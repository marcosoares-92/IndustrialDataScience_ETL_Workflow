{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Seggregation of the Dataset and Mean Difference Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _ETL Workflow Notebook 5_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "1. Calculating general statistics from a given column; \n",
    "2. Getting data quantiles from a given column; \n",
    "3. Getting a particular P-percent quantile limit; \n",
    "4. Applying a list of row filters to a dataframe; \n",
    "5. Selecting subsets from a dataframe (using row filters) and labelling these subsets; \n",
    "6. Performing Analysis of Variance (ANOVA) and obtaining boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.oneway import anova_oneway\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", load_all_sheets_at_once = False, sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading Excel files:\n",
    "    \n",
    "    # load_all_sheets_at_once = False - This parameter has effect only when for Excel files.\n",
    "    # If load_all_sheets_at_once = True, the function will return a list of dictionaries, each\n",
    "    # dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "    # value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "    # and its value will be the pandas dataframe object obtained from that sheet.\n",
    "    # This argument has preference over sheet_to_load. If it is True, all sheets will be loaded.\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\\n\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\\n\")\n",
    "        # For Excel type files, Pandas automatically detects the decimal separator and requires only the parameter parse_dates.\n",
    "        # Firstly, the argument infer_datetime_format was present on read_excel function, but was removed.\n",
    "        # From version 1.4 (beta, in 10 May 2022), it will be possible to pass the parameter 'decimal' to\n",
    "        # read_excel function for detecting decimal cases in strings. For numeric variables, it is not needed, though\n",
    "        \n",
    "        if (load_all_sheets_at_once == True):\n",
    "            \n",
    "            # Corresponds to setting sheet_name = None\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "            \n",
    "            # xlsx_doc is a dictionary containing the sheet names as keys, and dataframes as items.\n",
    "            # Let's convert it to the desired format.\n",
    "            # Dictionary dict, dict.keys() is the array of keys; dict.values() is an array of the values;\n",
    "            # and dict.items() is an array of tuples with format ('key', value)\n",
    "            \n",
    "            # Create a list of returned datasets:\n",
    "            list_of_datasets = []\n",
    "            \n",
    "            # Let's iterate through the array of tuples. The first element returned is the key, and the\n",
    "            # second is the value\n",
    "            for sheet_name, dataframe in (xlsx_doc.items()):\n",
    "                # sheet_name = key; dataframe = value\n",
    "                # Define the dictionary with the standard format:\n",
    "                df_dict = {'sheet': sheet_name,\n",
    "                            'df': dataframe}\n",
    "                \n",
    "                # Add the dictionary to the list:\n",
    "                list_of_datasets.append(df_dict)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(f\"A total of {len(list_of_datasets)} dataframes were retrieved from the Excel file.\\n\")\n",
    "            print(f\"The dataframes correspond to the following Excel sheets: {list(xlsx_doc.keys())}\\n\")\n",
    "            print(\"Returning a list of dictionaries. Each dictionary contains the key \\'sheet\\', with the original sheet name; and the key \\'df\\', with the Pandas dataframe object obtained.\\n\")\n",
    "            print(f\"Check the 10 first rows of the dataframe obtained from the first sheet, named {list_of_datasets[0]['sheet']}:\\n\")\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            except: # regular mode\n",
    "                print((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            return list_of_datasets\n",
    "            \n",
    "        elif (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_obj_to_pandas_dataframe (json_obj_to_convert, json_obj_type = 'list', json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "    # dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "    # example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "    # structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "    # file containing JSON, you could read the txt and save its content as a string.\n",
    "    # json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "    # 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "    # 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]    \n",
    "\n",
    "    # json_obj_type = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "    # json_obj_type = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "\n",
    "    \n",
    "    if (json_obj_type == 'string'):\n",
    "        # Use the json.loads method to convert the string to json\n",
    "        json_file = json.loads(json_obj_to_convert)\n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "        # like a dataframe.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    \n",
    "    elif (json_obj_type == 'list'):\n",
    "        \n",
    "        # make the json_file the object itself:\n",
    "        json_file = json_obj_to_convert\n",
    "    \n",
    "    else:\n",
    "        print (\"Enter a valid JSON object type: \\'list\\', in case the JSON object is a list of dictionaries in JSON format; or \\'string\\', if the JSON is stored as a text (string variable).\")\n",
    "        return \"error\"\n",
    "    \n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for dataframe general characterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_general_characterization (df):\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    # Set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    # Show dataframe's header\n",
    "    print(\"Dataframe\\'s 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "\n",
    "    # Show dataframe's tail:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    print(\"Dataframe\\'s 10 last rows:\\n\")\n",
    "    try:\n",
    "        display(DATASET.tail(10))\n",
    "    except:\n",
    "        print(DATASET.tail(10))\n",
    "    \n",
    "    # Show dataframe's shape:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_shape  = DATASET.shape\n",
    "    print(\"Dataframe\\'s shape = (number of rows, number of columns) =\\n\")\n",
    "    try:\n",
    "        display(df_shape)\n",
    "    except:\n",
    "        print(df_shape)\n",
    "    \n",
    "    # Show dataframe's columns:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_columns_array = DATASET.columns\n",
    "    print(\"Dataframe\\'s columns =\\n\")\n",
    "    try:\n",
    "        display(df_columns_array)\n",
    "    except:\n",
    "        print(df_columns_array)\n",
    "    \n",
    "    # Show dataframe's columns types:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_dtypes = DATASET.dtypes\n",
    "    # Now, the df_dtypes seroes has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.rename.html#pandas.Index.rename\n",
    "    # To access the Index object, we call the index attribute from Pandas dataframe.\n",
    "    # By setting inplace = True, we modify the object inplace, by simply calling the method:\n",
    "    df_dtypes.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    # Let's also modify the series label or name:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rename.html\n",
    "    df_dtypes.rename('dtype_series', inplace = True)\n",
    "    print(\"Dataframe\\'s variables types:\\n\")\n",
    "    try:\n",
    "        display(df_dtypes)\n",
    "    except:\n",
    "        print(df_dtypes)\n",
    "    \n",
    "    # Show dataframe's general statistics for numerical variables:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_general_statistics = DATASET.describe()\n",
    "    print(\"Dataframe\\'s general (summary) statistics for numeric variables:\\n\")\n",
    "    try:\n",
    "        display(df_general_statistics)\n",
    "    except:\n",
    "        print(df_general_statistics)\n",
    "    \n",
    "    # Show total of missing values for each variable:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    total_of_missing_values_series = DATASET.isna().sum()\n",
    "    # This is a series which uses the original column names as index\n",
    "    proportion_of_missing_values_series = DATASET.isna().mean()\n",
    "    percent_of_missing_values_series = proportion_of_missing_values_series * 100\n",
    "    missingness_dict = {'count_of_missing_values': total_of_missing_values_series,\n",
    "                       'proportion_of_missing_values': proportion_of_missing_values_series,\n",
    "                       'percent_of_missing_values': percent_of_missing_values_series}\n",
    "    \n",
    "    df_missing_values = pd.DataFrame(data = missingness_dict)\n",
    "    # Now, the dataframe has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    df_missing_values.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    \n",
    "    # Create a one row dataframe with the missingness for the whole dataframe:\n",
    "    # Pass the scalars as single-element lists or arrays:\n",
    "    one_row_data = {'dataframe_column': ['missingness_accross_rows'],\n",
    "                    'count_of_missing_values': [len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any'))],\n",
    "                    'proportion_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))],\n",
    "                    'percent_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))*100]\n",
    "                    }\n",
    "    one_row_df = pd.DataFrame(data = one_row_data)\n",
    "    one_row_df.set_index('dataframe_column', inplace = True)\n",
    "    \n",
    "    # Append this one_row_df to df_missing_values:\n",
    "    df_missing_values = pd.concat([df_missing_values, one_row_df])\n",
    "    \n",
    "    print(\"Missing values on each feature; and missingness considering all rows from the dataframe:\")\n",
    "    print(\"(note: \\'missingness_accross_rows\\' was calculated by: checking which rows have at least one missing value (NA); and then comparing total rows with NAs with total rows in the dataframe).\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(df_missing_values)\n",
    "    except:\n",
    "        print(df_missing_values)\n",
    "    \n",
    "    return df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for column filtering (selecting); ordering; or renaming all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_order_or_rename_columns (df, columns_list, mode = 'select_or_order_columns'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # MODE = 'select_or_order_columns' for filtering only the list of columns passed as columns_list,\n",
    "    # and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "    # the order of elements on the list will be the new order of columns.\n",
    "\n",
    "    # MODE = 'rename_columns' for renaming the columns with the names passed as columns_list. In this\n",
    "    # mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "    # the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "    # will result into columns with incorrect names.\n",
    "    \n",
    "    # columns_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: columns_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{DATASET.columns}\\n\")\n",
    "    \n",
    "    if ((columns_list is None) | (columns_list == np.nan)):\n",
    "        # empty list\n",
    "        columns_list = []\n",
    "    \n",
    "    if (len(columns_list) == 0):\n",
    "        print(\"Please, input a valid list of columns.\\n\")\n",
    "        return DATASET\n",
    "    \n",
    "    if (mode == 'select_or_order_columns'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        DATASET = DATASET[columns_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\\n\")\n",
    "        print(\"Check the new dataframe:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET)\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET)\n",
    "        \n",
    "    elif (mode == 'rename_columns'):\n",
    "        \n",
    "        # Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(columns_list) == len(DATASET.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\\n\")\n",
    "            return DATASET\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            DATASET.columns = columns_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\\n\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\\n\")\n",
    "            print(\"Check the new dataframe:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(DATASET)\n",
    "\n",
    "            except: # regular mode\n",
    "                print(DATASET)\n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'select_or_order_columns\\' or \\'rename_columns\\'.\")\n",
    "        return DATASET\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for applying a list of row filters to a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def APPLY_ROW_FILTERS_LIST (df, list_of_row_filters):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Warning: this function filter the rows and results into a smaller dataset, since it removes the non-selected entries.\")\n",
    "    print(\"If you want to pass a filter to simply label the selected rows, use the function LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\")\n",
    "    \n",
    "    # This function applies filters to the dataframe and remove the non-selected entries.\n",
    "    \n",
    "    # df: dataframe to be analyzed.\n",
    "    \n",
    "    ## define the filters and only them define the filters list\n",
    "    # EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "    # boolean_filter1 = ((None) & (None)) \n",
    "    # (condition1 AND (&) condition2)\n",
    "    # boolean_filter2 = ((None) | (None)) \n",
    "    # condition1 OR (|) condition2\n",
    "    \n",
    "    # boolean filters result into boolean values True or False.\n",
    "\n",
    "    ## Examples of filters:\n",
    "    ## filter1 = (condition 1) & (condition 2)\n",
    "    ## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "    ## filter2 = (condition)\n",
    "    ## filter2 = (df['column3'] <= 2.5)\n",
    "    ## filter3 = (df['column4'] > 10.7)\n",
    "    ## filter3 = (condition 1) | (condition 2)\n",
    "    ## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "\n",
    "    ## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "    ## <= (lower or equal); == (equal); != (different)\n",
    "\n",
    "    ## concatenation operators: & (and): the filter is True only if the \n",
    "    ## two conditions concatenated through & are True\n",
    "    ## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "    ## through | are True.\n",
    "    ## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "\n",
    "    ## separate conditions with parentheses. Use parentheses to define a order\n",
    "    ## of definition of the conditions:\n",
    "    ## filter = ((condition1) & (condition2)) | (condition3)\n",
    "    ## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "    ## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "    ## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "    ## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "    ## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "    ## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "    # The negative of this condition may be acessed with ~ operator:\n",
    "    ##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "    ## Also, you may use isna() method as filter for missing values:\n",
    "    ## filter = (dataframe_column_series).isna()\n",
    "    ## or, for not missing: ~(dataframe_column_series).isna()\n",
    "    \n",
    "    # list_of_row_filters: list of boolean filters to be applied to the dataframe\n",
    "    # e.g. list_of_row_filters = [filter1]\n",
    "    # applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "    # boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "    # That is because the function will loop through the list of filters.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3, filter4]\n",
    "    # will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "    # Notice that the filters must be declared in the order you want to apply them.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Get the original index and convert it to a list\n",
    "    original_index = list(DATASET.index)\n",
    "    \n",
    "    # Loop through the filters list, applying the filters sequentially:\n",
    "    # Each element of the list is identified as 'boolean_filter'\n",
    "    \n",
    "    if (len(list_of_row_filters) > 0):\n",
    "        \n",
    "        # Start a list of indices that were removed. That is because we must\n",
    "        # remove these elements from the boolean filter series before filtering, avoiding\n",
    "        # the index mismatch.\n",
    "        removed_indices = []\n",
    "        \n",
    "        # Now, loop through other rows in the list_of_row_filters:\n",
    "        for boolean_series in list_of_row_filters:\n",
    "            \n",
    "            if (len(removed_indices) > 0):\n",
    "                # Drop rows in list removed_indices. Set inplace = True to remove by simply applying\n",
    "                # the method:\n",
    "                # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "                boolean_series.drop(labels = removed_indices, axis = 0, inplace = True)\n",
    "            \n",
    "            # Apply the filter:\n",
    "            DATASET = DATASET[boolean_series]\n",
    "            \n",
    "            # Finally, let's update the list of removed indices:\n",
    "            for index in original_index:\n",
    "                if ((index not in list(DATASET.index)) & (index not in removed_indices)):\n",
    "                    removed_indices.append(index)\n",
    "    \n",
    "    \n",
    "    # Reset index:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully filtered the dataframe. Check the 10 first rows of the filtered and returned dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating general statistics from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GENERAL_STATISTICS (df, column_to_analyze):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe to be analyzed.\n",
    "    \n",
    "    #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "    # will analyze column named as 'col1'\n",
    "    analyzed_series = df[column_to_analyze]\n",
    "    \n",
    "    general_stats = analyzed_series.describe()\n",
    "    print(f\"General descriptive statistics from variable {column_to_analyze}, ignoring missing values:\\n\")\n",
    "    print(general_stats)\n",
    "    print(\"\\n\") #line break\n",
    "    print(\"Interpretation: count = total of values evaluated. Missing values are ignored; mean = mean value of the series, ignoring missing values; std = standard deviation of the series, ignoring missing values; min = minimum observed value for the analyzed series; 25\\% = 0.25 quantile (1st-quartile): 25\\% of data fall below (<=) this value; 50\\% = 2nd-quartile: 50\\% <= this value; 75\\% = 3rd-quartile - 75\\% of data <= this value; max = maximum observed value for the analyzed series.\")\n",
    "    print(\"This function shows the general statistics only for numerical variables. The results were returned as the dataframe general_stats.\")\n",
    "    \n",
    "    # Return only the dataframe\n",
    "    return general_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for getting data quantiles from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_QUANTILES (df, column_to_analyze):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe to be analyzed.\n",
    "    \n",
    "    #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "    # will analyze column named as 'col1'\n",
    "    analyzed_series = df[column_to_analyze]\n",
    "    \n",
    "    list_of_quantiles = []\n",
    "    list_of_values = []\n",
    "    list_of_interpretation = []\n",
    "    \n",
    "    #First element: minimum\n",
    "    list_of_quantiles.append(0)\n",
    "    list_of_values.append(analyzed_series.min())\n",
    "    list_of_interpretation.append(f\"Minimum value of {column_to_analyze}\")\n",
    "    \n",
    "    i = 0.05\n",
    "    #Start from the 5% quantile\n",
    "    while (i <= 0.95):\n",
    "        \n",
    "        list_of_quantiles.append(i)\n",
    "        list_of_values.append(analyzed_series.quantile(i))\n",
    "        list_of_interpretation.append(f\"{i}-quantile: {i*100}\\% of data fall below this value\")\n",
    "        \n",
    "        i = i + 0.05\n",
    "    \n",
    "    # Last element: maximum value\n",
    "    list_of_quantiles.append(1)\n",
    "    list_of_values.append(analyzed_series.max())\n",
    "    list_of_interpretation.append(f\"Maximum value of {column_to_analyze}\")\n",
    "    \n",
    "    # Summarize the lists as a dictionary and convert the dictionary to a dataframe:\n",
    "    quantiles_dict = {\"quantile\": list_of_quantiles, f\"value_of_{column_to_analyze}\": list_of_values,\n",
    "                     \"interpretation\": list_of_interpretation}\n",
    "    \n",
    "    quantiles_summ_df = pd.DataFrame(data = quantiles_dict)\n",
    "    \n",
    "    print(\"Quantiles returned as dataframe quantiles_summ_df. Check it below:\\n\")\n",
    "    print(quantiles_summ_df)\n",
    "    \n",
    "    return quantiles_summ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for getting a particular P-percent quantile limit**\n",
    "- input P as a percent, from 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_P_PERCENT_QUANTILE_LIM (df, column_to_analyze, p_percent = 100):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe to be analyzed.\n",
    "    \n",
    "    #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "    # will analyze column named as 'col1'\n",
    "    \n",
    "    # p_percent: float value from 0 to 100 representing the percent of the quantile\n",
    "    # if p_percent = 31.2, then 31.2% of the data will fall below the returned value\n",
    "    # if p_percent = 75, then 75% of the data will fall below the returned value\n",
    "    # if p_percent = 0, the minimum value is returned.\n",
    "    # if p_percent = 100, the maximum value is returned.\n",
    "    \n",
    "    analyzed_series = df[column_to_analyze]\n",
    "    \n",
    "    #convert the quantile to fraction\n",
    "    quantile_fraction = p_percent/100.0 #.0 to guarantee a float result\n",
    "    \n",
    "    if (quantile_fraction < 0):\n",
    "        print(\"Invalid percent value - it cannot be lower than zero.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    elif (quantile_fraction == 0):\n",
    "        #get the minimum value\n",
    "        quantile_lim = analyzed_series.min()\n",
    "        print(f\"Minimum value of {column_to_analyze} = {quantile_lim}\")\n",
    "    \n",
    "    elif (quantile_fraction == 1):\n",
    "        #get the maximum value\n",
    "        quantile_lim = analyzed_series.max()\n",
    "        print(f\"Maximum value of {column_to_analyze} = {quantile_lim}\")\n",
    "        \n",
    "    else:\n",
    "        #get the quantile\n",
    "        quantile_lim = analyzed_series.quantile(quantile_fraction)\n",
    "        print(f\"{quantile_fraction}-quantile: {p_percent}\\% of data fall below this value\")\n",
    "    \n",
    "    return quantile_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for selecting subsets from a dataframe (using row filters) and labelling these subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LABEL_DATAFRAME_SUBSETS (df, list_of_row_filters, new_labels_list = None, new_labelled_column_name = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Attention: this function selects subsets from the dataframe and label them, allowing the seggregation of the data.\")\n",
    "    print(\"If you want to filter the dataframe to eliminate non-selected rows, use the function APPLY_ROW_FILTERS_LIST\")\n",
    "    \n",
    "    ## This function selects subsets of the dataframe by applying a list\n",
    "    ## of row filters, and then it labels each one of the filtered subsets.\n",
    "    \n",
    "    # df: dataframe to be analyzed.\n",
    "    \n",
    "    ## define the filters and only them define the filters list\n",
    "    # EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "    # boolean_filter1 = ((None) & (None)) \n",
    "    # (condition1 AND (&) condition2)\n",
    "    # boolean_filter2 = ((None) | (None)) \n",
    "    # condition1 OR (|) condition2\n",
    "    \n",
    "    # boolean filters result into boolean values True or False.\n",
    "\n",
    "    ## Examples of filters:\n",
    "    ## filter1 = (condition 1) & (condition 2)\n",
    "    ## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "    ## filter2 = (condition)\n",
    "    ## filter2 = (df['column3'] <= 2.5)\n",
    "    ## filter3 = (df['column4'] > 10.7)\n",
    "    ## filter3 = (condition 1) | (condition 2)\n",
    "    ## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "\n",
    "    ## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "    ## <= (lower or equal); == (equal); != (different)\n",
    "\n",
    "    ## concatenation operators: & (and): the filter is True only if the \n",
    "    ## two conditions concatenated through & are True\n",
    "    ## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "    ## through | are True.\n",
    "    \n",
    "    ## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "    ## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "    ## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "    ## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "\n",
    "    ## separate conditions with parentheses. Use parentheses to define a order\n",
    "    ## of definition of the conditions:\n",
    "    ## filter = ((condition1) & (condition2)) | (condition3)\n",
    "    ## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "    ## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "    \n",
    "    # list_of_row_filters: list of boolean filters to be applied to the dataframe\n",
    "    # e.g. list_of_row_filters = [filter1]\n",
    "    # applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "    # boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "    # That is because the function will loop through the list of filters.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3, filter4]\n",
    "    # will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "    # Notice that the filters must be declared in the order you want to apply them.\n",
    "        \n",
    "    # Here, each filter will be associated with one label. Therefore, if you want to apply \n",
    "    # a filter with several conditions, combine them using the & and | operators.\n",
    "    \n",
    "    # The labels are input as a list of strings or of discrete integer values. There must be\n",
    "    # exactly one filter to each label: the filter will be used to obtain a subset from\n",
    "    # the dataframe, and the label will be applied to all the rows of this subset.\n",
    "    # e.g. list_of_row_filters = [filter1, filter2], new_labels_list = ['label1', 'label2']\n",
    "    # In this case, the rows of the dataset obtained when applying filter1 will receive the\n",
    "    # string 'label1' as label. In turns, \n",
    "    # rows selected from filter2 will be labelled as 'label2'.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3], new_labels_list = [1, 2, 3]\n",
    "    # Here, the labels are integers: rows filtered through filter1 are marked as 1;\n",
    "    # rows selected by filter2 are labelled as 2; and rows selected from filter3 are marked\n",
    "    # as 3.\n",
    "    \n",
    "    # Warning: the sequence of filters must be correspondent to the sequence of labels. \n",
    "    # Rows selected from the first filter are labelled with the first item from the labels\n",
    "    # list; rows selected by the 2nd filter are labelled with the 2nd element, and so on.\n",
    "    \n",
    "    # If no list of labels is provided, a list of integers starting from zero and with the\n",
    "    # same total of elements from list_of_row_filters will be created. For instance,\n",
    "    # if list_of_row_filters = [filter1, filter2, filter3, filter4], the list of labels\n",
    "    # [0, 1, 2, 3] would be created. Again, the rows selected from filter1 would be labelled\n",
    "    # as 0; rows selected from filter 2 would be labelled as 1; and so on.\n",
    "    \n",
    "    # new_labelled_column_name = None. Alternatively, declare a string (inside quotes), with\n",
    "    # the name of the column containing the applied labels. For example, if\n",
    "    # new_labelled_column_name = 'labels', then the new labels are saved in the column 'labels'\n",
    "    # If no new_labelled_column_name is provided, the default name = \"df_label\" is provided.\n",
    "    \n",
    "    if (new_labelled_column_name is None):\n",
    "        \n",
    "        new_labelled_column_name = \"df_label\"\n",
    "    \n",
    "    if (new_labels_list is None):\n",
    "        \n",
    "        #Create a list of integers with the same length as list_of_row_filters\n",
    "        # start the list:\n",
    "        new_labels_list = []\n",
    "        # Append the value zero:\n",
    "        new_labels_list.append(0)\n",
    "        \n",
    "        #check the total of elements of list list_of_row_filters:\n",
    "        total_of_filters = len(list_of_row_filters)\n",
    "        \n",
    "        #loop from i = 1 to i = (total_of_filters - 1), index of the last element from\n",
    "        #list total_of_filters. Append these values to the list of new labels:\n",
    "        for i in range (1, total_of_filters):\n",
    "            #i goes from 1 to (total_of_filters - 1).\n",
    "            # the element 0 was already added\n",
    "            new_labels_list.append(i)\n",
    "            # now we have a list of integers from 0 to (total_of_filters - 1), where each\n",
    "            # number is a label to be applied to the rows selected from one of the filters.\n",
    "        \n",
    "    # Let's select rows applying the filters, and label them according    \n",
    "    \n",
    "    #Loop through the filters list, applying the filters sequentially:\n",
    "    # Each element of the list is identified as 'boolean_filter'\n",
    "    for i in range (0, total_of_filters):\n",
    "        # Here, i ranges from 0 to total_of_filters - 1, index of the last element\n",
    "        # We specifically declare 0 as the first element to avoid problems derived\n",
    "        # from the fact that a counting variable i was already used and could have a\n",
    "        # value in the memory. The value 0 is the default when only one value is declared\n",
    "        # as argument from range\n",
    "        \n",
    "        #select the boolean filter from list_of_row_filters. It is the i-th indexed element \n",
    "        boolean_filter = list_of_row_filters[i]\n",
    "        # Select the correspondent label from new_labels_list:\n",
    "        applied_label = new_labels_list[i]\n",
    "        \n",
    "        #Apply the filter to select a group of rows, and apply the correspondent label\n",
    "        # to the selected rows\n",
    "        \n",
    "        # syntax: dataset.loc[dataset['column_filtered'] <= 0.87, 'labelled_column'] = 1\n",
    "        # which is equivalent to dataset.loc[(filter), 'labelled_column'] = label\n",
    "        df.loc[(boolean_filter), new_labelled_column_name] = applied_label\n",
    "    \n",
    "    print(\"Successfully labelled the dataframe. Check the 10 first rows of the labelled and returned dataframe:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for performing Analysis of Variance (ANOVA) and obtaining boxplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "azdata_cell_guid": "8ca7e1cf-e19f-4eae-98e6-780392820baa",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def anova_boxplot (df, column_to_analyze, column_with_group_labels = None, confidence_level = 0.95, boxplot_notch = False, boxplot_vertically_oriented = True, boxplot_patch_artist = False,  reference_value = None, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "        \n",
    "        print (\"If an error message is shown, update statsmodels. Declare and run a cell as:\")\n",
    "        print (\"!pip install statsmodels --upgrade\")\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        from statsmodels.stats.oneway import anova_oneway\n",
    "        \n",
    "        # df: dataframe to be analyzed.\n",
    "    \n",
    "        #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "        # will analyze column named as 'col1'\n",
    "        \n",
    "        # column_with_group_labels: name of the column registering groups, labels, or\n",
    "        # factors correspondent to each row. e.g. column_with_group_labels = \"group\"\n",
    "        # if the correspondent group is saved as column named 'group'.\n",
    "        # If no name is provided, then the default name is used:\n",
    "        # COLUMN_WITH_GROUP_LABELS = COLUMN_TO_ANALYZE + \"_label\"\n",
    "        \n",
    "        if (column_with_group_labels is None):\n",
    "            \n",
    "            column_with_group_labels = column_to_analyze + \"_label\"\n",
    "    \n",
    "        # CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "        # Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "        # Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "        # to get less restrictive results.\n",
    "        alpha = 1 - confidence_level\n",
    "        \n",
    "        # reference_value: keep it as None or add a float value.\n",
    "        # This reference value will be shown as a red constant line on the plot to be compared\n",
    "        # with the boxes. e.g. reference_value = 1.0 will plot a red line passing through\n",
    "        # column_to_analyze = 1.0\n",
    "        \n",
    "        # boxplot_notch = False\n",
    "        # Manipulate parameter notch (boolean, default: False) from the boxplot object\n",
    "        # Whether to draw a notched boxplot (True), or a rectangular boxplot (False). \n",
    "        # The notches represent the confidence interval (CI) around the median. \n",
    "        # The documentation for bootstrap describes how the locations of the notches are \n",
    "        # computed by default, but their locations may also be overridden by setting the \n",
    "        # conf_intervals parameter.\n",
    "        \n",
    "        # boxplot_vertically_oriented = True\n",
    "        # Manipulate the parameter vert (boolean, default: True)\n",
    "        # If True, draws vertical boxes. If False, draw horizontal boxes.\n",
    "        \n",
    "        # boxplot_patch_artist = False\n",
    "        # Manipulate parameter patch_artist (boolean, default: False)\n",
    "        # If False produces boxes with the Line2D artist. Otherwise, boxes are drawn \n",
    "        # with Patch artists.\n",
    "        # Check documentation:\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html\n",
    "        \n",
    "        # Create a local copy of the dataframe to be manipulated:\n",
    "        DATASET = df\n",
    "        \n",
    "        # Sort the dataframe by the groups/factors column (column_with_group_labels):\n",
    "        DATASET = DATASET.sort_values(column_with_group_labels, ascending = True)\n",
    "        \n",
    "        # Reset the index of the new dataframe:\n",
    "        DATASET = DATASET.reset_index(drop = True)\n",
    "        \n",
    "        # Get a series for the analyzed values and for the \n",
    "        # column with the correspondent labels:\n",
    "        analyzed_series = DATASET[column_to_analyze]\n",
    "        groups_series = DATASET[column_with_group_labels]\n",
    "        \n",
    "        # Get a series of unique values of groups\n",
    "        # Convert the series of columns into a NumPy array\n",
    "        array_of_groups = np.array(groups_series)\n",
    "        \n",
    "        #Keep only the unique values into this array:\n",
    "        array_of_groups = np.unique(array_of_groups)\n",
    "        \n",
    "        #Get the total of groups\n",
    "        total_of_groups = len(array_of_groups)\n",
    "        \n",
    "        \n",
    "        # Create a dictionary of NumPy arrays. This dictionary will store NumPy arrays\n",
    "        # correspondent to the elements of each group:\n",
    "        values_for_each_group_dict = {}\n",
    "        \n",
    "        # Loop through each element of the array_of_groups (array with unique values).\n",
    "        # The elements of array_of_groups are identified as 'group'\n",
    "        for group in array_of_groups:\n",
    "            \n",
    "            #start a list to store the values:\n",
    "            analyzed_vals_list = []\n",
    "            \n",
    "            # Now loop through each row of the dataframe. The dataframe row index go from\n",
    "            # zero to len(DATASET) - 1:\n",
    "            for i in range((len(DATASET) - 1)):\n",
    "                #loop go from i = 0 to i = len(DATASET) - 1, index of the last row\n",
    "                \n",
    "                # if the row in index is correspondent to the grouping/label 'group',\n",
    "                # then store the analyzed value into list analyzed_vals_list:\n",
    "                if (groups_series[i] == group):\n",
    "                    \n",
    "                    analyzed_vals_list.append(analyzed_series[i])\n",
    "            \n",
    "            # lets convert the lists into NumPy arrays:\n",
    "            analyzed_vals_array = np.array(analyzed_vals_list)\n",
    "            \n",
    "            # Now let's update the dictionary values_for_each_group_dict\n",
    "            # 1. Set the 'key' of the dictionary as the name of the group\n",
    "            dict_key = group\n",
    "            \n",
    "            # Use the update method to add the key 'dict_key', and the \n",
    "            # array analyzed_vals_array as the value correspondent to the key.\n",
    "            \n",
    "            # syntax: dict.update({\"key\": value})\n",
    "            # check: https://www.w3schools.com/python/ref_dictionary_update.asp\n",
    "            \n",
    "            values_for_each_group_dict.update({dict_key: analyzed_vals_array})\n",
    "        \n",
    "        # Now, the dictionary values_for_each_group_dict stores each group name as its keys\n",
    "        # and the arrays of analyzed values as the correspondent object.\n",
    "        \n",
    "        # Notice that the keys of the dictionary were created with the same order of\n",
    "        # array_of_groups, so there is no risk of losing the correspondence\n",
    "        \n",
    "        # Create a humongous list of arrays, containing the arrays stored into the\n",
    "        # dictionary. It is necessary for the statsmodels API:\n",
    "        humongous_list = []\n",
    "        \n",
    "        # Loop through each key of the dictionary:\n",
    "        for group in values_for_each_group_dict.keys():\n",
    "            # loop through each element of the array of keys from the dictionary.\n",
    "            # each element is named 'group'.\n",
    "            # append the correspondent array as an element from humongous_list\n",
    "            humongous_list.append(values_for_each_group_dict[group])\n",
    "        \n",
    "        #Now, we can pass the humongous_list as argument for the one-way Anova:\n",
    "        anova_one_way_summary = anova_oneway(humongous_list, groups = array_of_groups, use_var = 'bf', welch_correction=True, trim_frac=0)\n",
    "        # When use_var = 'bf', variances are not assumed to be equal across samples.\n",
    "        # Check documentation: \n",
    "        # https://www.statsmodels.org/stable/generated/statsmodels.stats.oneway.anova_oneway.html\n",
    "        \n",
    "        # The information is stored in a tuple (f_statistic, p-value)\n",
    "        # f_statistic: Test statistic for k-sample mean comparison which is approximately \n",
    "        # F-distributed.\n",
    "        # p-value: If use_var=\"bf\", then the p-value is based on corrected degrees of freedom following Mehrotra 1997.\n",
    "        f_statistic = anova_one_way_summary[0]\n",
    "        p_value = anova_one_way_summary[1]\n",
    "        \n",
    "        print(f\"Probability that the means of the groups are the same = {100*p_value}\\% (p-value = {p_value})\")\n",
    "        print(f\"Calculated F-statistic for the variances = {f_statistic}\")\n",
    "        \n",
    "        if (p_value <= alpha):\n",
    "            print(f\"For a confidence level of {confidence_level*100}\\%, we can reject the null hypothesis.\")\n",
    "            print(f\"The means are different for a {confidence_level*100}\\% confidence level.\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"For a confidence level of {confidence_level*100}\\%, we can accept the null hypothesis.\")\n",
    "            print(f\"The means are equal for a {confidence_level*100}\\% confidence level.\")\n",
    "        \n",
    "        print(\"Check ANOVA summary:\\n\")\n",
    "        print(anova_one_way_summary)\n",
    "        # When printing the summary, the set of supporting information of the Tuple are shown.\n",
    "        \n",
    "        anova_summary_dict = {'F_statistic': f_statistic, 'p_value': p_value}\n",
    "        \n",
    "        #Now, let's obtain the boxplot\n",
    "        fig, ax = plt.subplots()\n",
    "                    \n",
    "        # rectangular box plot\n",
    "        # The arrays of each group are the elements of the list humongous_list\n",
    "        boxplot_returned_dict = ax.boxplot(humongous_list, labels = array_of_groups, notch = boxplot_notch, vert = boxplot_vertically_oriented, patch_artist = boxplot_patch_artist)\n",
    "        \n",
    "        # boxplot_returned_dict: A dictionary mapping each component of the boxplot to \n",
    "        # a list of the Line2D instances created. That dictionary has the following keys \n",
    "        # (assuming vertical boxplots):\n",
    "        # boxes: the main body of the boxplot showing the quartiles and the median's \n",
    "        # confidence intervals if enabled.\n",
    "        # medians: horizontal lines at the median of each box.\n",
    "        # whiskers: the vertical lines extending to the most extreme, non-outlier data \n",
    "        # points.\n",
    "        # caps: the horizontal lines at the ends of the whiskers.\n",
    "        # fliers: points representing data that extend beyond the whiskers (fliers).\n",
    "        # means: points or lines representing the means.\n",
    "          \n",
    "        ax.set_title(f\"Boxplot of {column_to_analyze} by {col_with_group_indication}\")\n",
    "        \n",
    "        if (boxplot_vertically_oriented == True):\n",
    "            # generate vertically-oriented boxplot\n",
    "        \n",
    "            ax.set(ylabel = column_to_analyze)\n",
    "            # If the boxplot was horizontally oriented, this label should be the X instead.\n",
    "            # The X labels were already defined when creating the boxplot\n",
    "                    \n",
    "            if not (reference_value is None):\n",
    "                # Add an horizontal reference_line to compare against the boxes:\n",
    "                # If the boxplot was horizontally-oriented, this line should be vertical instead.\n",
    "                ax.axhline(reference_value, color = 'red', linestyle = 'dashed', label = 'Reference value')\n",
    "                # axhline generates an horizontal (h) line on ax\n",
    "                \n",
    "        else:\n",
    "            # In case it is False, generate horizontally-oriented plot:\n",
    "            ax.set(xlabel = column_to_analyze)\n",
    "            # The Y labels were already defined when creating the boxplot\n",
    "                    \n",
    "            if not (reference_value is None):\n",
    "                # Add an horizontal reference_line to compare against the boxes:\n",
    "                # If the boxplot was horizontally-oriented, this line should be vertical instead.\n",
    "                ax.axvline(reference_value, color = 'red', linestyle = 'dashed', label = 'Reference value')\n",
    "                # axvline generates a vertical (v) line on ax\n",
    "        \n",
    "            # Now, we can add general components of the graphic:\n",
    "            #Create an almost transparent horizontal line to guide the eyes without distracting\n",
    "            ax.yaxis.grid(grid, linestyle='-', which = 'major', color = 'lightgrey', alpha = 0.5)\n",
    "            ax.xaxis.grid(grid, linestyle='-', which = 'major', color = 'lightgrey', alpha = 0.5)       \n",
    "            \n",
    "            #ROTATE X AXIS IN XX DEGREES\n",
    "            plt.xticks(rotation = x_axis_rotation)\n",
    "            # XX = 70 DEGREES x_axis (Default)\n",
    "            #ROTATE Y AXIS IN XX DEGREES:\n",
    "            plt.yticks(rotation = y_axis_rotation)\n",
    "            # XX = 0 DEGREES y_axis (Default)\n",
    "            ax.legend()\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"/\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = \"boxplot\"\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 110 dpi\n",
    "                    png_resolution_dpi = 110\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            #fig.tight_layout()\n",
    "\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\n\") #line break\n",
    "            print(\"Successfully returned 2 dictionaries: anova_summary_dict (dictionary storing ANOVA F-test and p-value); and boxplot_returned_dict (dictionary mapping each component of the boxplot).\\n\")\n",
    "            \n",
    "            print(\"Boxplot interpretation:\")\n",
    "            print(\"Boxplot presents the following key visual components:\")\n",
    "            print(\"The main box represents the Interquartile Range (IQR). It represents the data that is from quartile Q1 to quartile Q3.\")\n",
    "            print(\"Q1 = 1st quartile of the dataset. 25% of values lie below this level (i.e., it is the 0.25-quantile or percentile).\")\n",
    "            print(\"Q2 = 2nd quartile of the dataset. 50% of values lie above and below this level (i.e., it is the 0.50-quantile or percentile).\")\n",
    "            print(\"Q3 = 3rd quartile of the dataset. 75% of values lie below and 25% lie above this level (i.e., it is the 0.75-quantile or percentile).\")\n",
    "            print(\"Boxplot main box (the IQR) is divided by an horizontal line if it is vertically-oriented; or by a vertical line if it is horizontally-oriented.\")\n",
    "            print(\"This line represents the median: it is the midpoint of the dataset.\")\n",
    "            print(\"There are lines extending beyond the main boxes limits. This lines end in horizontal limits, if the boxplot is vertically oriented; or in vertical limits, for an horizontal plot.\")\n",
    "            print(\"The minimum limit of the boxplot is defined as: Q1 - (1.5) x (IQR width) = Q1 - 1.5*(Q3-Q1)\")\n",
    "            print(\"The maximum limit of the boxplot is defined as: Q3 + (1.5) x (IQR width) = Q3 + 1.5*(Q3-Q1)\")\n",
    "            print(\"Finally, there are isolated points (circles) on the plot.\")\n",
    "            print(\"These points lie below the minimum bar, or above the maximum bar line. They are defined as outliers.\")\n",
    "            # https://nickmccullum.com/python-visualization/boxplot/\n",
    "            \n",
    "            return anova_summary_dict, boxplot_returned_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Create dataframe local copies to manipulate, avoiding that Pandas operates on\n",
    "    # the original objects; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    \n",
    "    # Start a list of copied dataframes:\n",
    "    LIST_OF_DATAFRAMES = []\n",
    "    \n",
    "    # Loop through each element from list_of_dataframes:\n",
    "    for dataframe in list_of_dataframes:\n",
    "        \n",
    "        # create a copy of the object:\n",
    "        copied_df = dataframe.copy(deep = True)\n",
    "        # Append this element to the LIST_OF_DATAFRAMES:\n",
    "        LIST_OF_DATAFRAMES.append(copied_df)\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "        \n",
    "        # In this case, we must save a list of columns of each one of the dataframes, containing\n",
    "        # the different column names observed. That is because the concat method eliminates the\n",
    "        # original column names when AXIS = 1\n",
    "        # We can start the LIST_OF_COLUMNS as the columns from the first object on the\n",
    "        # LIST_OF_DATAFRAMES, eliminating one iteration cycle. Since the columns method generates\n",
    "        # an array, we use the list attribute to convert the array to a regular list:\n",
    "        \n",
    "        i = 0\n",
    "        analyzed_df = LIST_OF_DATAFRAMES[i]\n",
    "        LIST_OF_COLUMNS = list(analyzed_df.columns)\n",
    "        \n",
    "        # Now, loop through each other element on LIST_OF_DATAFRAMES. Since index 0 was already\n",
    "        # considered, start from index 1:\n",
    "        for i in range (1, len(LIST_OF_DATAFRAMES)):\n",
    "            \n",
    "            analyzed_df = LIST_OF_DATAFRAMES[i]\n",
    "            \n",
    "            # Now, loop through each column, named 'col', from the list of columns of analyzed_df:\n",
    "            for col in list(analyzed_df.columns):\n",
    "                \n",
    "                # If 'col' is not in LIST_OF_COLUMNS, append it to the list with its current name.\n",
    "                # The order of the columns on the concatenated dataframe will be the same (the order\n",
    "                # they appear):\n",
    "                if not (col in LIST_OF_COLUMNS):\n",
    "                    LIST_OF_COLUMNS.append(col)\n",
    "                \n",
    "                else:\n",
    "                    # There is already a column with this name. So, append col with a suffix:\n",
    "                    LIST_OF_COLUMNS.append(col + \"_df_\" + str(i))\n",
    "                    \n",
    "        # Now, we have a list of all column names, that we will use for retrieving the headers after\n",
    "        # concatenation.\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\\n\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\\n\")\n",
    "        concat_df = pd.concat(LIST_OF_DATAFRAMES, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(LIST_OF_DATAFRAMES, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    if (AXIS == 1):\n",
    "        # If we concatentated columns, we lost the columns' names (headers). So, use the list\n",
    "        # LIST_OF_COLUMNS as the new headers for this case:\n",
    "        concat_df.columns = LIST_OF_COLUMNS\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(concat_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for time series visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_vis (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "     \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "        \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        print(\"Data to be plotted in a same column.\\n\")\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "    \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to plot. Please, provide valid arguments.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_to_analyze \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        if (add_scatter_dots == True):\n",
    "            MARKER = 'o'\n",
    "            \n",
    "        else:\n",
    "            MARKER = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_timestamp\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"timestamp\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = MARKER, color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"time_series_vis\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        #plt.figure(figsize = (12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions for histogram visualization**\n",
    "- Function `histogram`: ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons.\n",
    "- Function `histogram_alternative`: histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram (df, column_to_analyze, normal_curve_overlay = True, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ideal bin interval calculated through Montgomery's method. \n",
    "    # Histogram is obtained from this calculated bin size.\n",
    "    # Douglas C. Montgomery (2009). Introduction to Statistical Process Control, \n",
    "    # Sixth Edition, John Wiley & Sons.\n",
    "    \n",
    "    # column_to_analyze: string with the name of the column that will be analyzed.\n",
    "    # column_to_analyze = 'col1' obtain a histogram from column 1.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Sort by the column to analyze (ascending order) and reset the index:\n",
    "    DATASET = DATASET.sort_values(by = column_to_analyze, ascending = True)\n",
    "    \n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = (df[column_to_analyze])rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "\n",
    "    # General stats\n",
    "    mu = (DATASET[column_to_analyze]).mean() \n",
    "    median = (DATASET[column_to_analyze]).median()\n",
    "    sigma = (DATASET[column_to_analyze]).std() \n",
    "\n",
    "    # bin-size\n",
    "    lowest = (DATASET[column_to_analyze]).min()\n",
    "    highest = (DATASET[column_to_analyze]).max()\n",
    "    sample_size = (DATASET[column_to_analyze]).count()\n",
    "    range_hist = abs(highest - lowest)\n",
    "    n_cells = int(np.rint((sample_size)**(0.5)))\n",
    "    # We must use the int function to guarantee that the ncells will store an\n",
    "    # integer number of cells (we cannot have a fraction of a sentence).\n",
    "    # The int function guarantees that the variable will be stored as an integer.\n",
    "    # The numpy.rint(a) function rounds elements of the array to the nearest integer.\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "    # For values exactly halfway between rounded decimal values, \n",
    "    # NumPy rounds to the nearest even value. \n",
    "    # Thus 1.5 and 2.5 round to 2.0; -0.5 and 0.5 round to 0.0; etc.\n",
    "    \n",
    "    if (n_cells <= 1):\n",
    "        \n",
    "        # Manually set to 5 cells:\n",
    "        n_cells = 5\n",
    "    \n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    bin_size = range_hist/n_cells  \n",
    "    \n",
    "    # 1st bin:\n",
    "    \n",
    "    inf_bin_lim = lowest\n",
    "    sup_bin_lim = inf_bin_lim + bin_size\n",
    "    bin_mean = (inf_bin_lim + sup_bin_lim)/2\n",
    "    \n",
    "    boolean_filter = ((DATASET[column_to_analyze] >= inf_bin_lim) & (DATASET[column_to_analyze] <= sup_bin_lim))\n",
    "    # Use the filter to input the value bin_mean to the column named 'bin_center'\n",
    "    # dataset.loc[dataset[\"CatVar\"] == 'Value1', \"EncodedColumn\"] = 1\n",
    "    DATASET.loc[boolean_filter, 'bin_center'] = bin_mean        \n",
    "    \n",
    "    while (highest > sup_bin_lim):\n",
    "        \n",
    "        # Update the limits:\n",
    "        inf_bin_lim = sup_bin_lim\n",
    "        sup_bin_lim = inf_bin_lim + bin_size\n",
    "        bin_mean = (inf_bin_lim + sup_bin_lim)/2\n",
    "        \n",
    "        boolean_filter = ((DATASET[column_to_analyze] >= inf_bin_lim) & (DATASET[column_to_analyze] <= sup_bin_lim))\n",
    "        \n",
    "        # Use the filter to input the value bin_mean to the column named 'bin_center'\n",
    "        # dataset.loc[dataset[\"CatVar\"] == 'Value1', \"EncodedColumn\"] = 1\n",
    "        DATASET.loc[boolean_filter, 'bin_center'] = bin_mean\n",
    "    \n",
    "    # Now, select only the columns column_to_analyze and 'bin_center':\n",
    "    DATASET = DATASET[[column_to_analyze, 'bin_center']]\n",
    "    \n",
    "    # Group by bin_center in terms of counting:\n",
    "    DATASET = DATASET.groupby(by = 'bin_center', as_index = False, sort = True)[column_to_analyze].count()\n",
    "    \n",
    "    # Rename the columns:\n",
    "    DATASET.columns = ['bin_center', 'count']\n",
    "    \n",
    "    # Get a lists of bin_center and column_to_analyze:\n",
    "    list_of_bins = list(DATASET['bin_center'])\n",
    "    list_of_counts = list(DATASET['count'])\n",
    "    \n",
    "    # get the maximum count:\n",
    "    max_count = DATASET['count'].max()\n",
    "    # Get the index of the max count:\n",
    "    max_count_index = list_of_counts.index(max_count)\n",
    "    \n",
    "    # Get the value bin_center correspondent to the max count (maximum probability):\n",
    "    bin_of_max_proba = list_of_bins[max_count_index] \n",
    "    number_of_bins = len(DATASET) # Total of elements on the frequency table\n",
    "    \n",
    "    string_for_title = \" - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "    \n",
    "    if not (plot_title is None):\n",
    "        plot_title = plot_title + string_for_title\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        # Set graphic title\n",
    "        plot_title = f\"histogram_of_{column_to_analyze}\" + string_for_title\n",
    "\n",
    "    if (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        horizontal_axis_title = column_to_analyze\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        vertical_axis_title = \"Counting/Frequency\"\n",
    "        \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        if (sigma == 0):\n",
    "            print(\"Impossible to obtain a normal curve overlayed, because the standard deviation is zero.\\n\")\n",
    "            print(\"The analyzed variable is constant throughout the whole sample space.\\n\")\n",
    "            \n",
    "        else:\n",
    "            # create lists to store the normal curve. Center the normal curve in the bin\n",
    "            # of maximum bar (max probability, which will not be the mean if the curve\n",
    "            # is skewed). For normal distributions, this value will be the mean and the median.\n",
    "\n",
    "            # set the lowest value x used for obtaining the normal curve as bin_of_max_proba - 4*sigma\n",
    "            # the highest x will be bin_of_max_proba - 4*sigma\n",
    "            # each value will be created by incrementing (0.10)*sigma\n",
    "\n",
    "            x = (bin_of_max_proba - (4 * sigma))\n",
    "            x_of_normal = [x]\n",
    "\n",
    "            while (x < (bin_of_max_proba + (4 * sigma))):\n",
    "\n",
    "                x = x + (0.10)*(sigma)\n",
    "                x_of_normal.append(x)\n",
    "\n",
    "            # Convert the list to a NumPy array, so that it is possible to perform element-wise\n",
    "            # (vectorial) operations:\n",
    "            x_of_normal = np.array(x_of_normal)\n",
    "\n",
    "            # Create an array of the normal curve y, applying the normal curve equation:\n",
    "            # normal curve = 1/(sigma* ((2*pi)**(0.5))) * exp(-((x-mu)**2)/(2*(sigma**2)))\n",
    "            # where pi = 3,14...., and exp is the exponential function (base e)\n",
    "            # Let's center the normal curve on bin_of_max_proba\n",
    "            y_normal = (1 / (sigma* (np.sqrt(2 * (np.pi))))) * (np.exp(-0.5 * (((1 / sigma) * (x_of_normal - bin_of_max_proba)) ** 2)))\n",
    "            y_normal = np.array(y_normal)\n",
    "\n",
    "            # Pick the maximum value obtained for y_normal:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "            y_normal_max = np.amax(y_normal)\n",
    "\n",
    "            # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "            # with y_normal_max:\n",
    "            correction_factor = max_count/(y_normal_max)\n",
    "\n",
    "            # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "            y_normal = y_normal * correction_factor\n",
    "    \n",
    "    x_hist = DATASET['bin_center']\n",
    "    y_hist = DATASET['count']\n",
    "    \n",
    "    # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    ax.bar(x_hist, y_hist, alpha = OPACITY, label = f'counting_of\\n{column_to_analyze}', color = 'darkblue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    if ((normal_curve_overlay == True) & (sigma > 0)):\n",
    "    \n",
    "        # add normal curve\n",
    "        ax.plot(x_of_normal, y_normal, color = 'red', linestyle = 'dashed', alpha = OPACITY, label = 'Adjusted\\nnormal_curve')\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(horizontal_axis_title)\n",
    "    ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    ax.grid(grid) # show grid or not\n",
    "    ax.legend(loc = 'upper right')\n",
    "    # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "    # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "    # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "      \n",
    "    stats_dict = {\n",
    "                  'statistics': ['mean', 'median', 'standard_deviation', f'lowest_{column_to_analyze}', \n",
    "                                f'highest_{column_to_analyze}', 'count_of_values', 'number_of_bins', \n",
    "                                 'bin_size', 'bin_of_max_proba', 'count_on_bin_of_max_proba'],\n",
    "                  'value': [mu, median, sigma, lowest, highest, sample_size, number_of_bins,\n",
    "                           bin_size, bin_of_max_proba, max_count]\n",
    "                 }\n",
    "    \n",
    "    # Convert it to a Pandas dataframe setting the list 'statistics' as the index:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "    general_stats = pd.DataFrame(data = stats_dict)\n",
    "    \n",
    "    # Set the column 'statistics' as the index of the dataframe, using set_index method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\n",
    "    \n",
    "    # If inplace = True, modifies the DataFrame in place (do not create a new object).\n",
    "    # Then, we do not create an object equal to the expression. We simply apply the method (so,\n",
    "    # None is returned from the method):\n",
    "    general_stats.set_index(['statistics'], inplace = True)\n",
    "    \n",
    "    print(\"Check the general statistics from the analyzed variable:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(general_stats)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(general_stats)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the frequency table:\\n\")\n",
    "    \n",
    "    try:    \n",
    "        display(DATASET)    \n",
    "    except:\n",
    "        print(DATASET)\n",
    "\n",
    "    return general_stats, DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_alternative (df, column_to_analyze, total_of_bins, normal_curve_overlay = True, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # column_to_analyze: string with the name of the column that will be analyzed.\n",
    "    # column_to_analyze = 'col1' obtain a histogram from column 1.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Sort by the column to analyze (ascending order) and reset the index:\n",
    "    DATASET = DATASET.sort_values(by = column_to_analyze, ascending = True)\n",
    "    \n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    y_hist = DATASET[column_to_analyze]\n",
    "    \n",
    "    # General stats\n",
    "    mu = (DATASET[column_to_analyze]).mean() \n",
    "    median = (DATASET[column_to_analyze]).median()\n",
    "    sigma = (DATASET[column_to_analyze]).std() \n",
    "\n",
    "    # bin-size\n",
    "    lowest = (DATASET[column_to_analyze]).min()\n",
    "    highest = (DATASET[column_to_analyze]).max()\n",
    "    sample_size = (DATASET[column_to_analyze]).count()\n",
    "    \n",
    "    # Retrieve the histogram array hist_array\n",
    "    fig, ax = plt.subplots() # (0,0) not to show the plot now:\n",
    "    hist_array = plt.hist(y_hist, bins = total_of_bins)\n",
    "    plt.delaxes(ax) # this will delete ax, so that it will not be plotted.\n",
    "    plt.show()\n",
    "    print(\"\") # use this print not to mix with the final plot\n",
    "    \n",
    "    # hist_array is an array of arrays:\n",
    "    # hist_array = (array([count_1, count_2, ..., cont_n]), array([bin_center_1,...,\n",
    "    # bin_center_n])), where n = total_of_bins\n",
    "    # hist_array[0] is the array of countings for each bin, whereas hist_array[1] is\n",
    "    # the array of the bin center, i.e., the central value of the analyzed variable for\n",
    "    # that bin.\n",
    "    \n",
    "    # It is possible that the hist_array[0] contains more elements than hist_array[1].\n",
    "    # This happens when the last bins created by the division contain zero elements.\n",
    "    # In this case, we have to pad the sequence of hist_array[0], completing it with zeros.\n",
    "    \n",
    "    MAX_LENGTH = max(len(hist_array[0]), len(hist_array[1])) # Get the length of the longest sequence\n",
    "    SEQUENCES = [list(hist_array[0]), list(hist_array[1])] # get a list of sequences to pad.\n",
    "    # Notice that we applied the list attribute to create a list of lists\n",
    "    \n",
    "    # We cannot pad with the function pad_sequences from tensorflow because it converts all values\n",
    "    # to integers. Then, we have to pad the sequences by looping through the elements from SEQUENCES:\n",
    "    \n",
    "    # Start a support_list\n",
    "    support_list = []\n",
    "    \n",
    "    # loop through each sequence in SEQUENCES:\n",
    "    for sequence in SEQUENCES:\n",
    "        # add a zero at the end of the sequence until its length reaches MAX_LENGTH\n",
    "        while (len(sequence) < MAX_LENGTH):\n",
    "            \n",
    "            sequence.append(0)\n",
    "        \n",
    "        # append the sequence to support_list:\n",
    "        support_list.append(sequence)\n",
    "        \n",
    "    # Tuples and arrays are immutable. It means they do not support assignment, i.e., we cannot\n",
    "    # do tuple[0] = variable. Since arrays support vectorial (element-wise) operations, we can\n",
    "    # modify the whole array making it equals to support_list at once by using function np.array:\n",
    "    hist_array = np.array(support_list)\n",
    "    \n",
    "    # Get the bin_size as the average difference between successive elements from support_list[1]:\n",
    "    \n",
    "    diff_lists = []\n",
    "    \n",
    "    for i in range (1, len(support_list[1])):\n",
    "        \n",
    "        diff_lists.append(support_list[1][i] - support_list[1][(i-1)])\n",
    "    \n",
    "    # Now, get the mean value as the bin_size:\n",
    "    bin_size = np.amax(np.array(diff_lists))\n",
    "    \n",
    "    # Let's get the frequency table, which will be saved on DATASET (to get the code\n",
    "    # equivalent to the code for the function 'histogram'):\n",
    "    \n",
    "    DATASET = pd.DataFrame(data = {'bin_center': hist_array[1], 'count': hist_array[0]})\n",
    "    \n",
    "    # Get a lists of bin_center and column_to_analyze:\n",
    "    list_of_bins = list(hist_array[1])\n",
    "    list_of_counts = list(hist_array[0])\n",
    "    \n",
    "    # get the maximum count:\n",
    "    max_count = DATASET['count'].max()\n",
    "    # Get the index of the max count:\n",
    "    max_count_index = list_of_counts.index(max_count)\n",
    "    \n",
    "    # Get the value bin_center correspondent to the max count (maximum probability):\n",
    "    bin_of_max_proba = list_of_bins[max_count_index]\n",
    "    bin_after_the_max_proba = list_of_bins[(max_count_index + 1)] # the next bin\n",
    "    number_of_bins = len(DATASET) # Total of elements on the frequency table\n",
    "    \n",
    "    string_for_title = \" - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "    \n",
    "    if not (plot_title is None):\n",
    "        plot_title = plot_title + string_for_title\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        # Set graphic title\n",
    "        plot_title = f\"histogram_of_{column_to_analyze}\" + string_for_title\n",
    "\n",
    "    if (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        horizontal_axis_title = column_to_analyze\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        vertical_axis_title = \"Counting/Frequency\"\n",
    "        \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        if (sigma == 0):\n",
    "            print(\"Impossible to obtain a normal curve overlayed, because the standard deviation is zero.\\n\")\n",
    "            print(\"The analyzed variable is constant throughout the whole sample space.\\n\")\n",
    "            \n",
    "        else:\n",
    "            # create lists to store the normal curve. Center the normal curve in the bin\n",
    "            # of maximum bar (max probability, which will not be the mean if the curve\n",
    "            # is skewed). For normal distributions, this value will be the mean and the median.\n",
    "\n",
    "            # set the lowest value x used for obtaining the normal curve as center_of_bin_of_max_proba - 4*sigma\n",
    "            # the highest x will be center_of_bin_of_max_proba - 4*sigma\n",
    "            # each value will be created by incrementing (0.10)*sigma\n",
    "\n",
    "            # The arrays created by the plt.hist method present the value of the extreme left \n",
    "            # (the beginning) of the histogram bars, not the bin center. So, let's add half of the bin size\n",
    "            # to the bin_of_max_proba, so that the adjusted normal will be positioned on the center of the\n",
    "            # bar of maximum probability. We can do it by taking the average between bin_of_max_proba\n",
    "            # and the following bin, bin_after_the_max_proba:\n",
    "\n",
    "            center_of_bin_of_max_proba = (bin_of_max_proba + bin_after_the_max_proba)/2\n",
    "\n",
    "            x = (center_of_bin_of_max_proba - (4 * sigma))\n",
    "            x_of_normal = [x]\n",
    "\n",
    "            while (x < (center_of_bin_of_max_proba + (4 * sigma))):\n",
    "\n",
    "                x = x + (0.10)*(sigma)\n",
    "                x_of_normal.append(x)\n",
    "\n",
    "            # Convert the list to a NumPy array, so that it is possible to perform element-wise\n",
    "            # (vectorial) operations:\n",
    "            x_of_normal = np.array(x_of_normal)\n",
    "\n",
    "            # Create an array of the normal curve y, applying the normal curve equation:\n",
    "            # normal curve = 1/(sigma* ((2*pi)**(0.5))) * exp(-((x-mu)**2)/(2*(sigma**2)))\n",
    "            # where pi = 3,14...., and exp is the exponential function (base e)\n",
    "            # Let's center the normal curve on center_of_bin_of_max_proba\n",
    "            y_normal = (1 / (sigma* (np.sqrt(2 * (np.pi))))) * (np.exp(-0.5 * (((1 / sigma) * (x_of_normal - center_of_bin_of_max_proba)) ** 2)))\n",
    "            y_normal = np.array(y_normal)\n",
    "\n",
    "            # Pick the maximum value obtained for y_normal:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "            y_normal_max = np.amax(y_normal)\n",
    "\n",
    "            # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "            # with y_normal_max:\n",
    "            correction_factor = max_count/(y_normal_max)\n",
    "\n",
    "            # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "            y_normal = y_normal * correction_factor\n",
    "    \n",
    "    # values needed for the standard matplotlib barchart:\n",
    "    #x_hist = DATASET['bin_center']\n",
    "    #y_hist = DATASET['count']\n",
    "    \n",
    "    # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    ax.hist(y_hist, bins = total_of_bins, alpha = OPACITY, label = f'counting_of\\n{column_to_analyze}', color='darkblue')\n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    #ax.bar(x_hist, y_hist, label = f'counting_of\\n{column_to_analyze}', color = 'darkblue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    if ((normal_curve_overlay == True) & (sigma > 0)):\n",
    "    \n",
    "        # add normal curve\n",
    "        ax.plot(x_of_normal, y_normal, color = 'red', linestyle = 'dashed', alpha = OPACITY, label = 'Adjusted\\nnormal_curve')\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(horizontal_axis_title)\n",
    "    ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    ax.grid(grid) # show grid or not\n",
    "    ax.legend(loc = 'upper right')\n",
    "    # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "    # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "    # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "      \n",
    "    stats_dict = {\n",
    "                  'statistics': ['mean', 'median', 'standard_deviation', f'lowest_{column_to_analyze}', \n",
    "                                f'highest_{column_to_analyze}', 'count_of_values', 'number_of_bins', \n",
    "                                 'bin_size', 'bin_of_max_proba', 'count_on_bin_of_max_proba'],\n",
    "                  'value': [mu, median, sigma, lowest, highest, sample_size, number_of_bins,\n",
    "                           bin_size, bin_of_max_proba, max_count]\n",
    "                 }\n",
    "    \n",
    "    # Convert it to a Pandas dataframe setting the list 'statistics' as the index:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "    general_stats = pd.DataFrame(data = stats_dict)\n",
    "    \n",
    "    # Set the column 'statistics' as the index of the dataframe, using set_index method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\n",
    "    \n",
    "    # If inplace = True, modifies the DataFrame in place (do not create a new object).\n",
    "    # Then, we do not create an object equal to the expression. We simply apply the method (so,\n",
    "    # None is returned from the method):\n",
    "    general_stats.set_index(['statistics'], inplace = True)\n",
    "    \n",
    "    print(\"Check the general statistics from the analyzed variable:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(general_stats)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(general_stats)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the frequency table:\\n\")\n",
    "    \n",
    "    try:    \n",
    "        display(DATASET)    \n",
    "    except:\n",
    "        print(DATASET)\n",
    "\n",
    "    return general_stats, DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_pd_dataframe_as_csv (dataframe_obj_to_be_exported, new_file_name_without_extension, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_obj_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # new_file_name_without_extension - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "    # will export a file 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name_without_extension)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_obj_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_without_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for importing or exporting models, lists, or dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_list_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_or_list_file_name = None, directory_path = '', model_type = 'keras', dict_or_list_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickle as pkl\n",
    "    import dill\n",
    "    import tensorflow as tf\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_or_list_only' if only a dictionary or list will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    # model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_or_list_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_or_list_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep dictionary_or_list_file_name = None if no \n",
    "    # dictionary or list will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type = 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "    # lambda layers. Such models are compressed as tar.gz.\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_or_list_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_or_list_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_or_list_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root (empty string):\n",
    "        directory_path = \"\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_or_list_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_or_list_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary or list.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_or_list_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'keras_lambda'):\n",
    "                model_extension = 'tar.gz'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary or list {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary or list successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_lambda'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    # Try accessing the tar.gz file directly from the environment:\n",
    "                    model_path = key\n",
    "                    # to access from the dictionary:\n",
    "                    # model_path = colab_files_dict[key]\n",
    "                    \n",
    "                    # Extract to a temporary 'tmp' directory:\n",
    "                    try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                        ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        from tarfile import TarFile\n",
    "                        # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                        # https://docs.python.org/3/library/tarfile.html\n",
    "                        # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                        tar_file = TarFile.open(model_path, mode = 'r:gz')\n",
    "                        tar_file.extractall(\"tmp/\")\n",
    "                        tar_file.close()\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # Extract to a temporary 'tmp' directory:\n",
    "                    try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                        ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        from tarfile import TarFile\n",
    "                        # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                        # https://docs.python.org/3/library/tarfile.html\n",
    "                        # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                        tar_file = TarFile.open(model_path, mode = 'r:gz')\n",
    "                        tar_file.extractall(\"tmp/\")\n",
    "                        tar_file.close()\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    print(f\"TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "\n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "\n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_or_list_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary or list {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary or list successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_lambda'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    \n",
    "                    # Save your model in the SavedModel format\n",
    "                    model_to_export.save('saved_model/my_model')\n",
    "                    \n",
    "                    try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                        ! tar -czvf model_path saved_model/\n",
    "                    \n",
    "                    except NotFoundError:\n",
    "                        \n",
    "                        from tarfile import TarFile\n",
    "                        # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                        # https://docs.python.org/3/library/tarfile.html\n",
    "                        # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                        tar_file = TarFile.open(model_path, mode = 'w:gz')\n",
    "                        tar_file.add('saved_model/')\n",
    "                        tar_file.close()\n",
    "                    \n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    files.download(key)\n",
    "                    print(f\"TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # Save your model in the SavedModel format\n",
    "                    model_to_export.save('saved_model/my_model')\n",
    "                    \n",
    "                    try:\n",
    "                        # Compress the directory using tar\n",
    "                        ! tar -czvf model_path saved_model/\n",
    "                    \n",
    "                    except NotFoundError:\n",
    "                        \n",
    "                        from tarfile import TarFile\n",
    "                        # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                        # https://docs.python.org/3/library/tarfile.html\n",
    "                        # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                        tar_file = TarFile.open(model_path, mode = 'w:gz')\n",
    "                        tar_file.add('saved_model/')\n",
    "                        tar_file.close()\n",
    "                        \n",
    "                    print(f\"TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying a list of row filters to a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this function filter the rows and results into a smaller dataset, \n",
    "# since it removes the non-selected entries.\n",
    "# If you want to pass a filter to simply label the selected rows, use the function \n",
    "# LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 and (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 or (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "# The negative of this condition may be acessed with ~ operator:\n",
    "##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "## Also, you may use isna() method as filter for missing values:\n",
    "## filter = (dataframe_column_series).isna()\n",
    "## or, for not missing: ~(dataframe_column_series).isna()\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "# Filtered dataframe saved as filtered_df\n",
    "# Simply modify this object on the left of equality:\n",
    "filtered_df = APPLY_ROW_FILTERS_LIST (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating general statistics from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "# Statistics dataframe saved as general_stats. \n",
    "# Simply modify this object on the left of equality:\n",
    "general_stats = GENERAL_STATISTICS (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting data quantiles from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "# Quantiles dataframe saved as quantiles_summ_df\n",
    "# Simply modify this object on the left of equality:\n",
    "quantiles_summ_df = GET_QUANTILES (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting a particular P-percent quantile limit (P is a percent, from 0 to 100)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "P_PERCENT = 100\n",
    "# P_PERCENT: float value from 0 to 100 representing the percent of the quantile\n",
    "# if P_PERCENT = 31.2, then 31.2% of the data will fall below the returned value\n",
    "# if P_PERCENT = 75, then 75% of the data will fall below the returned value\n",
    "# if P_PERCENT = 0, the minimum value is returned.\n",
    "# if P_PERCENT = 100, the maximum value is returned.\n",
    "\n",
    "# P-Percent quantile limit returned as the float value quantile_lim\n",
    "# Simply modify this variable on the left of equality:\n",
    "quantile_lim = GET_P_PERCENT_QUANTILE_LIM (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, p_percent = P_PERCENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selecting subsets from a dataframe (using row filters) and labelling these subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention: this function selects subsets from the dataframe and label them, \n",
    "# allowing the seggregation of the data.\n",
    "# If you want to filter the dataframe to eliminate non-selected rows, use the \n",
    "# function APPLY_ROW_FILTERS_LIST\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 AND (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 OR (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. list_of_row_filters = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "NEW_LABELS_LIST = None\n",
    "# Here, each filter will be associated with one label. Therefore, if you want to apply \n",
    "# a filter with several conditions, combine them using the & and | operators.\n",
    "    \n",
    "# The labels are input as a list of strings or of discrete integer values. There must be\n",
    "# exactly one filter to each label: the filter will be used to obtain a subset from\n",
    "# the dataframe, and the label will be applied to all the rows of this subset.\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1, filter2], NEW_LABELS_LIST = ['label1', 'label2']\n",
    "# In this case, the rows of the dataset obtained when applying filter1 will receive the\n",
    "# string 'label1' as label. In turns, rows selected from filter2 will be labelled as \n",
    "# 'label2'.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3], NEW_LABELS_LIST = [1, 2, 3]\n",
    "# Here, the labels are integers: rows filtered through filter1 are marked as 1;\n",
    "# rows selected by filter2 are labelled as 2; and rows selected from filter3 are marked\n",
    "# as 3.\n",
    "    \n",
    "# Warning: the sequence of filters must be correspondent to the sequence of labels. \n",
    "# Rows selected from the first filter are labelled with the first item from the labels\n",
    "# list; rows selected by the 2nd filter are labelled with the 2nd element, and so on.\n",
    "    \n",
    "# If no list of labels is provided, a list of integers starting from zero and with the\n",
    "# same total of elements from LIST_OF_ROW_FILTERS will be created. For instance,\n",
    "# if lLIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4], the list of labels\n",
    "# [0, 1, 2, 3] would be created. Again, the rows selected from filter1 would be labelled\n",
    "# as 0; rows selected from filter 2 would be labelled as 1; and so on.\n",
    "\n",
    "NEW_LABELLED_COLUMN_NAME = None\n",
    "# NEW_LABELLED_COLUMN_NAME = None. Alternatively, declare a string (inside quotes), with\n",
    "# the name of the column containing the applied labels. For example, if\n",
    "# NEW_LABELLED_COLUMN_NAME = 'labels', then the new labels are saved in the column 'labels'\n",
    "# If no NEW_LABELLED_COLUMN_NAME is provided, the default name = \"df_label\" is provided.\n",
    "\n",
    "# Labelled dataframe saved as labelled_df\n",
    "# Simply modify this object on the left of equality:\n",
    "labelled_df = LABEL_DATAFRAME_SUBSETS (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS, new_labels_list = NEW_LABELS_LIST, new_labelled_column_name = NEW_LABELLED_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performing Analysis of Variance (ANOVA) and obtaining boxplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "COLUMN_WITH_GROUP_LABELS = None\n",
    "# COLUMN_WITH_GROUP_LABELS: name of the column registering groups, labels, or\n",
    "# factors correspondent to each row. e.g. COLUMN_WITH_GROUP_LABELS = \"group\"\n",
    "# if the correspondent group is registered in a column named 'group'.\n",
    "# If no name is provided, then the default name is used:\n",
    "# COLUMN_WITH_GROUP_LABELS = COLUMN_TO_ANALYZE + \"_label\"\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "# CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "# Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "# to get less restrictive results.\n",
    "\n",
    "REFERENCE_VALUE = None\n",
    "# reference_value: keep it as None or add a float value.\n",
    "# This reference value will be shown as a red constant line on the plot to be compared\n",
    "# with the boxes. e.g. reference_value = 1.0 will plot a red line passing through\n",
    "# column_to_analyze = 1.0\n",
    "BOXPLOT_NOTCH = False\n",
    "# Manipulate parameter notch (boolean, default: False) from the boxplot object\n",
    "# Whether to draw a notched boxplot (True), or a rectangular boxplot (False). \n",
    "# The notches represent the confidence interval (CI) around the median. \n",
    "BOXPLOT_VERTICALLY_ORIENTED = True\n",
    "# Manipulate the parameter vert (boolean, default: True)\n",
    "# If True, draws vertical boxes. If False, draw horizontal boxes.\n",
    "BOXPLOT_PATCH_ARTIST = False \n",
    "# Manipulate parameter patch_artist (boolean, default: False)\n",
    "# If False produces boxes with the Line2D artist. Otherwise, boxes are drawn \n",
    "# with Patch artists.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# Dictionary storing ANOVA F-test and p-value returned as anova_summary_dict\n",
    "# Dictionary mapping each component of the boxplot returned as boxplot_returned_dict\n",
    "# Simply modify these objects on the left of equality:\n",
    "anova_summary_dict, boxplot_returned_dict = anova_boxplot (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_group_labels = COLUMN_WITH_GROUP_LABELS, confidence_level = CONFIDENCE_LEVEL, boxplot_notch = BOXPLOT_NOTCH, boxplot_vertically_oriented = BOXPLOT_VERTICALLY_ORIENTED, boxplot_patch_artist = BOXPLOT_PATCH_ARTIST,  reference_value = REFERENCE_VALUE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: automatically calculate the ideal histogram bin size\n",
    "- The ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'analyzed_variable'\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# COLUMN_TO_ANALYZE = 'column1'\n",
    "\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#New dataframes saved as general_stats and frequency_table.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_stats, frequency_table = histogram (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, normal_curve_overlay = NORMAL_CURVE_OVERLAY, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: set number of bins\n",
    "- Use this one if the distance between data is too small, or if the histogram function did not return a valid histogram.\n",
    "- Here, the histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'analyzed_variable'\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# COLUMN_TO_ANALYZE = 'column1'\n",
    "\n",
    "TOTAL_OF_BINS = 50\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided. Check comments after the histogram_alternative\n",
    "# function call).\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#New dataframes saved as general_stats and frequency_table.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_stats, frequency_table = histogram_alternative (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, total_of_bins = TOTAL_OF_BINS, normal_curve_overlay = NORMAL_CURVE_OVERLAY, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
