{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Characterization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _ETL Workflow Notebook 2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "1. Dataframe general characterization; \n",
    "2. Characterizing categorical variables; \n",
    "3. Plotting bar charts;\n",
    "4. Calculating cumulative statistics;\n",
    "5. Dealing with missing values; \n",
    "6. Obtaining the correlation plot; \n",
    "7. Obtaining scatter plots and simple linear regressions; \n",
    "8. Visualizing time series; \n",
    "9. Visualizing histograms; \n",
    "10. Testing normality and visualizing the probability plot;\n",
    "11. Testing and visualizing probability plots for different statistical distributions;\n",
    "12. Filtering (selecting); or renaming columns; \n",
    "13. Dropping specific columns or rows from the dataframe; \n",
    "14. Removing duplicate rows from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                        # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                        #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                        # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                        # parsing speed by 5-10x.\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                        # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                        #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                        # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                        # parsing speed by 5-10x.\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\")\n",
    "            \n",
    "        if (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Dictionaries, possibly with nested dictionaries (JSON formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_obj_to_dataframe (json_obj_to_convert, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    json_file = json.loads(json_obj_to_convert)\n",
    "    # json.load() : This method is used to parse JSON from URL or file.\n",
    "    # json.loads(): This method is used to parse string with JSON content.\n",
    "    # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "    # like a dataframe.\n",
    "    # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object {json_obj_to_convert} converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for dataframe general characterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_gen_charac (df):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Dataframe 10 first rows:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_shape  = df.shape\n",
    "    print(f\"Dataframe shape (rows, columns) = {df_shape}.\")\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_columns_list = df.columns\n",
    "    print(f\"Dataframe columns list = {df_columns_list}.\")\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_dtypes = df.dtypes\n",
    "    print(\"Dataframe variables types:\")\n",
    "    print(df_dtypes)\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_general_statistics = df.describe()\n",
    "    print(\"Dataframe general statistics (numerical variables):\")\n",
    "    print(df_general_statistics)\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_missing_values = df.isna().sum()\n",
    "    print(\"Total of missing values for each feature:\")\n",
    "    print(df_missing_values)\n",
    "    \n",
    "    return df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for characterizing categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charac_cat_var (df, categorical_var_name, encode_var = False, new_encoded_col_name = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "           \n",
    "        \n",
    "            # Encoding syntax:\n",
    "            # dataset.loc[dataset[\"CatVar\"] == 'Value1', \"EncodedColumn\"] = 1\n",
    "            # dataset.loc[boolean_filter, EncodedColumn\"] = value,\n",
    "            # boolean_filter = (dataset[\"CatVar\"] == 'Value1') will be True when the \n",
    "            # equality is verified. The .loc method filters the dataframe, accesses the\n",
    "            # column declared after the comma and inputs the value defined (e.g. value = 1)\n",
    "            \n",
    "    \n",
    "    #WARNING: Use this function to a analyze a single column from a dataframe.\n",
    "    # The pandas .unique method and the numpy.unique function can handle a single series\n",
    "    # (column) or 1D arrays.\n",
    "    \n",
    "    #WARNING: The first return is the unique values summary/encoding table; \n",
    "    # the second one will be the dataframe with the encoded column. This second \n",
    "    # dataframe is returned only when encode_var = True\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name of the column \n",
    "    #to be analyzed. e.g. categorical_var_name = \"column1\"\n",
    "    \n",
    "    # encode_var = False - keep it False not to associate an integer value to each \n",
    "    # possible category. Alternatively, set encode_var = True to associate an integer\n",
    "    # starting from zero to each categorical value. This should not be used in case there\n",
    "    # is no order to be associated. Also, the encoding will be performed in accordance to \n",
    "    # the order of occurence in the dataframe df. In case there is no order, the One-Hot\n",
    "    # Encoding should be used.\n",
    "    # WARNING: if encoded_var = True, the function will return a new dataframe containing\n",
    "    # a column correspondent to the column input as 'categorical_var_name' but with the\n",
    "    # correspondent integer values.\n",
    "    \n",
    "    # new_encoded_col_name = None - This parameter shows effects only when encode_var = True.\n",
    "    # It refers to the column that will be created on the original dataframe containing the\n",
    "    # integer values associated to the categorical values.\n",
    "    # Input a string inside quotes to define a name (header) for the column of the summary\n",
    "    # dataframe to store the integer correspondent to each categorical value.\n",
    "    # e.g. new_encoded_col_name = \"integer_vals\". If no name is provided, the standard name\n",
    "    # formed by the string concatenation categorical_var_name + \"_encoded\" will be automatically\n",
    "    # assigned.\n",
    "    \n",
    "    # Get unique vals and respective counts.\n",
    "    # Let's use the more primitive NumPy method, less prone to be updated. It is equivalent\n",
    "    # to use the pandas .unique method, followed by the pandas .value_counts method.\n",
    "    \n",
    "    # Create a copy of the dataframe to eliminate the missing values from it\n",
    "    df_cleaned = df.dropna(axis = 0)\n",
    "    \n",
    "    #Now, check the unique values of the categorical variable:\n",
    "    unique_vals_list, counts_of_occurences = np.unique(df_cleaned[categorical_var_name], return_counts = True)\n",
    "    # The functions for counting unique values may raise exception errors when there are\n",
    "    # missing values. This does not happen when the missing values were incorrectly read as\n",
    "    # strings.\n",
    "    \n",
    "    # Let's normalize the counts_of_occurences array to obtain a correspondent list in percent\n",
    "    total_sum = np.sum(counts_of_occurences) #np.sum let's sum the whole array\n",
    "    \n",
    "    percent_of_occurence = ((counts_of_occurences)/(total_sum)) * 100\n",
    "    #By dividing the whole array by the total sum, we obtain an array where each element is\n",
    "    # the fraction of that element in the original array. These values are converted to \n",
    "    # percents after being multiplied by 100.\n",
    "    \n",
    "    # Let's create a Pandas dataframe summarizing these information:\n",
    "    # Create the dictionary from the arrays:\n",
    "    unique_summary = {\n",
    "                        \"Unique_values_of_categorical_variable\": unique_vals_list,\n",
    "                        \"Counts_of_occurences\": counts_of_occurences,\n",
    "                        \"Percent_of_occurence\": percent_of_occurence\n",
    "                     }\n",
    "    \n",
    "    # convert the dictionary to a Pandas dataframe:\n",
    "    unique_vals_summary = pd.DataFrame(data = unique_summary)\n",
    "    \n",
    "    if (encode_var == True):\n",
    "        \n",
    "        # Create a column on the unique_vals_summary dataframe to store the integer number,\n",
    "        # and copy the dataframe index to it. Then, the numbers will go from zero to the\n",
    "        # index of the last categorical value.\n",
    "        unique_vals_summary['Value_encoding'] = unique_vals_summary.index\n",
    "        # pandas .index method creates a range from zero to (len(dataframe) - 1), step = 1.\n",
    "     \n",
    "        # Loop through the unique_vals_summary, check the \"Unique_values_of_categorical_variable\"\n",
    "        # and the correspondent 'Value_encoding':\n",
    "        \n",
    "        for i in range(len(unique_vals_summary) - 1):\n",
    "            #len(unique_vals_summary) is the total of rows in the dataframe unique_vals_summary.\n",
    "            # This loop goes from i = 0 (index of the first row), \n",
    "            # to i = len(unique_vals_summary) - 1, index of the last row.\n",
    "            \n",
    "            # use the .iloc pandas method to subset an element (i, j) from the dataframe.\n",
    "            # i = row, j = column.\n",
    "            # j = 0 is the column \"Unique_values_of_categorical_variable\" (1st column);\n",
    "            # j = 3 is the column 'Value_encoding' (4th column).\n",
    "            \n",
    "            # Select the categorical value (column 0):\n",
    "            cat_val = unique_vals_summary.iloc[i, 0]\n",
    "            # Select the encoded value (column 3):\n",
    "            encoded_val = unique_vals_summary.iloc[i, 3]\n",
    "            \n",
    "            # Check if there is no name for the new column\n",
    "            if (new_encoded_col_name is None):\n",
    "                \n",
    "                #if there is not new_encoded_col_name, create one as the default:\n",
    "                new_encoded_col_name = categorical_var_name + \"_encoded\"\n",
    "            \n",
    "            # Create a boolean filter that checks if the entry on the dataframe corresponds\n",
    "            # to cat_val:\n",
    "            boolean_filter = (df[categorical_var_name] == cat_val)\n",
    "            # boolean_filter = True if the row contains the cat_val in its categorical column.\n",
    "            \n",
    "            #Input the encoded value when the filter is True:\n",
    "            df.loc[boolean_filter, new_encoded_col_name] = encoded_val\n",
    "            # In each iteration, the boolean_filter and the encoded_val are updated.\n",
    "            # Then, the command applies the boolean_filter to the dataframe df, and access\n",
    "            # the column declared after the comma. Then, it sets the values of the column as\n",
    "            # equals to the value after the equality, encoded_val.\n",
    "            # It is much more efficient than looping through the each row of the dataset, \n",
    "            # check the values on the categorical variable, and then input the integer value. \n",
    "            \n",
    "            # Notice that we used df, not df_cleaned, avoiding the elimination of the\n",
    "            # rows containing missing values.\n",
    "        \n",
    "        #Now that the dataframe was encoded, return the new dataframe\n",
    "        \n",
    "        # Print the encoding table\n",
    "        print(\"Returning dataset with encoded column and the unique values summary table.\")\n",
    "        print(\"Unique values summary table:\\n\")\n",
    "        print(unique_vals_summary)\n",
    "        \n",
    "        return unique_vals_summary, df\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #No encoded column will be added to the dataset\n",
    "        print(\"Unique values summary table:\\n\")\n",
    "        print(unique_vals_summary)\n",
    "    \n",
    "        return unique_vals_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for plotting the bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def bar_chart (df, categorical_var_name, response_var_name, aggregate_function = 'sum', add_suffix_to_aggregated_col = True, suffix = None, calculate_and_plot_cumulative_percent = True, orientation = 'vertical', limit_of_plotted_categories = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # df: dataframe being analyzed\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name \n",
    "    # of the column to be analyzed. e.g. \n",
    "    # categorical_var_name = \"column1\"\n",
    "    \n",
    "    # response_var_name: string (inside quotes) containing the name \n",
    "    # of the column that stores the response correspondent to the\n",
    "    # categories. e.g. response_var_name = \"response_feature\" \n",
    "    \n",
    "    # aggregate_function = 'sum': String defining the aggregation \n",
    "    # method that will be applied. Possible values:\n",
    "    # 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "    # 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "    # '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "    # '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "    # '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "    # and '95_percent_quantile'.\n",
    "    # To use another aggregate function, the method must be added to the\n",
    "    # dictionary of methods agg_methods_dict, defined in the function.\n",
    "    # If None or an invalid function is input, 'sum' will be used.\n",
    "    \n",
    "    # add_suffix_to_aggregated_col = True will add a suffix to the\n",
    "    # aggregated column. e.g. 'responseVar_mean'. If add_suffix_to_aggregated_col \n",
    "    # = False, the aggregated column will have the original column name.\n",
    "    \n",
    "    # suffix = None. Keep it None if no suffix should be added, or if\n",
    "    # the name of the aggregate function should be used as suffix, after\n",
    "    # \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "    # \"_\" sign in the beginning of this string to separate the suffix from\n",
    "    # the original column name. e.g. if the response variable is 'Y' and\n",
    "    # suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "    \n",
    "    # calculate_and_plot_cumulative_percent = True to calculate and plot\n",
    "    # the line of cumulative percent, or \n",
    "    # calculate_and_plot_cumulative_percent = False to omit it.\n",
    "    # This feature is only shown when aggregate_function = 'sum', 'median',\n",
    "    # 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "    # another aggregate is selected.\n",
    "    \n",
    "    # orientation = 'vertical' is the standard, and plots vertical bars\n",
    "    # (perpendicular to the X axis). In this case, the categories are shown\n",
    "    # in the X axis, and the correspondent responses are in Y axis.\n",
    "    # Alternatively, orientation = 'horizontal' results in horizontal bars.\n",
    "    # In this case, categories are in Y axis, and responses in X axis.\n",
    "    # If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "    \n",
    "    # Note: to obtain a Pareto chart, keep aggregate_function = 'sum',\n",
    "    # plot_cumulative_percent = True, and orientation = 'vertical'.\n",
    "    \n",
    "    # limit_of_plotted_categories: integer value that represents\n",
    "    # the maximum of categories that will be plot. Keep it None to plot\n",
    "    # all categories. Alternatively, set an integer value. e.g.: if\n",
    "    # limit_of_plotted_categories = 4, but there are more categories,\n",
    "    # the dataset will be sorted in descending order and: 1) The remaining\n",
    "    # categories will be sum in a new category named 'others' if the\n",
    "    # aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "    # omitted from the plot, for other aggregate functions. Notice that\n",
    "    # it limits only the variables in the plot: all of them will be\n",
    "    # returned in the dataframe.\n",
    "    # Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "    # columns will be aggregated as 'others' even if there is a single column\n",
    "    # beyond the limit.\n",
    "    \n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    \n",
    "    DATASET = df\n",
    "    \n",
    "    # Create the dictionary of possible aggregates, to define the\n",
    "    # aggregation method, according to the set by the user:\n",
    "    agg_methods_dict = {\n",
    "        \n",
    "        'median': DATASET.groupby(categorical_var_name)[response_var_name].median(),\n",
    "        'mean': DATASET.groupby(categorical_var_name)[response_var_name].mean(),\n",
    "        'mode': DATASET.groupby(categorical_var_name)[response_var_name].mode(),\n",
    "        'sum': DATASET.groupby(categorical_var_name)[response_var_name].sum(),\n",
    "        'min': DATASET.groupby(categorical_var_name)[response_var_name].min(),\n",
    "        'max': DATASET.groupby(categorical_var_name)[response_var_name].max(),\n",
    "        'variance': DATASET.groupby(categorical_var_name)[response_var_name].var(),\n",
    "        'standard_deviation': DATASET.groupby(categorical_var_name)[response_var_name].std(),\n",
    "        '10_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.10),\n",
    "        '20_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.20),\n",
    "        '25_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.25),\n",
    "        '30_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.30),\n",
    "        '40_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.40),\n",
    "        '50_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.50),\n",
    "        '60_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.60),\n",
    "        '70_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.70),\n",
    "        '75_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.75),\n",
    "        '80_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.80),\n",
    "        '90_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.90),\n",
    "        '95_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.95)\n",
    "    }\n",
    "    \n",
    "    # check if the function was not set in the dictionary. If not,\n",
    "    # use 'sum'\n",
    "    if (aggregate_function not in (agg_methods_dict.keys())):\n",
    "        \n",
    "        aggregate_function = 'sum'\n",
    "        print(\"Invalid or no aggregation function input, so using the default \\'sum\\'.\")\n",
    "    \n",
    "    # Select the method in the dictionary and apply it. To access a value\n",
    "    # 'val' correspondent to the key 'key' from a dictionary dict, we\n",
    "    # declare: dict['key'], just as accessing a column from a dataframe.\n",
    "    \n",
    "    # The value will be the application of the method itself, i.e., the\n",
    "    # dataset will be aggregated:\n",
    "    DATASET = agg_methods_dict[aggregate_function]\n",
    "    \n",
    "    # If an aggregate function different from 'sum', 'mean', 'median' or 'mode' \n",
    "    # is used with plot_cumulative_percent = True, \n",
    "    # set plot_cumulative_percent = False:\n",
    "    # (check if aggregate function is not in the list of allowed values):\n",
    "    if ((aggregate_function not in ['sum', 'mean', 'median', 'mode']) & (calculate_and_plot_cumulative_percent == True)):\n",
    "        \n",
    "        calculate_and_plot_cumulative_percent = False\n",
    "        print(\"The cumulative percent is only calculated when aggregate_function = \\'sum\\', \\'mean\\', \\'median\\', or \\'mode\\'. So, plot_cumulative_percent was set as False.\")\n",
    "    \n",
    "    # Guarantee that the columns from the aggregated dataset have the correct\n",
    "    \n",
    "    # Let's create a list of the new column names\n",
    "    # The first element is categorical_var_name, which is not modified:\n",
    "    list_of_cols = [categorical_var_name]\n",
    "    \n",
    "    # Check if add_suffix_to_aggregated_col is False. If it is, simply\n",
    "    # repeat the original response_var_name:\n",
    "    if (add_suffix_to_aggregated_col == False):\n",
    "        \n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    else:\n",
    "        # Let's add a suffix. Check if suffix is None. If it is,\n",
    "        # set \"_\" + aggregate_function as suffix:\n",
    "        \n",
    "        if (suffix is None):\n",
    "            suffix = \"_\" + aggregate_function\n",
    "        \n",
    "        # Now, append response_var_name + suffix to the list to\n",
    "        # create the name of the new aggregated column:\n",
    "        response_var_name = response_var_name + suffix\n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    # Now, rename the columns of the aggregated dataset as the list\n",
    "    # list_of_cols:\n",
    "    DATASET.columns = list_of_cols\n",
    "    \n",
    "    # Let's sort the dataframe.\n",
    "    \n",
    "    # Order the dataframe in descending order by the response.\n",
    "    # If there are equal responses, order them by category, in\n",
    "    # ascending order; put the missing values in the first position\n",
    "    # To pass multiple columns and multiple types of ordering, we use\n",
    "    # lists. If there was a single column to order by, we would declare\n",
    "    # it as a string. If only one order of ascending was used, we would\n",
    "    # declare it as a simple boolean\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    \n",
    "    DATASET = DATASET.sort_values(by = [response_var_name, categorical_var_name], ascending = [False, True], na_position = 'first')\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # plot_cumulative_percent = True, create a column to store the\n",
    "    # cumulative percent:\n",
    "    if (calculate_and_plot_cumulative_percent): \n",
    "        # Run the following code if the boolean value is True (implicity)\n",
    "        \n",
    "        # Calculate the total sum of the array correspondent to\n",
    "        # the column (series) response_var_name\n",
    "        total_sum = np.sum(np.array(DATASET[response_var_name]))\n",
    "        \n",
    "        # Create a column series for the cumulative sum:\n",
    "        cumsum_col = response_var_name + \"_cumsum\"\n",
    "        DATASET[cumsum_col] = DATASET[response_var_name].cumsum()\n",
    "        \n",
    "        # Now, create a column for the accumulated percent\n",
    "        # by dividing cumsum_col by total_sum and multiplying it by\n",
    "        # 100 (%):\n",
    "        cum_pct_col = response_var_name + \"_cum_pct\"\n",
    "        DATASET[cum_pct_col] = (DATASET[cumsum_col])/(total_sum)*100\n",
    "        print(f\"Successfully calculated cumulative sum and cumulative percent correspondent to the response variable {response_var_name}.\")\n",
    "    \n",
    "    print(\"Successfully aggregated and ordered the dataset to plot. Check the 10 first rows of this returned dataset:\\n\")\n",
    "    print(DATASET.head(10))\n",
    "    \n",
    "    # Check if the total of plotted categories is limited:\n",
    "    if not (limit_of_plotted_categories is None):\n",
    "        \n",
    "        # Since the value is not None, we have to limit it\n",
    "        # Check if the limit is lower than or equal to the length of the dataframe.\n",
    "        # If it is, we simply copy the columns to the series (there is no need of\n",
    "        # a memory-consuming loop or of applying the head method to a local copy\n",
    "        # of the dataframe):\n",
    "        df_length = len(DATASET)\n",
    "            \n",
    "        if (limit_of_plotted_categories <= df_length):\n",
    "            # Simply copy the columns to the graphic series:\n",
    "            categories = DATASET[categorical_var_name]\n",
    "            responses = DATASET[response_var_name]\n",
    "            # If there is a cum_pct column, copy it to a series too:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = plotted_df[cum_pct_col]\n",
    "        \n",
    "        else:\n",
    "            # The limit is lower than the total of categories,\n",
    "            # so we actually have to limit the size of plotted df:\n",
    "        \n",
    "            # If aggregate_function is not 'sum', we simply apply\n",
    "            # the head method to obtain the first rows (number of\n",
    "            # rows input as parameter; if no parameter is input, the\n",
    "            # number of 5 rows is used):\n",
    "            if (aggregate_function != 'sum'):\n",
    "                # Limit to the number limit_of_plotted_categories:\n",
    "                # create another local copy of the dataframe not to\n",
    "                # modify the returned dataframe object:\n",
    "                plotted_df = DATASET.head(limit_of_plotted_categories)\n",
    "\n",
    "                # Create the series of elements to plot:\n",
    "                categories = plotted_df[categorical_var_name]\n",
    "                responses = plotted_df[response_var_name]\n",
    "                # If the cumulative percent was obtained, create the series for it:\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = plotted_df[cum_pct_col]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Firstly, copy the elements that will be kept to x, y and (possibly) cum_pct\n",
    "                # lists.\n",
    "                # Start the lists:\n",
    "                categories = []\n",
    "                responses = []\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = [] # start this list only if its needed to save memory\n",
    "\n",
    "                for i in range (0, limit_of_plotted_categories):\n",
    "                    # i goes from 0 (first index) to limit_of_plotted_categories - 1\n",
    "                    # (index of the last category to be kept):\n",
    "                    # copy the elements from the DATASET to the list\n",
    "                    # category is the 1st column (column 0); response is the 2nd (col 1);\n",
    "                    # and cumulative percent is the 4th (col 3):\n",
    "                    categories.append(DATASET.iloc[i, 0])\n",
    "                    responses.append(DATASET.iloc[i, 1])\n",
    "                    \n",
    "                    if (calculate_and_plot_cumulative_percent):\n",
    "                        cum_pct.append(DATASET.iloc[i, 3]) # only if there is something to iloc\n",
    "                    \n",
    "                # Now, i = limit_of_plotted_categories - 1\n",
    "                # Create a variable to store the sum of other responses\n",
    "                other_responses = 0\n",
    "                # loop from i = limit_of_plotted_categories to i = df_length-1, index\n",
    "                # of the last element. Notice that this loop may have a single call, if there\n",
    "                # is only one element above the limit:\n",
    "                for i in range (limit_of_plotted_categories, (df_length - 1)):\n",
    "                    \n",
    "                    other_responses = other_responses + (DATASET.iloc[i, 1])\n",
    "                \n",
    "                # Now, add the last elements to the series:\n",
    "                # The last category is named 'others':\n",
    "                categories.append('others')\n",
    "                # The correspondent aggregated response is the value \n",
    "                # stored in other_responses:\n",
    "                responses.append(other_responses)\n",
    "                # The cumulative percent is 100%, since this must be the sum of all\n",
    "                # elements (the previous ones plus the ones aggregated as 'others'\n",
    "                # must totalize 100%).\n",
    "                # On the other hand, the cumulative percent is stored only if needed:\n",
    "                cum_pct.append(100)\n",
    "    \n",
    "    else:\n",
    "        # This is the situation where there is no limit of plotted categories. So, we\n",
    "        # simply copy the columns to the plotted series (it is equivalent to the \n",
    "        # situation where there is a limit, but the limit is equal or inferior to the\n",
    "        # size of the dataframe):\n",
    "        categories = DATASET[categorical_var_name]\n",
    "        responses = DATASET[response_var_name]\n",
    "        # If there is a cum_pct column, copy it to a series too:\n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            cum_pct = plotted_df[cum_pct_col]\n",
    "    \n",
    "    \n",
    "    # Now the data is prepared and we only have to plot \n",
    "    # categories, responses, and cum_pct:\n",
    "    \n",
    "    # Set labels and titles for the case they are None\n",
    "    if (plot_title is None):\n",
    "        plot_title = f\"Bar_chart_for_{response_var_name}_by_{categorical_var_name}\"\n",
    "    \n",
    "    if (horizontal_axis_title is None):\n",
    "\n",
    "        horizontal_axis_title = categorical_var_name\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Notice that response_var_name already has the suffix indicating the\n",
    "        # aggregation function\n",
    "        vertical_axis_title = response_var_name\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    plt.title(plot_title)\n",
    "    ax1.set_xlabel(horizontal_axis_title)\n",
    "    ax1.set_ylabel(vertical_axis_title, color = 'blue')\n",
    "    \n",
    "    if (orientation == 'horizontal'):\n",
    "        \n",
    "        # Horizontal bars used - barh method (bar horizontal):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "        # Now, the categorical variables stored in series categories must be\n",
    "        # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "        # must be in the horizontal axis X.\n",
    "        ax1.barh(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.barh(y, x, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            # Here, the x axis must be the cum_pct value, and the Y\n",
    "            # axis must be categories (it must be correspondent to the\n",
    "            # bar chart)\n",
    "            ax2.plot(cum_pct, categories, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('x', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "        \n",
    "    else: \n",
    "        # If None or an invalid orientation was used, set it as vertical\n",
    "        # Use Matplotlib standard bar method (vertical bar):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "        \n",
    "        # In this standard case, the categorical variables (categories) are positioned\n",
    "        # as X, and the responses as Y:\n",
    "        ax1.bar(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.bar(x, y, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(categories, cum_pct, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('y', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "    \n",
    "    # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "    # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "    # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "    # constant line).\n",
    "    \n",
    "    # Now the parameters specific to the configurations are finished, so we can go back\n",
    "    # to the general code:\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"bar_chart\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating cumulative statistics**\n",
    "- Cumulative sum (cumsum); cumulative product (cumprod); cumulative maximum (cummax); cumulative minimum (cummin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_stats (df, column_to_analyze, cumulative_statistic = 'sum', new_cum_stats_col_name = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # cumulative_statistic: the statistic that will be calculated. The cumulative\n",
    "    # statistics allowed are: 'sum' (for cumulative sum, cumsum); 'product' \n",
    "    # (for cumulative product, cumprod); 'max' (for cumulative maximum, cummax);\n",
    "    # and 'min' (for cumulative minimum, cummin).\n",
    "    \n",
    "    # new_cum_stats_col_name = None or string (inside quotes), \n",
    "    # containing the name of the new column created for storing the cumulative statistic\n",
    "    # calculated. \n",
    "    # e.g. new_cum_stats_col_name = \"cum_stats\" will create a column named as 'cum_stats'.\n",
    "    # If its None, the new column will be named as column_to_analyze + \"_\" + [selected\n",
    "    # cumulative function] ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "     \n",
    "    \n",
    "    #WARNING: Use this function to a analyze a single column from a dataframe.\n",
    "    \n",
    "    if ((cumulative_statistic not in ['sum', 'product', 'max', 'min']) | (cumulative_statistic is None)):\n",
    "        \n",
    "        print(\"Please, select a valid method for calculating the cumulative statistics: sum, product, max, or min.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if (new_cum_stats_col_name is None):\n",
    "            # set the standard name\n",
    "            # column_to_analyze + \"_\" + [selected cumulative function] \n",
    "            # ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "            # cumulative_statistic variable stores ['sum', 'product', 'max', 'min']\n",
    "            # we must concatenate \"cum\" to the left of this string:\n",
    "            new_cum_stats_col_name = column_to_analyze + \"_\" + \"cum\" + cumulative_statistic\n",
    "        \n",
    "        # create a local copy of the dataframe to manipulate:\n",
    "        DATASET = df\n",
    "        # The series to be analyzed is stored as DATASET[column_to_analyze]\n",
    "        \n",
    "        # Now apply the correct method\n",
    "        # the dictionary dict_of_methods correlates the input cumulative_statistic to the\n",
    "        # correct Pandas method to be applied to the dataframe column\n",
    "        dict_of_methods = {\n",
    "            \n",
    "            'sum': DATASET[column_to_analyze].cumsum(),\n",
    "            'product': DATASET[column_to_analyze].cumprod(),\n",
    "            'max': DATASET[column_to_analyze].cummax(),\n",
    "            'min': DATASET[column_to_analyze].cummin()\n",
    "        }\n",
    "        \n",
    "        # To access the value (method) correspondent to a given key (input as \n",
    "        # cumulative_statistic): dictionary['key'], just as if accessing a column from\n",
    "        # a dataframe. In this case, the method is accessed as:\n",
    "        # dict_of_methods[cumulative_statistic], since cumulative_statistic is itself the key\n",
    "        # of the dictionary of methods.\n",
    "        \n",
    "        # store the resultant of the method in a new column of DATASET \n",
    "        # named as new_cum_stats_col_name\n",
    "        DATASET[new_cum_stats_col_name] = dict_of_methods[cumulative_statistic]\n",
    "        \n",
    "        print(f\"The cumulative {cumulative_statistic} statistic was successfully calculated and added as the column \\'{new_cum_stats_col_name}\\' of the returned dataframe.\")\n",
    "        print(\"Check the new dataframe's 10 first rows:\\n\")\n",
    "        print(DATASET.head(10))\n",
    "        \n",
    "        return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for dealing with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values (df, subset_columns_list = None, drop_missing_val = True, fill_missing_val = False, eliminate_only_completely_empty_rows = False, minimum_number_of_mis_vals = None, value_to_fill = None, fill_method = \"fill_with_zeros\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #subset_columns_list = list of columns to look for missing values. Only missing values\n",
    "    # in these columns will be considered for deciding which columns to remove.\n",
    "    # Declare it as a list of strings inside quotes containing the columns' names to look at,\n",
    "    # even if this list contains a single element. e.g. subset_columns_list = ['column1']\n",
    "    # will check only 'column1'; whereas subset_columns_list = ['col1', 'col2', 'col3'] will\n",
    "    # chek the columns named as 'col1', 'col2', and 'col3'.\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "    \n",
    "    #drop_missing_val = True to eliminate the rows containing missing values.\n",
    "    \n",
    "    #fill_missing_val = False. Set this to True to activate the mode for filling the missing\n",
    "    # values.\n",
    "    \n",
    "    #eliminate_only_completely_empty_rows = False - This parameter shows effect only when\n",
    "    # drop_missing_val = True. If you set eliminate_only_completely_empty_rows = True, then\n",
    "    # only the rows where all the columns are missing will be eliminated.\n",
    "    # If you define a subset, then only the rows where all the subset columns are missing\n",
    "    # will be eliminated.\n",
    "    \n",
    "    #minimum_number_of_mis_vals = None - This parameter shows effect only when\n",
    "    # drop_missing_val = True. If you set minimum_number_of_mis_vals equals to an integer value,\n",
    "    # then only the rows where at least this integer number columns are missing will be \n",
    "    # eliminated. e.g. if minimum_number_of_mis_vals = 2, only rows containing missing values\n",
    "    # for at least 2 columns will be eliminated.\n",
    "    # If you define a subset, then only the rows where all the subset columns are missing\n",
    "    # will be eliminated.\n",
    "    \n",
    "    #value_to_fill = None - This parameter shows effect only when\n",
    "    # fill_missing_val = True. Set this parameter as a float value to fill all missing\n",
    "    # values with this value. e.g. value_to_fill = 0 will fill all missing values with\n",
    "    # the number 0. You can also pass a function call like \n",
    "    # value_to_fill = np.sum(dataset['col1']). In this case, the missing values will be\n",
    "    # filled with the sum of the series dataset['col1']\n",
    "    # Alternatively, you can also input a string to fill the missing values. e.g.\n",
    "    # value_to_fill = 'text' will fill all the missing values with the string \"text\".\n",
    "    \n",
    "    #fill_method = \"fill_with_zeros\". - This parameter shows effect only \n",
    "    # when fill_missing_val = True.\n",
    "    # Alternatively: fill_method = \"fill_with_zeros\" - fill all the missing values with 0\n",
    "    # fill_method = \"fill_with_value_to_fill\" - fill the missing values with the value\n",
    "    # defined as the parameter value_to_fill\n",
    "    # fill_method = \"fill_with_avg\" - fill the missing values with the average value for \n",
    "    # each column.\n",
    "    # fill_method = \"fill_by_interpolating\" - fill by interpolating the previous and the \n",
    "    # following value. A linear interpolation will be used.\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\n",
    "    # fill_method = \"ffill\" - Forward fill: fills missing with previous value.\n",
    "    \n",
    "    #WARNING: if the fillna method is selected (fill_missing_val == True), but no filling\n",
    "    # methodology is selected, the missing values of the dataset will be filled with 0.\n",
    "    # The same applies when a non-valid fill methodology is selected.\n",
    "    # Pandas fillna method does not allow us to fill only a selected subset.\n",
    "    \n",
    "    #WARNING: if fill_method == \"fill_with_value_to_fill\" but value_to_fill is None, the \n",
    "    # missing values will be filled with the value 0.\n",
    "    \n",
    "    boolean_filter1 = (drop_missing_val == True) & (fill_missing_val == False)\n",
    "    #Notice that the presence of the parameter fill_missing_val will make this filter False,\n",
    "    # even if drop_missing_val is erroneously set as True.\n",
    "    boolean_filter2 = (boolean_filter1) & (subset_columns_list is None)\n",
    "    #These filters are True only if both conditions inside parentheses are True.\n",
    "    # The operator & is equivalent to 'And' (intersection).\n",
    "    # The operator | is equivalent to 'Or' (union).\n",
    "    \n",
    "    boolean_filter3 = (fill_missing_val == True) & (fill_method is None)\n",
    "    #boolean_filter3 represents the situation where the fillna method was selected, but\n",
    "    # no filling method was set.\n",
    "    \n",
    "    boolean_filter4 = (value_to_fill is None) & (fill_method == \"fill_with_value_to_fill\")\n",
    "    #boolean_filter4 represents the situation where the fillna method will be used and the\n",
    "    # user selected to fill the missing values with 'value_to_fill', but did not set a value\n",
    "    # for 'value_to_fill'.\n",
    "    \n",
    "    if (boolean_filter1 == True):\n",
    "        \n",
    "        if (boolean_filter2 == True):\n",
    "            \n",
    "            if (eliminate_only_completely_empty_rows == True):\n",
    "                #Eliminate only completely empty rows\n",
    "                cleaned_df = cleaned_df.dropna(axis = 0, how = \"all\")\n",
    "                # if axis = 1, dropna will eliminate each column containing missing values.\n",
    "            \n",
    "            elif (minimum_number_of_mis_vals is not None):\n",
    "                #eliminate only rows containing at least 'minimum_number_of_mis_vals' missing vals\n",
    "                cleaned_df = cleaned_df.dropna(axis = 0, tresh = minimum_number_of_mis_vals)\n",
    "            \n",
    "            else:\n",
    "                #Eliminate all rows containing missing values.\n",
    "                #The only parameter is drop_missing_val\n",
    "                cleaned_df = cleaned_df.dropna(axis=0)\n",
    "        \n",
    "        else:\n",
    "            #In this case, there is a subset for applying the Pandas dropna method.\n",
    "            #Only the coluns in the subset 'subset_columns_list' will be analyzed.\n",
    "                  \n",
    "            if (eliminate_only_completely_empty_rows == True):\n",
    "                #Eliminate only completely empty rows\n",
    "                cleaned_df = cleaned_df.dropna(subset = subset_columns_list, how = \"all\")\n",
    "            \n",
    "            elif (minimum_number_of_mis_vals is not None):\n",
    "                #eliminate only rows containing at least 'minimum_number_of_mis_vals' missing vals\n",
    "                cleaned_df = cleaned_df.dropna(subset = subset_columns_list, tresh = minimum_number_of_mis_vals)\n",
    "            \n",
    "            else:\n",
    "                #Eliminate all rows containing missing values.\n",
    "                #The only parameter is drop_missing_val\n",
    "                cleaned_df = cleaned_df.dropna(subset = subset_columns_list)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In this case, the user set a value for the parameter fill_missing_val, turning\n",
    "        # the boolean filter 1 to False. That's for avoiding the removal of the missing\n",
    "        # values, since the user wants to fill the missing data.\n",
    "        # Pandas fillna method documentation:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
    "        \n",
    "        if (boolean_filter3 == True):\n",
    "            #fillna method was selected, but no filling method was set.\n",
    "            # Then, filling with zero.\n",
    "            cleaned_df = cleaned_df.fillna(0)\n",
    "        \n",
    "        elif (boolean_filter4 == True):\n",
    "            #fill_method == \"fill_with_value_to_fill\" but value_to_fill is None.\n",
    "            # Then, filling with zero.\n",
    "            cleaned_df = cleaned_df.fillna(0)\n",
    "        \n",
    "        else:\n",
    "            #A filling methodology was selected.\n",
    "            if (fill_method == \"fill_with_zeros\"):\n",
    "                cleaned_df = cleaned_df.fillna(0)\n",
    "            \n",
    "            elif (fill_method == \"fill_with_value_to_fill\"):\n",
    "                cleaned_df = cleaned_df.fillna(value_to_fill)\n",
    "            \n",
    "            elif (fill_method == \"fill_with_avg\"):\n",
    "                \n",
    "                #1. Get dataframe's columns list:\n",
    "                df_cols = df.columns\n",
    "                #2. Start an empty list that will store all of the average values:\n",
    "                avg_list = []\n",
    "                #3. Loop through each column of df_cols.\n",
    "                # Calculate the average value for the series df[column].\n",
    "                # Append this value to avg_list.\n",
    "                \n",
    "                for column in df_cols:\n",
    "                    #check all elements of df_cols, named 'column'\n",
    "                    # Calculate the average of the series df[column]\n",
    "                    # and append it to the avg_list\n",
    "                    avg_list.append(np.average(df[column]))\n",
    "               \n",
    "                #Now, we have a list containing the average value correspondent\n",
    "                # to each column of the dataframe, named avg_list.\n",
    "                # 5. Create a dictionary informing the columns and the values\n",
    "                # to be input in each column:\n",
    "                fill_dict = {df_cols: avg_list}\n",
    "                # This dictionary correlates each column to its average value.\n",
    "                \n",
    "                #6. Finally, use this dictionary to fill the missing values of each column\n",
    "                # with the average value of that column\n",
    "                cleaned_df = cleaned_df.fillna(value = fill_dict)\n",
    "                #The method will search the column name in fill_dict (key of the dictionary),\n",
    "                # and will use the correspondent value (average) to fill the missing values.\n",
    "            \n",
    "            elif (fill_method == \"fill_by_interpolating\"):\n",
    "                cleaned_df = cleaned_df.interpolate(method='linear')\n",
    "                #For using a polynomial interpolation:\n",
    "                # cleaned_df = cleaned_df.interpolate(method='polynomial', order = 2)\n",
    "                # Modify the order to use other degrees of polynomials (order = 2 for 2nd order).\n",
    "            \n",
    "            elif (fill_method == \"ffill\"):\n",
    "                cleaned_df = cleaned_df.fillna(method = fill_method)\n",
    "            \n",
    "            else:\n",
    "                print(\"No valid filling methodology was selected. Then, filling missing values with 0.\")\n",
    "                cleaned_df = cleaned_df.fillna(0)\n",
    "        \n",
    "        \n",
    "    #Reset index before returning the cleaned dataframe:\n",
    "    cleaned_df = df.reset_index(drop = True)\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for obtaining the correlation plot**\n",
    "- The Pandas method dataset.corr() calculates the Pearson's correlation coefficients R.\n",
    "- Pearson's correlation coefficients R go from -1 to 1.\n",
    "- These coefficients are R, not R².\n",
    "\n",
    "#### To obtain the coefficients R², we raise the results to the 2nd power, i.e., we calculate (dataset.corr())**2\n",
    "- R² goes from 0 to 1, where 1 represents the perfect correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot (df, show_masked_plot = True, responses_to_return_corr = None, set_returned_limit = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    #show_masked_plot = True - keep as True if you want to see a cleaned version of the plot\n",
    "    # where a mask is applied.\n",
    "    \n",
    "    #responses_to_return_corr - keep as None to return the full correlation tensor.\n",
    "    # If you want to display the correlations for a particular group of features, input them\n",
    "    # as a list, even if this list contains a single element. Examples:\n",
    "    # responses_to_return_corr = ['response1'] for a single response\n",
    "    # responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "    # responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "    # of a column of the dataset that represents a response variable.\n",
    "    # WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "    # of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "    \n",
    "    # set_returned_limit = None - This variable will only present effects in case you have\n",
    "    # provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "    # to return all of the correlation coefficients; or, alternatively, \n",
    "    # provide an integer number to limit the total of coefficients returned. \n",
    "    # e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "    correlation_matrix = df.corr(method='pearson')\n",
    "    \n",
    "    if (show_masked_plot == False):\n",
    "        #Show standard plot\n",
    "        \n",
    "        plt.figure()\n",
    "        sns.heatmap((correlation_matrix)**2, annot=True, fmt=\".2f\")\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(24,8));\n",
    "\n",
    "    #Oncee the pandas method .corr() calculates R, we raised it to the second power \n",
    "    # to obtain R². R² goes from zero to 1, where 1 represents the perfect correlation.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Show masked (cleaner) plot instead of the standard one\n",
    "        \n",
    "        plt.figure()\n",
    "        # Mask for the upper triangle\n",
    "        mask = np.zeros_like((correlation_matrix)**2)\n",
    "\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        # Generate a custom diverging colormap\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        # Heatmap with mask and correct aspect ratio\n",
    "        sns.heatmap(((correlation_matrix)**2), mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(24,8));\n",
    "\n",
    "        #Again, the method dataset.corr() calculates R within the variables of dataset.\n",
    "        #To calculate R², we simply raise it to the second power: (dataset.corr()**2)\n",
    "    \n",
    "    #Sort the values of correlation_matrix in Descending order:\n",
    "    \n",
    "    if (responses_to_return_corr is not None):\n",
    "        \n",
    "        #Select only the desired responses, by passing the list responses_to_return_corr\n",
    "        # as parameter for column filtering:\n",
    "        correlation_matrix = correlation_matrix[responses_to_return_corr]\n",
    "        \n",
    "        #Now sort the values according to the responses, by passing the list\n",
    "        # responses_to_return_corr as the parameter\n",
    "        correlation_matrix = correlation_matrix.sort_values(by = responses_to_return_corr, ascending = False)\n",
    "        \n",
    "        # If a limit of coefficients was determined, apply it:\n",
    "        if (set_returned_limit is not None):\n",
    "                \n",
    "                correlation_matrix = correlation_matrix.head(set_returned_limit)\n",
    "                #Pandas .head(X) method returns the first X rows of the dataframe.\n",
    "                # Here, it returns the defined limit of coefficients, set_returned_limit.\n",
    "                # The default .head() is X = 5.\n",
    "        \n",
    "        print(correlation_matrix)\n",
    "    \n",
    "    print(\"ATTENTION: The correlation plots show the linear correlations R², which go from 0 (none correlation) to 1 (perfect correlation). Obviously, the main diagonal always shows R² = 1, since the data is perfectly correlated to itself.\")\n",
    "    print(\"The returned correlation matrix, on the other hand, presents the linear coefficients of correlation R, not R². R values go from -1 (perfect negative correlation) to 1 (perfect positive correlation).\")\n",
    "    print(\"None of these coefficients take non-linear relations and the presence of a multiple linear correlation in account. For these cases, it is necessary to calculate R² adjusted, which takes in account the presence of multiple preditors and non-linearities.\")\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for obtaining scatter plots and simple linear regressions**\n",
    "- Here, only a single prediction variable will be analyzed by once.\n",
    "- The plots will show Y x X, where X is the predict or independent variable.\n",
    "- The linear regressions will be of the type Y = aX + b, i.e., a single pair (X, Y) analyzed.\n",
    "\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "azdata_cell_guid": "a1b4ca0e-b55f-4fce-9e94-8ae0a0ac171a",
    "tags": [
     "CELL_10"
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_plot_lin_reg (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 0, y_axis_rotation = 0, show_linear_reg = True, grid = True, add_splines_lines = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110): \n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (show_linear_reg == True):\n",
    "        estatisticas = []\n",
    "        estatisticas.append(\"Linear Fitting:\")\n",
    "        estatisticas.append(\"R² = \")\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (x1 is None):\n",
    "        \n",
    "        if not (lab1 is None):\n",
    "            label_1 = lab1\n",
    "        else:\n",
    "            label_1 = \"Y1 x X1\"\n",
    "        \n",
    "        #falsa negativa: passa se os valores nao forem nulos\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = 'o', color='blue', label=label_1)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta1 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg1 = stats.linregress(x1, y1)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg1 = x1.sort_values()\n",
    "            x_reg1 = x_reg1.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg1 = (reg1).intercept + (reg1).slope*(x_reg1)\n",
    "            #gerar string da reta\n",
    "            string1 = \"y = %.2f*x + %.2f\" %((reg1).slope, (reg1).intercept)\n",
    "            reta1.append(string1)\n",
    "            #calcular R2\n",
    "            r_sq1 = (reg1).rvalue**2\n",
    "            reta1.append(r_sq1)\n",
    "            print(\"\\nLinear Fitting 1: \" + string1)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 1) = %.4f\" %(r_sq1))\n",
    "            string_label1 = 'Linear regression: ' + label_1\n",
    "            ax.plot(x_reg1, y_reg1,  linestyle='-', marker='', color='blue', label = string_label1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #roda apenas se ambos estiverem presentes\n",
    "                \n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = 'o', color='red', label=label_2)    \n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta2 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg2 = stats.linregress(x2, y2)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg2 = x2.sort_values()\n",
    "            x_reg2 = x_reg2.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg2 = (reg2).intercept + (reg2).slope*(x_reg2)\n",
    "            #gerar string da reta\n",
    "            string2 = \"y = %.2f*x + %.2f\" %((reg2).slope, (reg2).intercept)\n",
    "            reta2.append(string2)\n",
    "            #calcular R2\n",
    "            r_sq2 = (reg2).rvalue**2\n",
    "            reta2.append(r_sq2)\n",
    "            print(\"\\nLinear Fitting 2: \" + string2)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 2) = %.4f\" %(r_sq2))\n",
    "            string_label2 = 'Linear regression: ' + label_2\n",
    "            ax.plot(x_reg2, y_reg2,  linestyle='-', marker='', color='red', label = string_label2)\n",
    "        \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = 'o', color='green', label=label_3)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta3 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg3 = stats.linregress(x3, y3)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg3 = x3.sort_values()\n",
    "            x_reg3 = x_reg3.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg3 = (reg3).intercept + (reg3).slope*(x_reg3)\n",
    "            #gerar string da reta\n",
    "            string3 = \"y = %.2f*x + %.2f\" %((reg3).slope, (reg3).intercept)\n",
    "            reta3.append(string3)\n",
    "            #calcular R2\n",
    "            r_sq3 = (reg3).rvalue**2\n",
    "            reta3.append(r_sq3)\n",
    "            print(\"\\nLinear Fitting 3: \" + string3)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 3) = %.4f\" %(r_sq3))\n",
    "            string_label3 = 'Linear regression: ' + label_3\n",
    "            ax.plot(x_reg3, y_reg3,  linestyle='-', marker='', color='green', label = string_label3)\n",
    "        \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = 'o', color='black', label=label_4)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta4 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg4 = stats.linregress(x4, y4)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg4 = x4.sort_values()\n",
    "            x_reg4 = x_reg4.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg4 = (reg4).intercept + (reg4).slope*(x_reg4)\n",
    "            #gerar string da reta\n",
    "            string4 = \"y = %.2f*x + %.2f\" %((reg4).slope, (reg4).intercept)\n",
    "            reta4.append(string4)\n",
    "            #calcular R2\n",
    "            r_sq4 = (reg4).rvalue**2\n",
    "            reta4.append(r_sq4)\n",
    "            print(\"\\nLinear Fitting 4: \" + string4)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 4) = %.4f\" %(r_sq4))\n",
    "            string_label4 = 'Linear regression: ' + label_4\n",
    "            ax.plot(x_reg4, y_reg4,  linestyle='-', marker='', color='black', label = string_label4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = 'o', color='magenta', label=label_5)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta5 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg5 = stats.linregress(x5, y5)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg5 = x5.sort_values()\n",
    "            x_reg5 = x_reg5.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg5 = (reg5).intercept + (reg5).slope*(x_reg5)\n",
    "            #gerar string da reta\n",
    "            string5 = \"y = %.2f*x + %.2f\" %((reg5).slope, (reg5).intercept)\n",
    "            reta5.append(string5)\n",
    "            #calcular R2\n",
    "            r_sq5 = (reg5).rvalue**2\n",
    "            reta5.append(r_sq5)\n",
    "            print(\"\\nLinear Fitting 5: \" + string5)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 5) = %.4f\" %(r_sq5))\n",
    "            string_label5 = 'Linear regression: ' + label_5\n",
    "            ax.plot(x_reg5, y_reg5,  linestyle='-', marker='', color='magenta', label = string_label5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = 'o', color='yellow', label=label_6)\n",
    "            \n",
    "        if (show_linear_reg == True):\n",
    "            reta6 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg6 = stats.linregress(x6, y6)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg6 = x6.sort_values()\n",
    "            x_reg6 = x_reg6.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg6 = (reg6).intercept + (reg6).slope*(x_reg6)\n",
    "            #gerar string da reta\n",
    "            string6 = \"y = %.2f*x + %.2f\" %((reg6).slope, (reg6).intercept)\n",
    "            reta6.append(string6)\n",
    "            #calcular R2\n",
    "            r_sq6 = (reg6).rvalue**2\n",
    "            reta6.append(r_sq6)\n",
    "            print(\"\\nLinear Fitting 6: \" + string6)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 6) = %.4f\" %(r_sq6))\n",
    "            string_label6 = 'Linear regression: ' + label_6\n",
    "            ax.plot(x_reg6, y_reg6,  linestyle='-', marker='', color='yellow', label = string_label6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #titulo do grafico\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #Titulo do eixo X\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Titulo do eixo Y\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"scatter_plot_lin_reg\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    if (show_linear_reg == True):\n",
    "        \n",
    "        if not (x2 is None):\n",
    "            \n",
    "            if not (x3 is None):\n",
    "                \n",
    "                if not (x4 is None):\n",
    "                    \n",
    "                    if not (x5 is None):\n",
    "                        \n",
    "                        if not (x6 is None):\n",
    "                            #todos estao presentes\n",
    "                            d = {'Statistics': estatisticas,\n",
    "                                 label_1: reta1,\n",
    "                                 label_2: reta2,\n",
    "                                 label_3: reta3,\n",
    "                                 label_4: reta4,\n",
    "                                 label_5: reta5,\n",
    "                                 label_6: reta6}\n",
    "                        \n",
    "                        else:\n",
    "                            #apenas 5 estão presentes:\n",
    "                            d = {'Statistics': estatisticas,\n",
    "                                 label_1: reta1,\n",
    "                                 label_2: reta2,\n",
    "                                 label_3: reta3,\n",
    "                                 label_4: reta4,\n",
    "                                 label_5: reta5}\n",
    "                    \n",
    "                    else:\n",
    "                        #apenas 4 estão presentes:\n",
    "                        d = {'Statistics': estatisticas,\n",
    "                             label_1: reta1,\n",
    "                             label_2: reta2,\n",
    "                             label_3: reta3,\n",
    "                             label_4: reta4}\n",
    "                \n",
    "                else:\n",
    "                    #apenas 3 estão presentes:\n",
    "                    d = {'Statistics': estatisticas,\n",
    "                         label_1: reta1,\n",
    "                         label_2: reta2,\n",
    "                         label_3: reta3}\n",
    "            \n",
    "            else:\n",
    "                #apenas 2 estão presentes:\n",
    "                d = {'Statistics': estatisticas,\n",
    "                     label_1: reta1,\n",
    "                     label_2: reta2}\n",
    "        \n",
    "        else:\n",
    "            #apenas 1 esta presente:\n",
    "            d = {'Statistics': estatisticas,\n",
    "                 label_1: reta1}\n",
    "        \n",
    "        lin_reg_summary = pd.DataFrame(data = d)\n",
    "        \n",
    "        return lin_reg_summary     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for time series visualization**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "azdata_cell_guid": "ef571494-3eb2-4dcc-9ebb-00c1fd6e9aad",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def time_series_vis (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (add_scatter_dots == True):\n",
    "        marker_value = 'o'\n",
    "    else:\n",
    "        marker_value = ''\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (lab1 is None):\n",
    "        \n",
    "        label_1 = lab1\n",
    "    \n",
    "    else:\n",
    "        label_1 = \"Y1\"\n",
    "\n",
    "    if not (x1 is None):\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = marker_value, color='blue', label=label_1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #runs only when both are present\n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = marker_value, color='red', label=label_2)\n",
    "    \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = marker_value, color='green', label=label_3)\n",
    "    \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = marker_value, color='black', label=label_4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = marker_value, color='magenta', label=label_5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = marker_value, color='yellow', label=label_6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #graphic's title\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #X-axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Y-axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"time_series_vis\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions for histogram visualization**\n",
    "- Function `histogram`: ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons.\n",
    "- Function `histogram_alternative`: histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "azdata_cell_guid": "9d2dc092-e26a-4491-a056-644c9412de6a",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def histogram (y, bar_width, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, normal_curve_overlay = True, data_units_label = None, y_title = None, histogram_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ideal bin interval calculated through Montgomery's method. \n",
    "    # Histogram is obtained from this calculated bin size.\n",
    "    # Douglas C. Montgomery (2009). Introduction to Statistical Process Control, \n",
    "    # Sixth Edition, John Wiley & Sons.\n",
    "    \n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "\n",
    "    y = y.reset_index(drop=True)\n",
    "    #faz com que os indices desta serie sejam consecutivos e a partir de zero\n",
    "\n",
    "    #Estatisticas gerais: media (mu) e desvio-padrao (sigma)\n",
    "    mu = y.mean() \n",
    "    sigma = y.std() \n",
    "\n",
    "    #Calculo do bin-size\n",
    "    highest = y.max()\n",
    "    lowest = y.min()\n",
    "    rangehist = highest - lowest\n",
    "    rangehist = abs(rangehist)\n",
    "    #garante que sera um numero positivo\n",
    "    samplesize = y.count() #contagem do total de entradas\n",
    "    ncells = (samplesize)**0.5 #potenciacao: ** - raiz quadrada de samplesize\n",
    "    #resultado da raiz quadrada e sempre positivo\n",
    "    ncells = round(ncells) #numero \"redondo\" mais proximo\n",
    "    ncells = int(ncells) #parte inteira do numero arredondado\n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    binsize = rangehist/ncells\n",
    "    binsize = round(binsize)\n",
    "    binsize = int(binsize) #precisa ser inteiro\n",
    "    \n",
    "    #Construcao da tabela de frequencias\n",
    "\n",
    "    j = 0 #indice da tabela de frequencias\n",
    "    #Este indice e diferente do ordenamento dos valores em ordem crescente\n",
    "    xhist = []\n",
    "    #Lista vazia que contera os x do histograma\n",
    "    yhist = []\n",
    "    #Listas vazia que conteras o y do histograma\n",
    "    hist_labels = []\n",
    "    #Esta lista gravara os limites da barra na forma de strings\n",
    "\n",
    "    pontomediodabarra = lowest + binsize/2 \n",
    "    limitedabarra = lowest + binsize\n",
    "    #ponto medio da barra \n",
    "    #limite da primeira barra do histograma\n",
    "    seriedohist1 = y\n",
    "    seriedohist1 = seriedohist1.sort_values(ascending=True)\n",
    "    #serie com os valores em ordem crescente\n",
    "    seriedohist1 = seriedohist1.reset_index(drop=True)\n",
    "    #garante que a nova serie tenha indices consecutivos, iniciando em zero\n",
    "    i = 0 #linha inicial da serie do histograma em ordem crescente\n",
    "    valcomparado = seriedohist1[i]\n",
    "    #primeiro valor da serie, o mais baixo\n",
    "\n",
    "    while (j <= (ncells-1)):\n",
    "        \n",
    "        #para quando termina o numero de linhas da tabela\n",
    "        xhist.append(pontomediodabarra)\n",
    "        #tempo da tabela de frequencias\n",
    "        cont = 0\n",
    "        #variavel de contagem do histograma\n",
    "        #contagem deve ser reiniciada\n",
    "       \n",
    "        if (i < samplesize):\n",
    "            #2 condicionais para impedir que um termo de indice inexistente\n",
    "            #seja acessado\n",
    "            while (valcomparado <= limitedabarra) and (valcomparado < highest):\n",
    "                #o segundo criterio garante a parada em casos em que os dados sao\n",
    "                #muito proximos\n",
    "                    cont = cont + 1 #adiciona contagem a tabela de frequencias\n",
    "                    i = i + 1\n",
    "                    \n",
    "                    if (i < samplesize): \n",
    "                        valcomparado = seriedohist1[i]\n",
    "        \n",
    "        yhist.append(cont) #valor de ocorrencias contadas\n",
    "        \n",
    "        limite_infdabarra = pontomediodabarra - binsize/2\n",
    "        rotulo = \"%.2f - %.2f\" %(limite_infdabarra, limitedabarra)\n",
    "        #intervalo da tabela de frequencias\n",
    "        #%.2f: 2 casas decimais de aproximação\n",
    "        hist_labels.append(rotulo)\n",
    "        \n",
    "        pontomediodabarra = pontomediodabarra + binsize\n",
    "        #tanto os pontos medios quanto os limites se deslocam do mesmo intervalo\n",
    "        \n",
    "        limitedabarra = limitedabarra + binsize\n",
    "        #proxima barra\n",
    "        \n",
    "        j = j + 1\n",
    "    \n",
    "    #Temos que verificar se o valor maximo foi incluido\n",
    "    #isso porque o processo de aproximacao por numero inteiro pode ter\n",
    "    #arredondado para baixo e excluido o limite superior\n",
    "    #Porem, note que na ultima iteracao o limite superior da barra foi \n",
    "    #somado de binsize, mas como j ja e maior que ncells-1, o loop parou\n",
    "    \n",
    "    #assim, o limitedabarra nesse momento e o limite da barra que seria\n",
    "    #construida em seguida, nao da ultima barra da tabela de frequencias\n",
    "    #isso pode fazer com que esta barra ja seja maior que o highest\n",
    "    \n",
    "    #note porem que nao aumentamos o valor do limite inferior da barra\n",
    "    #por isso, basta vermos se ele mais o binsize sao menores que o valor mais alto\n",
    "        \n",
    "    while ((limite_infdabarra+binsize) < highest):\n",
    "        \n",
    "        #vamos criar novas linhas ate que o ponto mais alto do histograma\n",
    "        #tenha sido contado\n",
    "        ncells = ncells + 1 #adiciona uma linha a tabela de frequencias\n",
    "        xhist.append(pontomediodabarra)\n",
    "        \n",
    "        cont = 0 #variavel de contagem do histograma\n",
    "        \n",
    "        while (valcomparado <= limitedabarra):\n",
    "                cont = cont + 1 #adiciona contagem a tabela de frequencias\n",
    "                i = i + 1\n",
    "                if (i < samplesize):\n",
    "                    valcomparado = seriedohist1[i]\n",
    "                    #apenas se i ainda nao e maior que o total de dados\n",
    "                \n",
    "                else: \n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        #parar o loop se i atingiu um tamanho maior que a quantidade \n",
    "        #de dados.Temos que ter este cuidado porque estamos acrescentando\n",
    "        #mais linhas a tabela de frequencias para corrigir a aproximacao\n",
    "        #de ncells por um numero inteiro\n",
    "        \n",
    "        yhist.append(cont) #valor de ocorrencias contadas\n",
    "        \n",
    "        limite_infdabarra = pontomediodabarra - binsize/2\n",
    "        rotulo = \"%.2f - %.2f\" %(limite_infdabarra, limitedabarra)\n",
    "        #intervalo da tabela de frequencias - 2 casas decimais\n",
    "        hist_labels.append(rotulo)\n",
    "        \n",
    "        pontomediodabarra = pontomediodabarra + binsize\n",
    "        #tanto os pontos medios quanto os limites se deslocam do mesmo intervalo\n",
    "        \n",
    "        limitedabarra = limitedabarra + binsize\n",
    "        #proxima barra\n",
    "        \n",
    "    estatisticas_col1 = []\n",
    "    #contera as descricoes das colunas da tabela de estatisticas gerais\n",
    "    estatisticas_col2 = []\n",
    "    #contera os valores da tabela de estatisticas gerais\n",
    "    \n",
    "    estatisticas_col1.append(\"Count of data evaluated\")\n",
    "    estatisticas_col2.append(samplesize)\n",
    "    estatisticas_col1.append(\"Average (mu)\")\n",
    "    estatisticas_col2.append(mu)\n",
    "    estatisticas_col1.append(\"Standard deviation (sigma)\")\n",
    "    estatisticas_col2.append(sigma)\n",
    "    estatisticas_col1.append(\"Highest value\")\n",
    "    estatisticas_col2.append(highest)\n",
    "    estatisticas_col1.append(\"Lowest value\")\n",
    "    estatisticas_col2.append(lowest)\n",
    "    estatisticas_col1.append(\"Data range (maximum value - lowest value)\")\n",
    "    estatisticas_col2.append(rangehist)\n",
    "    estatisticas_col1.append(\"Bin size (bar width)\")\n",
    "    estatisticas_col2.append(binsize)\n",
    "    estatisticas_col1.append(\"Total rows in frequency table\")\n",
    "    estatisticas_col2.append(ncells)\n",
    "    #como o comando append grava linha a linha em sequencia, garantimos\n",
    "    #a correspondencia das colunas\n",
    "    #Assim como em qualquer string, incluindo de rotulos de graficos\n",
    "    #os \\n sao lidos como quebra de linha\n",
    "    \n",
    "    d1 = {\"General Statistics\": estatisticas_col1, \"Calculated Value\": estatisticas_col2}\n",
    "    #dicionario das duas series, para criar o dataframe com as descricoes\n",
    "    estatisticas_gerais = pd.DataFrame(data = d1)\n",
    "    \n",
    "    #Casos os títulos estejam presentes (valor nao e None):\n",
    "    #vamos utiliza-los\n",
    "    #Caso contrario, vamos criar nomenclaturas genericas para o histograma\n",
    "    \n",
    "    eixo_y = \"Counting/Frequency\"\n",
    "    \n",
    "    if not (data_units_label is None):\n",
    "        xlabel = data_units_label\n",
    "    \n",
    "    else:\n",
    "        xlabel = \"Frequency\\n table data\"\n",
    "    \n",
    "    if not (y_title is None):\n",
    "        eixo_x = y_title\n",
    "        #lembre-se que no histograma, os dados originais vao pro eixo X\n",
    "        #O eixo Y vira o eixo da contagem/frequencia daqueles dados\n",
    "    \n",
    "    else:\n",
    "        eixo_x = \"X: Mean value of the interval\"\n",
    "    \n",
    "    if not (histogram_title is None):\n",
    "        string1 = \"- $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        main_label = histogram_title + string1\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        main_label = \"Data Histogram - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        #os simbolos $\\ $ substituem o simbolo pela letra grega\n",
    "    \n",
    "    d2 = {\"Considered interval\": hist_labels, eixo_x: xhist, eixo_y: yhist}\n",
    "    #dicionario que compoe a tabela de frequencias\n",
    "    tab_frequencias = pd.DataFrame(data = d2)\n",
    "    #cria a tabela de frequencias como um dataframe de saida\n",
    "   \n",
    "    #parametros da normal ja calculados:\n",
    "    #mu e sigma\n",
    "    #numero de bins: ncells\n",
    "    #limites de especificacao: lsl,usl - target\n",
    "    \n",
    "    #valor maximo do histograma\n",
    "    max_hist = max(yhist)\n",
    "    #seleciona o valor maximo da serie, para ajustar a curva normal\n",
    "    #isso porque a normal é criada com valores entre 0 e 1\n",
    "    #multiplicando ela por max_hist, fazemos ela se adequar a altura do histograma\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        #construir a normal ajustada/esperada\n",
    "        #vamos criar pontos ao redor da media mu - 4sigma ate mu + 4sigma, \n",
    "        #de modo a garantir a quase totalidade da curva normal. \n",
    "        #O incremento será de 0.10 sigma a cada iteracao\n",
    "        x_inf = mu -(4)*sigma\n",
    "        x_sup = mu + 4*sigma\n",
    "        x_inc = (0.10)*sigma\n",
    "        \n",
    "        x_normal_adj = []\n",
    "        y_normal_adj = []\n",
    "        \n",
    "        x_adj = x_inf\n",
    "        y_adj = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (x_adj - mu))**2))\n",
    "        x_normal_adj.append(x_adj)\n",
    "        y_normal_adj.append(y_adj)\n",
    "        \n",
    "        while(x_adj < x_sup): \n",
    "            \n",
    "            x_adj = x_adj + x_inc\n",
    "            y_adj = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (x_adj - mu))**2))\n",
    "            x_normal_adj.append(x_adj)\n",
    "            y_normal_adj.append(y_adj)\n",
    "        \n",
    "        #vamos ajustar a altura da curva ao histograma. Para isso, precisamos\n",
    "        #calcular quantas vezes o ponto mais alto do histograma é maior que o ponto\n",
    "        #mais alto da normal (chamaremos essa relação de fator). A seguir,\n",
    "        #multiplicamos cada elemento da normal por este mesmo fator\n",
    "        max_normal = max(y_normal_adj) \n",
    "        #maximo da normal ajustada, numero entre 0 e 1\n",
    "        \n",
    "        fator = (max_hist)/(max_normal)\n",
    "        size_normal = len(y_normal_adj) #quantidade de dados criados\n",
    "        \n",
    "        i = 0\n",
    "        while (i < size_normal):\n",
    "            y_normal_adj[i] = (y_normal_adj[i])*(fator)\n",
    "            i = i + 1\n",
    "    \n",
    "    #Fazer o grafico\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "    \n",
    "        #adicionar a normal\n",
    "        ax.plot(x_normal_adj, y_normal_adj, color = 'black', label = 'Adjusted/expected\\n normal curve')\n",
    "    \n",
    "    ax.set_xlabel(eixo_x)\n",
    "    ax.set_ylabel(eixo_y)\n",
    "    ax.set_title(main_label)\n",
    "    ax.set_xticks(xhist)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(grid)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"General statistics:\\n\")\n",
    "    print(estatisticas_gerais)\n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Frequency table:\\n\")\n",
    "    print(tab_frequencias)\n",
    "\n",
    "    return estatisticas_gerais, tab_frequencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "azdata_cell_guid": "3fe31404-56d2-4590-87a3-58d4ead49706",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def histogram_alternative (y, total_of_bins, bar_width, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, data_units_label = None, y_title = None, histogram_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "    \n",
    "    #this variable here is to simply guarantee the compatibility of the function,\n",
    "    # with no extensive code modifications. It has no real effect.\n",
    "    normal_curve_overlay = True\n",
    "    \n",
    "\n",
    "    y = y.reset_index(drop=True)\n",
    "    #faz com que os indices desta serie sejam consecutivos e a partir de zero\n",
    "\n",
    "    #Estatisticas gerais: media (mu) e desvio-padrao (sigma)\n",
    "    mu = y.mean() \n",
    "    sigma = y.std() \n",
    "\n",
    "    #Calculo do bin-size\n",
    "    highest = y.max()\n",
    "    lowest = y.min()\n",
    "    rangehist = highest - lowest\n",
    "    rangehist = abs(rangehist)\n",
    "    #garante que sera um numero positivo\n",
    "    samplesize = y.count() #contagem do total de entradas\n",
    "    ncells = (samplesize)**0.5 #potenciacao: ** - raiz quadrada de samplesize\n",
    "    #resultado da raiz quadrada e sempre positivo\n",
    "    ncells = round(ncells) #numero \"redondo\" mais proximo\n",
    "    ncells = int(ncells) #parte inteira do numero arredondado\n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    binsize = rangehist/ncells\n",
    "    binsize = round(binsize)\n",
    "    binsize = int(binsize) #precisa ser inteiro\n",
    "    \n",
    "    #Construcao da tabela de frequencias\n",
    "\n",
    "    j = 0 #indice da tabela de frequencias\n",
    "    #Este indice e diferente do ordenamento dos valores em ordem crescente\n",
    "    xhist = []\n",
    "    #Lista vazia que contera os x do histograma\n",
    "    yhist = []\n",
    "    #Listas vazia que conteras o y do histograma\n",
    "    hist_labels = []\n",
    "    #Esta lista gravara os limites da barra na forma de strings\n",
    "\n",
    "    pontomediodabarra = lowest + binsize/2 \n",
    "    limitedabarra = lowest + binsize\n",
    "    #ponto medio da barra \n",
    "    #limite da primeira barra do histograma\n",
    "    seriedohist1 = y\n",
    "    seriedohist1 = seriedohist1.sort_values(ascending=True)\n",
    "    #serie com os valores em ordem crescente\n",
    "    seriedohist1 = seriedohist1.reset_index(drop=True)\n",
    "    #garante que a nova serie tenha indices consecutivos, iniciando em zero\n",
    "    i = 0 #linha inicial da serie do histograma em ordem crescente\n",
    "    valcomparado = seriedohist1[i]\n",
    "    #primeiro valor da serie, o mais baixo\n",
    "        \n",
    "    estatisticas_col1 = []\n",
    "    #contera as descricoes das colunas da tabela de estatisticas gerais\n",
    "    estatisticas_col2 = []\n",
    "    #contera os valores da tabela de estatisticas gerais\n",
    "    \n",
    "    estatisticas_col1.append(\"Count of data evaluated\")\n",
    "    estatisticas_col2.append(samplesize)\n",
    "    estatisticas_col1.append(\"Average (mu)\")\n",
    "    estatisticas_col2.append(mu)\n",
    "    estatisticas_col1.append(\"Standard deviation (sigma)\")\n",
    "    estatisticas_col2.append(sigma)\n",
    "    estatisticas_col1.append(\"Highest value\")\n",
    "    estatisticas_col2.append(highest)\n",
    "    estatisticas_col1.append(\"Lowest value\")\n",
    "    estatisticas_col2.append(lowest)\n",
    "    estatisticas_col1.append(\"Data range (maximum value - lowest value)\")\n",
    "    estatisticas_col2.append(rangehist)\n",
    "    estatisticas_col1.append(\"Bin size (bar width)\")\n",
    "    estatisticas_col2.append(binsize)\n",
    "    estatisticas_col1.append(\"Total rows in frequency table\")\n",
    "    estatisticas_col2.append(ncells)\n",
    "    #como o comando append grava linha a linha em sequencia, garantimos\n",
    "    #a correspondencia das colunas\n",
    "    #Assim como em qualquer string, incluindo de rotulos de graficos\n",
    "    #os \\n sao lidos como quebra de linha\n",
    "    \n",
    "    d1 = {\"General Statistics\": estatisticas_col1, \"Calculated Value\": estatisticas_col2}\n",
    "    #dicionario das duas series, para criar o dataframe com as descricoes\n",
    "    estatisticas_gerais = pd.DataFrame(data = d1)\n",
    "    \n",
    "    #Casos os títulos estejam presentes (valor nao e None):\n",
    "    #vamos utiliza-los\n",
    "    #Caso contrario, vamos criar nomenclaturas genericas para o histograma\n",
    "    \n",
    "    eixo_y = \"Counting/Frequency\"\n",
    "    \n",
    "    if not (data_units_label is None):\n",
    "        xlabel = data_units_label\n",
    "    \n",
    "    else:\n",
    "        xlabel = \"Frequency\\n table data\"\n",
    "    \n",
    "    if not (y_title is None):\n",
    "        eixo_x = y_title\n",
    "        #lembre-se que no histograma, os dados originais vao pro eixo X\n",
    "        #O eixo Y vira o eixo da contagem/frequencia daqueles dados\n",
    "    \n",
    "    else:\n",
    "        eixo_x = \"X: Mean value of the interval\"\n",
    "    \n",
    "    if not (histogram_title is None):\n",
    "        string1 = \"- $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        main_label = histogram_title + string1\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        main_label = \"Data Histogram - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        #os simbolos $\\ $ substituem o simbolo pela letra grega\n",
    "    \n",
    "    d2 = {\"Considered interval\": hist_labels, eixo_x: xhist, eixo_y: yhist}\n",
    "    #dicionario que compoe a tabela de frequencias\n",
    "    tab_frequencias = pd.DataFrame(data = d2)\n",
    "    #cria a tabela de frequencias como um dataframe de saida\n",
    "    \n",
    "    #parametros da normal ja calculados:\n",
    "    #mu e sigma\n",
    "    #numero de bins: ncells\n",
    "    #limites de especificacao: lsl,usl - target\n",
    "    \n",
    "    #valor maximo do histograma\n",
    "    #max_hist = max(yhist)\n",
    "    #seleciona o valor maximo da serie, para ajustar a curva normal\n",
    "    #isso porque a normal é criada com valores entre 0 e 1\n",
    "    #multiplicando ela por max_hist, fazemos ela se adequar a altura do histograma\n",
    "    \n",
    "    \n",
    "    #Fazer o grafico\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    ax.hist(y, bins = total_of_bins, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    #ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    ax.set_xlabel(eixo_x)\n",
    "    ax.set_ylabel(eixo_y)\n",
    "    ax.set_title(main_label)\n",
    "    #ax.set_xticks(xhist)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(grid)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram_alternative\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"General statistics:\\n\")\n",
    "    print(estatisticas_gerais)\n",
    "    # This function is supposed to be used in cases where the differences between data\n",
    "    # is very small. In such cases, there will be no trust values calculated for the \n",
    "    # frequency table. Therefore, we omit it here, but it can be accessed from the\n",
    "    # returned dataframe.\n",
    "\n",
    "    return estatisticas_gerais, tab_frequencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "CELL_10"
    ]
   },
   "outputs": [],
   "source": [
    "def test_data_normality (y, alpha = 0.10, show_probability_plot = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "    # Check https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    # Check https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    # WARNING: The statistical tests require at least 20 samples\n",
    "    \n",
    "    # Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "    # Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "    # results.\n",
    "    \n",
    "    # y = series of data that will be tested.\n",
    "    # y = dataset['Y']\n",
    "    \n",
    "    #Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "    # variable Y (normal distribution tested). \n",
    "    # Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "    \n",
    "    lista1 = []\n",
    "    #esta lista sera a primeira coluna, com as descrições das demais\n",
    "    lista1.append(\"p-value: probability that data is described by the normal distribution.\")\n",
    "    lista1.append(\"Probability of being described by the normal distribution (\\%).\")\n",
    "    lista1.append(\"alpha\")\n",
    "    lista1.append(\"Criterium: is not described by normal if p < alpha = %.3f.\" %(alpha))\n",
    "    #%.3f apresenta f com 3 casas decimais\n",
    "    #%f se refere a uma variavel float\n",
    "    #informa ao usuario o valor definido para a rejeição\n",
    "    lista1.append(\"Are data described by the normal?\")\n",
    "    #Note que o comando append adiciona os elementos em sequencia, linha a linha\n",
    "    #nao se especifica indice, pois ja esta subentendido que esta na proxima\n",
    "    #linha\n",
    "    \n",
    "    #Scipy.stats’ normality test\n",
    "    # It is based on D’Agostino and Pearson’s test that combines \n",
    "    # skew and kurtosis to produce an omnibus test of normality.\n",
    "    _, scipystats_test_pval = stats.normaltest(y)\n",
    "    # The underscore indicates an output to be ignored, which is s^2 + k^2, \n",
    "    # where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.\n",
    "    # https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    #create list with only the p-val\n",
    "    p_scipy = []\n",
    "    p_scipy.append(scipystats_test_pval) #p-value\n",
    "    p_scipy.append(100*scipystats_test_pval) #p in percent\n",
    "    p_scipy.append(alpha)\n",
    "    \n",
    "    if (scipystats_test_pval < alpha):\n",
    "        p_scipy.append(\"p = %.3f < %.3f\" %(scipystats_test_pval, alpha))\n",
    "        p_scipy.append(\"Data not described by normal.\")\n",
    "    else:\n",
    "        p_scipy.append(\"p = %.3f >= %.3f\" %(scipystats_test_pval, alpha))\n",
    "        p_scipy.append(\"Data described by normal.\")    \n",
    "    \n",
    "    #Lilliefors’ test\n",
    "    lilliefors_test = diagnostic.kstest_normal(y, dist='norm', pvalmethod='table')\n",
    "    #Return: linha 1: ksstat: float\n",
    "    #Kolmogorov-Smirnov test statistic with estimated mean and variance.\n",
    "    #Linha 2: p-value:float\n",
    "    #If the pvalue is lower than some threshold, e.g. 0.10, then we can reject the Null hypothesis that the sample comes from a normal distribution.\n",
    "    \n",
    "    #criar lista apenas com o p-valor\n",
    "    p_lillie = []\n",
    "    p_lillie.append(lilliefors_test[1]) #p-valor\n",
    "    p_lillie.append(100*lilliefors_test[1]) #p em porcentagem\n",
    "    p_lillie.append(alpha)\n",
    "    \n",
    "    if (lilliefors_test[1] < alpha):\n",
    "        p_lillie.append(\"p = %.3f < %.3f\" %(lilliefors_test[1], alpha))\n",
    "        p_lillie.append(\"Data not described by normal.\")\n",
    "    else:\n",
    "        p_lillie.append(\"p = %.3f >= %.3f\" %(lilliefors_test[1], alpha))\n",
    "        p_lillie.append(\"Data described by normal.\")\n",
    "        \n",
    "    \n",
    "    #Anderson-Darling\n",
    "    ad_test = diagnostic.normal_ad(y, axis=0)\n",
    "    #Return: Linha 1: ad2: float\n",
    "    #Anderson Darling test statistic.\n",
    "    #Linha 2: p-val: float\n",
    "    #The p-value for hypothesis that the data comes from a normal distribution with unknown mean and variance.\n",
    "    \n",
    "    #criar lista apenas com o p-valor\n",
    "    p_ad = []\n",
    "    p_ad.append(ad_test[1]) #p-valor\n",
    "    p_ad.append(100*ad_test[1]) #p em porcentagem\n",
    "    p_ad.append(alpha)\n",
    "    \n",
    "    if (ad_test[1] < alpha):\n",
    "        p_ad.append(\"p = %.3f < %.3f\" %(ad_test[1], alpha))\n",
    "        p_ad.append(\"Data not described by normal.\")\n",
    "    else:\n",
    "        p_ad.append(\"p = %.3f >= %.3f\" %(ad_test[1], alpha))\n",
    "        p_ad.append(\"Data described by normal.\")\n",
    "    \n",
    "    #NOTA: o comando %f apresenta a variavel float com todas as casas\n",
    "    #decimais possiveis. Se desejamos um numero certo de casas decimais\n",
    "    #acrescentamos esse numero a frente. Exemplos: %.1f: 1 casa decimal\n",
    "    # %.2f: 2 casas; %.3f: 3 casas decimais, %.4f: 4 casas\n",
    "    \n",
    "    data_normality_dict = {'Parameters and Interpretation': lista1, 'D’Agostino and Pearson normality test': , 'Lilliefors Test': p_lillie, 'Anderson-Darling Test': p_ad}\n",
    "    \n",
    "    #dicionario dos valores obtidos\n",
    "    data_normality_res = pd.DataFrame(data = data_normality_dict)\n",
    "    #dataframe de saída\n",
    "    \n",
    "    print(\"Check data normality results:\\n\")\n",
    "    print(data_normality_res)\n",
    "    print(\"\\n\") #line break\n",
    "    \n",
    "    # Calculate data skewness and kurtosis\n",
    "    \n",
    "    # Skewness\n",
    "    data_skew = stats.skew(y)\n",
    "    # skewness = 0 : normally distributed.\n",
    "    # skewness > 0 : more weight in the left tail of the distribution.\n",
    "    # skewness < 0 : more weight in the right tail of the distribution.\n",
    "    # https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "    \n",
    "    # Kurtosis\n",
    "    data_kurtosis = stats.kurtosis(y, fisher = True)\n",
    "    # scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function \n",
    "    # calculates the kurtosis (Fisher or Pearson) of a data set. It is the the fourth \n",
    "    # central moment divided by the square of the variance. \n",
    "    # It is a measure of the “tailedness” i.e. descriptor of shape of probability \n",
    "    # distribution of a real-valued random variable. \n",
    "    # In simple terms, one can say it is a measure of how heavy tail is compared \n",
    "    # to a normal distribution.\n",
    "    # fisher parameter: fisher : Bool; Fisher’s definition is used (normal 0.0) if True; \n",
    "    # else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "    # https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "    print(\"A normal distribution should present no skewness (distribution distortion); and no kurtosis (long-tail).\")\n",
    "    print(f\"For the data analyzed: skewness = {data_skew}; kurtosis = {data_kurtosis}\")\n",
    "    \n",
    "    if (data_skew < 0):\n",
    "        \n",
    "        print(f\"Skewness {data_skew} < 0: more weight in the left tail of the distribution.\")\n",
    "    \n",
    "    elif (data_skew > 0):\n",
    "        \n",
    "        print(f\"Skewness {data_skew} > 0: more weight in the right tail of the distribution.\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Skewness {data_skew} = 0: no distortion of the distribution.\")\n",
    "    \n",
    "    \n",
    "    print(f\"Data kurtosis = {data_kurtosis}\")\n",
    "    \n",
    "    if (data_kurtosis == 0):\n",
    "        \n",
    "        print(\"Data kurtosis = 0. No long-tail effects detected.\")\n",
    "    \n",
    "    #Calculate the mode of the distribution:\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "    data_mode = stats.mode(y, axis = None)[0]\n",
    "    # returns an array of arrays. The first array is called mode=array and contains the mode.\n",
    "    # Axis: Default is 0. If None, compute over the whole array.\n",
    "    # we set axis = None to compute the general mode.\n",
    "    \n",
    "    #Create general statistics dictionary:\n",
    "    general_statistics_dict = {\n",
    "        \n",
    "        \"Count_of_analyzed_values\": len(y)\n",
    "        \"Data_mean\": np.mean(y),\n",
    "        \"Data_mean_ignoring_missing_values\": np.nanmean(y),\n",
    "        \"Data_variance\": np.var(y),\n",
    "        \"Data_variance_ignoring_missing_values\": np.nanvar(y),\n",
    "        \"Data_standard_deviation\": np.std(y),\n",
    "        \"Data_standard_deviation_ignoring_missing_values\": np.nanstd(y),\n",
    "        \"Data_skewness\": data_skew,\n",
    "        \"Data_kurtosis\": data_kurtosis,\n",
    "        \"Data_mode\": data_mode\n",
    "        \n",
    "    }\n",
    "    \n",
    "    print(\"Skewness and kurtosis successfully returned in the dictionary general_statistics_dict.\\n\")\n",
    "    print(general_statistics_dict)\n",
    "    print(\"/n\")\n",
    "    \n",
    "    if (show_probability_plot == True):\n",
    "        #Obtain the probability plot  \n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.set_title(\"Probability Plot for Normal Distribution\")\n",
    "\n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 70 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)   \n",
    "\n",
    "        res = stats.probplot(y, dist = 'norm', fit = True, plot = ax)\n",
    "        #This function resturns a tuple, so we must store it into res\n",
    "        \n",
    "        #Other distributions to check, see scipy Stats documentation. \n",
    "        # you could test dist=stats.loggamma, where stats was imported from scipy\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "\n",
    "        ax.grid(grid)\n",
    "        ax.legend()\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"probability_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "    \n",
    "    return data_normality_res, general_statistics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for testing and visualizing probability plots for different statistical distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "CELL_10"
    ]
   },
   "outputs": [],
   "source": [
    "def test_stat_distribution (y, statistical_distribution = 'lognormal', show_probability_plot = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "    # Check https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    # Check https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    # WARNING: The statistical tests require at least 20 samples\n",
    "    \n",
    "    # Attention: if you want to test a normal distribution, use the function \n",
    "    # test_data_normality.Function test_data_normality tests normality through 3 methods \n",
    "    # and compare them: D’Agostino and Pearson’s; Lilliefors; and Anderson-Darling tests.\n",
    "    # The calculus of the p-value from the Anderson-Darling statistic is available only \n",
    "    # for some distributions. The function specific for the normality calculates these \n",
    "    # probabilities of following the normal.\n",
    "    # Here, the function is destined to test a variety of distributions, and so only the \n",
    "    # Anderson-Darling test is performed.\n",
    "        \n",
    "    #statistical_distribution: string (inside quotes) containing the tested statistical \n",
    "    # distribution.\n",
    "    # Notice: if data Y follow a 'lognormal', log(Y) follow a normal\n",
    "    # Poisson is a special case from 'gamma' distribution.\n",
    "    ## There are 91 accepted statistical distributions:\n",
    "    # 'alpha', 'anglit', 'arcsine', 'beta', 'beta_prime', 'bradford', 'burr', 'burr12', \n",
    "    # 'cauchy', 'skewed_cauchy', 'chi', 'chi-squared', 'cosine', 'double_gamma', \n",
    "    # 'double_weibull', 'erlang', 'exponential', 'exponentiated_weibull', 'exponential_power',\n",
    "    # 'fatigue_life_birnbaum-saunders', 'fisk_log_logistic', 'folded_cauchy', 'folded_normal',\n",
    "    # 'F', 'gamma', 'generalized_logistic', 'generalized_pareto', 'generalized_exponential', \n",
    "    # 'generalized_extreme_value', 'generalized_gamma', 'generalized_half-logistic', \n",
    "    # 'generalized_hyperbolic', 'generalized_inverse_gaussian', 'generalized_normal', \n",
    "    # 'gilbrat', 'gompertz_truncated_gumbel', 'gumbel', 'gumbel_left-skewed', 'half-cauchy', \n",
    "    # 'half-normal', 'half-logistic', 'hyperbolic_secant', 'gauss_hypergeometric', \n",
    "    # 'inverted_gamma', 'inverse_normal', 'inverted_weibull', 'johnson_SB', 'johnson_SU', \n",
    "    # 'KSone', 'KStwo', 'KStwobign', 'laplace', 'asymmetric_laplace', 'left-skewed_levy', \n",
    "    # 'levy', 'logistic', 'log_laplace', 'log_gamma', 'lognormal', 'log-uniform', 'maxwell', \n",
    "    # 'mielke_Beta-Kappa', 'nakagami', 'noncentral_chi-squared', 'noncentral_F', \n",
    "    # 'noncentral_t', 'normal', 'normal_inverse_gaussian', 'pareto', 'lomax', \n",
    "    # 'power_lognormal', 'power_normal', 'power-function', 'R', 'rayleigh', 'rice', \n",
    "    # 'reciprocal_inverse_gaussian', 'semicircular', 'studentized_range', 'student-t', \n",
    "    # 'trapezoidal', 'triangular', 'truncated_exponential', 'truncated_normal', 'tukey-lambda',\n",
    "    # 'uniform', 'von_mises', 'wald', 'weibull_maximum_extreme_value', \n",
    "    # 'weibull_minimum_extreme_value', 'wrapped_cauchy'\n",
    "    \n",
    "    # y = series of data that will be tested.\n",
    "    # y = dataset['Y']\n",
    "    \n",
    "    #Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "    # variable Y (normal distribution tested). \n",
    "    # Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "    \n",
    "    # Calculate data skewness and kurtosis\n",
    "    \n",
    "    # Skewness\n",
    "    data_skew = stats.skew(y)\n",
    "    # skewness = 0 : normally distributed.\n",
    "    # skewness > 0 : more weight in the left tail of the distribution.\n",
    "    # skewness < 0 : more weight in the right tail of the distribution.\n",
    "    # https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "    \n",
    "    # Kurtosis\n",
    "    data_kurtosis = stats.kurtosis(y, fisher = True)\n",
    "    # scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function \n",
    "    # calculates the kurtosis (Fisher or Pearson) of a data set. It is the the fourth \n",
    "    # central moment divided by the square of the variance. \n",
    "    # It is a measure of the “tailedness” i.e. descriptor of shape of probability \n",
    "    # distribution of a real-valued random variable. \n",
    "    # In simple terms, one can say it is a measure of how heavy tail is compared \n",
    "    # to a normal distribution.\n",
    "    # fisher parameter: fisher : Bool; Fisher’s definition is used (normal 0.0) if True; \n",
    "    # else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "    # https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "    print(\"A normal distribution should present no skewness (distribution distortion); and no kurtosis (long-tail).\")\n",
    "    print(f\"For the data analyzed: skewness = {data_skew}; kurtosis = {data_kurtosis}\")\n",
    "    \n",
    "    if (data_skew < 0):\n",
    "        \n",
    "        print(f\"Skewness {data_skew} < 0: more weight in the left tail of the distribution.\")\n",
    "    \n",
    "    elif (data_skew > 0):\n",
    "        \n",
    "        print(f\"Skewness {data_skew} > 0: more weight in the right tail of the distribution.\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Skewness {data_skew} = 0: no distortion of the distribution.\")\n",
    "    \n",
    "    \n",
    "    print(f\"Data kurtosis = {data_kurtosis}\")\n",
    "    \n",
    "    if (data_kurtosis == 0):\n",
    "        \n",
    "        print(\"Data kurtosis = 0. No long-tail effects detected.\")\n",
    "    \n",
    "    #Calculate the mode of the distribution:\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "    data_mode = stats.mode(y, axis = None)[0]\n",
    "    # returns an array of arrays. The first array is called mode=array and contains the mode.\n",
    "    # Axis: Default is 0. If None, compute over the whole array.\n",
    "    # we set axis = None to compute the general mode.\n",
    "    \n",
    "\n",
    "    # Lets define the statistic distribution dictionary:\n",
    "    # This are the callable Scipy objects which can be tested through Anderson-Darling test:\n",
    "    # They are listed and explained in: \n",
    "    # https://docs.scipy.org/doc/scipy/tutorial/stats/continuous.html\n",
    "    \n",
    "    # This dictionary correlates the input name of the distribution to the correct scipy.stats\n",
    "    # callable object\n",
    "    # There are 91 possible statistical distributions:\n",
    "    \n",
    "    print(\"If a compilation error is shown below, please update your Scipy version. Declare and run the following code into a separate cell:\")\n",
    "    print(\"! pip install scipy --upgrade\\n\")\n",
    "    \n",
    "    callable_statistical_distributions_dict = {\n",
    "        \n",
    "        'alpha': stats.alpha, 'anglit': stats.anglit, 'arcsine': stats.arcsine,\n",
    "        'beta': stats.beta, 'beta_prime': stats.betaprime, 'bradford': stats.bradford,\n",
    "        'burr': stats.burr, 'burr12': stats.burr12, 'cauchy': stats.cauchy,\n",
    "        'skewed_cauchy': stats.skewcauchy, 'chi': stats.chi, 'chi-squared': stats.chi2,\n",
    "        'cosine': stats.cosine, 'double_gamma': stats.dgamma, 'double_weibull': stats.dweibull,\n",
    "        'erlang': stats.erlang, 'exponential': stats.expon, 'exponentiated_weibull': stats.exponweib,\n",
    "        'exponential_power': stats.exponpow, 'fatigue_life_birnbaum-saunders': stats.fatiguelife,\n",
    "        'fisk_log_logistic': stats.fisk, 'folded_cauchy': stats.foldcauchy,\n",
    "        'folded_normal': stats.foldnorm, 'F': stats.f, 'gamma': stats.gamma,\n",
    "        'generalized_logistic': stats.genlogistic, 'generalized_pareto': stats.genpareto,\n",
    "        'generalized_exponential': stats.genexpon, 'generalized_extreme_value': stats.genextreme,\n",
    "        'generalized_gamma': stats.gengamma, 'generalized_half-logistic': stats.genhalflogistic,\n",
    "        'generalized_hyperbolic': stats.genhyperbolic, 'generalized_inverse_gaussian': stats.geninvgauss,\n",
    "        'generalized_normal': stats.gennorm, 'gilbrat': stats.gilbrat,\n",
    "        'gompertz_truncated_gumbel': stats.gompertz, 'gumbel': stats.gumbel_r,\n",
    "        'gumbel_left-skewed': stats.gumbel_l, 'half-cauchy': stats.halfcauchy, 'half-normal': stats.halfnorm,\n",
    "        'half-logistic': stats.halflogistic, 'hyperbolic_secant': stats.hypsecant,\n",
    "        'gauss_hypergeometric': stats.gausshyper, 'inverted_gamma': stats.invgamma,\n",
    "        'inverse_normal': stats.invgauss, 'inverted_weibull': stats.invweibull,\n",
    "        'johnson_SB': stats.johnsonsb, 'johnson_SU': stats.johnsonsu, 'KSone': stats.ksone,\n",
    "        'KStwo': stats.kstwo, 'KStwobign': stats.kstwobign, 'laplace': stats.laplace,\n",
    "        'asymmetric_laplace': stats.laplace_asymmetric, 'left-skewed_levy': stats.levy_l,\n",
    "        'levy': stats.levy, 'logistic': stats.logistic, 'log_laplace': stats.loglaplace,\n",
    "        'log_gamma': stats.loggamma, 'lognormal': stats.lognorm, 'log-uniform': stats.loguniform,\n",
    "        'maxwell': stats.maxwell, 'mielke_Beta-Kappa': stats.mielke, 'nakagami': stats.nakagami,\n",
    "        'noncentral_chi-squared': stats.ncx2, 'noncentral_F': stats.ncf, 'noncentral_t': stats.nct,\n",
    "        'normal': stats.norm, 'normal_inverse_gaussian': stats.norminvgauss, 'pareto': stats.pareto,\n",
    "        'lomax': stats.lomax, 'power_lognormal': stats.powerlognorm, 'power_normal': stats.powernorm,\n",
    "        'power-function': stats.powerlaw, 'R': stats.rdist, 'rayleigh': stats.rayleigh,\n",
    "        'rice': stats.rayleigh, 'reciprocal_inverse_gaussian': stats.recipinvgauss,\n",
    "        'semicircular': stats.semicircular, 'studentized_range': stats.studentized_range,\n",
    "        'student-t': stats.t, 'trapezoidal': stats.trapezoid, 'triangular': stats.triang,\n",
    "        'truncated_exponential': stats.truncexpon, 'truncated_normal': stats.truncnorm,\n",
    "        'tukey-lambda': stats.tukeylambda, 'uniform': stats.uniform, 'von_mises': stats.vonmises,\n",
    "        'wald': stats.wald, 'weibull_maximum_extreme_value': stats.weibull_max,\n",
    "        'weibull_minimum_extreme_value': stats.weibull_min, 'wrapped_cauchy': stats.wrapcauchy\n",
    "                \n",
    "    }\n",
    "    \n",
    "    # Get a list of keys from this dictionary, to compare with the selected string:\n",
    "    list_of_dictionary_keys = callable_statistical_distributions_dict.keys()\n",
    "    \n",
    "    #check if an invalid string was provided using the in method:\n",
    "    # The string must be in the list of dictionary keys\n",
    "    boolean_filter = statistical_distribution in list_of_dictionary_keys\n",
    "    # if it is the list, boolean_filter == True. If it is not, boolean_filter == False\n",
    "    \n",
    "    if (boolean_filter == False):\n",
    "        \n",
    "        print(f\"Please, select a valid statistical distribution to test: {list_of_dictionary_keys}\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Access the object correspondent to the distribution provided. To do so,\n",
    "        # simply access dict['key1'], where 'key1' is a key from a dictionary dict ={\"key1\": 'val1'}\n",
    "        # Access just as accessing a column from a dataframe.\n",
    "        \n",
    "        print(\"ATTENTION: If you want to test a normal distribution, use the function test_data_normality.\")\n",
    "        print(\"Function test_data_normality tests normality through 3 methods and compare them: D’Agostino and Pearson’s; Lilliefors; and Anderson-Darling tests.\")\n",
    "        print(\"The calculus of the p-value from the Anderson-Darling statistic is available only for some distributions. The function specific for the normality calculates these probabilities of following the normal.\")\n",
    "        print(\"Here, the function is destined to test a variety of distributions, and so only the Anderson-Darling test is performed.\\n\")\n",
    "        \n",
    "        DISTRIBUTION = callable_statistical_distributions_dict[statistical_distribution]\n",
    "        \n",
    "        anderson_darling_statistic = diagnostic.anderson_statistic(y, dist = DISTRIBUTION, fit = True, axis=0)\n",
    "        print(f\"Anderson-Darling statistic for the distribution {statistical_distribution} = {anderson_darling_statistic}\")\n",
    "        print(\"The Anderson–Darling test assesses whether a sample comes from a specified distribution. It makes use of the fact that, when given a hypothesized underlying distribution and assuming the data does arise from this distribution, the cumulative distribution function (CDF) of the data can be assumed to follow a uniform distribution. The data can be then tested for uniformity with a distance test (Shapiro 1980).\")\n",
    "        # source: https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\n",
    "        \n",
    "        #Fit the distribution and get its parameters\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html\n",
    "        \n",
    "        distribution_parameters = DISTRIBUTION.fit(y, method = \"MLE\")\n",
    "        # With method=\"MLE\" (default), the fit is computed by minimizing the negative \n",
    "        # log-likelihood function. A large, finite penalty (rather than infinite negative \n",
    "        # log-likelihood) is applied for observations beyond the support of the distribution. \n",
    "        # With method=\"MM\", the fit is computed by minimizing the L2 norm of the relative errors \n",
    "        # between the first k raw (about zero) data moments and the corresponding distribution \n",
    "        # moments, where k is the number of non-fixed parameters. \n",
    "        \n",
    "        # distribution_parameters: Estimates for any shape parameters (if applicable), \n",
    "        # followed by those for location and scale.\n",
    "        print(f\"Distribution shape parameters calculated for {statistical_distribution}: {distribution_parameters}\")\n",
    "        \n",
    "         #Create general statistics dictionary:\n",
    "        general_statistics_dict = {\n",
    "\n",
    "            \"Count_of_analyzed_values\": len(y)\n",
    "            \"Data_mean\": np.mean(y),\n",
    "            \"Data_mean_ignoring_missing_values\": np.nanmean(y),\n",
    "            \"Data_variance\": np.var(y),\n",
    "            \"Data_variance_ignoring_missing_values\": np.nanvar(y),\n",
    "            \"Data_standard_deviation\": np.std(y),\n",
    "            \"Data_standard_deviation_ignoring_missing_values\": np.nanstd(y),\n",
    "            \"Data_skewness\": data_skew,\n",
    "            \"Data_kurtosis\": data_kurtosis,\n",
    "            \"Data_mode\": data_mode,\n",
    "            \"Anderson-Darling_statistic_A\": anderson_darling_statistic,\n",
    "            \"Distribution_parameters\": distribution_parameters\n",
    "\n",
    "        }\n",
    "\n",
    "        print(\"General statistics successfully returned as the dictionary general_statistics_dict.\\n\")\n",
    "        print(general_statistics_dict)\n",
    "        print(\"/n\")\n",
    "        \n",
    "        # The critical values for the Anderson-Darling test are dependent on the \n",
    "        # specific distribution that is being tested. Tabulated values and formulas have been \n",
    "        # published (Stephens, 1974, 1976, 1977, 1979) for a few specific distributions (normal, \n",
    "        # lognormal, exponential, Weibull, logistic, extreme value type 1). The test is a \n",
    "        # one-sided test and the hypothesis that the distribution is of a specific form is \n",
    "        # rejected if the test statistic, A, is greater than the critical value. Note \n",
    "        # that for a given distribution, the Anderson-Darling statistic may be \n",
    "        # multiplied by a constant (which usually depends on the sample size, n). \n",
    "        # These constants are given in the various papers by Stephens. In the sample \n",
    "        # output below, this is the \"adjusted Anderson-Darling\" statistic. This is what \n",
    "        # should be compared against the critical values. Also, be aware that different \n",
    "        # constants (and therefore critical values) have been published. You just \n",
    "        # need to be aware of what constant was used for a given set of critical values \n",
    "        # (the needed constant is typically given with the critical values).\n",
    "        # https://itl.nist.gov/div898/handbook/eda/section3/eda3e.htm\n",
    "\n",
    "        if (show_probability_plot == True):\n",
    "            #Obtain the probability plot  \n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            ax.set_title(f\"Probability Plot for {statistical_distribution} Distribution\")\n",
    "\n",
    "            #ROTATE X AXIS IN XX DEGREES\n",
    "            plt.xticks(rotation = x_axis_rotation)\n",
    "            # XX = 70 DEGREES x_axis (Default)\n",
    "            #ROTATE Y AXIS IN XX DEGREES:\n",
    "            plt.yticks(rotation = y_axis_rotation)\n",
    "            # XX = 0 DEGREES y_axis (Default)   \n",
    "\n",
    "            res = stats.probplot(y, dist = DISTRIBUTION, fit = True, plot = ax)\n",
    "            #This function resturns a tuple, so we must store it into res\n",
    "                   \n",
    "            #Other distributions to check, see scipy Stats documentation. \n",
    "            # you could test dist=stats.loggamma, where stats was imported from scipy\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "\n",
    "            ax.grid(grid)\n",
    "            ax.legend()\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"/\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = \"probability_plot\"\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 110 dpi\n",
    "                    png_resolution_dpi = 110\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            #fig.tight_layout()\n",
    "\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "\n",
    "        return general_statistics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for column filtering (selecting); or column renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_filter_rename (df, cols_list, mode = 'filter'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #mode = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "    #mode = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "    \n",
    "    #cols_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{df.columns}\")\n",
    "    \n",
    "    if (mode == 'filter'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        df = df[cols_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\")\n",
    "        \n",
    "    elif (mode == 'rename'):\n",
    "        \n",
    "        #Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(cols_list) == len(df.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\")\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            df.columns = cols_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'filter\\' or \\'rename\\'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for dropping specific columns or rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns (df, what_to_drop = 'columns', cols_list = None, row_index_list = None, reset_index_after_drop = True):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # check https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html?highlight=drop\n",
    "    \n",
    "    # what_to_drop = 'columns' for removing the columns specified by their names (headers)\n",
    "    # in cols_list (a list of strings).\n",
    "    # what_to_drop = 'rows' for removing the rows specified by their indices in\n",
    "    # row_index_list (a list of integers). Remember that the indexing starts from zero, i.e.,\n",
    "    # the first row is row number zero.\n",
    "    \n",
    "    # cols_list = list of strings containing the names (headers) of the columns to be removed\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # remove columns 'col1', 'col2', and 'col3' from the dataframe.\n",
    "    # If a single column will be dropped, you can declare it as a string (outside a list)\n",
    "    # e.g. cols_list = 'col1'; or cols_list = ['col1']\n",
    "    \n",
    "    # row_index_list = a list of integers containing the indices of the rows that will be dropped.\n",
    "    # e.g. row_index_list = [0, 1, 2] will drop the rows with indices 0 (1st row), 1 (2nd row), and\n",
    "    # 2 (third row). Again, if a single row will be dropped, you can declare it as an integer (outside\n",
    "    # a list).\n",
    "    # e.g. row_index_list = 20 or row_index_list = [20] to drop the row with index 20 (21st row).\n",
    "    \n",
    "    # reset_index_after_drop = True. keep it True to restarting the indexing numeration after dropping.\n",
    "    # Alternatively, set reset_index_after_drop = False to keep the original numeration (the removed indices\n",
    "    # will be missing).\n",
    "    \n",
    "    #Create a local copy of the dataframe to manipulate:\n",
    "    DATASET = df\n",
    "    \n",
    "    if (what_to_drop == 'columns'):\n",
    "        \n",
    "        if (cols_list is None):\n",
    "            #check if a list was not input:\n",
    "            print(\"Input a list of columns cols_list to be dropped.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            #Drop the columns in cols_list:\n",
    "            DATASET.drop(columns = cols_list)\n",
    "            print(f\"The columns in {cols_list} headers list were successfully removed.\\n\")\n",
    "    \n",
    "    elif (what_to_drop == 'rows'):\n",
    "        \n",
    "        if (row_index_list is None):\n",
    "            #check if a list was not input:\n",
    "            print(\"Input a list of rows indices row_index_list to be dropped.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            #Drop the rows in row_index_list:\n",
    "            DATASET.drop(row_index_list)\n",
    "            print(f\"The rows in {row_index_list} indices list were successfully removed.\\n\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Input a valid string as what_to_drop, rows or columns.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    if (reset_index_after_drop == True):\n",
    "        \n",
    "        #restart the indexing\n",
    "        DATASET = DATASET.reset_index(drop = True)\n",
    "        print(\"The indices of the dataset were successfully restarted.\\n\")\n",
    "    \n",
    "    print(\"Check the 10 first rows from the returned dataset:\\n\")\n",
    "    print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for removing duplicate rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_rows (df, list_of_columns_to_analyze = None, which_row_to_keep = 'first', reset_index_after_drop = True):\n",
    "    \n",
    "    import pandas as pd\n",
    "    # check https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\n",
    "    \n",
    "    # if list_of_columns_to_analyze = None, the whole dataset will be analyzed, i.e., rows\n",
    "    # will be removed only if they have same values for all columns from the dataset.\n",
    "    # Alternatively, pass a list of columns names (strings), if you want to remove rows with\n",
    "    # same values for that combination of columns. Pass it as a list, even if there is a single column\n",
    "    # being declared.\n",
    "    # e.g. list_of_columns_to_analyze = ['column1'] will check only 'column1'. Entries with same value\n",
    "    # on 'column1' will be considered duplicates and will be removed.\n",
    "    # list_of_columns_to_analyze = ['col1', 'col2',  'col3'] will analyze the combination of 3 columns:\n",
    "    # 'col1', 'col2', and 'col3'. Only rows with same value for these 3 columns will be considered\n",
    "    # duplicates and will be removed.\n",
    "    \n",
    "    # which_row_to_keep = 'first' will keep the first detected row and remove all other duplicates. If\n",
    "    # None or an invalid string is input, this method will be selected.\n",
    "    # which_row_to_keep = 'last' will keep only the last detected duplicate row, and remove all the others.\n",
    "    \n",
    "    # reset_index_after_drop = True. keep it True to restarting the indexing numeration after dropping.\n",
    "    # Alternatively, set reset_index_after_drop = False to keep the original numeration (the removed indices\n",
    "    # will be missing).\n",
    "    \n",
    "    #Create a local copy of the dataframe to manipulate:\n",
    "    DATASET = df\n",
    "    \n",
    "    if (which_row_to_keep == 'last'):\n",
    "        \n",
    "        #keep only the last duplicate.\n",
    "        if (list_of_columns_to_analyze is None):\n",
    "            # use the whole dataset\n",
    "            DATASET = DATASET.drop_duplicates(keep = 'last')\n",
    "            print(f\"The rows with duplicate entries were successfully removed.\")\n",
    "            print(\"Only the last one of the duplicate entries was kept in the dataset.\\n\")\n",
    "        \n",
    "        else:\n",
    "            #use the subset of columns\n",
    "            if (list_of_columns_to_analyze is None):\n",
    "                #check if a list was not input:\n",
    "                print(\"Input a list of columns list_of_columns_to_analyze to be analyzed.\")\n",
    "                return \"error\"\n",
    "        \n",
    "            else:\n",
    "                #Drop the columns in cols_list:\n",
    "                DATASET = DATASET.drop_duplicates(subset = list_of_columns_to_analyze, keep = 'last')\n",
    "                print(f\"The rows with duplicate values for the columns in {list_of_columns_to_analyze} headers list were successfully removed.\")\n",
    "                print(\"Only the last one of the duplicate entries was kept in the dataset.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #keep only the first duplicate.\n",
    "        if (list_of_columns_to_analyze is None):\n",
    "            # use the whole dataset\n",
    "            DATASET = DATASET.drop_duplicates()\n",
    "            print(f\"The rows with duplicate entries were successfully removed.\")\n",
    "            print(\"Only the first one of the duplicate entries was kept in the dataset.\\n\")\n",
    "        \n",
    "        else:\n",
    "            #use the subset of columns\n",
    "            if (list_of_columns_to_analyze is None):\n",
    "                #check if a list was not input:\n",
    "                print(\"Input a list of columns list_of_columns_to_analyze to be analyzed.\")\n",
    "                return \"error\"\n",
    "        \n",
    "            else:\n",
    "                #Drop the columns in cols_list:\n",
    "                DATASET = DATASET.drop_duplicates(subset = list_of_columns_to_analyze)\n",
    "                print(f\"The rows with duplicate values for the columns in {list_of_columns_to_analyze} headers list were successfully removed.\")\n",
    "                print(\"Only the first one of the duplicate entries was kept in the dataset.\\n\")\n",
    "    \n",
    "    if (reset_index_after_drop == True):\n",
    "        \n",
    "        #restart the indexing\n",
    "        DATASET = DATASET.reset_index(drop = True)\n",
    "        print(\"The indices of the dataset were successfully restarted.\\n\")\n",
    "    \n",
    "    print(\"Check the 10 first rows from the returned dataset:\\n\")\n",
    "    print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe (dataframe_to_be_exported, new_file_name, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # NEW_FILE_NAME - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. NEW_FILE_NAME = \"my_file\" will export a file\n",
    "    # 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_with_csv_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values = df_gen_charac (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characterizing the categorical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: do not return a new column with encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: Use this function to a analyze a single column from a dataframe.\n",
    "# The pandas .unique method and the numpy.unique function can handle a single series\n",
    "# (column) or 1D arrays.\n",
    "    \n",
    "#WARNING: The first return is the unique values summary/encoding table; \n",
    "# the second one will be the dataframe with the encoded column. This second \n",
    "# dataframe is returned only when encode_var = True\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = \"categorical_column_name\"\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name of the column \n",
    "#to be analyzed. e.g. CATEGORICAL_VAR_NAME = \"column1\"\n",
    "# Alternatively: substitute the string by the name of the column where the categorical\n",
    "# variable is. Keep it inside quotes. e.g. if the categorical values are in the column\n",
    "# named 'col1', set CATEGORICAL_VAR_NAME = 'col1'\n",
    "\n",
    "ENCODE_VAR = False\n",
    "# ENCODE_VAR = False - keep it False not to associate an integer value to each \n",
    "# possible category. Alternatively, set ENCODE_VAR = True to associate an integer\n",
    "# starting from zero to each categorical value. This should not be used in case there\n",
    "# is no order to be associated. Also, the encoding will be performed in accordance to \n",
    "# the order of occurence in the dataframe df. In case there is no order, the One-Hot\n",
    "# Encoding should be used.\n",
    "# WARNING: if ENCODE_VAR = True, the function will return a new dataframe containing\n",
    "# a column correspondent to the column input as 'CATEGORICAL_VAR_NAME' but with the\n",
    "# correspondent integer values.\n",
    "\n",
    "NEW_ENCODED_COL_NAME = None\n",
    "# NEW_ENCODED_COL_NAME = None - This parameter shows effects only when ENCODED_VAR = True.\n",
    "# It refers to the column that will be created on the original dataframe containing the\n",
    "# integer values associated to the categorical values.\n",
    "# Input a string inside quotes to define a name (header) for the column of the summary\n",
    "# dataframe to store the integer correspondent to each categorical value.\n",
    "# e.g. NEW_ENCODED_COL_NAME = \"integer_vals\". If no name is provided, the standard name\n",
    "# formed by the string concatenation categorical_var_name + \"_encoded\" will be automatically\n",
    "# assigned.\n",
    "\n",
    "#New dataframe saved as unique_vals_summary. Simply modify this object on the left of equality:\n",
    "unique_vals_summary = charac_cat_var (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, encode_var = ENCODE_VAR, new_encoded_col_name = NEW_ENCODED_COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: return a new column with encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: Use this function to a analyze a single column from a dataframe.\n",
    "# The pandas .unique method and the numpy.unique function can handle a single series\n",
    "# (column) or 1D arrays.\n",
    "    \n",
    "#WARNING: The first return is the unique values summary/encoding table; \n",
    "# the second one will be the dataframe with the encoded column. This second \n",
    "# dataframe is returned only when encode_var = True\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = \"categorical_column_name\"\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name of the column \n",
    "#to be analyzed. e.g. CATEGORICAL_VAR_NAME = \"column1\"\n",
    "# Alternatively: substitute the string by the name of the column where the categorical\n",
    "# variable is. Keep it inside quotes. e.g. if the categorical values are in the column\n",
    "# named 'col1', set CATEGORICAL_VAR_NAME = 'col1'\n",
    "\n",
    "ENCODE_VAR = True\n",
    "# ENCODE_VAR = False - keep it False not to associate an integer value to each \n",
    "# possible category. Alternatively, set ENCODE_VAR = True to associate an integer\n",
    "# starting from zero to each categorical value. This should not be used in case there\n",
    "# is no order to be associated. Also, the encoding will be performed in accordance to \n",
    "# the order of occurence in the dataframe df. In case there is no order, the One-Hot\n",
    "# Encoding should be used.\n",
    "# WARNING: if ENCODE_VAR = True, the function will return a new dataframe containing\n",
    "# a column correspondent to the column input as 'CATEGORICAL_VAR_NAME' but with the\n",
    "# correspondent integer values.\n",
    "\n",
    "NEW_ENCODED_COL_NAME = None\n",
    "# NEW_ENCODED_COL_NAME = None - This parameter shows effects only when ENCODED_VAR = True.\n",
    "# It refers to the column that will be created on the original dataframe containing the\n",
    "# integer values associated to the categorical values.\n",
    "# Input a string inside quotes to define a name (header) for the column of the summary\n",
    "# dataframe to store the integer correspondent to each categorical value.\n",
    "# e.g. NEW_ENCODED_COL_NAME = \"integer_vals\". If no name is provided, the standard name\n",
    "# formed by the string concatenation categorical_var_name + \"_encoded\" will be automatically\n",
    "# assigned.\n",
    "\n",
    "# New dataframes saved as unique_vals_summary and encoded_df.\n",
    "# Simply modify these objects on the left of equality:\n",
    "unique_vals_summary, encoded_df = charac_cat_var (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, encode_var = ENCODE_VAR, new_encoded_col_name = NEW_ENCODED_COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting a bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a Pareto chart, keep aggregate_function = 'sum', plot_cumulative_percent = True, and orientation = 'vertical'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating cumulative statistics**\n",
    "- Cumulative sum (cumsum); cumulative product (cumprod); cumulative maximum (cummax); cumulative minimum (cummin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CUMULATIVE_STATISTIC = 'sum'\n",
    "# CUMULATIVE_STATISTIC: the statistic that will be calculated. The cumulative\n",
    "# statistics allowed are: 'sum' (for cumulative sum, cumsum); 'product' \n",
    "# (for cumulative product, cumprod); 'max' (for cumulative maximum, cummax);\n",
    "# and 'min' (for cumulative minimum, cummin).\n",
    "\n",
    "NEW_CUM_STATS_COL_NAME = None\n",
    "# NEW_CUM_STATS_COL_NAME = None or string (inside quotes), \n",
    "# containing the name of the new column created for storing the cumulative statistic\n",
    "# calculated. \n",
    "# e.g. NEW_CUM_STATS_COL_NAME = \"cum_stats\" will create a column named as 'cum_stats'.\n",
    "# If its None, the new column will be named as column_to_analyze + \"_\" + [selected\n",
    "# cumulative function] ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "\n",
    "# New dataframe saved as new_df\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df = calculate_cumulative_stats (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, cumulative_statistic = CUMULATIVE_STATISTIC, new_cum_stats_col_name = NEW_CUM_STATS_COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET_COLUMNS_LIST = None\n",
    "# SUBSET_COLUMNS_LIST = list of columns to look for missing values. Only missing values\n",
    "# in these columns will be considered for deciding which columns to remove.\n",
    "# Declare it as a list of strings inside quotes containing the columns' names to look at,\n",
    "# even if this list contains a single element. e.g. subset_columns_list = ['column1']\n",
    "# will check only 'column1'; whereas subset_columns_list = ['col1', 'col2', 'col3'] will\n",
    "# chek the columns named as 'col1', 'col2', and 'col3'.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "    \n",
    "DROP_MISSING_VAL = True\n",
    "# DROP_MISSING_VAL = True to eliminate the rows containing missing values.\n",
    "# Alternatively: DROP_MISSING_VAL = False to use the filling method.\n",
    "\n",
    "FILL_MISSING_VAL = False\n",
    "# FILL_MISSING_VAL = False. Set this to True to activate the mode for filling the missing\n",
    "# values.\n",
    "\n",
    "ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = False\n",
    "# ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = False - This parameter shows effect only when\n",
    "# DROP_MISSING_VAL = True. If you set ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = True, then\n",
    "# only the rows where all the columns are missing will be eliminated.\n",
    "# If you define a subset, then only the rows where all the subset columns are missing\n",
    "# will be eliminated.\n",
    "\n",
    "MINIMUM_NUMBER_OF_MISSING_VALS = None\n",
    "# MINIMUM_NUMBER_OF_MISSING_VALS = None - This parameter shows effect only when\n",
    "# DROP_MISSING_VAL = True. If you set MINIMUM_NUMBER_OF_MISSING_VALS equals to an integer \n",
    "# value, then only the rows where at least this integer number columns are missing will be \n",
    "# eliminated. e.g. if MINIMUM_NUMBER_OF_MISSING_VALS = 2, only rows containing missing \n",
    "# values for at least 2 columns will be eliminated.\n",
    "# If you define a subset, then only the rows where all the subset columns are missing\n",
    "# will be eliminated.\n",
    "\n",
    "VALUE_TO_FILL = None\n",
    "# VALUE_TO_FILL = None - This parameter shows effect only when\n",
    "# FILL_MISSING_VAL = True. Set this parameter as a float value to fill all missing\n",
    "# values with this value. e.g. VALUE_TO_FILL = 0 will fill all missing values with\n",
    "# the number 0. You can also pass a function call like \n",
    "# VALUE_TO_FILL = np.sum(dataset['col1']). In this case, the missing values will be\n",
    "# filled with the sum of the series dataset['col1']\n",
    "# Alternatively, you can also input a string to fill the missing values. e.g.\n",
    "# VALUE_TO_FILL = 'text' will fill all the missing values with the string \"text\".\n",
    "\n",
    "FILL_METHOD = \"fill_with_zeros\"\n",
    "# FILL_METHOD = \"fill_with_zeros\". - This parameter shows effect only \n",
    "# when FILL_MISSING_VAL = True.\n",
    "# Alternatively: FILL_METHOD = \"fill_with_zeros\" - fill all the missing values with 0\n",
    "# FILL_METHOD = \"fill_with_value_to_fill\" - fill the missing values with the value\n",
    "# defined as the parameter VALUE_TO_FILL\n",
    "# FILL_METHOD = \"fill_with_avg\" - fill the missing values with the average value for \n",
    "# each column.\n",
    "# FILL_METHOD = \"fill_by_interpolating\" - fill by interpolating the previous and the \n",
    "# following value. A linear interpolation will be used.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\n",
    "# FILL_METHOD = \"ffill\" - Forward fill: fills missing with previous value.\n",
    "    \n",
    "#WARNING: if the fillna method is selected (FILL_MISSING_VAL == True), but no filling\n",
    "# methodology is selected, the missing values of the dataset will be filled with 0.\n",
    "# The same applies when a non-valid fill methodology is selected.\n",
    "# Pandas fillna method does not allow us to fill only a selected subset.\n",
    "    \n",
    "#WARNING: if FILL_MISSING_VAL == \"fill_with_value_to_fill\" but VALUE_TO_FILL is None, the \n",
    "# missing values will be filled with the value 0.    \n",
    "\n",
    "#New dataframe saved as cleaned_df\n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = handle_missing_values (df = DATASET, subset_columns_list = SUBSET_COLUMNS_LIST, drop_missing_val = DROP_MISSING_VAL, fill_missing_val = FILL_MISSING_VAL, eliminate_only_completely_empty_rows = ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS, minimum_number_of_mis_vals = MINIMUM_NUMBER_OF_MISSING_VALS, value_to_fill = VALUE_TO_FILL, fill_method = FILL_METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtaining correlation plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SHOW_MASKED_PLOT = True\n",
    "#SHOW_MASKED_PLOT = True - keep as True if you want to see a cleaned version of the plot\n",
    "# where a mask is applied. Alternatively, SHOW_MASKED_PLOT = True, or \n",
    "# SHOW_MASKED_PLOT = False\n",
    "\n",
    "RESPONSES_TO_RETURN_CORR = None\n",
    "#RESPONSES_TO_RETURN_CORR - keep as None to return the full correlation tensor.\n",
    "# If you want to display the correlations for a particular group of features, input them\n",
    "# as a list, even if this list contains a single element. Examples:\n",
    "# responses_to_return_corr = ['response1'] for a single response\n",
    "# responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "# responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "# of a column of the dataset that represents a response variable.\n",
    "# WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "# of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "# Alternatively: a list containing strings (inside quotes) with the names of the response\n",
    "# columns that you want to see the correlations. Declare as a list even if it contains a\n",
    "# single element.\n",
    "\n",
    "SET_RETURNED_LIMIT = None\n",
    "# SET_RETURNED_LIMIT = None - This variable will only present effects in case you have\n",
    "# provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "# to return all of the correlation coefficients; or, alternatively, \n",
    "# provide an integer number to limit the total of coefficients returned. \n",
    "# e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframe saved as correlation_matrix. Simply modify this object on the left of equality:\n",
    "correlation_matrix = correlation_plot (df = DATASET, show_masked_plot = SHOW_MASKED_PLOT, responses_to_return_corr = RESPONSES_TO_RETURN_CORR, set_returned_limit = SET_RETURNED_LIMIT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtaining scatter plots and simple linear regressions**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "X1 = DATASET['X1']\n",
    "#Alternatively: None; or other column in quotes, substituting 'X1'\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "SHOW_LINEAR_REG = True\n",
    "#Alternatively: set SHOW_LINEAR_REG = True to plot the linear regressions graphics and show \n",
    "# the linear regressions calculated for each pair Y x X (i.e., each correlation \n",
    "# Y = aX + b, as well as the R² coefficient calculated). \n",
    "# Set SHOW_LINEAR_REG = False to omit both the linear regressions plots on the graphic, and\n",
    "# the correlations and R² coefficients obtained.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframe saved as lin_reg_summary. Simply modify this object on the left of equality:\n",
    "lin_reg_summary = scatter_plot_lin_reg (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_linear_reg = SHOW_LINEAR_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing time series**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#X1 = dataset.index to use the index as the axis itself\n",
    "X1 = (DATASET['DATE']).astype('datetime64[D]') \n",
    "#Alternatively: None; or other column in quotes, substituting 'DATE'\n",
    "# WARNING: Modify only the object in the first parenthesis: DATASET['DATE']\n",
    "# Do not modify the method .astype('datetime64[D]')\n",
    "#Remove .astype('datetime64[D]') if it is not a datetime.\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "#Notice that there is a data transforming step to guarantee that the 'DATE' was interpreted as a timestamp, not as object or string.\n",
    "#The astype method defines the type of variable as 'datetime64[D]'. If we wanted the timestamps to be resolved in seconds, we should use\n",
    "# 'datetime64[ns]'.\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "ADD_SCATTER_DOTS = False #Alternatively: True or False\n",
    "# If ADD_SCATTER_DOTS = False, the dots (scatter plot) are omitted, so only the lines\n",
    "# correspondent to the series are shown.\n",
    "\n",
    "# Notice that adding the dots and omitting the spline lines is equivalent to obtain a\n",
    "# scatter plot. If you want to do so, consider using the scatter_plot_lin_reg function, \n",
    "# capable of calculating the linear regressions.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "time_series_vis (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: automatically calculate the ideal histogram bin size\n",
    "- The ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "744224c5-7105-4256-a568-6c5443e28cae",
    "tags": [
     "CELL_9"
    ]
   },
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ANALYZED_VARIABLE = DATASET['analyzed_variable']\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# ANALYZED_VARIABLE = DATASET['column1']\n",
    "\n",
    "SET_GRAPHIC_BAR_WIDTH = 2.0\n",
    "# This parameter must be visually adjusted for each particular analyzed variable.\n",
    "# Manually set this parameter until you see only a minimal separation between successive\n",
    "# bars (i.e., you know that the bars are not overlapping, but they are not so distant that\n",
    "# the statistic distribution profile is not clear).\n",
    "# You can input any numeric value, and the order of magnitude will vary depending on the\n",
    "# dispersion and on the width of the sample space.\n",
    "# e.g. SET_GRAPHIC_BAR_WIDTH = 3; SET_GRAPHIC_BAR_WIDTH = 0.003\n",
    "\n",
    "X_AXIS_ROTATION = 70 \n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "DATA_UNITS_LABEL = None\n",
    "# Input a string inside quotes for setting a label for the X-axis, that will represent\n",
    "# The type of data that the histogram is evaluating, i.e., what the statistic distribution\n",
    "# shown by the histogram means.\n",
    "# e.g. if DATA_UNITS_LABEL = \"particle_diameter_in_nm\", the axis X will be labelled with\n",
    "# this string. Then, we can know that the diagram represents the distribution (counts of\n",
    "# data for each defined bin) of particles diameters.\n",
    "\n",
    "Y_TITLE = None\n",
    "#Alternatively: string inside quotes for vertical title. e.g. Y_TITLE = \"Analyzed_values\".\n",
    "\n",
    "HISTOGRAM_TITLE = None\n",
    "#Alternatively: string inside quotes for graphic title. e.g. \n",
    "# HISTOGRAM_TITLE = \"Analyzed_values_histogram\".\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframes saved as general_statistics and frequency_tab.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics, frequency_tab = histogram (y = ANALYZED_VARIABLE, bar_width = SET_GRAPHIC_BAR_WIDTH, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, normal_curve_overlay = NORMAL_CURVE_OVERLAY, data_units_label = DATA_UNITS_LABEL, y_title = Y_TITLE, histogram_title = HISTOGRAM_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: set number of bins\n",
    "- Use this one if the distance between data is too small, or if the histogram function did not return a valid histogram.\n",
    "- Here, the histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ANALYZED_VARIABLE = DATASET['analyzed_variable']\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# ANALYZED_VARIABLE = DATASET['column1']\n",
    "\n",
    "TOTAL_OF_BINS = 50\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided. Check comments after the histogram_alternative\n",
    "# function call).\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "\n",
    "SET_GRAPHIC_BAR_WIDTH = 2.0\n",
    "# This parameter must be visually adjusted for each particular analyzed variable.\n",
    "# Manually set this parameter until you see only a minimal separation between successive\n",
    "# bars (i.e., you know that the bars are not overlapping, but they are not so distant that\n",
    "# the statistic distribution profile is not clear).\n",
    "# You can input any numeric value, and the order of magnitude will vary depending on the\n",
    "# dispersion and on the width of the sample space.\n",
    "# e.g. SET_GRAPHIC_BAR_WIDTH = 3; SET_GRAPHIC_BAR_WIDTH = 0.003\n",
    "\n",
    "X_AXIS_ROTATION = 70 \n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "DATA_UNITS_LABEL = None\n",
    "# Input a string inside quotes for setting a label for the X-axis, that will represent\n",
    "# The type of data that the histogram is evaluating, i.e., what the statistic distribution\n",
    "# shown by the histogram means.\n",
    "# e.g. if DATA_UNITS_LABEL = \"particle_diameter_in_nm\", the axis X will be labelled with\n",
    "# this string. Then, we can know that the diagram represents the distribution (counts of\n",
    "# data for each defined bin) of particles diameters.\n",
    "\n",
    "Y_TITLE = None\n",
    "#Alternatively: string inside quotes for vertical title. e.g. Y_TITLE = \"Analyzed_values\".\n",
    "\n",
    "HISTOGRAM_TITLE = None\n",
    "#Alternatively: string inside quotes for graphic title. e.g. \n",
    "# HISTOGRAM_TITLE = \"Analyzed_values_histogram\".\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram_alternative.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframes saved as general_statistics and frequency_tab.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics, frequency_tab = histogram_alternative (y = ANALYZED_VARIABLE, total_of_bins = TOTAL_OF_BINS, bar_width = SET_GRAPHIC_BAR_WIDTH, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, data_units_label = DATA_UNITS_LABEL, y_title = Y_TITLE, histogram_title = HISTOGRAM_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "Y = DATASET['Y'] \n",
    "#Alternatively: other column in quotes, substituting 'Y'\n",
    "# e.g. Y = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "ALPHA = 0.10\n",
    "# Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "# Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "# results.\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframe saved as data_normality_res\n",
    "# Skewness kurtosis and general statistics dictionary returned as general_statistics_dict\n",
    "# Simply modify these objects on the left of equality:\n",
    "data_normality_res, general_statistics_dict = test_data_normality (y = Y, alpha = ALPHA, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing and visualizing probability plots for different statistical distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "# Attention: if you want to test a normal distribution, use the function \n",
    "# test_data_normality.Function test_data_normality tests normality through 3 methods \n",
    "# and compare them: D’Agostino and Pearson’s; Lilliefors; and Anderson-Darling tests.\n",
    "# The calculus of the p-value from the Anderson-Darling statistic is available only \n",
    "# for some distributions. The function specific for the normality calculates these \n",
    "# probabilities of following the normal.\n",
    "# Here, the function is destined to test a variety of distributions, and so only the \n",
    "# Anderson-Darling test is performed.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "Y = DATASET['Y'] \n",
    "#Alternatively: other column in quotes, substituting 'Y'\n",
    "# e.g. Y = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "STATISTICAL_DISTRIBUTION = 'lognormal'\n",
    "#STATISTICAL_DISTRIBUTION: string (inside quotes) containing the tested statistical \n",
    "# distribution. \n",
    "## Notice: if data Y follow a 'lognormal', log(Y) follow a normal\n",
    "## Poisson is a special case from 'gamma' distribution.\n",
    "## There are 91 accepted statistical distributions:\n",
    "# 'alpha', 'anglit', 'arcsine', 'beta', 'beta_prime', 'bradford', 'burr', 'burr12', 'cauchy',\n",
    "# 'skewed_cauchy', 'chi', 'chi-squared', 'cosine', 'double_gamma', 'double_weibull', \n",
    "# 'erlang', 'exponential', 'exponentiated_weibull', 'exponential_power',\n",
    "# 'fatigue_life_birnbaum-saunders', 'fisk_log_logistic', 'folded_cauchy', 'folded_normal',\n",
    "# 'F', 'gamma', 'generalized_logistic', 'generalized_pareto', 'generalized_exponential', \n",
    "# 'generalized_extreme_value', 'generalized_gamma', 'generalized_half-logistic', \n",
    "# 'generalized_hyperbolic', 'generalized_inverse_gaussian', 'generalized_normal', \n",
    "# 'gilbrat', 'gompertz_truncated_gumbel', 'gumbel', 'gumbel_left-skewed', 'half-cauchy', \n",
    "# 'half-normal', 'half-logistic', 'hyperbolic_secant', 'gauss_hypergeometric', \n",
    "# 'inverted_gamma', 'inverse_normal', 'inverted_weibull', 'johnson_SB', 'johnson_SU', \n",
    "# 'KSone', 'KStwo', 'KStwobign', 'laplace', 'asymmetric_laplace', 'left-skewed_levy', \n",
    "# 'levy', 'logistic', 'log_laplace', 'log_gamma', 'lognormal', 'log-uniform', 'maxwell', \n",
    "# 'mielke_Beta-Kappa', 'nakagami', 'noncentral_chi-squared', 'noncentral_F', \n",
    "# 'noncentral_t', 'normal', 'normal_inverse_gaussian', 'pareto', 'lomax', 'power_lognormal',\n",
    "# 'power_normal', 'power-function', 'R', 'rayleigh', 'rice', 'reciprocal_inverse_gaussian', \n",
    "# 'semicircular', 'studentized_range', 'student-t', 'trapezoidal', 'triangular', \n",
    "# 'truncated_exponential', 'truncated_normal', 'tukey-lambda', 'uniform', 'von_mises', \n",
    "# 'wald', 'weibull_maximum_extreme_value', 'weibull_minimum_extreme_value', 'wrapped_cauchy'\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# General statistics dictionary returned as general_statistics_dict\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics_dict = test_stat_distribution (y = Y, statistical_distribution = STATISTICAL_DISTRIBUTION, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering (selecting); or renaming columns of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'filter'\n",
    "# MODE = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "# MODE = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "\n",
    "COLS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = col_filter_rename (df = DATASET, cols_list = COLS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropping specific columns or rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "WHAT_TO_DROP = 'columns'\n",
    "# WHAT_TO_DROP = 'columns' for removing the columns specified by their names (headers)\n",
    "# in COLS_LIST (a list of strings).\n",
    "# WHAT_TO_DROP = 'rows' for removing the rows specified by their indices in\n",
    "# ROW_INDEX_LIST (a list of integers). Remember that the indexing starts from zero, i.e.,\n",
    "# the first row is row number zero.\n",
    "\n",
    "COLS_LIST = None\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to be removed\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# remove columns 'col1', 'col2', and 'col3' from the dataframe.\n",
    "# If a single column will be dropped, you can declare it as a string (outside a list)\n",
    "# e.g. COLS_LIST = 'col1'; or COLS_LIST = ['col1']\n",
    "\n",
    "ROW_INDEX_LIST = None\n",
    "# ROW_INDEX_LIST = a list of integers containing the indices of the rows that will be dropped.\n",
    "# e.g. ROW_INDEX_LIST = [0, 1, 2] will drop the rows with indices 0 (1st row), 1 (2nd row), and\n",
    "# 2 (third row). Again, if a single row will be dropped, you can declare it as an integer (outside\n",
    "# a list).\n",
    "# e.g. ROW_INDEX_LIST = 20 or ROW_INDEX_LIST = [20] to drop the row with index 20 (21st row).\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = drop_columns (df = DATASET, what_to_drop = WHAT_TO_DROP, cols_list = COLS_LIST, row_index_list = ROW_INDEX_LIST, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing duplicate rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_ANALYZE = None\n",
    "# if LIST_OF_COLUMNS_TO_ANALYZE = None, the whole dataset will be analyzed, i.e., rows\n",
    "# will be removed only if they have same values for all columns from the dataset.\n",
    "# Alternatively, pass a list of columns names (strings), if you want to remove rows with\n",
    "# same values for that combination of columns. Pass it as a list, even if there is a single column\n",
    "# being declared.\n",
    "# e.g. LIST_OF_COLUMNS_TO_ANALYZE = ['column1'] will check only 'column1'. Entries with same value\n",
    "# on 'column1' will be considered duplicates and will be removed.\n",
    "# LIST_OF_COLUMNS_TO_ANALYZE = ['col1', 'col2',  'col3'] will analyze the combination of 3 columns:\n",
    "# 'col1', 'col2', and 'col3'. Only rows with same value for these 3 columns will be considered\n",
    "# duplicates and will be removed.\n",
    "\n",
    "WHICH_ROW_TO_KEEP = 'first'\n",
    "# WHICH_ROW_TO_KEEP = 'first' will keep the first detected row and remove all other duplicates. If\n",
    "# None or an invalid string is input, this method will be selected.\n",
    "# WHICH_ROW_TO_KEEP = 'last' will keep only the last detected duplicate row, and remove all the others.\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = remove_duplicate_rows (df = DATASET, list_of_columns_to_analyze = LIST_OF_COLUMNS_TO_ANALYZE, which_row_to_keep = WHICH_ROW_TO_KEEP, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME = \"dataset\"\n",
    "# NEW_FILE_NAME - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_dataframe (dataframe_to_be_exported = DATAFRAME_TO_BE_EXPORTED, new_file_name = NEW_FILE_NAME, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Statiscal distribution skewness and kurtosis - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy stats.skew()\n",
    "- scipy.stats.skew(array, axis=0, bias=True) function calculates the skewness of the data set.\n",
    "    - skewness = 0 : normally distributed.\n",
    "    - skewness > 0 : more weight in the left tail of the distribution.\n",
    "    - skewness < 0 : more weight in the right tail of the distribution. \n",
    "\n",
    "Its basic formula:\n",
    "\n",
    "`skewness = (3 *(Mean - Median))/(Standard deviation)`\n",
    "\n",
    "Parameters :\n",
    "- array : Input array or object having the elements.\n",
    "- axis : Axis along which the skewness value is to be measured. By default axis = 0.\n",
    "- bias : Bool; calculations are corrected for statistical bias, if set to False.\n",
    "- Returns : Skewness value of the data set, along the axis.\n",
    "\n",
    "https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "\n",
    "Scipy uses a more general and complex formula, shown in its documentation:\n",
    "https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.skew.html\n",
    "\n",
    "### scipy stats.kurtosis()\n",
    "- scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function calculates the kurtosis (Fisher or Pearson) of a data set. \n",
    "- It is the the fourth central moment divided by the square of the variance. \n",
    "- It is a measure of the “tailedness” i.e. descriptor of shape of probability distribution of a real-valued random variable. \n",
    "- In simple terms, one can say it is a measure of how heavy tail is compared to a normal distribution.\n",
    "\n",
    "Its formula:\n",
    "\n",
    "`Kurtosis(X) = E[((X - mu)/sigma)^4]`\n",
    "\n",
    "Parameters :\n",
    "- array : Input array or object having the elements.\n",
    "- axis : Axis along which the kurtosis value is to be measured. By default axis = 0.\n",
    "- fisher : Bool; Fisher’s definition is used (normal 0.0) if True; else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "- bias : Bool; calculations are corrected for statistical bias, if set to False.\n",
    "- Returns : Kurtosis value of the normal distribution for the data set.\n",
    "\n",
    "https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
