{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "00750668-aee5-432d-a9c8-c3cb76951d6a"
   },
   "source": [
    "# **Dataset Characterization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "942bceb6-e596-462c-9ebc-39e8f01e1215"
   },
   "source": [
    "## _ETL Workflow Notebook 2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "83992759-51b4-4a5f-816c-ff79d50dccbf"
   },
   "source": [
    "## Content:\n",
    "1. Dataframe general characterization; \n",
    "2. Characterizing categorical variables;\n",
    "3. Removing all columns and rows that contain only missing values;\n",
    "4. Visualizing and characterizing distribution of missing values;\n",
    "5. Visualizing missingness across a variable, and comparing it to another variable (both numeric);\n",
    "6. Dealing with missing values; \n",
    "7. Obtaining the correlation plot;\n",
    "8. Plotting bar charts;\n",
    "9. Calculating cumulative statistics; \n",
    "10. Obtaining scatter plots and simple linear regressions;\n",
    "11. Performing the polynomial fitting; \n",
    "12. Visualizing time series; \n",
    "13. Visualizing histograms; \n",
    "14. Testing normality and visualizing the probability plot;\n",
    "15. Testing and visualizing probability plots for different statistical distributions;\n",
    "16. Filtering (selecting); ordering; or renaming columns from the dataframe;\n",
    "17. Renaming specific columns from the dataframe; or cleaning columns' labels;\n",
    "18. Dropping specific columns or rows from the dataframe; \n",
    "19. Removing duplicate rows from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# to update pip, unmark and run:\n",
    "# ! pip install pip --upgrade\n",
    "# to show if a library is installed and visualize its information, unmark and run\n",
    "# (e.g. tensorflow):\n",
    "# ! pip show tensorflow\n",
    "# To run a Python file (e.g idsw_etl.py) saved in the notebook's workspace directory,\n",
    "# unmark and run:\n",
    "# import idsw_etl\n",
    "# or:\n",
    "# import idsw_etl as etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "3f058d0c-6337-48e5-b0a0-f2ac1ab83d5a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "167f9ba8-6daf-472a-a25e-193932bed0ba"
   },
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "bc0913f8-4446-42db-acbb-e97f40200718",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1d4d5fc1-6b8d-4dca-a527-462530f8ad06"
   },
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", load_all_sheets_at_once = False, sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv. Also,\n",
    "    # html files and webpages may be also read.\n",
    "    \n",
    "    # You may input the path for an HTML file containing a table to be read; or \n",
    "    # a string containing the address for a webpage containing the table. The address must start\n",
    "    # with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
    "    # or as FILE_NAME_WITH_EXTENSION.\n",
    "    \n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading Excel files:\n",
    "    \n",
    "    # load_all_sheets_at_once = False - This parameter has effect only when for Excel files.\n",
    "    # If load_all_sheets_at_once = True, the function will return a list of dictionaries, each\n",
    "    # dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "    # value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "    # and its value will be the pandas dataframe object obtained from that sheet.\n",
    "    # This argument has preference over sheet_to_load. If it is True, all sheets will be loaded.\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    if (file_directory_path is None):\n",
    "        file_directory_path = ''\n",
    "    if (file_name_with_extension is None):\n",
    "        file_name_with_extension = ''\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    \n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    if(file_extension not in ['xls', 'xlsx', 'xlsm', 'xlsb', 'odf',\n",
    "                              'ods', 'odt', 'json', 'txt', 'csv', 'html']):\n",
    "        \n",
    "        # Check if it is a webpage by evaluating the 3 to 5 initial characters:\n",
    "        # Notice that 'https' contains 'http'\n",
    "        if ((file_path[:3] == 'www') | (file_path[:4] == '/www') | (file_path[:4] == 'http')| (file_path[:5] == '/http'):\n",
    "            file_extension = 'html'\n",
    "\n",
    "            # If the address starts with a slash (1st character), remove it:\n",
    "            if (file_path[0] == '/'):\n",
    "                # Pick all characters from index 1:\n",
    "                file_path = file_path[1:]\n",
    "    \n",
    "        \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\\n\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "            \n",
    "    elif (file_extension == 'html'):    \n",
    "        \n",
    "        if (has_header == True):\n",
    "            \n",
    "            dataset = pd.read_html(file_path, na_values = how_missing_values_are_registered, parse_dates = True, decimal = decimal_separator)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dataset = pd.read_html(file_path, header = None, na_values = how_missing_values_are_registered, parse_dates = True, decimal = decimal_separator)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\\n\")\n",
    "        # For Excel type files, Pandas automatically detects the decimal separator and requires only the parameter parse_dates.\n",
    "        # Firstly, the argument infer_datetime_format was present on read_excel function, but was removed.\n",
    "        # From version 1.4 (beta, in 10 May 2022), it will be possible to pass the parameter 'decimal' to\n",
    "        # read_excel function for detecting decimal cases in strings. For numeric variables, it is not needed, though\n",
    "        \n",
    "        if (load_all_sheets_at_once == True):\n",
    "            \n",
    "            # Corresponds to setting sheet_name = None\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "            \n",
    "            # xlsx_doc is a dictionary containing the sheet names as keys, and dataframes as items.\n",
    "            # Let's convert it to the desired format.\n",
    "            # Dictionary dict, dict.keys() is the array of keys; dict.values() is an array of the values;\n",
    "            # and dict.items() is an array of tuples with format ('key', value)\n",
    "            \n",
    "            # Create a list of returned datasets:\n",
    "            list_of_datasets = []\n",
    "            \n",
    "            # Let's iterate through the array of tuples. The first element returned is the key, and the\n",
    "            # second is the value\n",
    "            for sheet_name, dataframe in (xlsx_doc.items()):\n",
    "                # sheet_name = key; dataframe = value\n",
    "                # Define the dictionary with the standard format:\n",
    "                df_dict = {'sheet': sheet_name,\n",
    "                            'df': dataframe}\n",
    "                \n",
    "                # Add the dictionary to the list:\n",
    "                list_of_datasets.append(df_dict)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(f\"A total of {len(list_of_datasets)} dataframes were retrieved from the Excel file.\\n\")\n",
    "            print(f\"The dataframes correspond to the following Excel sheets: {list(xlsx_doc.keys())}\\n\")\n",
    "            print(\"Returning a list of dictionaries. Each dictionary contains the key \\'sheet\\', with the original sheet name; and the key \\'df\\', with the Pandas dataframe object obtained.\\n\")\n",
    "            print(f\"Check the 10 first rows of the dataframe obtained from the first sheet, named {list_of_datasets[0]['sheet']}:\\n\")\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            except: # regular mode\n",
    "                print((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            return list_of_datasets\n",
    "            \n",
    "        elif (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "133dafe4-5c47-47b9-a41a-e318822cb93f"
   },
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1064756d-282d-428b-bd9b-44f07d753cff",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def json_obj_to_pandas_dataframe (json_obj_to_convert, json_obj_type = 'list', json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "    # dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "    # example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "    # structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "    # file containing JSON, you could read the txt and save its content as a string.\n",
    "    # json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "    # 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "    # 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]    \n",
    "\n",
    "    # json_obj_type = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "    # json_obj_type = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "\n",
    "    \n",
    "    if (json_obj_type == 'string'):\n",
    "        # Use the json.loads method to convert the string to json\n",
    "        json_file = json.loads(json_obj_to_convert)\n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "        # like a dataframe.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    \n",
    "    elif (json_obj_type == 'list'):\n",
    "        \n",
    "        # make the json_file the object itself:\n",
    "        json_file = json_obj_to_convert\n",
    "    \n",
    "    else:\n",
    "        print (\"Enter a valid JSON object type: \\'list\\', in case the JSON object is a list of dictionaries in JSON format; or \\'string\\', if the JSON is stored as a text (string variable).\")\n",
    "        return \"error\"\n",
    "    \n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5972a95e-e5fb-4fcd-b37c-08939dcad145"
   },
   "source": [
    "# **Function for applying a list of row filters to a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e43aaba4-2846-4c0a-a290-cf81952691f5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def APPLY_ROW_FILTERS_LIST (df, list_of_row_filters):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Warning: this function filter the rows and results into a smaller dataset, since it removes the non-selected entries.\")\n",
    "    print(\"If you want to pass a filter to simply label the selected rows, use the function LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\")\n",
    "    \n",
    "    # This function applies filters to the dataframe and remove the non-selected entries.\n",
    "    \n",
    "    # df: dataframe to be analyzed.\n",
    "    \n",
    "    ## define the filters and only them define the filters list\n",
    "    # EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "    # boolean_filter1 = ((None) & (None)) \n",
    "    # (condition1 AND (&) condition2)\n",
    "    # boolean_filter2 = ((None) | (None)) \n",
    "    # condition1 OR (|) condition2\n",
    "    \n",
    "    # boolean filters result into boolean values True or False.\n",
    "\n",
    "    ## Examples of filters:\n",
    "    ## filter1 = (condition 1) & (condition 2)\n",
    "    ## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "    ## filter2 = (condition)\n",
    "    ## filter2 = (df['column3'] <= 2.5)\n",
    "    ## filter3 = (df['column4'] > 10.7)\n",
    "    ## filter3 = (condition 1) | (condition 2)\n",
    "    ## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "\n",
    "    ## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "    ## <= (lower or equal); == (equal); != (different)\n",
    "\n",
    "    ## concatenation operators: & (and): the filter is True only if the \n",
    "    ## two conditions concatenated through & are True\n",
    "    ## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "    ## through | are True.\n",
    "    ## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "\n",
    "    ## separate conditions with parentheses. Use parentheses to define a order\n",
    "    ## of definition of the conditions:\n",
    "    ## filter = ((condition1) & (condition2)) | (condition3)\n",
    "    ## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "    ## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "    ## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "    ## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "    ## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "    ## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "    # The negative of this condition may be acessed with ~ operator:\n",
    "    ##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "    ## Also, you may use isna() method as filter for missing values:\n",
    "    ## filter = (dataframe_column_series).isna()\n",
    "    ## or, for not missing: ~(dataframe_column_series).isna()\n",
    "    \n",
    "    # list_of_row_filters: list of boolean filters to be applied to the dataframe\n",
    "    # e.g. list_of_row_filters = [filter1]\n",
    "    # applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "    # boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "    # That is because the function will loop through the list of filters.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3, filter4]\n",
    "    # will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "    # Notice that the filters must be declared in the order you want to apply them.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Get the original index and convert it to a list\n",
    "    original_index = list(DATASET.index)\n",
    "    \n",
    "    # Loop through the filters list, applying the filters sequentially:\n",
    "    # Each element of the list is identified as 'boolean_filter'\n",
    "    \n",
    "    if (len(list_of_row_filters) > 0):\n",
    "        \n",
    "        # Start a list of indices that were removed. That is because we must\n",
    "        # remove these elements from the boolean filter series before filtering, avoiding\n",
    "        # the index mismatch.\n",
    "        removed_indices = []\n",
    "        \n",
    "        # Now, loop through other rows in the list_of_row_filters:\n",
    "        for boolean_series in list_of_row_filters:\n",
    "            \n",
    "            if (len(removed_indices) > 0):\n",
    "                # Drop rows in list removed_indices. Set inplace = True to remove by simply applying\n",
    "                # the method:\n",
    "                # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "                boolean_series.drop(labels = removed_indices, axis = 0, inplace = True)\n",
    "            \n",
    "            # Apply the filter:\n",
    "            DATASET = DATASET[boolean_series]\n",
    "            \n",
    "            # Finally, let's update the list of removed indices:\n",
    "            for index in original_index:\n",
    "                if ((index not in list(DATASET.index)) & (index not in removed_indices)):\n",
    "                    removed_indices.append(index)\n",
    "    \n",
    "    \n",
    "    # Reset index:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully filtered the dataframe. Check the 10 first rows of the filtered and returned dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6fac6bb3-893c-4294-8501-e5989cde3ff4"
   },
   "source": [
    "# **Function for dataframe general characterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5b505f72-a88c-41db-87f7-1a953595d957",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def df_general_characterization (df):\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    # Set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    # Show dataframe's header\n",
    "    print(\"Dataframe\\'s 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "\n",
    "    # Show dataframe's tail:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    print(\"Dataframe\\'s 10 last rows:\\n\")\n",
    "    try:\n",
    "        display(DATASET.tail(10))\n",
    "    except:\n",
    "        print(DATASET.tail(10))\n",
    "    \n",
    "    # Show dataframe's shape:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_shape  = DATASET.shape\n",
    "    print(\"Dataframe\\'s shape = (number of rows, number of columns) =\\n\")\n",
    "    try:\n",
    "        display(df_shape)\n",
    "    except:\n",
    "        print(df_shape)\n",
    "    \n",
    "    # Show dataframe's columns:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_columns_array = DATASET.columns\n",
    "    print(\"Dataframe\\'s columns =\\n\")\n",
    "    try:\n",
    "        display(df_columns_array)\n",
    "    except:\n",
    "        print(df_columns_array)\n",
    "    \n",
    "    # Show dataframe's columns types:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_dtypes = DATASET.dtypes\n",
    "    # Now, the df_dtypes seroes has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.rename.html#pandas.Index.rename\n",
    "    # To access the Index object, we call the index attribute from Pandas dataframe.\n",
    "    # By setting inplace = True, we modify the object inplace, by simply calling the method:\n",
    "    df_dtypes.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    # Let's also modify the series label or name:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rename.html\n",
    "    df_dtypes.rename('dtype_series', inplace = True)\n",
    "    print(\"Dataframe\\'s variables types:\\n\")\n",
    "    try:\n",
    "        display(df_dtypes)\n",
    "    except:\n",
    "        print(df_dtypes)\n",
    "    \n",
    "    # Show dataframe's general statistics for numerical variables:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_general_statistics = DATASET.describe()\n",
    "    print(\"Dataframe\\'s general (summary) statistics for numeric variables:\\n\")\n",
    "    try:\n",
    "        display(df_general_statistics)\n",
    "    except:\n",
    "        print(df_general_statistics)\n",
    "    \n",
    "    # Show total of missing values for each variable:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    total_of_missing_values_series = DATASET.isna().sum()\n",
    "    # This is a series which uses the original column names as index\n",
    "    proportion_of_missing_values_series = DATASET.isna().mean()\n",
    "    percent_of_missing_values_series = proportion_of_missing_values_series * 100\n",
    "    missingness_dict = {'count_of_missing_values': total_of_missing_values_series,\n",
    "                       'proportion_of_missing_values': proportion_of_missing_values_series,\n",
    "                       'percent_of_missing_values': percent_of_missing_values_series}\n",
    "    \n",
    "    df_missing_values = pd.DataFrame(data = missingness_dict)\n",
    "    # Now, the dataframe has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    df_missing_values.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    \n",
    "    # Create a one row dataframe with the missingness for the whole dataframe:\n",
    "    # Pass the scalars as single-element lists or arrays:\n",
    "    one_row_data = {'dataframe_column': ['missingness_accross_rows'],\n",
    "                    'count_of_missing_values': [len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any'))],\n",
    "                    'proportion_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))],\n",
    "                    'percent_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))*100]\n",
    "                    }\n",
    "    one_row_df = pd.DataFrame(data = one_row_data)\n",
    "    one_row_df.set_index('dataframe_column', inplace = True)\n",
    "    \n",
    "    # Append this one_row_df to df_missing_values:\n",
    "    df_missing_values = pd.concat([df_missing_values, one_row_df])\n",
    "    \n",
    "    print(\"Missing values on each feature; and missingness considering all rows from the dataframe:\")\n",
    "    print(\"(note: \\'missingness_accross_rows\\' was calculated by: checking which rows have at least one missing value (NA); and then comparing total rows with NAs with total rows in the dataframe).\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(df_missing_values)\n",
    "    except:\n",
    "        print(df_missing_values)\n",
    "    \n",
    "    return df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "92dcccd1-86f9-47fd-a0df-0ff1e4235515"
   },
   "source": [
    "# **Function for characterizing categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_categorical_variables (df, timestamp_tag_column = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # df: dataframe that will be analyzed\n",
    "    \n",
    "    # timestamp_tag_colum: name (header) of the column containing the\n",
    "    # timestamps. Keep timestamp_tag_column = None if the dataframe do not contain\n",
    "    # timestamps.\n",
    "       \n",
    "            # Encoding syntax:\n",
    "            # dataset.loc[dataset[\"CatVar\"] == 'Value1', \"EncodedColumn\"] = 1\n",
    "            # dataset.loc[boolean_filter, EncodedColumn\"] = value,\n",
    "            # boolean_filter = (dataset[\"CatVar\"] == 'Value1') will be True when the \n",
    "            # equality is verified. The .loc method filters the dataframe, accesses the\n",
    "            # column declared after the comma and inputs the value defined (e.g. value = 1)\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Get the list of columns:\n",
    "    cols_list = list(DATASET.columns)\n",
    "    \n",
    "    # Start a list of categorical columns:\n",
    "    categorical_list = []\n",
    "    is_categorical = 0 # start the variable\n",
    "    \n",
    "    # Start a timestamp list that will be empty if there is no timestamp_tag_column\n",
    "    timestamp_list = []\n",
    "    if (timestamp_tag_column is not None):\n",
    "        timestamp_list.append(timestamp_tag_column)\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    # Loop through all valid columns (cols_list)\n",
    "    for column in cols_list:\n",
    "        \n",
    "        # Check if the column is neither in timestamp_list nor in\n",
    "        # categorical_list yet:\n",
    "        \n",
    "        if ((column not in categorical_list) & (column not in timestamp_list)):\n",
    "            # Notice that, since we already selected the 'timestamp_obj', we remove the original timestamps.\n",
    "            column_data_type = DATASET[column].dtype\n",
    "            \n",
    "            if (column_data_type not in numeric_dtypes):\n",
    "                \n",
    "                # Append to categorical columns list:\n",
    "                categorical_list.append(column)\n",
    "    \n",
    "    # Subset the dataframe:\n",
    "    if (len(categorical_list) >= 1):\n",
    "        \n",
    "        DATASET = DATASET[categorical_list]\n",
    "        is_categorical = 1\n",
    "    \n",
    "    # Start a list to store the results:\n",
    "    summary_list = []\n",
    "    # It will be a list of dictionaries.\n",
    "    \n",
    "    # Loop through all variables on the list:\n",
    "    for categorical_var in categorical_list:\n",
    "        \n",
    "        # Get unique vals and respective counts.\n",
    "\n",
    "        # Start dictionary that will be appended as a new element from the list:\n",
    "        # The main dictionary will be an element of the list\n",
    "        unique_dict = {'categorical_variable': categorical_var}\n",
    "        \n",
    "        # Start a list of unique values:\n",
    "        unique_vals = []\n",
    "\n",
    "        # Now, check the unique values of the categorical variable:\n",
    "        unique_vals_array = DATASET[categorical_var].unique()\n",
    "        # unique_vals_array is a NumPy array containing the different values from the categorical variable.\n",
    "\n",
    "        # Total rows:\n",
    "        total_rows = len(DATASET)\n",
    "\n",
    "        # Check the total of missing values\n",
    "        # Set a boolean_filter for checking if the row contains a missing value\n",
    "        boolean_filter = DATASET[categorical_var].isna()\n",
    "\n",
    "        # Calculate the total of elements when applying the filter:\n",
    "        total_na = len(DATASET[boolean_filter])\n",
    "\n",
    "        # Create a dictionary for the missing values:\n",
    "        na_dict = {\n",
    "                    'value': np.nan, \n",
    "                    'counts_of_occurences': total_na,\n",
    "                    'percent_of_occurences': ((total_na/total_rows)*100)\n",
    "                    }\n",
    "        \n",
    "        \n",
    "        # Nest this dictionary as an element from the list unique_vals.\n",
    "        unique_vals.append(na_dict)\n",
    "        # notice that the dictionary was nested into a list, which will be itself\n",
    "        # nested as an element of the dictionary unique_dict\n",
    "        \n",
    "        # Now loop through each possible element on unique_vals_array\n",
    "        for unique_val in unique_vals_array:\n",
    "\n",
    "            # loop through each possible value of the array. The values are called 'unique_val'\n",
    "            # Check if the value is not none:\n",
    "            \n",
    "            # Depending on the type of variable, the following error may be raised:\n",
    "            # func 'isnan' not supported for the input types, and the inputs could not be safely coerced \n",
    "            # to any supported types according to the casting rule ''safe''\n",
    "            # To avoid it, we can set the variable as a string using the str attribute and check if\n",
    "            # the value is not neither 'nan' nor 'NaN'. That is because pandas will automatically convert\n",
    "            # identified null values to np.nan\n",
    "            \n",
    "            # So, since The unique method creates the strings 'nan' or 'NaN' for the missing values,\n",
    "            # if we read unique_val as string using the str attribute, we can filter out the\n",
    "            # values 'nan' or 'NaN', which may be present together with the None and the float\n",
    "            # np.nan:\n",
    "            if ((str(unique_val) != 'nan') & (str(unique_val) != 'NaN') & (unique_val is not None)):\n",
    "                # If one of these conditions is true, the value is None, 'NaN' or 'nan'\n",
    "                # so this condition does not run.\n",
    "                # It runs if at least one value is not a missing value\n",
    "                # (only when the value is neither None nor np.nan)\n",
    "\n",
    "                # create a filter to select only the entries where the column == unique_val:\n",
    "                boolean_filter = (DATASET[categorical_var] == unique_val)\n",
    "                # Calculate the total of elements when applying the filter:\n",
    "                total_elements = len(DATASET[boolean_filter])\n",
    "\n",
    "                # Create a dictionary for these values:\n",
    "                # Use the same keys as before:\n",
    "                cat_var_dict = {\n",
    "                    \n",
    "                                'value': unique_val, \n",
    "                                'counts_of_occurences': total_elements,\n",
    "                                'percent_of_occurences': ((total_elements/total_rows)*100)\n",
    "                    \n",
    "                                }\n",
    "                \n",
    "                # Nest this dictionary as an element from the list unique_vals.\n",
    "                unique_vals.append(cat_var_dict)\n",
    "                # notice that the dictionary was nested into a list, which will be itself\n",
    "                # nested as an element of the dictionary unique_dict\n",
    "        \n",
    "        # Nest the unique_vals list as an element of the dictionary unique_dict:\n",
    "        # Set 'unique_values' as the key, and unique_vals as value\n",
    "        unique_dict['unique_values'] = unique_vals\n",
    "        # Notice that unique_vals is a list where each element is a dictionary with information\n",
    "        # from a given unique value of the variable 'categorical_var' being analyzed.\n",
    "        \n",
    "        # Finally, append 'unique_dict' as an element of the list summary_list:\n",
    "        summary_list.append(unique_dict)\n",
    "        \n",
    "    \n",
    "    # We created a highly nested JSON structure with the following format:\n",
    "    \n",
    "    # summary_list = [\n",
    "    #          {\n",
    "    #            'categorical_variable': categorical_var1,\n",
    "    #            'unique_values': [\n",
    "    #                             {\n",
    "    #                                'value': np.nan, \n",
    "    #                               'counts_of_occurences': total_na,\n",
    "    #                               'percent_of_occurences': ((total_na/total_rows)*100)\n",
    "    #                      },  {\n",
    "    #\n",
    "    #                           'value': unique_val_1, \n",
    "    #                           'counts_of_occurences': total_elements_1,\n",
    "    #                           'percent_of_occurences': ((total_elements_1/total_rows)*100)\n",
    "    #               \n",
    "    #                     }, ... , {\n",
    "    #                           'value': unique_val_N, \n",
    "    #                           'counts_of_occurences': total_elements_N,\n",
    "    #                           'percent_of_occurences': ((total_elements_N/total_rows)*100)\n",
    "    #               \n",
    "    #                     }\n",
    "    #                    ]\n",
    "    #                 }, ... {\n",
    "    #                        'categorical_variable': categorical_var_M,\n",
    "    #                        'unique_values': [...]\n",
    "    #                       }\n",
    "    # ]\n",
    " \n",
    "    if (is_categorical == 1):\n",
    "        # Notice that, if !=1, the list is empty, so the previous loop is not executed.\n",
    "        # Now, call the same methods used in function json_obj_to_dataframe to \n",
    "        # flat the list of dictionaries, if they are present:\n",
    "    \n",
    "        JSON = summary_list\n",
    "        JSON_RECORD_PATH = 'unique_values'\n",
    "        JSON_FIELD_SEPARATOR = \"_\"\n",
    "        JSON_METADATA_PREFIX_LIST = ['categorical_variable']\n",
    "\n",
    "        cat_vars_summary = json_normalize(JSON, record_path = JSON_RECORD_PATH, sep = JSON_FIELD_SEPARATOR, meta = JSON_METADATA_PREFIX_LIST)\n",
    "        # JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "        # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "        # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "        # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "        print(\"\\n\") # line break\n",
    "        print(\"Finished analyzing the categorical variables. Check the summary dataframe:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(cat_vars_summary)\n",
    "\n",
    "        except: # regular mode\n",
    "            print(cat_vars_summary)\n",
    "        \n",
    "        return cat_vars_summary\n",
    "    \n",
    "    else:\n",
    "        print(\"The dataframe has no categorical variables to analyze.\\n\")\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "77c8b152-692a-4efe-9275-9576d230c05f"
   },
   "source": [
    "# **Function for removing all columns and rows that contain only missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_completely_blank_rows_and_columns (df, list_of_columns_to_ignore = None):\n",
    "        \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "        \n",
    "    # list_of_columns_to_ignore: if you do not want to check a specific column, pass its name\n",
    "    # (header) as an element from this list. It should be declared as a list even if it contains\n",
    "    # a single value.\n",
    "    # e.g. list_of_columns_to_ignore = ['column1'] will not analyze missing values in column named\n",
    "    # 'column1'; list_of_columns_to_ignore = ['col1', 'col2'] will ignore columns 'col1' and 'col2'\n",
    "        \n",
    "    # Create dataframe local copy to manipulate, avoiding that Pandas operates on\n",
    "    # the original object; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DATASET = df.copy(deep = True)\n",
    "        \n",
    "    # Get dataframe length:\n",
    "    df_length = len(DATASET)\n",
    "        \n",
    "    # Get list of columns from the dataframe:\n",
    "    df_columns = DATASET.columns\n",
    "        \n",
    "    # Get initial totals of rows or columns:\n",
    "    total_rows = len(DATASET)\n",
    "    total_cols = len(df_columns)\n",
    "        \n",
    "    # Get a list containing only columns to check:\n",
    "    cols_to_check = []\n",
    "        \n",
    "    # Check if there is a list of columns to ignore:\n",
    "    if not (list_of_columns_to_ignore is None):\n",
    "            \n",
    "        # Append all elements from df_columns that are not in the list\n",
    "        # to ignore:\n",
    "        for column in df_columns:\n",
    "            # loop through all elements named 'column' and check if it satisfies both conditions\n",
    "            if (column not in list_of_columns_to_ignore):\n",
    "                cols_to_check.append(column)\n",
    "            \n",
    "        # create a ignored dataframe and a checked df:\n",
    "        checked_df = DATASET[cols_to_check].copy(deep = True)\n",
    "        # Update total columns:\n",
    "        total_cols = len(checked_df.columns)\n",
    "            \n",
    "        ignored_df = DATASET[list_of_columns_to_ignore].copy(deep = True)\n",
    "        \n",
    "    else:\n",
    "        # There is no column to ignore, so we must check all columns:\n",
    "        checked_df = DATASET\n",
    "        # Update the list of columns to check:\n",
    "        cols_to_check = list(checked_df.columns)\n",
    "        \n",
    "    # To remove only rows or columns with only missing values, we set how = 'all' in\n",
    "    # dropna method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "        \n",
    "    # Remove rows that contain only missing values:\n",
    "        \n",
    "    checked_df = checked_df.dropna(axis = 0, how = 'all')\n",
    "    print(f\"{total_rows - len(checked_df)} rows were completely blank and were removed.\\n\")\n",
    "        \n",
    "    # Remove columns that contain only missing values:\n",
    "    checked_df = checked_df.dropna(axis = 1, how = 'all')\n",
    "    print(f\"{total_cols - len(checked_df.columns)} columns were completely blank and were removed.\\n\")\n",
    "        \n",
    "    # Check if at least one column was actually checked:\n",
    "    if (len(cols_to_check) > 0):\n",
    "\n",
    "        if not (list_of_columns_to_ignore is None): \n",
    "            # There is an ignored dataframe, so concatenate it with\n",
    "            # the checked dataframe:\n",
    "            DATASET = pd.concat([ignored_df, checked_df], axis = 1, join = \"inner\")\n",
    "            \n",
    "        else: \n",
    "            # Make the DATASET the checked_df itself:\n",
    "            DATASET = checked_df\n",
    "\n",
    "    # Now, reset the index:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "        \n",
    "    if (((total_rows - len(DATASET)) > 0) | ((total_cols - len(DATASET.columns)) > 0)):\n",
    "            \n",
    "        # There were modifications in the dataframe.\n",
    "        print(\"Check the first 10 rows of the new returned dataframe:\\n\")\n",
    "            \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET.head(10))\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET.head(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"No blank columns or rows were found. Returning the original dataframe.\\n\")\n",
    "        \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "106e9e2f-52b0-4aa2-ad98-c258fd83270c"
   },
   "source": [
    "# **Function for visualizing and characterizing distribution of missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "854c2e36-214b-4fad-884b-0931857f925a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def visualize_and_characterize_missing_values (df, slice_time_window_from = None, slice_time_window_to = None, aggregate_time_in_terms_of = None):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import missingno as msno\n",
    "    # misssingno package is built for visualizing missing values. \n",
    "    \n",
    "    # df: dataframe to be analyzed\n",
    "    \n",
    "    # slice_time_window_from and slice_time_window_to (timestamps). When analyzing time series,\n",
    "    # use these parameters to observe only values in a given time range.\n",
    "    \n",
    "    # slice_time_window_from: the inferior limit of the analyzed window. If you declare this value\n",
    "    # and keep slice_time_window_to = None, then you will analyze all values that comes after\n",
    "    # slice_time_window_from.\n",
    "    # slice_time_window_to: the superior limit of the analyzed window. If you declare this value\n",
    "    # and keep slice_time_window_from = None, then you will analyze all values until\n",
    "    # slice_time_window_to.\n",
    "    # If slice_time_window_from = slice_time_window_to = None, only the standard analysis with\n",
    "    # the whole dataset will be performed. If both values are specified, then the specific time\n",
    "    # window from 'slice_time_window_from' to 'slice_time_window_to' will be analyzed.\n",
    "    # e.g. slice_time_window_from = 'May-1976', and slice_time_window_to = 'Jul-1976'\n",
    "    # Notice that the timestamps must be declares in quotes, just as strings.\n",
    "    \n",
    "    # aggregate_time_in_terms_of = None. Keep it None if you do not want to aggregate the time\n",
    "    # series. Alternatively, set aggregate_time_in_terms_of = 'Y' or aggregate_time_in_terms_of = \n",
    "    # 'year' to aggregate the timestamps in years; set aggregate_time_in_terms_of = 'M' or\n",
    "    # 'month' to aggregate in terms of months; or set aggregate_time_in_terms_of = 'D' or 'day'\n",
    "    # to aggregate in terms of days.\n",
    "    \n",
    "    print(\"Possible reasons for missing data:\\n\")\n",
    "    print(\"One of the obvious reasons is that data is simply missing at random.\")\n",
    "    print(\"Other reasons might be that the missingness is dependent on another variable;\")\n",
    "    print(\"or it is due to missingness of the same variables or other variables.\\n\")\n",
    "    \n",
    "    print(\"Types of missingness:\\n\")\n",
    "    print(\"Identifying the missingness type helps narrow down the methodologies that you can use for treating missing data.\")\n",
    "    print(\"We can group the missingness patterns into 3 broad categories:\\n\")\n",
    "    \n",
    "    print(\"Missing Completely at Random (MCAR)\\n\")\n",
    "    print(\"Missingness has no relationship between any values, observed or missing.\")\n",
    "    print(\"Example: consider you have a class of students. There are a few students absent on any given day. The students are absent just randomly for their specific reasons. This is missing completely at random.\\n\")\n",
    "    \n",
    "    print(\"Missing at Random (MAR)\\n\")\n",
    "    print(\"There is a systematic relationship between missingness and other observed data, but not the missing data.\")\n",
    "    print(\"Example: consider the attendance in a classroom of students during winter, where many students are absent due to the bad weather. Although this might be at random, the hidden cause might be that students sitting closer might have contracted a fever.\\n\")\n",
    "    print(\"Missing at random means that there might exist a relationship with another variable.\")\n",
    "    print(\"In this example, the attendance is slightly correlated to the season of the year.\")\n",
    "    print(\"It\\'s important to notice that, for MAR, missingness is dependent only on the observed values; and not the other missing values.\\n\")\n",
    "    \n",
    "    print(\"Missing not at Random (MNAR)\\n\")\n",
    "    print(\"There is a relationship between missingness and its values, missing or non-missing.\")\n",
    "    print(\"Example: in our class of students, it is Sally\\'s birthday. Sally and many of her friends are absent to attend her birthday party. This is not at all random as Sally and only her friends are absent.\\n\")\n",
    "    \n",
    "    # set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Start the agg_dict, a dictionary that correlates the input aggregate_time_in_terms_of to\n",
    "    # the correspondent argument that must be passed to the matrix method:\n",
    "    agg_dict = {\n",
    "        \n",
    "        'year': 'Y',\n",
    "        'Y': 'Y',\n",
    "        'month': 'M',\n",
    "        'M': 'M',\n",
    "        'day': 'D',\n",
    "        'D':'D'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if not (aggregate_time_in_terms_of is None):\n",
    "        # access the frequency in the dictionary\n",
    "        frequency = agg_dict[aggregate_time_in_terms_of] \n",
    "    \n",
    "    df_length = len(DATASET)\n",
    "    print(f\"Count of rows from the dataframe =\\n\")\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(df_length)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(df_length)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Show total of missing values for each variable:\n",
    "    total_of_missing_values_series = DATASET.isna().sum()\n",
    "    # This is a series which uses the original column names as index\n",
    "    proportion_of_missing_values_series = DATASET.isna().mean()\n",
    "    percent_of_missing_values_series = proportion_of_missing_values_series * 100\n",
    "    missingness_dict = {'count_of_missing_values': total_of_missing_values_series,\n",
    "                       'proportion_of_missing_values': proportion_of_missing_values_series,\n",
    "                       'percent_of_missing_values': percent_of_missing_values_series}\n",
    "    \n",
    "    df_missing_values = pd.DataFrame(data = missingness_dict)\n",
    "    # Now, the dataframe has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    df_missing_values.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    \n",
    "    # Create a one row dataframe with the missingness for the whole dataframe:\n",
    "    # Pass the scalars as single-element lists or arrays:\n",
    "    one_row_data = {'dataframe_column': ['missingness_accross_rows'],\n",
    "                    'count_of_missing_values': [len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any'))],\n",
    "                    'proportion_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))],\n",
    "                    'percent_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))*100]\n",
    "                    }\n",
    "    one_row_df = pd.DataFrame(data = one_row_data)\n",
    "    one_row_df.set_index('dataframe_column', inplace = True)\n",
    "    \n",
    "    # Append this one_row_df to df_missing_values:\n",
    "    df_missing_values = pd.concat([df_missing_values, one_row_df])\n",
    "    \n",
    "    print(\"Missing values on each feature; and missingness considering all rows from the dataframe:\")\n",
    "    print(\"(note: \\'missingness_accross_rows\\' was calculated by: checking which rows have at least one missing value (NA); and then comparing total rows with NAs with total rows in the dataframe).\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(df_missing_values)    \n",
    "    except:\n",
    "        print(df_missing_values)\n",
    "    print(\"\\n\") # line_break\n",
    "    \n",
    "    print(\"Bar chart of the missing values - Nullity bar:\\n\")\n",
    "    msno.bar(DATASET)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "    print(\"The nullity bar allows us to visualize the completeness of the dataframe.\\n\")\n",
    "    \n",
    "    print(\"Nullity Matrix: distribution of missing values through the dataframe:\\n\")\n",
    "    msno.matrix(DATASET)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if not ((slice_time_window_from is None) | (slice_time_window_to is None)):\n",
    "        \n",
    "        # There is at least one of these two values for slicing:\n",
    "        if not ((slice_time_window_from is None) & (slice_time_window_to is None)):\n",
    "                # both are present\n",
    "                \n",
    "                if not (aggregate_time_in_terms_of is None):\n",
    "                    print(\"Nullity matrix for the defined time window and for the selected aggregation frequency:\\n\")\n",
    "                    msno.matrix(DATASET.loc[slice_time_window_from:slice_time_window_to], freq = frequency)\n",
    "                    \n",
    "                else:\n",
    "                    # do not aggregate:\n",
    "                    print(\"Nullity matrix for the defined time window:\\n\")\n",
    "                    msno.matrix(DATASET.loc[slice_time_window_from:slice_time_window_to])\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"\\n\")\n",
    "        \n",
    "        elif not (slice_time_window_from is None):\n",
    "            # slice only from the start. The condition where both limits were present was already\n",
    "            # checked. To reach this condition, only one is not None\n",
    "            # slice from 'slice_time_window_from' to the end of dataframe\n",
    "            \n",
    "                if not (aggregate_time_in_terms_of is None):\n",
    "                    print(\"Nullity matrix for the defined time window and for the selected aggregation frequency:\\n\")\n",
    "                    msno.matrix(DATASET.loc[slice_time_window_from:], freq = frequency)\n",
    "                \n",
    "                else:\n",
    "                    # do not aggregate:\n",
    "                    print(\"Nullity matrix for the defined time window:\\n\")\n",
    "                    msno.matrix(DATASET.loc[slice_time_window_from:])\n",
    "        \n",
    "                plt.show()\n",
    "                print(\"\\n\")\n",
    "            \n",
    "        else:\n",
    "        # equivalent to elif not (slice_time_window_to is None):\n",
    "            # slice only from the beginning to the upper limit. \n",
    "            # The condition where both limits were present was already checked. \n",
    "            # To reach this condition, only one is not None\n",
    "            # slice from the beginning to 'slice_time_window_to'\n",
    "            \n",
    "                if not (aggregate_time_in_terms_of is None):\n",
    "                    print(\"Nullity matrix for the defined time window and for the selected aggregation frequency:\\n\")\n",
    "                    msno.matrix(DATASET.loc[:slice_time_window_to], freq = frequency)\n",
    "                \n",
    "                else:\n",
    "                    # do not aggregate:\n",
    "                    print(\"Nullity matrix for the defined time window:\\n\")\n",
    "                    msno.matrix(DATASET.loc[:slice_time_window_to])\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"\\n\")\n",
    "    \n",
    "    else:\n",
    "        # Both slice limits are not. Let's check if we have to aggregate the dataframe:\n",
    "        if not (aggregate_time_in_terms_of is None):\n",
    "                print(\"Nullity matrix for the selected aggregation frequency:\\n\")\n",
    "                msno.matrix(DATASET, freq = frequency)\n",
    "                plt.show()\n",
    "                print(\"\\n\")\n",
    "    \n",
    "    print(\"The nullity matrix allows us to visualize the location of missing values in the dataset.\")\n",
    "    print(\"The nullity matrix describes the nullity in the dataset and appears blank wherever there are missing values.\")\n",
    "    print(\"It allows us to quickly analyze the patterns in missing values.\")\n",
    "    print(\"The sparkline on the right of the matrix summarizes the general shape of data completeness and points out the row with the minimum number of null values in the dataframe.\")\n",
    "    print(\"In turns, the nullity matrix shows the total counts of columns at its bottom.\")\n",
    "    print(\"We can previously slice the dataframe for a particular interval of analysis (e.g. slice the time interval) to obtain more clarity on the amount of missingness.\")\n",
    "    print(\"Slicing will be particularly helpful when analyzing large datasets.\\n\")\n",
    "    print(\"MCAR: plotting the missingness matrix plot (nullity matrix) for a MCAR variable will show values missing at random, with no correlation or clear pattern.\")\n",
    "    print(\"Correlation here implies the dependency of missing values on another variable present or absent.\\n\")\n",
    "    print(\"MAR: the nullity matrix for MAR can be visualized as the presence of many missing values for a given feature. In this case, there might be a reason for the missingness that cannot be directly observed.\\n\")\n",
    "    print(\"MNAR: the nullity matrix for MNAR shows a strong correlation between the missingness of two variables A and B.\")\n",
    "    print(\"This correlation is easily observable by sorting the dataframe in terms of A or B before obtaining the matrix.\\n\")\n",
    "    \n",
    "    print(\"Missingness Heatmap:\\n\")\n",
    "    msno.heatmap(DATASET)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"The missingness heatmap describes the correlation of missingness between columns.\")\n",
    "    print(\"The heatmap is a graph of correlation of missing values between columns.\")\n",
    "    print(\"It explains the dependencies of missingness between columns.\")\n",
    "    print(\"In simple terms, if the missingness for two columns are highly correlated, then the heatmap will show high values of coefficient of correlation R2 for them.\")\n",
    "    print(\"That is because columns where the missing values co-occur the maximum are highly related and vice-versa.\\n\")\n",
    "    print(\"In the graph, the redder the color, the lower the correlation between the missing values of the columns.\")\n",
    "    print(\"In turns, the bluer the color, the higher the correlation of missingness between the two variables.\\n\")\n",
    "    print(\"ATTENTION: before deciding if the missing values in one variable is correlated with other, so that they would be characterized as MAR or MNAR, check the total of missing values.\")\n",
    "    print(\"Even if the heatmap shows a certain degree of correlation, the number of missing values may be too small to substantiate that.\")\n",
    "    print(\"Missingness in very small number may be considered completely random, and missing values can be eliminated.\\n\")\n",
    "    \n",
    "    print(\"Missingness Dendrogram:\\n\")\n",
    "    msno.dendrogram(DATASET)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"A dendrogram is a tree diagram that groups similar objects in close branches.\")\n",
    "    print(\"So, the missingness dendrogram is a tree diagram of missingness that describes correlation of variables by grouping similarly missing columns together.\")\n",
    "    print(\"To interpret this graph, read it from a top-down perspective.\")\n",
    "    print(\"Cluster leaves which are linked together at a distance of zero fully predict one another\\'s presence.\")\n",
    "    print(\"In other words, when two variables are grouped together in the dendogram, one variable might always be empty while another is filled (the presence of one explains the missingness of the other), or they might always both be filled or both empty, and so on (the missingness of one explains the missigness of the other).\\n\")\n",
    "    \n",
    "    return df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9728d3cd-be42-4002-ba96-291b8b5d37f4"
   },
   "source": [
    "# **Function for visualizing missingness across a variable, and comparing it to another variable (both numeric)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "azdata_cell_guid": "e8b15ccc-f3d3-4b9b-b116-b914291168f5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def visualizing_and_comparing_missingness_across_numeric_vars (df, column_to_analyze, column_to_compare_with, show_interpreted_example = False, grid = True, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import os\n",
    "    # Two conditions require the os library, so we import it at the beginning of the function,\n",
    "    # to avoid importing it twice.\n",
    "    import shutil # component of the standard library to move or copy files.\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # column_to_analyze, column_to_compare_with: strings (in quotes).\n",
    "    # column_to_analyze is the column from the dataframe df that will be analyzed in terms of\n",
    "    # missingness; whereas column_to_compare_with is the column to which column_to_analyze will\n",
    "    # be compared.\n",
    "    # e.g. column_to_analyze = 'column1' will analyze 'column1' from df.\n",
    "    # column_to_compare_with = 'column2' will compare 'column1' against 'column2'\n",
    "    \n",
    "    # show_interpreted_example: set as True if you want to see an example of a graphic analyzed and\n",
    "    # interpreted.\n",
    "    \n",
    "    print(\"Missingness across a variable:\\n\")\n",
    "    print(\"In this analysis, we will graphically analyze the relationship between missing values and non-missing values.\")\n",
    "    print(\"To do so, we will start by visualizing the missingness of a variable against another variable.\")\n",
    "    print(\"The scatter plot will show missing values in one color, and non-missing values in other color.\")\n",
    "    print(\"It will allow us to visualize how missingness of a variable changes against another variable.\")\n",
    "    print(\"Analyzing the missingness of a variable against another variable helps you determine any relationships between missing and non-missing values.\")\n",
    "    print(\"This is very similar to how you found correlations of missingness between two columns.\")\n",
    "    print(\"In summary, we will plot a scatter plot to analyze if there is any correlation of missingness in one column against another column.\\n\")\n",
    "    \n",
    "    # To create the graph, we will use the matplotlib library. \n",
    "    # However, matplotlib skips all missing values while plotting. \n",
    "    # Therefore, we would need to first create a function that fills in dummy values for all the \n",
    "    # missing values in the DataFrame before plotting.\n",
    "    \n",
    "    # We will create a function 'fill_dummy_values' that fill in all columns in the DataFrame.\n",
    "    #The operations involve shifting and scaling the column range with a scaling factor.\n",
    "        \n",
    "    # We use a for loop to produce dummy values for all the columns in a given DataFrame. \n",
    "    # We can also define the scaling factor so that we can resize the range of dummy values. \n",
    "    # In addition to the previous steps of scaling and shifting the dummy values, we'll also have to \n",
    "    # create a copy of the DataFrame to fill in dummy values first. Let's now use this function to \n",
    "    # create our scatterplot.\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    # define a subfunction for filling the dummy values.\n",
    "    # In your function definition, set the default value of scaling_factor to be 0.075:\n",
    "    def fill_dummy_values(df, scaling_factor = 0.075):\n",
    "        \n",
    "        # To generate dummy values, we can use the 'rand()' function from 'numpy.random'. \n",
    "        # We first store the number of missing values in column_to_analyze to 'num_nulls'\n",
    "        # and then generate an array of random dummy values of the size 'num_nulls'. \n",
    "        # The generated dummy values appear as shown beside on the graph. \n",
    "        \n",
    "        # The rand function always outputs values between 0 and 1. \n",
    "        # However, you must observe that the values of both column_to_analyze and column_to_compare_with \n",
    "        # have their own ranges, that may be different. \n",
    "        # Hence we'll need to scale and shift the generated dummy values so that they nicely fit \n",
    "        # into the graph.\n",
    "        \n",
    "        from numpy.random import rand\n",
    "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html?msclkid=7414313ace7611eca18491dd4e7e86ae\n",
    "        \n",
    "        df_dummy = df.copy(deep = True)\n",
    "        # Get the list of columns from df_dummy.\n",
    "        # Use the list attribute to convert the array df_dummy.columns to list:\n",
    "        df_cols_list = list(df_dummy.columns)\n",
    "        \n",
    "        # Calculate the number of missing values in each column of the dummy DataFrame.\n",
    "        for col_name in df_dummy:\n",
    "            \n",
    "            col = df_dummy[col_name]\n",
    "            # Create a column informing if the element is missing (True)\n",
    "            # or not (False):\n",
    "            col_null = col.isnull()\n",
    "            # Calculate number of missing values in this column:\n",
    "            num_nulls = col_null.sum()\n",
    "            \n",
    "            # Return the index j of column col_name. \n",
    "            # Use the index method from lists, setting col_name as argument. It will return\n",
    "            # the index of col_name from the list of columns\n",
    "            # https://www.programiz.com/python-programming/methods/list/index#:~:text=The%20list%20index%20%28%29%20method%20can%20take%20a,-%20search%20the%20element%20up%20to%20this%20index?msclkid=a690b8dacfaa11ec8e84e10a50ae45ec\n",
    "            j = df_cols_list.index(col_name)\n",
    "            \n",
    "            # Check if the column is a text or timestamp. In this case, the type\n",
    "            # of column will be 'object'\n",
    "            if (col.dtype not in numeric_dtypes):\n",
    "                \n",
    "                # Try converting it to a datetime64 object:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    col = (col).astype('datetime64[ns]')\n",
    "                \n",
    "                except:\n",
    "                    \n",
    "                    # It is not a timestamp, so conversion was not possible.\n",
    "                    # Simply ignore it.\n",
    "                    pass\n",
    "                \n",
    "            # Now, try to perform the scale adjustment:\n",
    "            try:\n",
    "                # Calculate column range\n",
    "                col_range = (col.max() - col.min())\n",
    "\n",
    "                # Scale the random values to scaling_factor times col_range\n",
    "                # Calculate random values with the size of num_nulls.\n",
    "                # The rand() function takes in as argument the size of the array to be generated\n",
    "                # (i.e. the number num_nulls itself):\n",
    "                \n",
    "                try:\n",
    "                    dummy_values = (rand(num_nulls) - 2) * (scaling_factor) * (col_range) + (col.min())\n",
    "                \n",
    "                except:\n",
    "                    # It may be a timestamp, so we cannot multiply col_range and sum.\n",
    "                    dummy_values = (rand(num_nulls) - 2) * (scaling_factor) + (col.min())\n",
    "                \n",
    "                # We can shift the dummy values from 0 and 1 to -2 and -1 by subtracting 2, as in:\n",
    "                # (rand(num_nulls) - 2)\n",
    "                # By doing this, we make sure that the dummy values are always below or lesser than \n",
    "                # the actual values, as can be observed from the graph.\n",
    "                # So, by subtracting 2, we guarantee that the dummy values will be below the maximum \n",
    "                # possible.\n",
    "\n",
    "                # Next, scale your dummy values by scaling_factor and multiply them by col_range:\n",
    "                #  * (scaling_factor) * (col_range)\n",
    "                # Finally add the bias: the minimum observed for that column col.min():\n",
    "                # + (col.min())\n",
    "                # When we shift the values to the minimum (col.min()), we make sure that the dummy \n",
    "                # values are just below the actual values.\n",
    "\n",
    "                # Therefore, the procedure results in dummy values a distance apart from the \n",
    "                # actual values.\n",
    "\n",
    "                # Loop through the array of dummy values generated:\n",
    "                # Loop through each row of the dataframe:\n",
    "                \n",
    "                k = 0 # first element from the array of dummy values\n",
    "                for i in range (0, len(df_dummy)):\n",
    "\n",
    "                        # Check if the position is missing:\n",
    "                        boolean_filter = col_null[i]\n",
    "                        if (boolean_filter):\n",
    "\n",
    "                            # Run if it is True.\n",
    "                            # Fill the position in col_name with the dummy value\n",
    "                            # at the position k from the array of dummy values.\n",
    "                            # This array was created with a single element for each\n",
    "                            # missing value:\n",
    "                            df_dummy.iloc[i,j] = dummy_values[k]\n",
    "                            # go to the next element\n",
    "                            k = k + 1\n",
    "                \n",
    "            except:\n",
    "                # It was not possible, because it is neither numeric nor timestamp.\n",
    "                # Simply ignore it.\n",
    "                pass\n",
    "                \n",
    "        return df_dummy\n",
    "  \n",
    "    # We fill the dummy values to 'df_dummy' with the function `fill_dummy_values`. \n",
    "    # The graph can be plotted with 'df_dummy.plot()' of 'x=column_to_analyze', \n",
    "    # 'y=column_to_compare_with', 'kind=\"scatter\"' and 'alpha=0.5' for transparency. \n",
    "    \n",
    "    # Call the subfunction for filling the dummy values:\n",
    "    df_dummy = fill_dummy_values(df)\n",
    "    \n",
    "    # The object 'nullity' is the sum of the nullities of column_to_analyze and column_to_compare_with. \n",
    "    # It is a series of True and False values. \n",
    "    # True implies missing, while False implies not missing.\n",
    "    \n",
    "    # The nullity can be used to set the color of the data points with 'cmap=\"rainbow\"'. \n",
    "    # Thus, we obtain the graph that we require.\n",
    "    \n",
    "    # Set the nullity of column_to_analyze and column_to_compare_with:\n",
    "    nullity = ((df[column_to_analyze].isnull()) | (df[column_to_compare_with].isnull()))\n",
    "    # For setting different colors to the missing and non-missing values, you can simply add \n",
    "    # the nullity, or the sum of null values of both respective columns that you are plotting, \n",
    "    # calculated using the .isnull() method. The nullity returns a Series of True or False \n",
    "    # (i.e., a boolean filter) where:\n",
    "    # True - At least one of col1 or col2 is missing.\n",
    "    # False - Neither of col1 and col2 values are missing.\n",
    "\n",
    "    if (plot_title is None):\n",
    "        plot_title = \"missingness_of_\" + \"[\" + column_to_analyze + \"]\" + \"_vs_\" + \"[\" + column_to_compare_with + \"]\"\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    \n",
    "    # Create a scatter plot of column_to_analyze and column_to_compare_with \n",
    "    df_dummy.plot(x = column_to_analyze, y = column_to_compare_with, \n",
    "                        kind = 'scatter', alpha = 0.5,\n",
    "                        # Set color to nullity of column_to_analyze and column_to_compare_with\n",
    "                        # alpha: transparency. alpha = 0.5 = 50% of transparency.\n",
    "                        c = nullity,\n",
    "                        # The c argument controls the color of the points in the plot.\n",
    "                        cmap = 'rainbow',\n",
    "                        grid = grid,\n",
    "                        legend = True,\n",
    "                        title = plot_title)\n",
    "       \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"comparison_of_missing_values\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    print(\"Plot Legend:\") \n",
    "    print(\"1 = Missing value\")\n",
    "    print(\"0 = Non-missing value\")\n",
    "    \n",
    "    if (show_interpreted_example):\n",
    "        # Run if it is True. Requires TensorFlow to load. Load the extra library only\n",
    "        # if necessary:\n",
    "        from html2image import Html2Image\n",
    "        from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "        # img_to_array: convert the image into its numpy array representation\n",
    "        \n",
    "        # Download the images to the notebook's workspace:\n",
    "        \n",
    "        # Alternatively, use \"wget GNU\" (cannot use as .py file):\n",
    "        # Use the command !wget to download web content:\n",
    "        #example_na1 = !wget --no-check-certificate https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na1.PNG example_na1.png\n",
    "        #example_na2 = !wget --no-check-certificate https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na2.PNG example_na2.png\n",
    "        #example_na3 = !wget --no-check-certificate https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na3.PNG example_na3.png\n",
    "        #example_na4 = !wget --no-check-certificate https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na4.PNG example_na4.png\n",
    "        # When saving the !wget calls as variables, we silent the verbosity of the Wget GNU.\n",
    "        # Then, user do not see that a download has been made.\n",
    "        # To check the help from !wget GNU, type and run a cell with: \n",
    "        # ! wget --help\n",
    "        \n",
    "        url1 = \"https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na1.PNG\"\n",
    "        url2 = \"https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na2.PNG\"\n",
    "        url3 = \"https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na3.PNG\"\n",
    "        url4 = \"https://github.com/marcosoares-92/img_examples_guides/raw/main/example_na4.PNG\"\n",
    "        \n",
    "        # Create a new folder to store the images, if the folder do not exists:\n",
    "        new_dir = \"tmp\"\n",
    "        os.makedirs(new_dir, exist_ok = True)\n",
    "        # exist_ok = True creates the directory only if it does not exist.\n",
    "        \n",
    "        # Instantiate the class Html2Image:\n",
    "        html_img = Html2Image()\n",
    "        # Download the images:\n",
    "        # pypi.org/project/html2image/\n",
    "        img1 = html_img.screenshot(url = url1, save_as = \"example_na1.PNG\", size = (500, 500))\n",
    "        img2 = html_img.screenshot(url = url2, save_as = \"example_na2.PNG\", size = (500, 500))\n",
    "        img3 = html_img.screenshot(url = url3, save_as = \"example_na3.PNG\", size = (500, 500))\n",
    "        img4 = html_img.screenshot(url = url4, save_as = \"example_na4.PNG\", size = (500, 500))\n",
    "        # If size is omitted, the image is downloaded in the low-resolution default.\n",
    "        # save_as must be a file name, a path is not accepted.\n",
    "        # Make the output from the method equals to a variable eliminates its verbosity\n",
    "        \n",
    "        # Create the new paths for the images:\n",
    "        img1_path = os.path.join(new_dir, \"example_na1.PNG\")\n",
    "        img2_path = os.path.join(new_dir, \"example_na2.PNG\")\n",
    "        img3_path = os.path.join(new_dir, \"example_na3.PNG\")\n",
    "        img4_path = os.path.join(new_dir, \"example_na4.PNG\")\n",
    "        \n",
    "        # Move the image files to their new paths:\n",
    "        # use shutil.move(source, destination) method to move the files:\n",
    "        # pynative.com/python-move-files\n",
    "        # docs.python.org/3/library/shutil.html\n",
    "        shutil.move(\"example_na1.PNG\", img1_path)\n",
    "        shutil.move(\"example_na2.PNG\", img2_path)\n",
    "        shutil.move(\"example_na3.PNG\", img3_path)\n",
    "        shutil.move(\"example_na4.PNG\", img4_path)\n",
    "        \n",
    "        # Load the images and save them on variables:\n",
    "        sample_image1 = load_img(img1_path)\n",
    "        sample_image2 = load_img(img2_path)\n",
    "        sample_image3 = load_img(img3_path)\n",
    "        sample_image4 = load_img(img4_path)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Example of analysis:\\n\")\n",
    "        \n",
    "        print(\"Consider the following \\'diabetes\\' dataset, where scatterplot of \\'Serum_Insulin\\' and \\'BMI\\' illustrated below shows the non-missing values in purple and the missing values in red.\\n\")\n",
    "        \n",
    "        # Image example 1:\n",
    "        # show image with plt.imshow function:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        plt.imshow(sample_image1)\n",
    "        # If the image is black and white, you can color it with a cmap as \n",
    "        # fig.set_cmap('hot')\n",
    "        #set axis off:\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"The red points along the y-axis are the missing values of \\'Serum_Insulin\\' plotted against their \\'BMI\\' values.\\n\")\n",
    "        # Image example 2:\n",
    "        # show image with plt.imshow function:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        plt.imshow(sample_image2)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"Likewise, the points along the x-axis are the missing values of \\'BMI\\' against their \\'Serum_Insulin\\' values.\\n\")\n",
    "        # show image with plt.imshow function:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        plt.imshow(sample_image3)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"The bottom-left corner represents the missing values of both \\'BMI\\' and \\'Serum_Insulin\\'.\\n\")\n",
    "        # Image example 4:\n",
    "        # show image with plt.imshow function:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        plt.imshow(sample_image4)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"To interprete this graph, observe that the missing values of \\'Serum_Insulin\\' are spread throughout the \\'BMI\\' column.\")\n",
    "        print(\"Thus, we do not observe any specific correlation between the missingness of \\'Serum_Insulin\\' and \\'BMI\\'.\\n\")\n",
    "        \n",
    "        # Finally, before finishing the function, \n",
    "        # delete (remove) the files from the notebook's workspace.\n",
    "        # The os.remove function deletes a file or directory specified.\n",
    "        os.remove(img1_path)\n",
    "        os.remove(img2_path)\n",
    "        os.remove(img3_path)\n",
    "        os.remove(img4_path)\n",
    "        \n",
    "        # Check if the tmp folder is empty:\n",
    "        size = os.path.getsize(new_dir)\n",
    "        # os.path.getsize returns the total size in Bytes from a folder or a file.\n",
    "        \n",
    "        # Get the list of sub-folders, files or subdirectories (the content) from the folder:\n",
    "        list_of_contents = os.listdir(new_dir)\n",
    "        # doc.python.org/3/library/os.html\n",
    "        # It returns a list of strings representing the paths of each file or directory \n",
    "        # in the analyzed folder.\n",
    "        \n",
    "        # If the size is 0 and the length of the list_of_contents is also zero (i.e., there is no\n",
    "        # previous sub-directory created), then remove the directory:\n",
    "        if ((size == 0) & (len(list_of_contents) == 0)):\n",
    "            os.rmdir(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ef4fdcd6-ada5-4637-9d94-e8c229eac786"
   },
   "source": [
    "# **Function for dealing with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "azdata_cell_guid": "8fc19162-a506-443c-b797-d06966f6711a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def handle_missing_values (df, subset_columns_list = None, drop_missing_val = True, fill_missing_val = False, eliminate_only_completely_empty_rows = False, min_number_of_non_missing_val_for_a_row_to_be_kept = None, value_to_fill = None, fill_method = \"fill_with_zeros\", interpolation_order = 'linear'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    # numpy has no function mode, but scipy's stats module has.\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html?msclkid=ccd9aaf2cb1b11ecb57c6f4b3e03a341\n",
    "    # Pandas dropna method: remove rows containing missing values.\n",
    "    # Pandas fillna method: fill missing values.\n",
    "    # Pandas interpolate method: fill missing values with interpolation:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\n",
    "    \n",
    "    \n",
    "    # subset_columns_list = list of columns to look for missing values. Only missing values\n",
    "    # in these columns will be considered for deciding which columns to remove.\n",
    "    # Declare it as a list of strings inside quotes containing the columns' names to look at,\n",
    "    # even if this list contains a single element. e.g. subset_columns_list = ['column1']\n",
    "    # will check only 'column1'; whereas subset_columns_list = ['col1', 'col2', 'col3'] will\n",
    "    # chek the columns named as 'col1', 'col2', and 'col3'.\n",
    "    # ATTENTION: Subsets are considered only for dropping missing values, not for filling.\n",
    "    \n",
    "    # drop_missing_val = True to eliminate the rows containing missing values.\n",
    "    \n",
    "    # fill_missing_val = False. Set this to True to activate the mode for filling the missing\n",
    "    # values.\n",
    "    \n",
    "    # eliminate_only_completely_empty_rows = False - This parameter shows effect only when\n",
    "    # drop_missing_val = True. If you set eliminate_only_completely_empty_rows = True, then\n",
    "    # only the rows where all the columns are missing will be eliminated.\n",
    "    # If you define a subset, then only the rows where all the subset columns are missing\n",
    "    # will be eliminated.\n",
    "    \n",
    "    # min_number_of_non_missing_val_for_a_row_to_be_kept = None - \n",
    "    # This parameter shows effect only when drop_missing_val = True. \n",
    "    # If you set min_number_of_non_missing_val_for_a_row_to_be_kept equals to an integer value,\n",
    "    # then only the rows where at least this integer number of non-missing values will be kept\n",
    "    # after dropping the NAs.\n",
    "    # e.g. if min_number_of_non_missing_val_for_a_row_to_be_kept = 2, only rows containing at\n",
    "    # least two columns without missing values will be kept.\n",
    "    # If you define a subset, then the criterium is applied only to the subset.\n",
    "    \n",
    "    # value_to_fill = None - This parameter shows effect only when\n",
    "    # fill_missing_val = True. Set this parameter as a float value to fill all missing\n",
    "    # values with this value. e.g. value_to_fill = 0 will fill all missing values with\n",
    "    # the number 0. You can also pass a function call like \n",
    "    # value_to_fill = np.sum(dataset['col1']). In this case, the missing values will be\n",
    "    # filled with the sum of the series dataset['col1']\n",
    "    # Alternatively, you can also input a string to fill the missing values. e.g.\n",
    "    # value_to_fill = 'text' will fill all the missing values with the string \"text\".\n",
    "    \n",
    "    # You can also input a dictionary containing the column(s) to be filled as key(s);\n",
    "    # and the values to fill as the correspondent values. For instance:\n",
    "    # value_to_fill = {'col1': 10} will fill only 'col1' with value 10.\n",
    "    # value_to_fill = {'col1': 0, 'col2': 'text'} will fill 'col1' with zeros; and will\n",
    "    # fill 'col2' with the value 'text'\n",
    "    \n",
    "    # fill_method = \"fill_with_zeros\". - This parameter shows effect only \n",
    "    # when fill_missing_val = True.\n",
    "    # Alternatively: fill_method = \"fill_with_zeros\" - fill all the missing values with 0\n",
    "    \n",
    "    # fill_method = \"fill_with_value_to_fill\" - fill the missing values with the value\n",
    "    # defined as the parameter value_to_fill\n",
    "    \n",
    "    # fill_method = \"fill_with_avg_or_mode\" - fill the missing values with the average value for \n",
    "    # each column, if the column is numeric; or fill with the mode, if the column is categorical.\n",
    "    # The mode is the most commonly observed value.\n",
    "    \n",
    "    # fill_method = \"ffill\" - Forward (pad) fill: propagate last valid observation forward \n",
    "    # to next valid.\n",
    "    # fill_method = 'bfill' - backfill: use next valid observation to fill gap.\n",
    "    # fill_method = 'nearest' - 'ffill' or 'bfill', depending if the point is closest to the\n",
    "    # next or to the previous non-missing value.\n",
    "    \n",
    "    # fill_method = \"fill_by_interpolating\" - fill by interpolating the previous and the \n",
    "    # following value. For categorical columns, it fills the\n",
    "    # missing with the previous value, just as like fill_method = 'ffill'\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\n",
    "    \n",
    "    # interpolation_order: order of the polynomial used for interpolating if fill_method =\n",
    "    # \"fill_by_interpolating\". If interpolation_order = None, interpolation_order = 'linear',\n",
    "    # or interpolation_order = 1, a linear (1st-order polynomial) will be used.\n",
    "    # If interpolation_order is an integer > 1, then it will represent the polynomial order.\n",
    "    # e.g. interpolation_order = 2, for a 2nd-order polynomial; interpolation_order = 3 for a\n",
    "    # 3rd-order, and so on.\n",
    "    \n",
    "    # WARNING: if the fillna method is selected (fill_missing_val == True), but no filling\n",
    "    # methodology is selected, the missing values of the dataset will be filled with 0.\n",
    "    # The same applies when a non-valid fill methodology is selected.\n",
    "    # Pandas fillna method does not allow us to fill only a selected subset.\n",
    "    \n",
    "    # WARNING: if fill_method == \"fill_with_value_to_fill\" but value_to_fill is None, the \n",
    "    # missing values will be filled with the value 0.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of df to manipulate.\n",
    "    # The methods used in this function can modify the original object itself. So,\n",
    "    # here we apply the copy method setting deep = True\n",
    "    cleaned_df = df.copy(deep = True)\n",
    "   \n",
    "    if (subset_columns_list is None):\n",
    "        # all the columns are considered:\n",
    "        total_columns = cleaned_df.shape[1]\n",
    "    \n",
    "    else:\n",
    "        # Only the columns in the subset are considered.\n",
    "        # Total columns is the length of the list of columns to subset:\n",
    "        total_columns = len(subset_columns_list)\n",
    "        \n",
    "    # thresh argument of dropna method: int, optional - Require that many non-NA values.\n",
    "    # This is the minimum of non-missing values that a row must have in order to be kept:\n",
    "    THRESHOLD = min_number_of_non_missing_val_for_a_row_to_be_kept\n",
    "    \n",
    "    if ((drop_missing_val is None) & (fill_missing_val is None)):\n",
    "        print(\"No valid input set for neither \\'drop_missing_val\\' nor \\'fill_missing_val\\'. Then, setting \\'drop_missing_val\\' = True and \\'fill_missing_val\\' = False.\\n\")\n",
    "        drop_missing_val = True\n",
    "        fill_missing_val = False\n",
    "    \n",
    "    elif (drop_missing_val is None):\n",
    "        # The condition where both were missing was already tested. This one is tested only when the\n",
    "        # the first if was not run.\n",
    "        drop_missing_val = False\n",
    "        fill_missing_val = True\n",
    "    \n",
    "    elif (fill_missing_val is None):\n",
    "        drop_missing_val = True\n",
    "        fill_missing_val = False\n",
    "    \n",
    "    elif ((drop_missing_val == True) & (fill_missing_val == True)):\n",
    "        print(\"Both options \\'drop_missing_val\\' and \\'fill_missing_val\\' set as True. Then, selecting \\'drop_missing_val\\', which has preference.\\n\")\n",
    "        fill_missing_val = False\n",
    "    \n",
    "    elif ((drop_missing_val == False) & (fill_missing_val == False)):\n",
    "        print(\"Both options \\'drop_missing_val\\' and \\'fill_missing_val\\' set as False. Then, setting \\'drop_missing_val\\' = True.\\n\")\n",
    "        drop_missing_val = True\n",
    "    \n",
    "    boolean_filter1 = (drop_missing_val == True)\n",
    "\n",
    "    boolean_filter2 = (boolean_filter1) & (subset_columns_list is None)\n",
    "    # These filters are True only if both conditions inside parentheses are True.\n",
    "    # The operator & is equivalent to 'And' (intersection).\n",
    "    # The operator | is equivalent to 'Or' (union).\n",
    "    \n",
    "    boolean_filter3 = (fill_missing_val == True) & (fill_method is None)\n",
    "    # boolean_filter3 represents the situation where the fillna method was selected, but\n",
    "    # no filling method was set.\n",
    "    \n",
    "    boolean_filter4 = (value_to_fill is None) & (fill_method == \"fill_with_value_to_fill\")\n",
    "    # boolean_filter4 represents the situation where the fillna method will be used and the\n",
    "    # user selected to fill the missing values with 'value_to_fill', but did not set a value\n",
    "    # for 'value_to_fill'.\n",
    "    \n",
    "    if (boolean_filter1 == True):\n",
    "        # drop missing values\n",
    "        \n",
    "        print(\"Dropping rows containing missing values, accordingly to the provided parameters.\\n\")\n",
    "        \n",
    "        if (boolean_filter2 == True):\n",
    "            # no subset to filter\n",
    "            \n",
    "            if (eliminate_only_completely_empty_rows == True):\n",
    "                #Eliminate only completely empty rows\n",
    "                cleaned_df = cleaned_df.dropna(axis = 0, how = \"all\")\n",
    "                # if axis = 1, dropna will eliminate each column containing missing values.\n",
    "            \n",
    "            elif (min_number_of_non_missing_val_for_a_row_to_be_kept is not None):\n",
    "                # keep only rows containing at least the specified number of non-missing values:\n",
    "                cleaned_df = cleaned_df.dropna(axis = 0, thresh = THRESHOLD)\n",
    "            \n",
    "            else:\n",
    "                #Eliminate all rows containing missing values.\n",
    "                #The only parameter is drop_missing_val\n",
    "                cleaned_df = cleaned_df.dropna(axis = 0)\n",
    "        \n",
    "        else:\n",
    "            #In this case, there is a subset for applying the Pandas dropna method.\n",
    "            #Only the coluns in the subset 'subset_columns_list' will be analyzed.\n",
    "                  \n",
    "            if (eliminate_only_completely_empty_rows == True):\n",
    "                #Eliminate only completely empty rows\n",
    "                cleaned_df = cleaned_df.dropna(subset = subset_columns_list, how = \"all\")\n",
    "            \n",
    "            elif (min_number_of_non_missing_val_for_a_row_to_be_kept is not None):\n",
    "                # keep only rows containing at least the specified number of non-missing values:\n",
    "                cleaned_df = cleaned_df.dropna(subset = subset_columns_list, thresh = THRESHOLD)\n",
    "            \n",
    "            else:\n",
    "                #Eliminate all rows containing missing values.\n",
    "                #The only parameter is drop_missing_val\n",
    "                cleaned_df = cleaned_df.dropna(subset = subset_columns_list)\n",
    "        \n",
    "        print(\"Finished dropping of missing values.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Filling missing values.\\n\")\n",
    "        \n",
    "        # In this case, the user set a value for the parameter fill_missing_val to fill \n",
    "        # the missing data.\n",
    "        \n",
    "        # Check if a filling dictionary was passed as value_to_fill:\n",
    "        if (type(value_to_fill) == dict):\n",
    "            \n",
    "            print(f\"Applying the filling dictionary. Filling columns {value_to_fill.keys()} with the values {value_to_fill.values()}, respectively.\\n\")\n",
    "            cleaned_df = cleaned_df.fillna(value = value_to_fill)\n",
    "        \n",
    "        elif (boolean_filter3 == True):\n",
    "            # If this condition was reached, no filling dictionary was input.\n",
    "            # fillna method was selected, but no filling method was set.\n",
    "            # Then, filling with zero.\n",
    "            print(\"No filling method defined, so filling missing values with 0.\\n\")\n",
    "            cleaned_df = cleaned_df.fillna(0)\n",
    "        \n",
    "        elif (boolean_filter4 == True):\n",
    "            # fill_method == \"fill_with_value_to_fill\" but value_to_fill is None.\n",
    "            # Then, filling with zero.\n",
    "            print(\"No value input for filling, so filling missing values with 0.\\n\")\n",
    "            cleaned_df = cleaned_df.fillna(0)\n",
    "        \n",
    "        else:\n",
    "            # A filling methodology was selected.\n",
    "            if (fill_method == \"fill_with_zeros\"):\n",
    "                print(\"Filling missing values with 0.\\n\")\n",
    "                cleaned_df = cleaned_df.fillna(0)\n",
    "            \n",
    "            elif (fill_method == \"fill_with_value_to_fill\"):\n",
    "                print(f\"Filling missing values with {value_to_fill}.\\n\")\n",
    "                cleaned_df = cleaned_df.fillna(value_to_fill)\n",
    "            \n",
    "            elif ((fill_method == \"fill_with_avg_or_mode\") | (fill_method == \"fill_by_interpolating\")):\n",
    "                \n",
    "                # We must separate the dataset into numerical columns and categorical columns\n",
    "                # 1. Get dataframe's columns list:\n",
    "                df_cols = cleaned_df.columns\n",
    "                \n",
    "                # 2. start a list for the numeric and a list for the text (categorical) columns:\n",
    "                numeric_list = []\n",
    "                categorical_list = []\n",
    "                # List the possible numeric data types for a Pandas dataframe column:\n",
    "                numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "                \n",
    "                # 3. Loop through each column on df_cols, to put it in the correspondent type of column:\n",
    "                for column in df_cols:\n",
    "                    \n",
    "                    # Check if the column is neither in numeric_list nor in\n",
    "                    # categorical_list yet:\n",
    "                    if ((column not in numeric_list) & (column not in categorical_list)):\n",
    "                        \n",
    "                        column_data_type = cleaned_df[column].dtype\n",
    "\n",
    "                        if (column_data_type not in numeric_dtypes):\n",
    "\n",
    "                            # Append to categorical columns list:\n",
    "                            categorical_list.append(column)\n",
    "\n",
    "                        else:\n",
    "                            # Append to numerical columns list:\n",
    "                            numeric_list.append(column)\n",
    "                \n",
    "                   \n",
    "                # Create variables to map if both are present.\n",
    "                is_categorical = 0\n",
    "                is_numeric = 0\n",
    "\n",
    "                # Create two subsets:\n",
    "                if (len(categorical_list) > 0):\n",
    "\n",
    "                    # Has at least one column:\n",
    "                    df_categorical = df_copy.copy(deep = True)\n",
    "                    df_categorical = df_categorical[categorical_list]\n",
    "                    is_categorical = 1\n",
    "\n",
    "                if (len(numeric_list) > 0):\n",
    "\n",
    "                    df_numeric = df_copy.copy(deep = True)\n",
    "                    df_numeric = df_numeric[numeric_list]\n",
    "                    is_numeric = 1\n",
    "                \n",
    "                # Notice that the variables is_numeric and is_categorical have value 1 only when the subsets\n",
    "                # are present.\n",
    "                is_cat_num = is_categorical + is_numeric\n",
    "                # is_cat_num = 0 if no valid dataset was input.\n",
    "                # is_cat_num = 2 if both subsets are present.\n",
    "        \n",
    "                # Now, we have two subsets , one for the categoricals, other\n",
    "                # for the numeric. It will avoid trying to fill categorical columns with the\n",
    "                # mean values.\n",
    "                \n",
    "                if (fill_method == \"fill_with_avg_or_mode\"):\n",
    "                    \n",
    "                    # Start a filling dictionary:\n",
    "                    fill_dict = {}\n",
    "                    \n",
    "                    print(\"Filling missing values with the average values (numeric variables); or with the modes (categorical variables). The mode is the most commonly observed value of the categorical variable.\\n\")\n",
    "                    \n",
    "                    if (is_numeric == 1):\n",
    "                        \n",
    "                        for column in numeric_list:\n",
    "                            \n",
    "                            # add column as the key, and the mean as the value:\n",
    "                            fill_dict[column] = df_numeric[column].mean()\n",
    "                    \n",
    "                    if (is_categorical == 1):\n",
    "                        \n",
    "                        for column in categorical_list:\n",
    "                            \n",
    "                            mode_array = stats.mode(df_categorical[column])\n",
    "                            \n",
    "                            # The function stats.mode(X) returns an array as: \n",
    "                            # ModeResult(mode=array(['a'], dtype='<U1'), count=array([2]))\n",
    "                            # If we select the first element from this array, stats.mode(X)[0], \n",
    "                            # the function will return an array as array(['a'], dtype='<U1'). \n",
    "                            # We want the first element from this array stats.mode(X)[0][0], \n",
    "                            # which will return a string like 'a':\n",
    "                            try:\n",
    "                                fill_dict[column] = mode_array[0][0]\n",
    "\n",
    "                            except IndexError:\n",
    "                                # This error is generated when trying to access an array storing no values.\n",
    "                                # (i.e., with missing values). Since there is no dimension, it is not possible\n",
    "                                # to access the [0][0] position. In this case, simply append the np.nan \n",
    "                                # the (missing value):\n",
    "                                fill_dict[column] = np.nan\n",
    "                    \n",
    "                    # Now, fill_dict contains the mapping of columns (keys) and \n",
    "                    # correspondent values for imputation with the method fillna.\n",
    "                    # It is equivalent to use:\n",
    "                    # from sklearn.impute import SimpleImputer\n",
    "                    # mean_imputer = SimpleImputer(strategy='mean')\n",
    "                    # mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                    # as in the advanced imputation function.\n",
    "                        \n",
    "                    # In fillna documentation, we see that the argument 'value' must have a dictionary\n",
    "                    # with this particular format as input:\n",
    "                    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna\n",
    "\n",
    "                    # This dictionary correlates each column to its average value.\n",
    "\n",
    "                    #6. Finally, use this dictionary to fill the missing values of each column\n",
    "                    # with the average value of that column\n",
    "                    cleaned_df = cleaned_df.fillna(value = fill_dict)\n",
    "                    # The method will search the column name in fill_dict (key of the dictionary),\n",
    "                    # and will use the correspondent value (average) to fill the missing values.\n",
    "                   \n",
    "\n",
    "                elif (fill_method == \"fill_by_interpolating\"):\n",
    "                    # Pandas interpolate method\n",
    "                    \n",
    "                    # Separate the dataframes into a dataframe for filling with interpolation (numeric\n",
    "                    # variables); and a dataframe for forward filling (categorical variables).\n",
    "                    \n",
    "                    # Before subsetting, check if the list is not empty.\n",
    "                    \n",
    "                    if (is_numeric == 1):\n",
    "                    \n",
    "                        if (type(interpolation_order) == int):\n",
    "                            # an integer number was input\n",
    "\n",
    "                            if (interpolation_order > 1):\n",
    "\n",
    "                                print(f\"Performing interpolation of numeric variables with {interpolation_order}-degree polynomial to fill missing values.\\n\")\n",
    "                                df_numeric = df_numeric.interpolate(method = 'polynomial', order = interpolation_order)\n",
    "\n",
    "                            else:\n",
    "                                # 1st order or invalid order (0 or negative) was used\n",
    "                                print(\"Performing linear interpolation of numeric variables to fill missing values.\\n\")\n",
    "                                df_numeric = df_numeric.interpolate(method = 'linear')\n",
    "\n",
    "                        else:\n",
    "                            # 'linear', None or invalid text was input:\n",
    "                            print(\"Performing linear interpolation of numeric variables to fill missing values.\\n\")\n",
    "                            df_numeric = df_numeric.interpolate(method = 'linear')\n",
    "                    \n",
    "                    # Now, we finished the interpolation of the numeric variables. Let's check if\n",
    "                    # there are categorical variables to forward fill.\n",
    "                    if (is_categorical == 1):\n",
    "                        \n",
    "                        # Now, fill missing values by forward filling:\n",
    "                        print(\"Using forward filling to fill missing values of the categorical variables.\\n\")\n",
    "                        df_categorical = df_categorical.fillna(method = \"ffill\")\n",
    "                    \n",
    "                    # Now, let's check if there are both a numeric_subset and a text_subset to merge\n",
    "                    \n",
    "                    if (is_cat_num == 2):\n",
    "                        # Both subsets are present.\n",
    "                        # Concatenate the dataframes in the columns axis (append columns):\n",
    "                        cleaned_df = pd.concat([df_numeric, df_categorical], axis = 1, join = \"inner\")\n",
    "\n",
    "                    elif (is_categorical == 1):\n",
    "                        # There is only the categorical subset:\n",
    "                        cleaned_df = df_categorical\n",
    "\n",
    "                    elif (is_numeric == 1):\n",
    "                        # There is only the numeric subset:\n",
    "                        cleaned_df = df_numeric\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"No valid dataset provided, so returning the input dataset itself.\\n\")\n",
    "            \n",
    "            elif ((fill_method == \"ffill\") | (fill_method == \"bfill\")):\n",
    "                # use forward or backfill\n",
    "                cleaned_df = cleaned_df.fillna(method = fill_method)\n",
    "            \n",
    "            elif (fill_method == \"nearest\"):\n",
    "                # nearest: applies the 'bfill' or 'ffill', depending if the point\n",
    "                # is closes to the previous or to the next non-missing value.\n",
    "                # It is a Pandas dataframe interpolation method, not a fillna one.\n",
    "                cleaned_df = cleaned_df.interpolate(method = 'nearest')\n",
    "            \n",
    "            else:\n",
    "                print(\"No valid filling methodology was selected. Then, filling missing values with 0.\\n\")\n",
    "                cleaned_df = cleaned_df.fillna(0)\n",
    "        \n",
    "        \n",
    "    #Reset index before returning the cleaned dataframe:\n",
    "    cleaned_df = cleaned_df.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    print(f\"Number of rows of the dataframe before cleaning = {df.shape[0]} rows.\")\n",
    "    print(f\"Number of rows of the dataframe after cleaning = {cleaned_df.shape[0]} rows.\")\n",
    "    print(f\"Percentual variation of the number of rows = {(df.shape[0] - cleaned_df.shape[0])/(df.shape[0]) * 100} %\\n\")\n",
    "    print(\"Check the 10 first rows of the cleaned dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(cleaned_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(cleaned_df.head(10))\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8aa8b98c-4646-4cbd-9db5-832e4aebb7d6"
   },
   "source": [
    "# **Function for advanced imputation on time series data: finding the best imputation strategy for missing values on a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "azdata_cell_guid": "0aa71679-b3b3-4a23-ae24-a1cc1d820ff3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def adv_imputation_missing_values (df, column_to_fill, timestamp_tag_column = None, test_value_to_fill = None, show_imputation_comparison_plots = True):\n",
    "    \n",
    "    # Check DataCamp course Dealing with Missing Data in Python\n",
    "    # https://app.datacamp.com/learn/courses/dealing-with-missing-data-in-python\n",
    "    \n",
    "    # This function handles only one column by call, whereas handle_missing_values can process the whole\n",
    "    # dataframe at once.\n",
    "    # The strategies used for handling missing values is different here. You can use the function to\n",
    "    # process data that does not come from time series, but only plot the graphs for time series data.\n",
    "    \n",
    "    # This function is more indicated for dealing with missing values on time series data than handle_missing_values.\n",
    "    # This function will search for the best imputer for a given column.\n",
    "    # It can process both numerical and categorical columns.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import linregress\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from fancyimpute import KNN, IterativeImputer\n",
    "    \n",
    "    # column_to_fill: string (in quotes) indicating the column with missing values to fill.\n",
    "    # e.g. if column_to_fill = 'col1', imputations will be performed on column 'col1'.\n",
    "    \n",
    "    # timestamp_tag_column = None. string containing the name of the column with the timestamp. \n",
    "    # If timestamp_tag_column is None, the index will be used for testing different imputations.\n",
    "    # be the time series reference. declare as a string under quotes. This is the column from \n",
    "    # which we will extract the timestamps or values with temporal information. e.g.\n",
    "    # timestamp_tag_column = 'timestamp' will consider the column 'timestamp' a time column.\n",
    "    \n",
    "    # test_value_to_fill: the function will test the imputation of a constant. Specify this constant here\n",
    "    # or the tested constant will be zero. e.g. test_value_to_fill = None will test the imputation of 0.\n",
    "    # test_value_to_fill = 10 will test the imputation of value zero.\n",
    "    \n",
    "    # show_imputation_comparison_plots = True. Keep it True to plot the scatter plot comparison\n",
    "    # between imputed and original values, as well as the Kernel density estimate (KDE) plot.\n",
    "    # Alternatively, set show_imputation_comparison_plots = False to omit the plots.\n",
    "    \n",
    "    # The following imputation techniques will be tested, and the best one will be automatically\n",
    "    # selected: mean_imputer, median_imputer, mode_imputer, constant_imputer, linear_interpolation,\n",
    "    # quadratic_interpolation, cubic_interpolation, nearest_interpolation, bfill_imputation,\n",
    "    # ffill_imputation, knn_imputer, mice_imputer (MICE = Multiple Imputations by Chained Equations).\n",
    "    \n",
    "    # MICE: Performs multiple regressions over random samples of the data; \n",
    "    # Takes the average of multiple regression values; and imputes the missing feature value for the \n",
    "    # data point.\n",
    "    # KNN (K-Nearest Neighbor): Selects K nearest or similar data points using all the \n",
    "    # non-missing features. It takes the average of the selected data points to fill in the missing \n",
    "    # feature.\n",
    "    # These are Machine Learning techniques to impute missing values.\n",
    "    # KNN finds most similar points for imputing.\n",
    "    # MICE performs multiple regression for imputing. MICE is a very robust model for imputation.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of df to manipulate.\n",
    "    # The methods used in this function can modify the original object itself. So,\n",
    "    # here we apply the copy method setting deep = True\n",
    "    cleaned_df = df.copy(deep = True)\n",
    "    \n",
    "    subset_columns_list = [column_to_fill] # only the column indicated.\n",
    "    total_columns = 1 # keep the homogeneity with the previous function\n",
    "    \n",
    "    # Get the list of columns of the dataframe:\n",
    "    df_cols = list(cleaned_df.columns)\n",
    "    # Get the index j of the column_to_fill:\n",
    "    j = df_cols.index(column_to_fill)\n",
    "    print(f\"Filling missing values on column {column_to_fill}. This is the column with index {j} in the original dataframe.\\n\")\n",
    "\n",
    "    # Firstly, let's process the timestamp column and save it as x. \n",
    "    # That is because datetime objects cannot be directly applied to linear regressions and\n",
    "    # numeric procedure. We must firstly convert it to an integer scale capable of preserving\n",
    "    # the distance relationships.\n",
    "   \n",
    "    # Check if there is a timestamp_tag_column. If not, make the index the timestamp:\n",
    "    if (timestamp_tag_column is None):\n",
    "        \n",
    "        timestamp_tag_column = column_to_fill + \"_index\"\n",
    "        \n",
    "        # Create the x array\n",
    "        x = np.array(cleaned_df.index)\n",
    "        \n",
    "    else:\n",
    "        # Run only if there was a timestamp column originally.\n",
    "        # sort this dataframe by timestamp_tag_column and column_to_fill:\n",
    "        cleaned_df = cleaned_df.sort_values(by = [timestamp_tag_column, column_to_fill], ascending = [True, True])\n",
    "        # restart index:\n",
    "        cleaned_df = cleaned_df.reset_index(drop = True)\n",
    "        \n",
    "        # If timestamp_tag_column is an object, the user may be trying to pass a date as x. \n",
    "        # So, let's try to convert it to datetime:\n",
    "        if ((cleaned_df[timestamp_tag_column].dtype == 'O') | (cleaned_df[timestamp_tag_column].dtype == 'object')):\n",
    "\n",
    "            try:\n",
    "                cleaned_df[timestamp_tag_column] = (cleaned_df[timestamp_tag_column]).astype('datetime64[ns]')\n",
    "                        \n",
    "            except:\n",
    "                # Simply ignore it\n",
    "                pass\n",
    "        \n",
    "        ts_array = np.array(cleaned_df[timestamp_tag_column])\n",
    "        \n",
    "        # Check if the elements from array x are np.datetime64 objects. Pick the first\n",
    "        # element to check:\n",
    "        if (type(ts_array[0]) == np.datetime64):\n",
    "            # In this case, performing the linear regression directly in X will\n",
    "            # return an error. We must associate a sequential number to each time.\n",
    "            # to keep the distance between these integers the same as in the original sequence\n",
    "            # let's define a difference of 1 ns as 1. The 1st timestamp will be zero, and the\n",
    "            # addition of 1 ns will be an addition of 1 unit. So a timestamp recorded 10 ns\n",
    "            # after the time zero will have value 10. At the end, we divide every element by\n",
    "            # 10**9, to obtain the correspondent distance in seconds.\n",
    "                \n",
    "            # start a list for the associated integer timescale. Put the number zero,\n",
    "            # associated to the first timestamp:\n",
    "            int_timescale = [0]\n",
    "                \n",
    "            # loop through each element of the array x, starting from index 1:\n",
    "            for i in range(1, len(ts_array)):\n",
    "                    \n",
    "                # calculate the timedelta between x[i] and x[i-1]:\n",
    "                # The delta method from the Timedelta class converts the timedelta to\n",
    "                # nanoseconds, guaranteeing the internal compatibility:\n",
    "                timedelta = pd.Timedelta(ts_array[i] - ts_array[(i-1)]).delta\n",
    "                    \n",
    "                # Sum this timedelta (integer number of nanoseconds) to the\n",
    "                # previous element from int_timescale, and append the result to the list:\n",
    "                int_timescale.append((timedelta + int_timescale[(i-1)]))\n",
    "                \n",
    "            # Now convert the new scale (that preserves the distance between timestamps)\n",
    "            # to NumPy array:\n",
    "            int_timescale = np.array(int_timescale)\n",
    "            \n",
    "            # Divide by 10**9 to obtain the distances in seconds, reducing the order of\n",
    "            # magnitude of the integer numbers (the division is allowed for arrays).\n",
    "            # make it the timestamp array ts_array itself:\n",
    "            ts_array = int_timescale / (10**9)\n",
    "            # Now, reduce again the order of magnitude through division by (60*60)\n",
    "            # It will obtain the ts_array in hour:\n",
    "            ts_array = int_timescale / (60*60)\n",
    "            \n",
    "        # make x the ts_array itself:\n",
    "        x = ts_array\n",
    "    \n",
    "    column_data_type = cleaned_df[column_to_fill].dtype\n",
    "    \n",
    "    # Pre-process the column if it is categorical\n",
    "    if ((column_data_type == 'O') | (column_data_type == 'object')):\n",
    "        \n",
    "        # Ordinal encoding: let's associate integer sequential numbers to the categorical column\n",
    "        # to apply the advanced encoding techniques. Even though the one-hot encoding could perform\n",
    "        # the same task and would, in fact, better, since there may be no ordering relation, the\n",
    "        # ordinal encoding is simpler and more suitable for this particular task:\n",
    "        \n",
    "        # Create Ordinal encoder\n",
    "        ord_enc = OrdinalEncoder()\n",
    "        \n",
    "        # Select non-null values of the column in the dataframe:\n",
    "        series_on_df = cleaned_df[column_to_fill]\n",
    "        \n",
    "        # Reshape series_on_df to shape (-1, 1)\n",
    "        reshaped_vals = series_on_df.values.reshape(-1, 1)\n",
    "        \n",
    "        # Fit the ordinal encoder to the reshaped column_to_fill values:\n",
    "        encoded_vals = ord_enc.fit_transform(reshaped_vals)\n",
    "        \n",
    "        # Finally, store the values to non-null values of the column in dataframe\n",
    "        cleaned_df.iloc[:,j] = encoded_vals\n",
    "\n",
    "        # Max and minimum of the encoded range\n",
    "        max_encoded = max(encoded_vals)\n",
    "        min_encoded = min(encoded_vals)\n",
    "\n",
    "\n",
    "    # Start a list of imputations:\n",
    "    list_of_imputations = []\n",
    "    \n",
    "    subset_from_cleaned_df = cleaned_df.copy(deep = True)\n",
    "    subset_from_cleaned_df = subset_from_cleaned_df[subset_columns_list]\n",
    "\n",
    "    mean_imputer = SimpleImputer(strategy = 'mean')\n",
    "    list_of_imputations.append('mean_imputer')\n",
    "    \n",
    "    # Now, apply the fit_transform method from the imputer to fit it to the indicated column:\n",
    "    mean_imputer.fit(subset_from_cleaned_df)\n",
    "    # If you wanted to obtain constants for all columns, you should not specify a subset:\n",
    "    # imputer.fit_transform(cleaned_df)\n",
    "        \n",
    "    # create a column on the dataframe as 'mean_imputer':\n",
    "    cleaned_df['mean_imputer'] = mean_imputer.transform(subset_from_cleaned_df)\n",
    "        \n",
    "    # Create the median imputer:\n",
    "    median_imputer = SimpleImputer(strategy = 'median')\n",
    "    list_of_imputations.append('median_imputer')\n",
    "    median_imputer.fit(subset_from_cleaned_df)\n",
    "    cleaned_df['median_imputer'] = median_imputer.transform(subset_from_cleaned_df)\n",
    "    \n",
    "    # Create the mode imputer:\n",
    "    mode_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "    list_of_imputations.append('mode_imputer')\n",
    "    mode_imputer.fit(subset_from_cleaned_df)\n",
    "    cleaned_df['mode_imputer'] = mode_imputer.transform(subset_from_cleaned_df)\n",
    "    \n",
    "    # Create the constant value imputer:\n",
    "    if (test_value_to_fill is None):\n",
    "        test_value_to_fill = 0\n",
    "    \n",
    "    constant_imputer = SimpleImputer(strategy = 'constant', fill_value = test_value_to_fill)\n",
    "    list_of_imputations.append('constant_imputer')\n",
    "    constant_imputer.fit(subset_from_cleaned_df)\n",
    "    cleaned_df['constant_imputer'] = constant_imputer.transform(subset_from_cleaned_df)\n",
    "    \n",
    "    # Make the linear interpolation imputation:\n",
    "    linear_interpolation_df = cleaned_df[subset_columns_list].copy(deep = True)\n",
    "    linear_interpolation_df = linear_interpolation_df.interpolate(method = 'linear')\n",
    "    cleaned_df['linear_interpolation'] = linear_interpolation_df[column_to_fill]\n",
    "    list_of_imputations.append('linear_interpolation')\n",
    "        \n",
    "    # Interpolate 2-nd degree polynomial:\n",
    "    quadratic_interpolation_df = cleaned_df[subset_columns_list].copy(deep = True)\n",
    "    quadratic_interpolation_df = quadratic_interpolation_df.interpolate(method = 'polynomial', order = 2)\n",
    "    cleaned_df['quadratic_interpolation'] = quadratic_interpolation_df[column_to_fill]\n",
    "    list_of_imputations.append('quadratic_interpolation')\n",
    "        \n",
    "    # Interpolate 3-rd degree polynomial:\n",
    "    cubic_interpolation_df = cleaned_df[subset_columns_list].copy(deep = True)\n",
    "    cubic_interpolation_df = cubic_interpolation_df.interpolate(method = 'polynomial', order = 3)\n",
    "    cleaned_df['cubic_interpolation'] = cubic_interpolation_df[column_to_fill]\n",
    "    list_of_imputations.append('cubic_interpolation')\n",
    "    \n",
    "    # Nearest interpolation\n",
    "    # Similar to bfill and ffill, but uses the nearest\n",
    "    nearest_interpolation_df = cleaned_df[subset_columns_list].copy(deep = True)\n",
    "    nearest_interpolation_df = nearest_interpolation_df.interpolate(method = 'nearest')\n",
    "    cleaned_df['nearest_interpolation'] = nearest_interpolation_df[column_to_fill]\n",
    "    list_of_imputations.append('nearest_interpolation')\n",
    "    \n",
    "    # bfill and ffill:\n",
    "    bfill_df = cleaned_df[subset_columns_list].copy(deep = True)\n",
    "    ffill_df = cleaned_df[subset_columns_list].copy(deep = True)\n",
    "    \n",
    "    bfill_df = bfill_df.fillna(method = 'bfill')\n",
    "    cleaned_df['bfill_imputation'] = bfill_df[column_to_fill]\n",
    "    list_of_imputations.append('bfill_imputation')\n",
    "    \n",
    "    ffill_df = ffill_df.fillna(method = 'ffill')\n",
    "    cleaned_df['ffill_imputation'] = ffill_df[column_to_fill]\n",
    "    list_of_imputations.append('ffill_imputation')\n",
    "    \n",
    "    \n",
    "    # Now, we can go to the advanced machine learning techniques:\n",
    "    \n",
    "    # KNN Imputer:\n",
    "    # Initialize KNN\n",
    "    knn_imputer = KNN()\n",
    "    list_of_imputations.append('knn_imputer')\n",
    "    cleaned_df['knn_imputer'] = knn_imputer.fit_transform(subset_from_cleaned_df)\n",
    "    \n",
    "    # Initialize IterativeImputer\n",
    "    mice_imputer = IterativeImputer()\n",
    "    list_of_imputations.append('mice_imputer')\n",
    "    cleaned_df['mice_imputer'] = mice_imputer.fit_transform(subset_from_cleaned_df)\n",
    "    \n",
    "    # Now, let's create linear regressions for compare the performance of different\n",
    "    # imputation strategies.\n",
    "    # Firstly, start a dictionary to store\n",
    "    \n",
    "    imputation_performance_dict = {}\n",
    "\n",
    "    # Now, loop through each imputation and calculate the adjusted R²:\n",
    "    for imputation in list_of_imputations:\n",
    "        \n",
    "        y = cleaned_df[imputation]\n",
    "        \n",
    "        # fit the linear regression\n",
    "        slope, intercept, r, p, se = linregress(x, y)\n",
    "        \n",
    "        # Get the adjusted R² and add it as the key imputation of the dictionary:\n",
    "        imputation_performance_dict[imputation] = r**2\n",
    "    \n",
    "    # Select best R-squared\n",
    "    best_imputation = max(imputation_performance_dict, key = imputation_performance_dict.get)\n",
    "    print(f\"The best imputation strategy for the column {column_to_fill} is {best_imputation}.\\n\")\n",
    "    \n",
    "    \n",
    "    if (show_imputation_comparison_plots & ((column_data_type != 'O') & (column_data_type != 'object'))):\n",
    "        \n",
    "        # Firstly, converts the values obtained to closest integer (since we\n",
    "        # encoded the categorical values as integers, we cannot reconvert\n",
    "        # decimals):)): # run if it is True\n",
    "    \n",
    "        print(\"Check the Kernel density estimate (KDE) plot for the different imputations.\\n\")\n",
    "        labels_list = ['baseline\\ncomplete_case']\n",
    "        y = cleaned_df[column_to_fill]\n",
    "        X = cleaned_df[timestamp_tag_column] # not the converted scale\n",
    "\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "        \n",
    "        # Plot graphs of imputed DataFrames and the complete case\n",
    "        y.plot(kind = 'kde', c = 'red', linewidth = 3)\n",
    "\n",
    "        for imputation in list_of_imputations:\n",
    "            \n",
    "            labels_list.append(imputation)\n",
    "            y = cleaned_df[imputation]\n",
    "            y.plot(kind = 'kde')\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = 0)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = 0)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        ax.set_title(\"Kernel_density_estimate_plot_for_each_imputation\")\n",
    "        ax.set_xlabel(column_to_fill)\n",
    "        ax.set_ylabel(\"density\")\n",
    "\n",
    "        ax.grid(True) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(f\"Now, check the original time series compared with the values obtained through {best_imputation}:\\n\")\n",
    "        \n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "        \n",
    "        # Plot the imputed DataFrame in red dotted style\n",
    "        selected_imputation = cleaned_df[best_imputation]\n",
    "        ax.plot(X, selected_imputation, color = 'red', marker = 'o', linestyle = 'dotted', label = best_imputation)\n",
    "        \n",
    "        # Plot the original DataFrame with title\n",
    "        # Put a degree of transparency (35%) to highlight the imputation.\n",
    "        ax.plot(X, y, color = 'darkblue', alpha = 0.65, linestyle = '-', marker = '', label = (column_to_fill + \"_original\"))\n",
    "        \n",
    "        plt.xticks(rotation = 70)\n",
    "        plt.yticks(rotation = 0)\n",
    "        ax.set_title(column_to_fill + \"_original_vs_imputations\")\n",
    "        ax.set_xlabel(timestamp_tag_column)\n",
    "        ax.set_ylabel(column_to_fill)\n",
    "\n",
    "        ax.grid(True) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "        plt.show()\n",
    "        print(\"\\n\")\n",
    "    \n",
    "                \n",
    "    print(f\"Returning a dataframe where {best_imputation} strategy was used for filling missing values in {column_to_fill} column.\\n\")\n",
    "     \n",
    "    if (best_imputation == 'mice_imputer'):\n",
    "        print(\"MICE = Multiple Imputations by Chained Equations\")\n",
    "        print(\"MICE: Performs multiple regressions over random samples of the data.\")\n",
    "        print(\"It takes the average of multiple regression values and imputes the missing feature value for the data point.\")\n",
    "        print(\"It is a Machine Learning technique to impute missing values.\")\n",
    "        print(\"MICE performs multiple regression for imputing and is a very robust model for imputation.\\n\")\n",
    "    \n",
    "    elif (best_imputation == 'knn_imputer'):\n",
    "        print(\"KNN = K-Nearest Neighbor\")\n",
    "        print(\"KNN selects K nearest or similar data points using all the non-missing features.\")\n",
    "        print(\"It takes the average of the selected data points to fill in the missing feature.\")\n",
    "        print(\"It is a Machine Learning technique to impute missing values.\")\n",
    "        print(\"KNN finds most similar points for imputing.\\n\")\n",
    "    \n",
    "    # Make all rows from the column j equals to the selected imputer:\n",
    "    cleaned_df.iloc[:, j] = cleaned_df[best_imputation]\n",
    "    # If you wanted to make all rows from all columns equal to the imputer, you should declare:\n",
    "    # cleaned_df.iloc[:, :] = imputer\n",
    "    \n",
    "    # Drop all the columns created for storing different imputers:\n",
    "    # These columns were saved in the list list_of_imputations.\n",
    "    # Notice that the selected imputations were saved in the original column.\n",
    "    cleaned_df = cleaned_df.drop(columns = list_of_imputations)\n",
    "    \n",
    "    # Finally, let's reverse the ordinal encoding used in the beginning of the code to process object\n",
    "    # columns:\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (column_data_type not in numeric_dtypes):\n",
    "        \n",
    "        # Firstly, converts the values obtained to closest integer (since we\n",
    "        # encoded the categorical values as integers, we cannot reconvert\n",
    "        # decimals):\n",
    "        \n",
    "        cleaned_df[column_to_fill] = (np.rint(cleaned_df[column_to_fill]))\n",
    "        \n",
    "        # If a value is above the max_encoded, make it equals to the maximum.\n",
    "        # If it is below the minimum, make it equals to the minimum:\n",
    "        for k in range(0, len(cleaned_df)):\n",
    "\n",
    "            if (cleaned_df.iloc[k,j] > max_encoded):\n",
    "                cleaned_df.iloc[k,j] = max_encoded\n",
    "\n",
    "            elif (cleaned_df.iloc[k,j] < min_encoded):\n",
    "                cleaned_df.iloc[k,j] = min_encoded\n",
    "\n",
    "        new_series = cleaned_df[column_to_fill]\n",
    "        # We must use the int function to guarantee that the column_to_fill will store an\n",
    "        # integer number (we cannot have a fraction of an encoding).\n",
    "        # The int function guarantees that the variable will be stored as an integer.\n",
    "        # The numpy.rint(a) function rounds elements of the array to the nearest integer.\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        # For values exactly halfway between rounded decimal values, \n",
    "        # NumPy rounds to the nearest even value. \n",
    "        # Thus 1.5 and 2.5 round to 2.0; -0.5 and 0.5 round to 0.0; etc.\n",
    "        \n",
    "        # Reshape series_not_null to shape (-1, 1)\n",
    "        reshaped_vals = new_series.values.reshape(-1, 1)\n",
    "        \n",
    "        # Perform inverse transform of the ordinally encoded columns\n",
    "        cleaned_df[column_to_fill] = ord_enc.inverse_transform(reshaped_vals)\n",
    "\n",
    "\n",
    "    print(\"Check the 10 first rows from the cleaned dataframe:\\n\")\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(cleaned_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(cleaned_df.head(10))\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f7b7f842-98be-49ef-ad19-c149e432343a"
   },
   "source": [
    "# **Function for obtaining the correlation plot**\n",
    "- The Pandas method dataset.corr() calculates the Pearson's correlation coefficients R.\n",
    "- Pearson's correlation coefficients R go from -1 to 1.\n",
    "- These coefficients are R, not R².\n",
    "\n",
    "#### To obtain the coefficients R², we raise the results to the 2nd power, i.e., we calculate (dataset.corr())**2\n",
    "- R² goes from 0 to 1, where 1 represents the perfect correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2927e53d-4c3c-4ef3-9ec1-d03f003e7d2e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def correlation_plot (df, show_masked_plot = True, responses_to_return_corr = None, set_returned_limit = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    #show_masked_plot = True - keep as True if you want to see a cleaned version of the plot\n",
    "    # where a mask is applied.\n",
    "    \n",
    "    #responses_to_return_corr - keep as None to return the full correlation tensor.\n",
    "    # If you want to display the correlations for a particular group of features, input them\n",
    "    # as a list, even if this list contains a single element. Examples:\n",
    "    # responses_to_return_corr = ['response1'] for a single response\n",
    "    # responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "    # responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "    # of a column of the dataset that represents a response variable.\n",
    "    # WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "    # of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "    \n",
    "    # set_returned_limit = None - This variable will only present effects in case you have\n",
    "    # provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "    # to return all of the correlation coefficients; or, alternatively, \n",
    "    # provide an integer number to limit the total of coefficients returned. \n",
    "    # e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "    \n",
    "    # set a local copy of the dataset to perform the calculations:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    correlation_matrix = DATASET.corr(method = 'pearson')\n",
    "    \n",
    "    if (show_masked_plot == False):\n",
    "        #Show standard plot\n",
    "        \n",
    "        plt.figure(figsize = (12, 8))\n",
    "        sns.heatmap((correlation_matrix)**2, annot = True, fmt = \".2f\")\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    #Oncee the pandas method .corr() calculates R, we raised it to the second power \n",
    "    # to obtain R². R² goes from zero to 1, where 1 represents the perfect correlation.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Show masked (cleaner) plot instead of the standard one\n",
    "        # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize = (12, 8))\n",
    "        # Mask for the upper triangle\n",
    "        mask = np.zeros_like((correlation_matrix)**2)\n",
    "\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        # Generate a custom diverging colormap\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "\n",
    "        # Heatmap with mask and correct aspect ratio\n",
    "        sns.heatmap(((correlation_matrix)**2), mask = mask, cmap = cmap, center = 0,\n",
    "                    linewidths = .5)\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        #Again, the method dataset.corr() calculates R within the variables of dataset.\n",
    "        #To calculate R², we simply raise it to the second power: (dataset.corr()**2)\n",
    "    \n",
    "    #Sort the values of correlation_matrix in Descending order:\n",
    "    \n",
    "    if (responses_to_return_corr is not None):\n",
    "        \n",
    "        if (type(responses_to_return_corr) == str):\n",
    "            # If a string was input, put it inside a list\n",
    "            responses_to_return_corr = [responses_to_return_corr]\n",
    "        \n",
    "        #Select only the desired responses, by passing the list responses_to_return_corr\n",
    "        # as parameter for column filtering:\n",
    "        correlation_matrix = correlation_matrix[responses_to_return_corr]\n",
    "        # By passing a list as argument, we assure that the output is a dataframe\n",
    "        # and not a series, even if the list contains a single element.\n",
    "        \n",
    "        # Create a list of boolean variables == False, one False correspondent to\n",
    "        # each one of the responses\n",
    "        ascending_modes = [False for i in range(0, len(responses_to_return_corr))]\n",
    "        \n",
    "        #Now sort the values according to the responses, by passing the list\n",
    "        # response\n",
    "        correlation_matrix = correlation_matrix.sort_values(by = responses_to_return_corr, ascending = ascending_modes)\n",
    "        \n",
    "        # If a limit of coefficients was determined, apply it:\n",
    "        if (set_returned_limit is not None):\n",
    "                \n",
    "                correlation_matrix = correlation_matrix.head(set_returned_limit)\n",
    "                #Pandas .head(X) method returns the first X rows of the dataframe.\n",
    "                # Here, it returns the defined limit of coefficients, set_returned_limit.\n",
    "                # The default .head() is X = 5.\n",
    "    \n",
    "    print(\"ATTENTION: The correlation plots show the linear correlations R², which go from 0 (none correlation) to 1 (perfect correlation). Obviously, the main diagonal always shows R² = 1, since the data is perfectly correlated to itself.\\n\")\n",
    "    print(\"The returned correlation matrix, on the other hand, presents the linear coefficients of correlation R, not R². R values go from -1 (perfect negative correlation) to 1 (perfect positive correlation).\\n\")\n",
    "    print(\"None of these coefficients take non-linear relations and the presence of a multiple linear correlation in account. For these cases, it is necessary to calculate R² adjusted, which takes in account the presence of multiple preditors and non-linearities.\\n\")\n",
    "    \n",
    "    print(\"Correlation matrix - numeric results:\\n\")\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(correlation_matrix)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(correlation_matrix)\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "abf89077-4311-4fbe-9c4c-3162799cf24b"
   },
   "source": [
    "# **Function for plotting the bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`.\n",
    "- For obtaining the **data distribution of categorical variables**, select any numeric column as the response, and set `aggregate_function = 'count'`. You can also set `plot_cumulative_percent = True` to compare the frequencies of each possible value.\n",
    "\n",
    "### Use this function for obtaining the statistical distributions for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "azdata_cell_guid": "7d7b4bed-c51a-4875-a443-a5e9a6039e6e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def bar_chart (df, categorical_var_name, response_var_name, aggregate_function = 'sum', add_suffix_to_aggregated_col = True, suffix = None, calculate_and_plot_cumulative_percent = True, orientation = 'vertical', limit_of_plotted_categories = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy import stats\n",
    "    \n",
    "    # df: dataframe being analyzed\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name \n",
    "    # of the column to be analyzed. e.g. \n",
    "    # categorical_var_name = \"column1\"\n",
    "    \n",
    "    # response_var_name: string (inside quotes) containing the name \n",
    "    # of the column that stores the response correspondent to the\n",
    "    # categories. e.g. response_var_name = \"response_feature\" \n",
    "    \n",
    "    # aggregate_function = 'sum': String defining the aggregation \n",
    "    # method that will be applied. Possible values:\n",
    "    # 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "    # 'standard_deviation', '10_percent_quantile', '20_percent_quantile',\n",
    "    # '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "    # '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "    # '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "    # '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range',\n",
    "    # 'mean_standard_error', 'entropy'\n",
    "    # To use another aggregate function, you can use the .agg method, passing \n",
    "    # the aggregate as argument, such as in:\n",
    "    # .agg(scipy.stats.mode), \n",
    "    # where the argument is a Scipy aggregate function.\n",
    "    # If None or an invalid function is input, 'sum' will be used.\n",
    "    \n",
    "    # add_suffix_to_aggregated_col = True will add a suffix to the\n",
    "    # aggregated column. e.g. 'responseVar_mean'. If add_suffix_to_aggregated_col \n",
    "    # = False, the aggregated column will have the original column name.\n",
    "    \n",
    "    # suffix = None. Keep it None if no suffix should be added, or if\n",
    "    # the name of the aggregate function should be used as suffix, after\n",
    "    # \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "    # \"_\" sign in the beginning of this string to separate the suffix from\n",
    "    # the original column name. e.g. if the response variable is 'Y' and\n",
    "    # suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "    \n",
    "    # calculate_and_plot_cumulative_percent = True to calculate and plot\n",
    "    # the line of cumulative percent, or \n",
    "    # calculate_and_plot_cumulative_percent = False to omit it.\n",
    "    # This feature is only shown when aggregate_function = 'sum', 'median',\n",
    "    # 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "    # another aggregate is selected.\n",
    "    \n",
    "    # orientation = 'vertical' is the standard, and plots vertical bars\n",
    "    # (perpendicular to the X axis). In this case, the categories are shown\n",
    "    # in the X axis, and the correspondent responses are in Y axis.\n",
    "    # Alternatively, orientation = 'horizontal' results in horizontal bars.\n",
    "    # In this case, categories are in Y axis, and responses in X axis.\n",
    "    # If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "    \n",
    "    # Note: to obtain a Pareto chart, keep aggregate_function = 'sum',\n",
    "    # plot_cumulative_percent = True, and orientation = 'vertical'.\n",
    "    \n",
    "    # limit_of_plotted_categories: integer value that represents\n",
    "    # the maximum of categories that will be plot. Keep it None to plot\n",
    "    # all categories. Alternatively, set an integer value. e.g.: if\n",
    "    # limit_of_plotted_categories = 4, but there are more categories,\n",
    "    # the dataset will be sorted in descending order and: 1) The remaining\n",
    "    # categories will be sum in a new category named 'others' if the\n",
    "    # aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "    # omitted from the plot, for other aggregate functions. Notice that\n",
    "    # it limits only the variables in the plot: all of them will be\n",
    "    # returned in the dataframe.\n",
    "    # Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "    # columns will be aggregated as 'others' even if there is a single column\n",
    "    # beyond the limit.\n",
    "    \n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Before calling the method, we must guarantee that the variables may be\n",
    "    # used for that aggregate. Some aggregations are permitted only for numeric variables, so calling\n",
    "    # the methods before selecting the variables may raise warnings or errors.\n",
    "    \n",
    "    \n",
    "    list_of_aggregates = ['median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "                          'standard_deviation', '10_percent_quantile', '20_percent_quantile', \n",
    "                          '25_percent_quantile', '30_percent_quantile', '40_percent_quantile', \n",
    "                          '50_percent_quantile', '60_percent_quantile', '70_percent_quantile', \n",
    "                          '75_percent_quantile', '80_percent_quantile', '90_percent_quantile', \n",
    "                          '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range', \n",
    "                          'mean_standard_error', 'entropy']\n",
    "    \n",
    "    list_of_numeric_aggregates = ['median', 'mean', 'sum', 'min', 'max', 'variance',\n",
    "                                  'standard_deviation', '10_percent_quantile', '20_percent_quantile', \n",
    "                                  '25_percent_quantile', '30_percent_quantile', '40_percent_quantile', \n",
    "                                  '50_percent_quantile', '60_percent_quantile', '70_percent_quantile', \n",
    "                                  '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "                                  '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range', \n",
    "                                  'mean_standard_error']\n",
    "    \n",
    "    # Check if an invalid or no aggregation function was selected:\n",
    "    if ((aggregate_function not in (list_of_aggregates)) | (aggregate_function is None)):\n",
    "        \n",
    "        aggregate_function = 'sum'\n",
    "        print(\"Invalid or no aggregation function input, so using the default \\'sum\\'.\\n\")\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    # Check if a numeric aggregate was selected:\n",
    "    if (aggregate_function in list_of_numeric_aggregates):\n",
    "        \n",
    "        column_data_type = DATASET[response_var_name].dtype\n",
    "        \n",
    "        if (column_data_type not in numeric_dtypes):\n",
    "            \n",
    "                # If the Pandas series was defined as an object, it means it is categorical\n",
    "                # (string, date, etc).\n",
    "                print(\"Numeric aggregate selected, but categorical variable indicated as response variable.\")\n",
    "                print(\"Setting aggregate_function = \\'mode\\', to make aggregate compatible with data type.\\n\")\n",
    "                \n",
    "                aggregate_function = 'mode'\n",
    "    \n",
    "    else: # categorical aggregate function\n",
    "        \n",
    "        column_data_type = DATASET[response_var_name].dtype\n",
    "        \n",
    "        if ((column_data_type in numeric_dtypes) & (aggregate_function != 'count')):\n",
    "                # count is the only aggregate for categorical that can be used for numerical variables as well.\n",
    "                \n",
    "                print(\"Categorical aggregate selected, but numeric variable indicated as response variable.\")\n",
    "                print(\"Setting aggregate_function = \\'sum\\', to make aggregate compatible with data type.\\n\")\n",
    "                \n",
    "                aggregate_function = 'sum'\n",
    "    \n",
    "    # Before grouping, let's remove the missing values, avoiding the raising of TypeError.\n",
    "    # Pandas deprecated the automatic dropna with aggregation:\n",
    "    DATASET = DATASET.dropna(axis = 0)\n",
    "    \n",
    "    # Convert categorical_var_name to Pandas 'category' type. If the variable is represented by\n",
    "    # a number, the dataframe will be grouped in terms of an aggregation of the variable, instead\n",
    "    # of as a category. It will prevents this to happen:\n",
    "    DATASET[categorical_var_name] = DATASET[categorical_var_name].astype(\"category\")    \n",
    "    \n",
    "    # If an aggregate function different from 'sum', 'mean', 'median' or 'mode' \n",
    "    # is used with plot_cumulative_percent = True, \n",
    "    # set plot_cumulative_percent = False:\n",
    "    # (check if aggregate function is not in the list of allowed values):\n",
    "    if ((aggregate_function not in ['sum', 'mean', 'median', 'mode', 'count']) & (calculate_and_plot_cumulative_percent == True)):\n",
    "        \n",
    "        calculate_and_plot_cumulative_percent = False\n",
    "        print(\"The cumulative percent is only calculated when aggregate_function = \\'sum\\', \\'mean\\', \\'median\\', \\'mode\\', or \\'count\\'. So, plot_cumulative_percent was set as False.\")\n",
    "    \n",
    "    # Guarantee that the columns from the aggregated dataset have the correct names\n",
    "    \n",
    "    # Groupby according to the selection.\n",
    "    # Here, there is a great gain of performance in not using a dictionary of methods:\n",
    "    # If using a dictionary of methods, Pandas would calculate the results for each one of the methods.\n",
    "    \n",
    "    # Pandas groupby method documentation:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?msclkid=7b3531a6cff211ec9086f4edaddb94ba\n",
    "    # argument as_index = False: prevents the grouper variable to be set as index of the new dataframe.\n",
    "    # (default: as_index = True);\n",
    "    # dropna = False: do not removes the missing values (default: dropna = True, used here to avoid\n",
    "    # compatibility and version issues)\n",
    "    \n",
    "    if (aggregate_function == 'median'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg('median')\n",
    "\n",
    "    elif (aggregate_function == 'mean'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].mean()\n",
    "    \n",
    "    elif (aggregate_function == 'mode'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.mode)\n",
    "    \n",
    "    elif (aggregate_function == 'sum'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].sum()\n",
    "    \n",
    "    elif (aggregate_function == 'count'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].count()\n",
    "\n",
    "    elif (aggregate_function == 'min'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].min()\n",
    "    \n",
    "    elif (aggregate_function == 'max'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].max()\n",
    "    \n",
    "    elif (aggregate_function == 'variance'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].var()\n",
    "\n",
    "    elif (aggregate_function == 'standard_deviation'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].std()\n",
    "    \n",
    "    elif (aggregate_function == '10_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.10)\n",
    "    \n",
    "    elif (aggregate_function == '20_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.20)\n",
    "    \n",
    "    elif (aggregate_function == '25_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.25)\n",
    "    \n",
    "    elif (aggregate_function == '30_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.30)\n",
    "    \n",
    "    elif (aggregate_function == '40_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.40)\n",
    "    \n",
    "    elif (aggregate_function == '50_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.50)\n",
    "\n",
    "    elif (aggregate_function == '60_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.60)\n",
    "    \n",
    "    elif (aggregate_function == '70_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.30)\n",
    "\n",
    "    elif (aggregate_function == '75_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.75)\n",
    "\n",
    "    elif (aggregate_function == '80_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.80)\n",
    "    \n",
    "    elif (aggregate_function == '90_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.90)\n",
    "    \n",
    "    elif (aggregate_function == '95_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.95)\n",
    "\n",
    "    elif (aggregate_function == 'kurtosis'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.kurtosis)\n",
    "    \n",
    "    elif (aggregate_function == 'skew'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.skew)\n",
    "\n",
    "    elif (aggregate_function == 'interquartile_range'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.iqr)\n",
    "    \n",
    "    elif (aggregate_function == 'mean_standard_error'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.sem)\n",
    "    \n",
    "    else: # entropy\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.entropy)\n",
    "\n",
    "    \n",
    "    # List of columns of the aggregated dataset:\n",
    "    list_of_columns = list(DATASET.columns) # convert to a list\n",
    "    \n",
    "    if (add_suffix_to_aggregated_col == True):\n",
    "            \n",
    "        if (suffix is None):\n",
    "                \n",
    "            suffix = \"_\" + aggregate_function\n",
    "            \n",
    "        new_columns = [(str(name) + suffix) for name in list_of_columns]\n",
    "        # Do not consider the first element, which is the aggregate function with a suffix.\n",
    "        # Concatenate the correct name with the columns from the second element of the list:\n",
    "        new_columns = [categorical_var_name] + new_columns[1:]\n",
    "        # Make it the new columns:\n",
    "        DATASET.columns = new_columns\n",
    "        # Update the list of columns:\n",
    "        list_of_columns = DATASET.columns\n",
    "    \n",
    "    if (aggregate_function == 'mode'):\n",
    "        \n",
    "        # The columns was saved as a series of Tuples. Each row contains a tuple like:\n",
    "        # ([calculated_mode], [counting_of_occurrences]). We want only the calculated mode.\n",
    "        # On the other hand, if we do column[0], we will get the columns first row. So, we have to\n",
    "        # go through each column, retrieving only the mode:\n",
    "        \n",
    "        # Loop through each column:\n",
    "        for column in list_of_columns:\n",
    "            \n",
    "            # Save the series as a list:\n",
    "            list_of_modes_arrays = list(DATASET[column])\n",
    "            # Start a list of modes:\n",
    "            list_of_modes = []\n",
    "            \n",
    "            # Loop through each element from the list of arrays:\n",
    "            for mode_array in list_of_modes_arrays:\n",
    "                # mode array is like:\n",
    "                # ModeResult(mode=array([calculated_mode]), count=array([counting_of_occurrences]))\n",
    "                # To retrieve only the mode, we must access the element [0][0] from this array:\n",
    "                try:\n",
    "                    list_of_modes.append(mode_array[0][0])\n",
    "                \n",
    "                except IndexError:\n",
    "                    # This error is generated when trying to access an array storing no values.\n",
    "                    # (i.e., with missing values). Since there is no dimension, it is not possible\n",
    "                    # to access the [0][0] position. In this case, simply append the np.nan \n",
    "                    # the (missing value):\n",
    "                    list_of_modes.append(np.nan)\n",
    "            \n",
    "            # Make the list of modes the column itself:\n",
    "            DATASET[column] = list_of_modes\n",
    "    \n",
    "            \n",
    "    # the name of the response variable is now the second element from the list of column:\n",
    "    response_var_name = list(DATASET.columns)[1]\n",
    "    # the categorical variable name was not changed.\n",
    "    \n",
    "    # Let's sort the dataframe.\n",
    "    \n",
    "    # Order the dataframe in descending order by the response.\n",
    "    # If there are equal responses, order them by category, in\n",
    "    # ascending order; put the missing values in the first position\n",
    "    # To pass multiple columns and multiple types of ordering, we use\n",
    "    # lists. If there was a single column to order by, we would declare\n",
    "    # it as a string. If only one order of ascending was used, we would\n",
    "    # declare it as a simple boolean\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    \n",
    "    DATASET = DATASET.sort_values(by = [response_var_name, categorical_var_name], ascending = [False, True], na_position = 'first')\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    if (aggregate_function == 'count'):\n",
    "        \n",
    "        # Here, the column represents the counting, no matter the variable set as response.\n",
    "        DATASET.columns = [categorical_var_name, 'count_of_entries']\n",
    "        response_var_name = 'count_of_entries'\n",
    "    \n",
    "    # plot_cumulative_percent = True, create a column to store the\n",
    "    # cumulative percent:\n",
    "    if (calculate_and_plot_cumulative_percent): \n",
    "        # Run the following code if the boolean value is True (implicity)\n",
    "        # Only calculates cumulative percent in case aggregate is 'sum' or 'mode'\n",
    "        \n",
    "        # Create a column series for the cumulative sum:\n",
    "        cumsum_col = response_var_name + \"_cumsum\"\n",
    "        DATASET[cumsum_col] = DATASET[response_var_name].cumsum()\n",
    "        \n",
    "        # total sum is the last element from this series\n",
    "        # (i.e. the element with index len(DATASET) - 1)\n",
    "        total_sum = DATASET[cumsum_col][(len(DATASET) - 1)]\n",
    "        \n",
    "        # Now, create a column for the accumulated percent\n",
    "        # by dividing cumsum_col by total_sum and multiplying it by\n",
    "        # 100 (%):\n",
    "        cum_pct_col = response_var_name + \"_cum_pct\"\n",
    "        DATASET[cum_pct_col] = (DATASET[cumsum_col])/(total_sum) * 100\n",
    "        print(f\"Successfully calculated cumulative sum and cumulative percent correspondent to the response variable {response_var_name}.\")\n",
    "    \n",
    "    print(\"Successfully aggregated and ordered the dataset to plot. Check the 10 first rows of this returned dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    # Check if the total of plotted categories is limited:\n",
    "    if not (limit_of_plotted_categories is None):\n",
    "        \n",
    "        # Since the value is not None, we have to limit it\n",
    "        # Check if the limit is lower than or equal to the length of the dataframe.\n",
    "        # If it is, we simply copy the columns to the series (there is no need of\n",
    "        # a memory-consuming loop or of applying the head method to a local copy\n",
    "        # of the dataframe):\n",
    "        df_length = len(DATASET)\n",
    "            \n",
    "        if (df_length <= limit_of_plotted_categories):\n",
    "            # Simply copy the columns to the graphic series:\n",
    "            categories = DATASET[categorical_var_name]\n",
    "            responses = DATASET[response_var_name]\n",
    "            # If there is a cum_pct column, copy it to a series too:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = DATASET[cum_pct_col]\n",
    "        \n",
    "        else:\n",
    "            # The limit is lower than the total of categories,\n",
    "            # so we actually have to limit the size of plotted df:\n",
    "        \n",
    "            # If aggregate_function is not 'sum', we simply apply\n",
    "            # the head method to obtain the first rows (number of\n",
    "            # rows input as parameter; if no parameter is input, the\n",
    "            # number of 5 rows is used):\n",
    "            \n",
    "            # Limit to the number limit_of_plotted_categories:\n",
    "            # create another local copy of the dataframe not to\n",
    "            # modify the returned dataframe object:\n",
    "            plotted_df = DATASET.copy(deep = True).head(limit_of_plotted_categories)\n",
    "\n",
    "            # Create the series of elements to plot:\n",
    "            categories = list(plotted_df[categorical_var_name])\n",
    "            responses = list(plotted_df[response_var_name])\n",
    "            # If the cumulative percent was obtained, create the series for it:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = list(plotted_df[cum_pct_col])\n",
    "            \n",
    "            # Start variable to store the aggregates from the others:\n",
    "            other_responses = 0\n",
    "            \n",
    "            # Loop through each row from DATASET:\n",
    "            for i in range(0, len(DATASET)):\n",
    "                \n",
    "                # Check if the category is not in categories:\n",
    "                category = DATASET[categorical_var_name][i]\n",
    "                \n",
    "                if (category not in categories):\n",
    "                    \n",
    "                    # sum the value in the response variable to other_responses:\n",
    "                    other_responses = other_responses + DATASET[response_var_name][i]\n",
    "            \n",
    "            # Now we finished the sum of the other responses, let's add these elements to\n",
    "            # the lists:\n",
    "            categories.append(\"others\")\n",
    "            responses.append(other_responses)\n",
    "            # If there is a cumulative percent, append 100% to the list:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct.append(100)\n",
    "                # The final cumulative percent must be the total, 100%\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Firstly, copy the elements that will be kept to x, y and (possibly) cum_pct\n",
    "                # lists.\n",
    "                # Start the lists:\n",
    "                categories = []\n",
    "                responses = []\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = [] # start this list only if its needed to save memory\n",
    "\n",
    "                for i in range (0, limit_of_plotted_categories):\n",
    "                    # i goes from 0 (first index) to limit_of_plotted_categories - 1\n",
    "                    # (index of the last category to be kept):\n",
    "                    # copy the elements from the DATASET to the list\n",
    "                    # category is the 1st column (column 0); response is the 2nd (col 1);\n",
    "                    # and cumulative percent is the 4th (col 3):\n",
    "                    categories.append(DATASET.iloc[i, 0])\n",
    "                    responses.append(DATASET.iloc[i, 1])\n",
    "                    \n",
    "                    if (calculate_and_plot_cumulative_percent):\n",
    "                        cum_pct.append(DATASET.iloc[i, 3]) # only if there is something to iloc\n",
    "                    \n",
    "                # Now, i = limit_of_plotted_categories - 1\n",
    "                # Create a variable to store the sum of other responses\n",
    "                other_responses = 0\n",
    "                # loop from i = limit_of_plotted_categories to i = df_length-1, index\n",
    "                # of the last element. Notice that this loop may have a single call, if there\n",
    "                # is only one element above the limit:\n",
    "                for i in range (limit_of_plotted_categories, (df_length - 1)):\n",
    "                    \n",
    "                    other_responses = other_responses + (DATASET.iloc[i, 1])\n",
    "                \n",
    "                # Now, add the last elements to the series:\n",
    "                # The last category is named 'others':\n",
    "                categories.append('others')\n",
    "                # The correspondent aggregated response is the value \n",
    "                # stored in other_responses:\n",
    "                responses.append(other_responses)\n",
    "                # The cumulative percent is 100%, since this must be the sum of all\n",
    "                # elements (the previous ones plus the ones aggregated as 'others'\n",
    "                # must totalize 100%).\n",
    "                # On the other hand, the cumulative percent is stored only if needed:\n",
    "                cum_pct.append(100)\n",
    "    \n",
    "    else:\n",
    "        # This is the situation where there is no limit of plotted categories. So, we\n",
    "        # simply copy the columns to the plotted series (it is equivalent to the \n",
    "        # situation where there is a limit, but the limit is equal or inferior to the\n",
    "        # size of the dataframe):\n",
    "        categories = DATASET[categorical_var_name]\n",
    "        responses = DATASET[response_var_name]\n",
    "        # If there is a cum_pct column, copy it to a series too:\n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            cum_pct = DATASET[cum_pct_col]\n",
    "    \n",
    "    \n",
    "    # Now the data is prepared and we only have to plot \n",
    "    # categories, responses, and cum_pct:\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    # Set labels and titles for the case they are None\n",
    "    if (plot_title is None):\n",
    "        \n",
    "        if (aggregate_function == 'count'):\n",
    "            # The graph is the same count, no matter the response\n",
    "            plot_title = f\"Bar_chart_count_of_{categorical_var_name}\"\n",
    "        \n",
    "        else:\n",
    "            plot_title = f\"Bar_chart_for_{response_var_name}_by_{categorical_var_name}\"\n",
    "    \n",
    "    if (horizontal_axis_title is None):\n",
    "\n",
    "        horizontal_axis_title = categorical_var_name\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Notice that response_var_name already has the suffix indicating the\n",
    "        # aggregation function\n",
    "        vertical_axis_title = response_var_name\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize = (12, 8))\n",
    "    # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "\n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    plt.title(plot_title)\n",
    "    \n",
    "    if (orientation == 'horizontal'):\n",
    "        \n",
    "        # invert the axes in relation to the default (vertical, below)\n",
    "        ax1.set_ylabel(horizontal_axis_title)\n",
    "        ax1.set_xlabel(vertical_axis_title, color = 'darkblue')\n",
    "        \n",
    "        # Horizontal bars used - barh method (bar horizontal):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "        # Now, the categorical variables stored in series categories must be\n",
    "        # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "        # must be in the horizontal axis X.\n",
    "        ax1.barh(categories, responses, color = 'darkblue', alpha = OPACITY, label = categorical_var_name)\n",
    "        #.barh(y, x, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            # for the vertical orientation, we use the twinx. Here, we use twiny\n",
    "            ax2 = ax1.twiny()\n",
    "            # Here, the x axis must be the cum_pct value, and the Y\n",
    "            # axis must be categories (it must be correspondent to the\n",
    "            # bar chart)\n",
    "            ax2.plot(cum_pct, categories, '-ro', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('x', color = 'red')\n",
    "            ax2.set_xlabel(\"Cumulative Percent (%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        ax1.set_xlabel(horizontal_axis_title)\n",
    "        ax1.set_ylabel(vertical_axis_title, color = 'darkblue')\n",
    "        # If None or an invalid orientation was used, set it as vertical\n",
    "        # Use Matplotlib standard bar method (vertical bar):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "        \n",
    "        # In this standard case, the categorical variables (categories) are positioned\n",
    "        # as X, and the responses as Y:\n",
    "        ax1.bar(categories, responses, color = 'darkblue', alpha = OPACITY, label = categorical_var_name)\n",
    "        #.bar(x, y, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(categories, cum_pct, '-ro', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('y', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (%)\", color = 'red', rotation = 270)\n",
    "            # rotate the twin axis so that its label is inverted in relation to the main\n",
    "            # vertical axis.\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "    \n",
    "    # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "    # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "    # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "    # constant line).\n",
    "    \n",
    "    # Now the parameters specific to the configurations are finished, so we can go back\n",
    "    # to the general code:\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"bar_chart\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0cd5dc42-50f4-46dc-9eef-030dfc8c7060"
   },
   "source": [
    "# **Function for calculating cumulative statistics**\n",
    "- Cumulative sum (cumsum); cumulative product (cumprod); cumulative maximum (cummax); cumulative minimum (cummin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "azdata_cell_guid": "6487beb7-ab27-4e14-b006-16519990cd46",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def calculate_cumulative_stats (df, column_to_analyze, cumulative_statistic = 'sum', new_cum_stats_col_name = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # cumulative_statistic: the statistic that will be calculated. The cumulative\n",
    "    # statistics allowed are: 'sum' (for cumulative sum, cumsum); 'product' \n",
    "    # (for cumulative product, cumprod); 'max' (for cumulative maximum, cummax);\n",
    "    # and 'min' (for cumulative minimum, cummin).\n",
    "    \n",
    "    # new_cum_stats_col_name = None or string (inside quotes), \n",
    "    # containing the name of the new column created for storing the cumulative statistic\n",
    "    # calculated. \n",
    "    # e.g. new_cum_stats_col_name = \"cum_stats\" will create a column named as 'cum_stats'.\n",
    "    # If its None, the new column will be named as column_to_analyze + \"_\" + [selected\n",
    "    # cumulative function] ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "     \n",
    "    \n",
    "    #WARNING: Use this function to a analyze a single column from a dataframe.\n",
    "    \n",
    "    if ((cumulative_statistic not in ['sum', 'product', 'max', 'min']) | (cumulative_statistic is None)):\n",
    "        \n",
    "        print(\"Please, select a valid method for calculating the cumulative statistics: sum, product, max, or min.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if (new_cum_stats_col_name is None):\n",
    "            # set the standard name\n",
    "            # column_to_analyze + \"_\" + [selected cumulative function] \n",
    "            # ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "            # cumulative_statistic variable stores ['sum', 'product', 'max', 'min']\n",
    "            # we must concatenate \"cum\" to the left of this string:\n",
    "            new_cum_stats_col_name = column_to_analyze + \"_\" + \"cum\" + cumulative_statistic\n",
    "        \n",
    "        # create a local copy of the dataframe to manipulate:\n",
    "        DATASET = df.copy(deep = True)\n",
    "        # The series to be analyzed is stored as DATASET[column_to_analyze]\n",
    "        \n",
    "        # Now apply the correct method\n",
    "        # the dictionary dict_of_methods correlates the input cumulative_statistic to the\n",
    "        # correct Pandas method to be applied to the dataframe column\n",
    "        dict_of_methods = {\n",
    "            \n",
    "            'sum': DATASET[column_to_analyze].cumsum(),\n",
    "            'product': DATASET[column_to_analyze].cumprod(),\n",
    "            'max': DATASET[column_to_analyze].cummax(),\n",
    "            'min': DATASET[column_to_analyze].cummin()\n",
    "        }\n",
    "        \n",
    "        # To access the value (method) correspondent to a given key (input as \n",
    "        # cumulative_statistic): dictionary['key'], just as if accessing a column from\n",
    "        # a dataframe. In this case, the method is accessed as:\n",
    "        # dict_of_methods[cumulative_statistic], since cumulative_statistic is itself the key\n",
    "        # of the dictionary of methods.\n",
    "        \n",
    "        # store the resultant of the method in a new column of DATASET \n",
    "        # named as new_cum_stats_col_name\n",
    "        DATASET[new_cum_stats_col_name] = dict_of_methods[cumulative_statistic]\n",
    "        \n",
    "        print(f\"The cumulative {cumulative_statistic} statistic was successfully calculated and added as the column \\'{new_cum_stats_col_name}\\' of the returned dataframe.\\n\")\n",
    "        print(\"Check the new dataframe's 10 first rows:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET.head(10))\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET.head(10))\n",
    "        \n",
    "        return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f51b5b2e-3100-4be6-922c-5c5d0f00d49a"
   },
   "source": [
    "# **Function for obtaining scatter plots and simple linear regressions**\n",
    "- Here, only a single prediction variable will be analyzed by once.\n",
    "- The plots will show Y x X, where X is the predict or independent variable.\n",
    "- The linear regressions will be of the type Y = aX + b, i.e., a single pair (X, Y) analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "azdata_cell_guid": "ee736a8e-8f13-4214-8936-c7eb49ad2ee0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def scatter_plot_lin_reg (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], x_axis_rotation = 70, y_axis_rotation = 0, show_linear_reg = True, grid = True, add_splines_lines = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330): \n",
    "    \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from scipy import stats\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "        \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        print(\"Data to be plotted in a same column.\\n\")\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "    \n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "    \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to plot. Please, provide valid arguments.\\n\")\n",
    "        return \"error\" \n",
    "        # we return the value because this function always returns an object.\n",
    "        # In other functions, this return would be omitted.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        \n",
    "        # Now we pre-processed the data, we can obtain a final list of dictionaries, containing\n",
    "        # the linear regression information (it will be plotted only if the user asked to). Start\n",
    "        # a list to store all predictions:\n",
    "        list_of_dictionaries_with_series_and_predictions = []\n",
    "        \n",
    "        # Loop through each dictionary (element) on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            x_is_datetime = False\n",
    "            # boolean that will map if x is a datetime or not. Only change to True when it is.\n",
    "            \n",
    "            # Access keys 'x' and 'y' to retrieve the arrays.\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            \n",
    "            # Check if the elements from array x are np.datetime64 objects. Pick the first\n",
    "            # element to check:\n",
    "            \n",
    "            if (type(x[0]) == np.datetime64):\n",
    "                \n",
    "                x_is_datetime = True\n",
    "                \n",
    "            if (x_is_datetime):\n",
    "                # In this case, performing the linear regression directly in X will\n",
    "                # return an error. We must associate a sequential number to each time.\n",
    "                # to keep the distance between these integers the same as in the original sequence\n",
    "                # let's define a difference of 1 ns as 1. The 1st timestamp will be zero, and the\n",
    "                # addition of 1 ns will be an addition of 1 unit. So a timestamp recorded 10 ns\n",
    "                # after the time zero will have value 10. At the end, we divide every element by\n",
    "                # 10**9, to obtain the correspondent distance in seconds.\n",
    "                \n",
    "                # start a list for the associated integer timescale. Put the number zero,\n",
    "                # associated to the first timestamp:\n",
    "                int_timescale = [0]\n",
    "                \n",
    "                # loop through each element of the array x, starting from index 1:\n",
    "                for i in range(1, len(x)):\n",
    "                    \n",
    "                    # calculate the timedelta between x[i] and x[i-1]:\n",
    "                    # The delta method from the Timedelta class converts the timedelta to\n",
    "                    # nanoseconds, guaranteeing the internal compatibility:\n",
    "                    timedelta = pd.Timedelta(x[i] - x[(i-1)]).delta\n",
    "                    \n",
    "                    # Sum this timedelta (integer number of nanoseconds) to the\n",
    "                    # previous element from int_timescale, and append the result to the list:\n",
    "                    int_timescale.append((timedelta + int_timescale[(i-1)]))\n",
    "                \n",
    "                # Now convert the new scale (that preserves the distance between timestamps)\n",
    "                # to NumPy array:\n",
    "                int_timescale = np.array(int_timescale)\n",
    "                \n",
    "                # Divide by 10**9 to obtain the distances in seconds, reducing the order of\n",
    "                # magnitude of the integer numbers (the division is allowed for arrays)\n",
    "                int_timescale = int_timescale / (10**9)\n",
    "                \n",
    "                # Finally, use this timescale to obtain the linear regression:\n",
    "                lin_reg = stats.linregress(int_timescale, y = y)\n",
    "            \n",
    "            else:\n",
    "                # Obtain the linear regression object directly from x. Since x is not a\n",
    "                # datetime object, we can calculate the regression directly on it:\n",
    "                lin_reg = stats.linregress(x, y = y)\n",
    "                \n",
    "            # Retrieve the equation as a string.\n",
    "            # Access the attributes intercept and slope from the lin_reg object:\n",
    "            lin_reg_equation = \"y = %.2f*x + %.2f\" %((lin_reg).slope, (lin_reg).intercept)\n",
    "            # .2f: float with only two decimals\n",
    "                \n",
    "            # Retrieve R2 (coefficient of correlation) also as a string\n",
    "            r2_lin_reg = \"R²_lin_reg = %.4f\" %(((lin_reg).rvalue) ** 2)\n",
    "            # .4f: 4 decimals. ((lin_reg).rvalue) is the coefficient R. We\n",
    "            # raise it to the second power by doing **2, where ** is the potentiation.\n",
    "                \n",
    "            # Add these two strings to the dictionary\n",
    "            dictionary['lin_reg_equation'] = lin_reg_equation\n",
    "            dictionary['r2_lin_reg'] = r2_lin_reg\n",
    "                \n",
    "            # Now, as final step, let's apply the values x to the linear regression\n",
    "            # equation to obtain the predicted series used to plot the straight line.\n",
    "                \n",
    "            # The lists cannot perform vector operations like element-wise sum or product, \n",
    "            # but numpy arrays can. For example, [1, 2] + 1 would be interpreted as the try\n",
    "            # for concatenation of two lists, resulting in error. But, np.array([1, 2]) + 1\n",
    "            # is allowed, resulting in: np.array[2, 3].\n",
    "            # This and the fact that Scipy and Matplotlib are built on NumPy were the reasons\n",
    "            # why we converted every list to numpy arrays.\n",
    "            \n",
    "            # Save the predicted values as the array y_pred_lin_reg.\n",
    "            # Access the attributes intercept and slope from the lin_reg object.\n",
    "            # The equation is y = (slope * x) + intercept\n",
    "            \n",
    "            # Notice that again we cannot apply the equation directly to a timestamp.\n",
    "            # So once again we will apply the integer scale to obtain the predictions\n",
    "            # if we are dealing with datetime objects:\n",
    "            if (x_is_datetime):\n",
    "                y_pred_lin_reg = ((lin_reg).intercept) + ((lin_reg).slope) * (int_timescale)\n",
    "            \n",
    "            else:\n",
    "                # x is not a timestamp, so we can directly apply it to the regression\n",
    "                # equation:\n",
    "                y_pred_lin_reg = ((lin_reg).intercept) + ((lin_reg).slope) * (x)\n",
    "            \n",
    "            # Add this array to the dictionary with the key 'y_pred_lin_reg':\n",
    "            dictionary['y_pred_lin_reg'] = y_pred_lin_reg\n",
    "            \n",
    "            if (x_is_datetime):\n",
    "            \n",
    "                print(\"For performing the linear regression, a sequence of floats proportional to the timestamps was created. In this sequence, check on the returned object a dictionary containing the timestamps and the correspondent integers, that keeps the distance proportion between successive timestamps. The sequence was created by calculating the timedeltas as an integer number of nanoseconds, which were converted to seconds. The first timestamp was considered time = 0.\")\n",
    "                print(\"Notice that the regression equation is based on the use of this sequence of floats as X.\\n\")\n",
    "                \n",
    "                dictionary['warning'] = \"x is a numeric scale that was obtained from datetimes, preserving the distance relationships. It was obtained for allowing the polynomial fitting.\"\n",
    "                dictionary['numeric_to_datetime_correlation'] = {\n",
    "                    \n",
    "                    'x = 0': x[0],\n",
    "                    f'x = {max(int_timescale)}': x[(len(x) - 1)]\n",
    "                    \n",
    "                }\n",
    "                \n",
    "                dictionary['sequence_of_floats_correspondent_to_timestamps'] = {\n",
    "                                                                                'original_timestamps': x,\n",
    "                                                                                'sequence_of_floats': int_timescale\n",
    "                                                                                }\n",
    "                \n",
    "            # Finally, append this dictionary to list support_list:\n",
    "            list_of_dictionaries_with_series_and_predictions.append(dictionary)\n",
    "        \n",
    "        print(\"Returning a list of dictionaries. Each one contains the arrays of valid series and labels, and the equations, R² and values predicted by the linear regressions.\\n\")\n",
    "        \n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_and_predictions \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes, \n",
    "        # and all the information regarding the linear regression, including the predicted \n",
    "        # values for plotting.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_X\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"X\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_and_predictions:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = \"o\", color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "            if (show_linear_reg == True):\n",
    "                \n",
    "                # Plot the linear regression using the same color.\n",
    "                # Access the array of fitted Y's in the dictionary:\n",
    "                Y_PRED = dictionary['y_pred_lin_reg']\n",
    "                Y_PRED_LABEL = 'lin_reg_' + str(LABEL) # for the case where label is numeric\n",
    "                \n",
    "                ax.plot(X, Y_PRED,  linestyle = '-', marker = '', color = COLOR, alpha = OPACITY, label = Y_PRED_LABEL)\n",
    "\n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "        \n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"scatter_plot_lin_reg\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print(\"\\nLinear regression summaries (equations and R²):\\n\")\n",
    "            \n",
    "            for dictionary in list_of_dictionaries_with_series_and_predictions:\n",
    "                \n",
    "                print(f\"Linear regression summary for {dictionary['lab']}:\\n\")\n",
    "                \n",
    "                try:\n",
    "                    display(dictionary['lin_reg_equation'])\n",
    "                    display(dictionary['r2_lin_reg'])\n",
    "\n",
    "                except: # regular mode                  \n",
    "                    print(dictionary['lin_reg_equation'])\n",
    "                    print(dictionary['r2_lin_reg'])\n",
    "                \n",
    "                print(\"\\n\")\n",
    "         \n",
    "        \n",
    "        return list_of_dictionaries_with_series_and_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d042ac07-3ed5-4c83-964e-ba935fdae264"
   },
   "source": [
    "# **Function for polynomial fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "azdata_cell_guid": "9d275abc-acf1-408e-ad7f-3e4b79349ee4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def polynomial_fit (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], polynomial_degree = 6, calculate_roots = False, calculate_derivative = False, calculate_integral = False, x_axis_rotation = 70, y_axis_rotation = 0, show_polynomial_reg = True, grid = True, add_splines_lines = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    from numpy.polynomial.polynomial import Polynomial\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Check numpy.polynomial class API documentation for other polynomials \n",
    "    # (chebyshev, legendre, hermite, etc):\n",
    "    # https://numpy.org/doc/stable/reference/routines.polynomials.package.html#module-numpy.polynomial\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    # polynomial_degree = integer value representing the degree of the fitted polynomial.\n",
    "    \n",
    "    # calculate_derivative = False. Alternatively, set as True to calculate the derivative of the\n",
    "    #  fitted polynomial and add it as a column of the dataframe.\n",
    "    \n",
    "    # calculate_integral = False. Alternatively, set as True to calculate the integral of the\n",
    "    #  fitted polynomial and add it as a column of the dataframe.\n",
    "    \n",
    "    # calculate_roots = False.  Alternatively, set as True to calculate the roots of the\n",
    "    #  fitted polynomial and return them as a NumPy array.\n",
    "    \n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                \n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        x_is_datetime = True\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "            \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to fit. Please, provide valid arguments.\\n\")\n",
    "        return \"error\" \n",
    "        # we return the value because this function always returns an object.\n",
    "        # In other functions, this return would be omitted.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        \n",
    "        # Now we pre-processed the data, we can obtain a final list of dictionaries, containing\n",
    "        # the linear regression information (it will be plotted only if the user asked to). Start\n",
    "        # a list to store all predictions:\n",
    "        list_of_dictionaries_with_series_and_predictions = []\n",
    "        \n",
    "        # Loop through each dictionary (element) on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            x_is_datetime = False\n",
    "            # boolean that will map if x is a datetime or not. Only change to True when it is.\n",
    "            \n",
    "            # Access keys 'x' and 'y' to retrieve the arrays.\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            \n",
    "            # Check if the elements from array x are np.datetime64 objects. Pick the first\n",
    "            # element to check:\n",
    "            \n",
    "            if (type(x[0]) == np.datetime64):\n",
    "                \n",
    "                x_is_datetime = True\n",
    "            \n",
    "            if (x_is_datetime):\n",
    "                # In this case, performing the linear regression directly in X will\n",
    "                # return an error. We must associate a sequential number to each time.\n",
    "                # to keep the distance between these integers the same as in the original sequence\n",
    "                # let's define a difference of 1 ns as 1. The 1st timestamp will be zero, and the\n",
    "                # addition of 1 ns will be an addition of 1 unit. So a timestamp recorded 10 ns\n",
    "                # after the time zero will have value 10. At the end, we divide every element by\n",
    "                # 10**9, to obtain the correspondent distance in seconds.\n",
    "                \n",
    "                # start a list for the associated integer timescale. Put the number zero,\n",
    "                # associated to the first timestamp:\n",
    "                int_timescale = [0]\n",
    "                \n",
    "                # loop through each element of the array x, starting from index 1:\n",
    "                for i in range(1, len(x)):\n",
    "                    \n",
    "                    # calculate the timedelta between x[i] and x[i-1]:\n",
    "                    # The delta method from the Timedelta class converts the timedelta to\n",
    "                    # nanoseconds, guaranteeing the internal compatibility:\n",
    "                    timedelta = pd.Timedelta(x[i] - x[(i-1)]).delta\n",
    "                    \n",
    "                    # Sum this timedelta (integer number of nanoseconds) to the\n",
    "                    # previous element from int_timescale, and append the result to the list:\n",
    "                    int_timescale.append((timedelta + int_timescale[(i-1)]))\n",
    "                \n",
    "                # Now convert the new scale (that preserves the distance between timestamps)\n",
    "                # to NumPy array:\n",
    "                int_timescale = np.array(int_timescale)\n",
    "                \n",
    "                # Divide by 10**9 to obtain the distances in seconds, reducing the order of\n",
    "                # magnitude of the integer numbers (the division is allowed for arrays)\n",
    "                int_timescale = int_timescale / (10**9)\n",
    "                \n",
    "                # Finally, use this timescale to obtain the polynomial fit;\n",
    "                # Use the method .fit, passing X, Y, and degree as coefficients\n",
    "                # to fit the polynomial to data:\n",
    "                # Perform the least squares fit to data:\n",
    "                #create an instance (object) named 'pol' from the class Polynomial:\n",
    "                fitted_pol = Polynomial.fit(int_timescale, y, deg = polynomial_degree, full = False)\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                # Obtain the polynomial fitting object directly from x. Since x is not a\n",
    "                # datetime object, we can calculate the regression directly on it:\n",
    "                fitted_pol = Polynomial.fit(x, y, deg = polynomial_degree, full = False)\n",
    "   \n",
    "            # when full = True, the [resid, rank, sv, rcond] list is returned\n",
    "            # check: https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit\n",
    "\n",
    "            # This method returned a series named 'fitted_pol', with the values of Y predicted by the\n",
    "            # polynomial fitting. Now add it to the dictionary as fitted_polynomial_series:\n",
    "            dictionary['fitted_polynomial'] = fitted_pol\n",
    "            print(f\"{polynomial_degree} degree polynomial successfully fitted to data using the least squares method. The fitting Y ({lab}) values were added to the dataframe as the column \\'fitted_polynomial\\'.\")    \n",
    "            \n",
    "            # Get the polynomial coefficients array:\n",
    "            if (x_is_datetime):\n",
    "                coeff_array = Polynomial.fit(int_timescale, y, deg = polynomial_degree, full = False).coef\n",
    "            \n",
    "            else:\n",
    "                coeff_array = Polynomial.fit(x, y, deg = polynomial_degree, full = False).coef\n",
    "            \n",
    "            # Create a coefficient dictionary:\n",
    "            coeff_dict = {}\n",
    "            # Loop through each element from the array, and append it to the dictionary:\n",
    "            full_polynomial_format = \"a0\"\n",
    "            \n",
    "            for i in range(1, len(coeff_array)):\n",
    "                \n",
    "                    full_polynomial_format = full_polynomial_format + \" + a\" + str(i) + \"*x\" + str(i)\n",
    "            \n",
    "            coeff_dict['full_polynomial_format'] = full_polynomial_format\n",
    "            \n",
    "            for i in range(0, len(coeff_array)):\n",
    "                \n",
    "                # Create a key for the element i:\n",
    "                dict_key = \"a\" + str(i)\n",
    "                # Store the correspondent element on the array:\n",
    "                coeff_dict[dict_key] = coeff_array[i]\n",
    "            \n",
    "            if (x_is_datetime):\n",
    "                \n",
    "                coeff_dict['warning'] = \"x is a numeric scale that was obtained from datetimes, preserving the distance relationships. It was obtained for allowing the polynomial fitting.\"\n",
    "                coeff_dict['numeric_to_datetime_correlation'] = {\n",
    "                    \n",
    "                    'x = 0': x[0],\n",
    "                    f'x = {max(int_timescale)}': x[(len(x) - 1)]\n",
    "                    \n",
    "                }\n",
    "                \n",
    "                coeff_dict['sequence_of_floats_correspondent_to_timestamps'] = {\n",
    "                                                                                'original_timestamps': x,\n",
    "                                                                                'sequence_of_floats': int_timescale\n",
    "                                                                                }\n",
    "            \n",
    "            print(\"Polynomial summary:\\n\")\n",
    "            print(f\"Polynomial format = {full_polynomial_format}\\n\")\n",
    "            print(\"Polynomial coefficients:\")\n",
    "            \n",
    "            for i in range(0, len(coeff_array)):\n",
    "                print(f\"{str('a') + str(i)} = {coeff_array[i]}\")\n",
    "            \n",
    "            print(f\"Fitted polynomial = {dictionary['fitted_polynomial']}\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            if (x_is_datetime):\n",
    "                print(coeff_dict['warning'])\n",
    "                print(coeff_dict['numeric_to_datetime_correlation'])\n",
    "                print(\"\\n\")\n",
    "            \n",
    "            # Add it to the main dictionary:\n",
    "            dictionary['fitted_polynomial_coefficients'] = coeff_dict\n",
    "            \n",
    "            # Now, calculate the fitted series. Start a Pandas series as a copy from y:\n",
    "            fitted_series = y.copy()\n",
    "            # Make it zero:\n",
    "            fitted_series = 0\n",
    "            \n",
    "            # Now loop through the polynomial coefficients: ai*(x**i):\n",
    "            for i in range(0, len(coeff_array)):\n",
    "                \n",
    "                if (x_is_datetime):\n",
    "                    \n",
    "                    fitted_series = (coeff_array[i])*(int_timescale**(i))\n",
    "                \n",
    "                else:\n",
    "                    fitted_series = (coeff_array[i])*(x**(i))\n",
    "            \n",
    "            # Add the series to the dictionary:\n",
    "            dictionary['fitted_polynomial_series'] = fitted_series\n",
    "            \n",
    "            \n",
    "            if (calculate_derivative == True):\n",
    "        \n",
    "                # Calculate the derivative of the polynomial:\n",
    "                if (x_is_datetime):\n",
    "                    pol_deriv = Polynomial.fit(int_timescale, y, deg = polynomial_degree, full = False).deriv(m = 1)\n",
    "                \n",
    "                else:\n",
    "                    pol_deriv = Polynomial.fit(x, y, deg = polynomial_degree, full = False).deriv(m = 1)\n",
    "                # m (integer): order of the derivative. If m = 2, the second order is returned.\n",
    "                # This method returns a series. Check:\n",
    "                # https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.deriv.html#numpy.polynomial.polynomial.Polynomial.deriv\n",
    "\n",
    "                #Add pol_deriv series as a new key from the dictionary:\n",
    "                dictionary['fitted_polynomial_derivative'] = pol_deriv\n",
    "                print(\"1st Order derivative of the polynomial successfully calculated and added to the dictionary as the key \\'fitted_polynomial_derivative\\'.\\n\")\n",
    "\n",
    "            if (calculate_integral == True):\n",
    "\n",
    "                # Calculate the integral of the polynomial:\n",
    "                if (x_is_datetime):\n",
    "                    pol_integral = Polynomial.fit(int_timescale, y, deg = polynomial_degree, full = False).integ(m = 1)\n",
    "                \n",
    "                else:\n",
    "                    pol_integral = Polynomial.fit(x, y, deg = polynomial_degree, full = False).integ(m = 1)\n",
    "                # m (integer): The number of integrations to perform.\n",
    "                # This method returns a series. Check:\n",
    "                # https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.integ.html#numpy.polynomial.polynomial.Polynomial.integ\n",
    "\n",
    "                #Add pol_deriv series as a new key from the dictionary:\n",
    "                dictionary['fitted_polynomial_integral'] = pol_integral\n",
    "                print(\"Integral of the polynomial successfully calculated and added to the dictionary as the key \\'fitted_polynomial_integral\\'.\\n\")\n",
    "\n",
    "            if (calculate_roots == True):\n",
    "\n",
    "                # Calculate the roots of the polynomial:\n",
    "                if (x_is_datetime):\n",
    "                    roots_array = Polynomial.fit(int_timescale, y, deg = polynomial_degree, full = False).roots()\n",
    "                \n",
    "                else:\n",
    "                    roots_array = Polynomial.fit(x, y, deg = polynomial_degree, full = False).roots()\n",
    "                # This method returns an array with the polynomial roots. Check:\n",
    "                # https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.roots.html#numpy.polynomial.polynomial.Polynomial.roots\n",
    "\n",
    "                #Add it as the key polynomial_roots:\n",
    "                dictionary['polynomial_roots'] = roots_array\n",
    "                print(f\"Roots of the polynomial: {roots_array}.\\n\")\n",
    "                print(\"Roots added to the dictionary as the key \\'polynomial_roots\\'.\\n\")\n",
    "\n",
    "            # Finally, append this dictionary to list support_list:\n",
    "            list_of_dictionaries_with_series_and_predictions.append(dictionary)\n",
    "        \n",
    "        print(\"Returning a list of dictionaries. Each one contains the arrays of valid series and labels, as well as the regression statistics obtained.\\n\")\n",
    "        \n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_and_predictions \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes, \n",
    "        # and all the information regarding the linear regression, including the predicted \n",
    "        # values for plotting.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_X\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"X\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_and_predictions:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = \"o\", color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "            if (show_polynomial_reg == True):\n",
    "                \n",
    "                # Plot the linear regression using the same color.\n",
    "                # Access the array of fitted Y's in the dictionary:\n",
    "                Y_PRED = dictionary['fitted_polynomial_series']\n",
    "                Y_PRED_LABEL = 'fitted_pol_' + str(LABEL) # for the case where label is numeric\n",
    "                \n",
    "                ax.plot(X, Y_PRED,  linestyle = '-', marker = '', color = COLOR, alpha = OPACITY, label = Y_PRED_LABEL)\n",
    "\n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "        \n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"polynomial_fitting\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "        \n",
    "        return list_of_dictionaries_with_series_and_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8e9717e7-b418-4700-9f5b-bd40c73f84b2"
   },
   "source": [
    "# **Function for time series visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "azdata_cell_guid": "4e672125-e682-4d12-893c-3483b491c4d1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def time_series_vis (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "     \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "        \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        print(\"Data to be plotted in a same column.\\n\")\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "    \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to plot. Please, provide valid arguments.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_to_analyze \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        if (add_scatter_dots == True):\n",
    "            MARKER = 'o'\n",
    "            \n",
    "        else:\n",
    "            MARKER = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_timestamp\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"timestamp\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = MARKER, color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"time_series_vis\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        #plt.figure(figsize = (12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "24ffcd84-e21f-4489-a825-314d2ceba94b"
   },
   "source": [
    "# **Functions for histogram visualization**\n",
    "- Ideal number of bins is calculated through Montgomery's method.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class capability_analysis:\n",
    "            \n",
    "    # Initialize instance attributes.\n",
    "    # define the Class constructor, i.e., how are its objects:\n",
    "    def __init__ (self, df, column_with_variable_to_be_analyzed, specification_limits, total_of_bins = 10, alpha = 0.10):\n",
    "                \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # If the user passes the argument, use them. Otherwise, use the standard values.\n",
    "        # Set the class objects' attributes.\n",
    "        # Suppose the object is named plot. We can access the attribute as:\n",
    "        # plot.dictionary, for instance.\n",
    "        # So, we can save the variables as objects' attributes.\n",
    "        self.df = df\n",
    "        self.column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed\n",
    "        self.specification_limits = specification_limits\n",
    "        self.sample_size = df[column_with_variable_to_be_analyzed].count()\n",
    "        self.mu = (df[column_with_variable_to_be_analyzed]).mean() \n",
    "        self.median = (df[column_with_variable_to_be_analyzed]).median()\n",
    "        self.sigma = (df[column_with_variable_to_be_analyzed]).std()\n",
    "        self.lowest = (df[column_with_variable_to_be_analyzed]).min()\n",
    "        self.highest = (df[column_with_variable_to_be_analyzed]).max()\n",
    "        self.total_of_bins = total_of_bins\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Start a dictionary of constants\n",
    "        self.dict_of_constants = {}\n",
    "        # Get parameters to update later:\n",
    "        self.histogram_dict = {}\n",
    "        self.capability_dict = {}\n",
    "        self.normality_dict = {}\n",
    "        \n",
    "        print(\"WARNING: this capability analysis is based on the strong hypothesis that data follows the normal (Gaussian) distribution.\\n\")\n",
    "        \n",
    "    # Define the class methods.\n",
    "    # All methods must take an object from the class (self) as one of the parameters\n",
    "   \n",
    "    # Define a dictionary of constants.\n",
    "    # Each key in the dictionary corresponds to a number of samples in a subgroup.\n",
    "    # sample_size - This variable represents the total of labels or subgroups n. \n",
    "    # If there are multiple labels, this variable will be updated later.\n",
    "    \n",
    "    def check_data_normality (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from scipy import stats\n",
    "        from statsmodels.stats import diagnostic\n",
    "        \n",
    "        alpha = self.alpha\n",
    "        df = self.df\n",
    "        column_with_variable_to_be_analyzed = self.column_with_variable_to_be_analyzed\n",
    "        sample_size = self.sample_size\n",
    "        mu = self.mu \n",
    "        median = self.median\n",
    "        sigma = self.sigma\n",
    "        lowest = self.lowest\n",
    "        highest = self.highest\n",
    "        normality_dict = self.normality_dict # empty dictionary \n",
    "        \n",
    "        print(\"WARNING: The statistical tests require at least 20 samples.\\n\")\n",
    "        print(\"Interpretation:\")\n",
    "        print(\"p-value: probability that data is described by the normal distribution.\")\n",
    "        print(\"Criterion: the series is not described by normal if p < alpha = %.3f.\" %(alpha))\n",
    "        \n",
    "        if (sample_size < 20):\n",
    "            \n",
    "            print(f\"Unable to test series normality: at least 20 samples are needed, but found only {sample_size} entries for this series.\\n\")\n",
    "            normality_dict['WARNING'] = \"Series without the minimum number of elements (20) required to test the normality.\"\n",
    "            \n",
    "        else:\n",
    "            # Let's test the series.\n",
    "            y = df[column_with_variable_to_be_analyzed]\n",
    "            \n",
    "            # Scipy.stats’ normality test\n",
    "            # It is based on D’Agostino and Pearson’s test that combines \n",
    "            # skew and kurtosis to produce an omnibus test of normality.\n",
    "            _, scipystats_test_pval = stats.normaltest(y)\n",
    "            # The underscore indicates an output to be ignored, which is s^2 + k^2, \n",
    "            # where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.\n",
    "            # https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"D\\'Agostino and Pearson\\'s normality test (scipy.stats normality test):\")\n",
    "            print(f\"p-value = {scipystats_test_pval:e} = {scipystats_test_pval*100:.2f}% of probability of being normal.\")\n",
    "            # :e indicates the scientific notation; .2f: float with 2 decimal cases\n",
    "            \n",
    "            if (scipystats_test_pval < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(scipystats_test_pval, alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(scipystats_test_pval, alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['dagostino_pearson_p_val'] = scipystats_test_pval\n",
    "            normality_dict['dagostino_pearson_p_in_pct'] = scipystats_test_pval*100\n",
    "            \n",
    "            # Scipy.stats’ Shapiro-Wilk test\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n",
    "            shapiro_test = stats.shapiro(y)\n",
    "            # returns ShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n",
    "             \n",
    "            print(\"\\n\")\n",
    "            print(\"Shapiro-Wilk normality test:\")\n",
    "            print(f\"p-value = {shapiro_test[1]:e} = {(shapiro_test[1])*100:.2f}% of probability of being normal.\")\n",
    "            \n",
    "            if (shapiro_test[1] < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(shapiro_test[1], alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(shapiro_test[1], alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['shapiro_wilk_p_val'] = shapiro_test[1]\n",
    "            normality_dict['shapiro_wilk_p_in_pct'] = (shapiro_test[1])*100\n",
    "            \n",
    "            # Lilliefors’ normality test\n",
    "            lilliefors_test = diagnostic.kstest_normal(y, dist = 'norm', pvalmethod = 'table')\n",
    "            # Returns a tuple: index 0: ksstat: float\n",
    "            # Kolmogorov-Smirnov test statistic with estimated mean and variance.\n",
    "            # index 1: p-value:float\n",
    "            # If the pvalue is lower than some threshold, e.g. 0.10, then we can reject the Null hypothesis that the sample comes from a normal distribution.\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"Lilliefors\\'s normality test:\")\n",
    "            print(f\"p-value = {lilliefors_test[1]:e} = {(lilliefors_test[1])*100:.2f}% of probability of being normal.\")\n",
    "            \n",
    "            if (lilliefors_test[1] < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(lilliefors_test[1], alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(lilliefors_test[1], alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['lilliefors_p_val'] = lilliefors_test[1]\n",
    "            normality_dict['lilliefors_p_in_pct'] = (lilliefors_test[1])*100\n",
    "\n",
    "            # Anderson-Darling normality test\n",
    "            ad_test = diagnostic.normal_ad(y, axis = 0)\n",
    "            # Returns a tuple: index 0 - ad2: float\n",
    "            # Anderson Darling test statistic.\n",
    "            # index 1 - p-val: float\n",
    "            # The p-value for hypothesis that the data comes from a normal distribution with unknown mean and variance.\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"Anderson-Darling (AD) normality test:\")\n",
    "            print(f\"p-value = {ad_test[1]:e} = {(ad_test[1])*100:.2f}% of probability of being normal.\")\n",
    "            \n",
    "            if (ad_test[1] < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(ad_test[1], alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(ad_test[1], alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['anderson_darling_p_val'] = ad_test[1]\n",
    "            normality_dict['anderson_darling_p_in_pct'] = (ad_test[1])*100\n",
    "            \n",
    "            # Update the attribute:\n",
    "            self.normality_dict = normality_dict\n",
    "            \n",
    "            return self\n",
    "    \n",
    "    def get_constants (self):\n",
    "        \n",
    "        if (self.sample_size < 2):\n",
    "            \n",
    "            self.sample_size = 2\n",
    "            \n",
    "        if (self.sample_size <= 25):\n",
    "            \n",
    "            dict_of_constants = {\n",
    "                \n",
    "                2: {'A':2.121, 'A2':1.880, 'A3':2.659, 'c4':0.7979, '1/c4':1.2533, 'B3':0, 'B4':3.267, 'B5':0, 'B6':2.606, 'd2':1.128, '1/d2':0.8865, 'd3':0.853, 'D1':0, 'D2':3.686, 'D3':0, 'D4':3.267},\n",
    "                3: {'A':1.732, 'A2':1.023, 'A3':1.954, 'c4':0.8862, '1/c4':1.1284, 'B3':0, 'B4':2.568, 'B5':0, 'B6':2.276, 'd2':1.693, '1/d2':0.5907, 'd3':0.888, 'D1':0, 'D2':4.358, 'D3':0, 'D4':2.574},\n",
    "                4: {'A':1.500, 'A2':0.729, 'A3':1.628, 'c4':0.9213, '1/c4':1.0854, 'B3':0, 'B4':2.266, 'B5':0, 'B6':2.088, 'd2':2.059, '1/d2':0.4857, 'd3':0.880, 'D1':0, 'D2':4.698, 'D3':0, 'D4':2.282},\n",
    "                5: {'A':1.342, 'A2':0.577, 'A3':1.427, 'c4':0.9400, '1/c4':1.0638, 'B3':0, 'B4':2.089, 'B5':0, 'B6':1.964, 'd2':2.326, '1/d2':0.4299, 'd3':0.864, 'D1':0, 'D2':4.918, 'D3':0, 'D4':2.114},\n",
    "                6: {'A':1.225, 'A2':0.483, 'A3':1.287, 'c4':0.9515, '1/c4':1.0510, 'B3':0.030, 'B4':1.970, 'B5':0.029, 'B6':1.874, 'd2':2.534, '1/d2':0.3946, 'd3':0.848, 'D1':0, 'D2':5.078, 'D3':0, 'D4':2.004},\n",
    "                7: {'A':1.134, 'A2':0.419, 'A3':1.182, 'c4':0.9594, '1/c4':1.0423, 'B3':0.118, 'B4':1.882, 'B5':0.113, 'B6':1.806, 'd2':2.704, '1/d2':0.3698, 'd3':0.833, 'D1':0.204, 'D2':5.204, 'D3':0.076, 'D4':1.924},\n",
    "                8: {'A':1.061, 'A2':0.373, 'A3':1.099, 'c4':0.9650, '1/c4':1.0363, 'B3':0.185, 'B4':1.815, 'B5':0.179, 'B6':1.751, 'd2':2.847, '1/d2':0.3512, 'd3':0.820, 'D1':0.388, 'D2':5.306, 'D3':0.136, 'D4':1.864},\n",
    "                9: {'A':1.000, 'A2':0.337, 'A3':1.032, 'c4':0.9693, '1/c4':1.0317, 'B3':0.239, 'B4':1.761, 'B5':0.232, 'B6':1.707, 'd2':2.970, '1/d2':0.3367, 'd3':0.808, 'D1':0.547, 'D2':5.393, 'D3':0.184, 'D4':1.816},\n",
    "                10: {'A':0.949, 'A2':0.308, 'A3':0.975, 'c4':0.9727, '1/c4':1.0281, 'B3':0.284, 'B4':1.716, 'B5':0.276, 'B6':1.669, 'd2':3.078, '1/d2':0.3249, 'd3':0.797, 'D1':0.687, 'D2':5.469, 'D3':0.223, 'D4':1.777},\n",
    "                11: {'A':0.905, 'A2':0.285, 'A3':0.927, 'c4':0.9754, '1/c4':1.0252, 'B3':0.321, 'B4':1.679, 'B5':0.313, 'B6':1.637, 'd2':3.173, '1/d2':0.3152, 'd3':0.787, 'D1':0.811, 'D2':5.535, 'D3':0.256, 'D4':1.744},\n",
    "                12: {'A':0.866, 'A2':0.266, 'A3':0.886, 'c4':0.9776, '1/c4':1.0229, 'B3':0.354, 'B4':1.646, 'B5':0.346, 'B6':1.610, 'd2':3.258, '1/d2':0.3069, 'd3':0.778, 'D1':0.922, 'D2':5.594, 'D3':0.283, 'D4':1.717},\n",
    "                13: {'A':0.832, 'A2':0.249, 'A3':0.850, 'c4':0.9794, '1/c4':1.0210, 'B3':0.382, 'B4':1.618, 'B5':0.374, 'B6':1.585, 'd2':3.336, '1/d2':0.2998, 'd3':0.770, 'D1':1.025, 'D2':5.647, 'D3':0.307, 'D4':1.693},\n",
    "                14: {'A':0.802, 'A2':0.235, 'A3':0.817, 'c4':0.9810, '1/c4':1.0194, 'B3':0.406, 'B4':1.594, 'B5':0.399, 'B6':1.563, 'd2':3.407, '1/d2':0.2935, 'd3':0.763, 'D1':1.118, 'D2':5.696, 'D3':0.328, 'D4':1.672},\n",
    "                15: {'A':0.775, 'A2':0.223, 'A3':0.789, 'c4':0.9823, '1/c4':1.0180, 'B3':0.428, 'B4':1.572, 'B5':0.421, 'B6':1.544, 'd2':3.472, '1/d2':0.2880, 'd3':0.756, 'D1':1.203, 'D2':5.741, 'D3':0.347, 'D4':1.653},\n",
    "                16: {'A':0.750, 'A2':0.212, 'A3':0.763, 'c4':0.9835, '1/c4':1.0168, 'B3':0.448, 'B4':1.552, 'B5':0.440, 'B6':1.526, 'd2':3.532, '1/d2':0.2831, 'd3':0.750, 'D1':1.282, 'D2':5.782, 'D3':0.363, 'D4':1.637},\n",
    "                17: {'A':0.728, 'A2':0.203, 'A3':0.739, 'c4':0.9845, '1/c4':1.0157, 'B3':0.466, 'B4':1.534, 'B5':0.458, 'B6':1.511, 'd2':3.588, '1/d2':0.2787, 'd3':0.744, 'D1':1.356, 'D2':5.820, 'D3':0.378, 'D4':1.622},\n",
    "                18: {'A':0.707, 'A2':0.194, 'A3':0.718, 'c4':0.9854, '1/c4':1.0148, 'B3':0.482, 'B4':1.518, 'B5':0.475, 'B6':1.496, 'd2':3.640, '1/d2':0.2747, 'd3':0.739, 'D1':1.424, 'D2':5.856, 'D3':0.391, 'D4':1.608},\n",
    "                19: {'A':0.688, 'A2':0.187, 'A3':0.698, 'c4':0.9862, '1/c4':1.0140, 'B3':0.497, 'B4':1.503, 'B5':0.490, 'B6':1.483, 'd2':3.689, '1/d2':0.2711, 'd3':0.734, 'D1':1.487, 'D2':5.891, 'D3':0.403, 'D4':1.597},\n",
    "                20: {'A':0.671, 'A2':0.180, 'A3':0.680, 'c4':0.9869, '1/c4':1.0133, 'B3':0.510, 'B4':1.490, 'B5':0.504, 'B6':1.470, 'd2':3.735, '1/d2':0.2677, 'd3':0.729, 'D1':1.549, 'D2':5.921, 'D3':0.415, 'D4':1.585},\n",
    "                21: {'A':0.655, 'A2':0.173, 'A3':0.663, 'c4':0.9876, '1/c4':1.0126, 'B3':0.523, 'B4':1.477, 'B5':0.516, 'B6':1.459, 'd2':3.778, '1/d2':0.2647, 'd3':0.724, 'D1':1.605, 'D2':5.951, 'D3':0.425, 'D4':1.575},\n",
    "                22: {'A':0.640, 'A2':0.167, 'A3':0.647, 'c4':0.9882, '1/c4':1.0119, 'B3':0.534, 'B4':1.466, 'B5':0.528, 'B6':1.448, 'd2':3.819, '1/d2':0.2618, 'd3':0.720, 'D1':1.659, 'D2':5.979, 'D3':0.434, 'D4':1.566},\n",
    "                23: {'A':0.626, 'A2':0.162, 'A3':0.633, 'c4':0.9887, '1/c4':1.0114, 'B3':0.545, 'B4':1.455, 'B5':0.539, 'B6':1.438, 'd2':3.858, '1/d2':0.2592, 'd3':0.716, 'D1':1.710, 'D2':6.006, 'D3':0.443, 'D4':1.557},\n",
    "                24: {'A':0.612, 'A2':0.157, 'A3':0.619, 'c4':0.9892, '1/c4':1.0109, 'B3':0.555, 'B4':1.445, 'B5':0.549, 'B6':1.429, 'd2':3.895, '1/d2':0.2567, 'd3':0.712, 'D1':1.759, 'D2':6.031, 'D3':0.451, 'D4':1.548},\n",
    "                25: {'A':0.600, 'A2':0.153, 'A3':0.606, 'c4':0.9896, '1/c4':1.0105, 'B3':0.565, 'B4':1.435, 'B5':0.559, 'B6':1.420, 'd2':3.931, '1/d2':0.2544, 'd3':0.708, 'D1':1.806, 'D2':6.056, 'D3':0.459, 'D4':1.541},\n",
    "            }\n",
    "            \n",
    "            # Access the key:\n",
    "            dict_of_constants = dict_of_constants[self.sample_size]\n",
    "            \n",
    "        else: #>= 26\n",
    "            \n",
    "            dict_of_constants = {'A':(3/(self.sample_size**(0.5))), 'A2':0.153, \n",
    "                                 'A3':3/((4*(self.sample_size-1)/(4*self.sample_size-3))*(self.sample_size**(0.5))), \n",
    "                                 'c4':(4*(self.sample_size-1)/(4*self.sample_size-3)), \n",
    "                                 '1/c4':1/((4*(self.sample_size-1)/(4*self.sample_size-3))), \n",
    "                                 'B3':(1-3/(((4*(self.sample_size-1)/(4*self.sample_size-3)))*((2*(self.sample_size-1))**(0.5)))), \n",
    "                                 'B4':(1+3/(((4*(self.sample_size-1)/(4*self.sample_size-3)))*((2*(self.sample_size-1))**(0.5)))),\n",
    "                                 'B5':(((4*(self.sample_size-1)/(4*self.sample_size-3)))-3/((2*(self.sample_size-1))**(0.5))), \n",
    "                                 'B6':(((4*(self.sample_size-1)/(4*self.sample_size-3)))+3/((2*(self.sample_size-1))**(0.5))), \n",
    "                                 'd2':3.931, '1/d2':0.2544, 'd3':0.708, 'D1':1.806, 'D2':6.056, 'D3':0.459, 'D4':1.541}\n",
    "        \n",
    "        # Update the attribute\n",
    "        self.dict_of_constants = dict_of_constants\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_histogram_array (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        df = self.df\n",
    "        column_with_variable_to_be_analyzed = self.column_with_variable_to_be_analyzed\n",
    "        y_hist = df[column_with_variable_to_be_analyzed]\n",
    "        lowest = self.lowest\n",
    "        highest = self.highest\n",
    "        sample_size = self.sample_size\n",
    "        \n",
    "        # Number of bins set by the user:\n",
    "        total_of_bins = self.total_of_bins\n",
    "        \n",
    "        # Firstly, get the ideal bin-size according to the Montgomery's method:\n",
    "        # Douglas C. Montgomery (2009). Introduction to Statistical Process Control, \n",
    "        # Sixth Edition, John Wiley & Sons.\n",
    "        # Sort by the column to analyze (ascending order) and reset the index:\n",
    "        y_hist = y_hist.sort_values(ascending = True)\n",
    "        y_hist = y_hist.reset_index(drop = True)\n",
    "        #Calculo do bin size - largura do histograma:\n",
    "        #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "        #2: Calcular rangehist = highest - lowest\n",
    "        #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "        #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "        #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "        #5: Calcular binsize = (df[column_to_analyze])rangehist/(ncells)\n",
    "        #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "        #isso porque a largura do histograma tem que ser um numero positivo \n",
    "\n",
    "        # bin-size\n",
    "        range_hist = abs(highest - lowest)\n",
    "        n_cells = int(np.rint((sample_size)**(0.5)))\n",
    "        # We must use the int function to guarantee that the ncells will store an\n",
    "        # integer number of cells (we cannot have a fraction of a sentence).\n",
    "        # The int function guarantees that the variable will be stored as an integer.\n",
    "        # The numpy.rint(a) function rounds elements of the array to the nearest integer.\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        # For values exactly halfway between rounded decimal values, \n",
    "        # NumPy rounds to the nearest even value. \n",
    "        # Thus 1.5 and 2.5 round to 2.0; -0.5 and 0.5 round to 0.0; etc.\n",
    "        if (n_cells > 3):\n",
    "            \n",
    "            print(f\"Ideal number of histogram bins calculated through Montgomery's method = {n_cells} bins.\\n\")\n",
    "        \n",
    "        # Retrieve the histogram array hist_array\n",
    "        fig, ax = plt.subplots() # (0,0) not to show the plot now:\n",
    "        \n",
    "        # Get a 10-bins histogram:\n",
    "        hist_array = plt.hist(y_hist, bins = total_of_bins)\n",
    "        plt.delaxes(ax) # this will delete ax, so that it will not be plotted.\n",
    "        plt.show()\n",
    "        print(\"\") # use this print not to mix with the final plot\n",
    "\n",
    "        # hist_array is an array of arrays:\n",
    "        # hist_array = (array([count_1, count_2, ..., cont_n]), array([bin_center_1,...,\n",
    "        # bin_center_n])), where n = total_of_bins\n",
    "        # hist_array[0] is the array of countings for each bin, whereas hist_array[1] is\n",
    "        # the array of the bin center, i.e., the central value of the analyzed variable for\n",
    "        # that bin.\n",
    "\n",
    "        # It is possible that the hist_array[0] contains more elements than hist_array[1].\n",
    "        # This happens when the last bins created by the division contain zero elements.\n",
    "        # In this case, we have to pad the sequence of hist_array[0], completing it with zeros.\n",
    "\n",
    "        MAX_LENGTH = max(len(hist_array[0]), len(hist_array[1])) # Get the length of the longest sequence\n",
    "        SEQUENCES = [list(hist_array[0]), list(hist_array[1])] # get a list of sequences to pad.\n",
    "        # Notice that we applied the list attribute to create a list of lists\n",
    "\n",
    "        # We cannot pad with the function pad_sequences from tensorflow because it converts all values\n",
    "        # to integers. Then, we have to pad the sequences by looping through the elements from SEQUENCES:\n",
    "\n",
    "        # Start a support_list\n",
    "        support_list = []\n",
    "\n",
    "        # loop through each sequence in SEQUENCES:\n",
    "        for sequence in SEQUENCES:\n",
    "            # add a zero at the end of the sequence until its length reaches MAX_LENGTH\n",
    "            while (len(sequence) < MAX_LENGTH):\n",
    "\n",
    "                sequence.append(0)\n",
    "\n",
    "            # append the sequence to support_list:\n",
    "            support_list.append(sequence)\n",
    "\n",
    "        # Tuples and arrays are immutable. It means they do not support assignment, i.e., we cannot\n",
    "        # do tuple[0] = variable. Since arrays support vectorial (element-wise) operations, we can\n",
    "        # modify the whole array making it equals to support_list at once by using function np.array:\n",
    "        hist_array = np.array(support_list)\n",
    "\n",
    "        # Get the bin_size as the average difference between successive elements from support_list[1]:\n",
    "\n",
    "        diff_lists = []\n",
    "\n",
    "        for i in range (1, len(support_list[1])):\n",
    "\n",
    "            diff_lists.append(support_list[1][i] - support_list[1][(i-1)])\n",
    "\n",
    "        # Now, get the mean value as the bin_size:\n",
    "        bin_size = np.amax(np.array(diff_lists))\n",
    "\n",
    "        # Let's get the frequency table, which will be saved on DATASET (to get the code\n",
    "        # equivalent to the code for the function 'histogram'):\n",
    "\n",
    "        DATASET = pd.DataFrame(data = {'bin_center': hist_array[1], 'count': hist_array[0]})\n",
    "\n",
    "        # Get a lists of bin_center and column_to_analyze:\n",
    "        list_of_bins = list(hist_array[1])\n",
    "        list_of_counts = list(hist_array[0])\n",
    "\n",
    "        # get the maximum count:\n",
    "        max_count = DATASET['count'].max()\n",
    "        # Get the index of the max count:\n",
    "        max_count_index = list_of_counts.index(max_count)\n",
    "\n",
    "        # Get the value bin_center correspondent to the max count (maximum probability):\n",
    "        bin_of_max_proba = list_of_bins[max_count_index]\n",
    "        bin_after_the_max_proba = list_of_bins[(max_count_index + 1)] # the next bin\n",
    "        number_of_bins = len(DATASET) # Total of elements on the frequency table\n",
    "        \n",
    "        # Obtain a list of differences between bins\n",
    "        bins_diffs = [(list_of_bins[i] - list_of_bins[(i-1)]) for i in range (1, len(list_of_bins))]\n",
    "        # Convert it to Pandas series and use the mean method to retrieve the average bin size:\n",
    "        bin_size = pd.Series(bins_diffs).mean()\n",
    "        \n",
    "        self.histogram_dict = {'df': DATASET, 'list_of_bins': list_of_bins, 'list_of_counts': list_of_counts,\n",
    "                              'max_count': max_count, 'max_count_index': max_count_index,\n",
    "                              'bin_of_max_proba': bin_of_max_proba, 'bin_after_the_max_proba': bin_after_the_max_proba,\n",
    "                              'number_of_bins': number_of_bins, 'bin_size': bin_size}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_desired_normal (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get a normal completely (6s) in the specifications, and centered\n",
    "        # within these limits\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        histogram_dict = self.histogram_dict\n",
    "        max_count = histogram_dict['max_count']\n",
    "        \n",
    "        specification_limits = self.specification_limits\n",
    "        \n",
    "        lower_spec = specification_limits['lower_spec_lim']\n",
    "        upper_spec = specification_limits['upper_spec_lim']\n",
    "        \n",
    "        if (lower_spec is None):\n",
    "            \n",
    "            # There is no lower specification: everything below it is in the specifications.\n",
    "            # Make it mean - 6sigma (virtually infinite).\n",
    "            lower_spec = mu - 6*(sigma)\n",
    "            # Update the dictionary:\n",
    "            specification_limits['lower_spec_lim'] = lower_spec\n",
    "        \n",
    "        if (upper_spec is None):\n",
    "            \n",
    "            # There is no upper specification: everything above it is in the specifications.\n",
    "            # Make it mean + 6sigma (virtually infinite).\n",
    "            upper_spec = mu + 6*(sigma)\n",
    "            # Update the dictionary:\n",
    "            specification_limits['upper_spec_lim'] = upper_spec\n",
    "        \n",
    "        # Desired normal mu: center of the specification limits.\n",
    "        desired_mu = (lower_spec + upper_spec)/2\n",
    "        \n",
    "        # Desired sigma: 6 times the variation within the specific limits\n",
    "        desired_sigma = (upper_spec - lower_spec)/6\n",
    "        \n",
    "        if (desired_sigma == 0):\n",
    "            print(\"Impossible to obtain a normal curve overlayed, because the standard deviation is zero.\\n\")\n",
    "            print(\"The analyzed variable is constant throughout the whole sample space.\\n\")\n",
    "            \n",
    "            # Get a dictionary of empty lists for this case\n",
    "            desired_normal = {'x': [], 'y':[]}\n",
    "            \n",
    "        else:\n",
    "            # create lists to store the normal curve. Center the normal curve in the bin\n",
    "            # of maximum bar (max probability, which will not be the mean if the curve\n",
    "            # is skewed). For normal distributions, this value will be the mean and the median.\n",
    "\n",
    "            # set the lowest value x used for obtaining the normal curve as center_of_bin_of_max_proba - 4*sigma\n",
    "            # the highest x will be center_of_bin_of_max_proba - 4*sigma\n",
    "            # each value will be created by incrementing (0.10)*sigma\n",
    "\n",
    "            # The arrays created by the plt.hist method present the value of the extreme left \n",
    "            # (the beginning) of the histogram bars, not the bin center. So, let's add half of the bin size\n",
    "            # to the bin_of_max_proba, so that the adjusted normal will be positioned on the center of the\n",
    "            # bar of maximum probability. We can do it by taking the average between bin_of_max_proba\n",
    "            # and the following bin, bin_after_the_max_proba:\n",
    "            \n",
    "            # Let's create a normal around the desired mean value. Firstly, create the range X - 4s to\n",
    "            # X + 4s. The probabilities will be calculated for each value in this range:\n",
    "\n",
    "            x = (desired_mu - (4 * desired_sigma))\n",
    "            x_of_normal = [x]\n",
    "\n",
    "            while (x < (desired_mu + (4 * desired_sigma))):\n",
    "\n",
    "                x = x + (0.10)*(desired_sigma)\n",
    "                x_of_normal.append(x)\n",
    "\n",
    "            # Convert the list to a NumPy array, so that it is possible to perform element-wise\n",
    "            # (vectorial) operations:\n",
    "            x_of_normal = np.array(x_of_normal)\n",
    "\n",
    "            # Create an array of the normal curve y, applying the normal curve equation:\n",
    "            # normal curve = 1/(sigma* ((2*pi)**(0.5))) * exp(-((x-mu)**2)/(2*(sigma**2)))\n",
    "            # where pi = 3,14...., and exp is the exponential function (base e)\n",
    "            # Let's center the normal curve on desired_mu:\n",
    "            y_normal = (1 / (desired_sigma* (np.sqrt(2 * (np.pi))))) * (np.exp(-0.5 * (((1 / desired_sigma) * (x_of_normal - desired_mu)) ** 2)))\n",
    "            y_normal = np.array(y_normal)\n",
    "\n",
    "            # Pick the maximum value obtained for y_normal:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "            y_normal_max = np.amax(y_normal)\n",
    "\n",
    "            # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "            # with y_normal_max:\n",
    "            correction_factor = max_count/(y_normal_max)\n",
    "\n",
    "            # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "            y_normal = y_normal * correction_factor\n",
    "            # Now the probability density function (values originally from 0 to 1) has the same \n",
    "            # height as the histogram.\n",
    "            \n",
    "            desired_normal = {'x': x_of_normal, 'y': y_normal}\n",
    "        \n",
    "        # Nest the desired_normal dictionary into specification_limits dictionary:\n",
    "        specification_limits['desired_normal'] = desired_normal\n",
    "        # Update the attribute:\n",
    "        self.specification_limits = specification_limits\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_fitted_normal (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get a normal completely (6s) in the specifications, and centered\n",
    "        # within these limits\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        histogram_dict = self.histogram_dict\n",
    "        max_count = histogram_dict['max_count']\n",
    "        bin_of_max_proba = histogram_dict['bin_of_max_proba']\n",
    "        specification_limits = self.specification_limits\n",
    "        \n",
    "        if (sigma == 0):\n",
    "            print(\"Impossible to obtain a normal curve overlayed, because the standard deviation is zero.\\n\")\n",
    "            print(\"The analyzed variable is constant throughout the whole sample space.\\n\")\n",
    "            \n",
    "            # Get a dictionary of empty lists for this case\n",
    "            fitted_normal = {'x': [], 'y':[]}\n",
    "            \n",
    "        else:\n",
    "            # create lists to store the normal curve. Center the normal curve in the bin\n",
    "            # of maximum bar (max probability, which will not be the mean if the curve\n",
    "            # is skewed). For normal distributions, this value will be the mean and the median.\n",
    "\n",
    "            # set the lowest value x used for obtaining the normal curve as bin_of_max_proba - 4*sigma\n",
    "            # the highest x will be bin_of_max_proba - 4*sigma\n",
    "            # each value will be created by incrementing (0.10)*sigma\n",
    "\n",
    "            x = (bin_of_max_proba - (4 * sigma))\n",
    "            x_of_normal = [x]\n",
    "\n",
    "            while (x < (bin_of_max_proba + (4 * sigma))):\n",
    "\n",
    "                x = x + (0.10)*(sigma)\n",
    "                x_of_normal.append(x)\n",
    "\n",
    "            # Convert the list to a NumPy array, so that it is possible to perform element-wise\n",
    "            # (vectorial) operations:\n",
    "            x_of_normal = np.array(x_of_normal)\n",
    "\n",
    "            # Create an array of the normal curve y, applying the normal curve equation:\n",
    "            # normal curve = 1/(sigma* ((2*pi)**(0.5))) * exp(-((x-mu)**2)/(2*(sigma**2)))\n",
    "            # where pi = 3,14...., and exp is the exponential function (base e)\n",
    "            # Let's center the normal curve on bin_of_max_proba\n",
    "            y_normal = (1 / (sigma* (np.sqrt(2 * (np.pi))))) * (np.exp(-0.5 * (((1 / sigma) * (x_of_normal - bin_of_max_proba)) ** 2)))\n",
    "            y_normal = np.array(y_normal)\n",
    "\n",
    "            # Pick the maximum value obtained for y_normal:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "            y_normal_max = np.amax(y_normal)\n",
    "\n",
    "            # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "            # with y_normal_max:\n",
    "            correction_factor = max_count/(y_normal_max)\n",
    "\n",
    "            # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "            y_normal = y_normal * correction_factor\n",
    "            \n",
    "            fitted_normal = {'x': x_of_normal, 'y': y_normal}\n",
    "        \n",
    "        # Nest the fitted_normal dictionary into specification_limits dictionary:\n",
    "        specification_limits['fitted_normal'] = fitted_normal\n",
    "        # Update the attribute:\n",
    "        self.specification_limits = specification_limits\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_actual_pdf (self):\n",
    "        \n",
    "        # PDF: probability density function.\n",
    "        # KDE: Kernel density estimation: estimation of the actual probability density\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from scipy import stats\n",
    "        \n",
    "        df = self.df\n",
    "        column_with_variable_to_be_analyzed = self.column_with_variable_to_be_analyzed\n",
    "        array_to_analyze = np.array(df[column_with_variable_to_be_analyzed])\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        lowest = self.lowest\n",
    "        highest = self.highest\n",
    "        sample_size = self.sample_size\n",
    "        \n",
    "        histogram_dict = self.histogram_dict\n",
    "        max_count = histogram_dict['max_count']\n",
    "        specification_limits = self.specification_limits \n",
    "        \n",
    "        # Get the KDE object\n",
    "        kde = stats.gaussian_kde(array_to_analyze)\n",
    "        \n",
    "        # Here, kde may represent a distribution with high skewness and kurtosis. So, let's check\n",
    "        # if the intervals mu - 6s and mu + 6s are represented by the array:\n",
    "        inf_kde_lim = mu - 6*sigma\n",
    "        sup_kde_lim = mu + 6*sigma\n",
    "        \n",
    "        if (inf_kde_lim > min(list(array_to_analyze))):\n",
    "            # make the inferior limit the minimum value from the array:\n",
    "            inf_kde_lim = min(list(array_to_analyze))\n",
    "        \n",
    "        if (sup_kde_lim < max(list(array_to_analyze))):\n",
    "            # make the superior limit the minimum value from the array:\n",
    "            sup_kde_lim = max(list(array_to_analyze))\n",
    "        \n",
    "        # Let's obtain a X array, consisting with all values from which we will calculate the PDF:\n",
    "        new_x = inf_kde_lim\n",
    "        new_x_list = [new_x]\n",
    "        \n",
    "        while ((new_x) < sup_kde_lim):\n",
    "            # There is already the first element, so go to the next one.\n",
    "            new_x = new_x + (0.10)*sigma\n",
    "            new_x_list.append(new_x)\n",
    "        \n",
    "        # Convert the new_x_list to NumPy array, making it the array_to_analyze:\n",
    "        array_to_analyze = np.array(new_x_list)\n",
    "        \n",
    "        # Apply the pdf method to convert the array_to_analyze into the array of probabilities:\n",
    "        # i.e., calculate the probability for each one of the values in array_to_analyze:\n",
    "        # PDF: Probability density function\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.pdf.html#scipy.stats.gaussian_kde.pdf\n",
    "        array_of_probs = kde.pdf(array_to_analyze)\n",
    "        \n",
    "        # Pick the maximum value obtained for array_of_probs:\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "        array_of_probs_max = np.amax(array_of_probs)\n",
    "\n",
    "        # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "        # with array_of_probs_max:\n",
    "        correction_factor = max_count/(array_of_probs_max)\n",
    "\n",
    "        # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "        array_of_probs = array_of_probs * correction_factor\n",
    "        # Now the probability density function (values originally from 0 to 1) has the same \n",
    "        # height as the histogram.\n",
    "        \n",
    "        # Define a dictionary\n",
    "        # X of the probability density plot: values from the series being analyzed.\n",
    "        # Y of the probability density plot: probabilities calculated for each X.\n",
    "        actual_pdf = {'x': array_to_analyze, 'y': array_of_probs}\n",
    "        \n",
    "        # Nest the desired_normal dictionary into specification_limits dictionary:\n",
    "        specification_limits['actual_pdf'] = actual_pdf\n",
    "        # Update the attribute:\n",
    "        self.specification_limits = specification_limits\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_capability_indicators (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get a normal completely (6s) in the specifications, and centered\n",
    "        # within these limits\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        histogram_dict = self.histogram_dict\n",
    "        bin_of_max_proba = histogram_dict['bin_of_max_proba']\n",
    "        bin_after_the_max_proba = histogram_dict['bin_after_the_max_proba']\n",
    "        max_count = histogram_dict['max_count']\n",
    "        \n",
    "        specification_limits = self.specification_limits\n",
    "        lower_spec = specification_limits['lower_spec_lim']\n",
    "        upper_spec = specification_limits['upper_spec_lim']\n",
    "        desired_mu = (lower_spec + upper_spec)/2 \n",
    "        # center of the specification limits: we want the mean to be in the center of the\n",
    "        # specification limits\n",
    "        \n",
    "        range_spec = abs(upper_spec - lower_spec)\n",
    "        \n",
    "        # Get the constant:\n",
    "        self = self.get_constants()\n",
    "        dict_of_constants = self.dict_of_constants\n",
    "        constant = dict_of_constants['1/c4']\n",
    "        \n",
    "        # Calculate corrected sigma:\n",
    "        sigma_corrected = sigma*constant\n",
    "        \n",
    "        # Calculate the capability indicators, adding them to the\n",
    "        # capability_dict\n",
    "        cp = (range_spec)/(6*sigma_corrected)\n",
    "        cr = 100*(6*sigma_corrected)/(range_spec)\n",
    "        cm = (range_spec)/(8*sigma_corrected)\n",
    "        zu = (upper_spec - mu)/(sigma_corrected)\n",
    "        zl = (mu - lower_spec)/(sigma_corrected)\n",
    "        \n",
    "        z_min = min(zu, zl)\n",
    "        cpk = (z_min)/3\n",
    "\n",
    "        cpm_factor = 1 + ((mu - desired_mu)/sigma_corrected)**2\n",
    "        cpm_factor = cpm_factor**(0.5) # square root\n",
    "        cpm = (cp)/(cpm_factor)\n",
    "        \n",
    "        capability_dict = {'indicator': ['cp', 'cr', 'cm', 'zu', 'zl', 'z_min', 'cpk', 'cpm'], \n",
    "                            'value': [cp, cr, cm, zu, zl, z_min, cpk, cpm]}\n",
    "        # Already in format for pd.DataFrame constructor\n",
    "        \n",
    "        # Update the attribute:\n",
    "        self.capability_dict = capability_dict\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def capability_interpretation (self):\n",
    "       \n",
    "        print(\"Capable process: a process which attends its specifications.\")\n",
    "        print(\"Naturally, we want processes capable of attending the specifications.\\n\")\n",
    "        \n",
    "        print(\"Specification range:\")\n",
    "        print(\"Absolute value of the difference between the upper and the lower limits of specification.\\n\")\n",
    "        \n",
    "        print(\"6s interval:\")\n",
    "        print(\"Consider mean value = mu; standard deviation = s\")\n",
    "        print(\"For a normal distribution, 99.7% of the values range from its (mu - 3s) to (mu + 3s).\")\n",
    "        print(\"So, if the process follows the normal distribution, we can consider that virtually all of the data is in this range with 6s width.\\n\")\n",
    "        \n",
    "        print (\"Cp:\")\n",
    "        print (\"Relation between specification range and 6s.\\n\")\n",
    "        \n",
    "        print(\"Cr:\")\n",
    "        print(\"Usually, 6s > specification range.\")\n",
    "        print(\"So, the inverse of Cp is the fraction of 6s correspondent to the specification range.\")\n",
    "        print(\"Example: if 1/Cp = 0.2, then the specification range corresponds to 0.20 (20%) of the 6s interval.\")\n",
    "        print(\"Cr = 100 x (1/Cp) - the percent of 6s correspondent to the specification range.\")\n",
    "        print(\"Again, if 1/Cp = 0.2, then Cr = 20: the specification range corresponds to 20% of the 6s interval.\\n\")\n",
    "        \n",
    "        print(\"Cm:\")\n",
    "        print(\"It is a more generalized version of Cp.\")\n",
    "        print(\"Cm is the relation between specification range and 8s.\")\n",
    "        print(\"Then, even highly distant values from long-tailed curves are analyzed by this indicator.\\n\")\n",
    "        \n",
    "        print(\"Zu:\")\n",
    "        print(\"Represents how far is the mean of the values from the upper specification limit.\")\n",
    "        print(\"Zu = ([upper specification limit] - mu)/s\")\n",
    "        print(\"A higher Zu indicates a mean value lower than (and more distant from) the upper specification.\")\n",
    "        print(\"A negative Zu, in turns, indicates that the mean value is greater than the upper specification (i.e.: in average, specification is not attended).\\n\")\n",
    "        \n",
    "        print(\"Zl:\")\n",
    "        print(\"Represents how far is the mean of the values from the lower specification limit.\")\n",
    "        print(\"Zl = (mu - [lower specification limit])/s\\n\")\n",
    "        print(\"A higher Zl indicates a mean value higher than  (and more distant from) the lower specification.\")\n",
    "        print(\"A negative Zl, in turns, indicates that the mean value is inferior than the lower specification (i.e.: in average, specification is not attended).\\n\")\n",
    "        \n",
    "        print(\"Zmin:\")\n",
    "        print(\"It is the minimum value between Zu and Zl.\")\n",
    "        print(\"So, Zmin indicates which specification is more difficult for the process to attend: the upper or the lower one.\")\n",
    "        print(\"Example: if Zmin = Zl, the mean of the process is closer to the lower specification than it is from the upper specification.\")\n",
    "        print(\"If Zmin, Zu, and Zl are equal, than the process is equally distant from the two specifications.\")\n",
    "        print(\"Again, if Zmin is negative, at least one of the specifications is not attended.\\n\")\n",
    "        \n",
    "        print(\"Cpk:\")\n",
    "        print(\"This is the most fundamental capability indicator.\")\n",
    "        print(\"Consider again that 99.7% of the normally distributed data are within [(mu - 3s), (mu + 3s)].\")\n",
    "        print(\"Cpk = Zmin/3\")\n",
    "        print(\"Cpk = min((([upper specification limit] - mu)/3s), ((mu - [lower specification limit])/3s))\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Cpk simultaneously assess the process centrality, and if the process is capable of attending its specifications.\")\n",
    "        print(\"Here, the process centrality is verified as results which are well and simetrically distributed throughout the mean of the specification limits.\")\n",
    "        print(\"Basically, a perfectly-centralized process has its mean equally distant from both specifications\")\n",
    "        print(\"i.e., the mean is in the center of the specification interval.\")\n",
    "        print(\"Cpk = + 1 is usually considered the minimum value acceptable for a process.\")\n",
    "        print(\"Many quality programs define reaching Cpk = + 1.33 as their goal.\")\n",
    "        print(\"A 6-sigma process, in turns, is defined as a process with Cpk = + 2.\")\n",
    "        print(\"\\n\")\n",
    "        print(\"High values of Cpk indicate that the process is not only centralized, but that the differences\")\n",
    "        print(\"([upper specification limit] - mu) and (mu - [lower specification limit]) are greater than 3s.\")\n",
    "        print(\"Since mu +- 3s is the range for 99.7% of data, it indicates that most of the values generated fall in a range\")\n",
    "        print(\"that is only a fraction of the specification range.\")\n",
    "        print(\"So, it is easier for the process to attend the specifications.\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Cpk values inferior than 1 indicate that at least one of the intervals ([upper specification limit] - mu) and (mu - [lower specification limit])\")\n",
    "        print(\"is lower than 3s, i.e., the process naturally generates values beyond at least one of the specifications.\")\n",
    "        print(\"Low values of Cpk (in particular the negative ones) indicate not-centralized processes and processes not capable of attending their specifications.\")\n",
    "        print(\"So, lower (and, specially, more negative) Cpk: process' outputs more distant from the specifications.\\n\")\n",
    "        \n",
    "        print(\"Cpm:\")\n",
    "        print(\"This indicator is a more generalized version of the Cpk.\")\n",
    "        print(\"It basically consists on a standard normalization of the Cpk.\")\n",
    "        print(\"For that, a normalization factor is defined as:\")\n",
    "        print(\"factor = square root(1 + ((mu - target)/s)**2)\")\n",
    "        print(\"where target is the center of the specification limits, and **2 represents the second power (square)\")\n",
    "        print(\"Cpm = Cpk/(factor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "azdata_cell_guid": "871b3321-cbb3-4018-8d6e-c76c3867495c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def histogram (df, column_to_analyze, total_of_bins = 10, normal_curve_overlay = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # column_to_analyze: string with the name of the column that will be analyzed.\n",
    "    # column_to_analyze = 'col1' obtain a histogram from column 1.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Sort by the column to analyze (ascending order) and reset the index:\n",
    "    DATASET = DATASET.sort_values(by = column_to_analyze, ascending = True)\n",
    "    \n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # Create an instance (object) from class capability_analysis:\n",
    "    capability_obj = capability_analysis(df = DATASET, column_with_variable_to_be_analyzed = column_to_analyze, specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}, total_of_bins = total_of_bins)\n",
    "     \n",
    "    # Get histogram array:\n",
    "    capability_obj = capability_obj.get_histogram_array()\n",
    "    # Attribute .histogram_dict: dictionary with keys 'list_of_bins' and 'list_of_counts'.\n",
    "    \n",
    "    # Get fitted normal:\n",
    "    capability_obj = capability_obj.get_fitted_normal()\n",
    "    # Now the .specification_limits attribute contains the nested dict desired_normal = {'x': x_of_normal, 'y': y_normal}\n",
    "    # in key 'fitted_normal'.\n",
    "    \n",
    "    # Get the actual probability density function (PDF):\n",
    "    capability_obj = capability_obj.get_actual_pdf()\n",
    "    # Now the dictionary in the attribute .specification_limits has the nested dict actual_pdf = {'x': array_to_analyze, 'y': array_of_probs}\n",
    "    # in key 'actual_pdf'.\n",
    "    \n",
    "    # Retrieve general statistics:\n",
    "    stats_dict = {\n",
    "        \n",
    "        'sample_size': capability_obj.sample_size,\n",
    "        'mu': capability_obj.mu,\n",
    "        'median': capability_obj.median,\n",
    "        'sigma': capability_obj.sigma,\n",
    "        'lowest': capability_obj.lowest,\n",
    "        'highest': capability_obj.highest\n",
    "    }\n",
    "    \n",
    "    # Retrieve the histogram dict:\n",
    "    histogram_dict = capability_obj.histogram_dict\n",
    "    \n",
    "    # Retrieve the specification limits dictionary updated:\n",
    "    specification_limits = capability_obj.specification_limits\n",
    "    # Retrieve the desired normal and actual PDFs dictionaries:\n",
    "    fitted_normal = specification_limits['fitted_normal']\n",
    "    actual_pdf = specification_limits['actual_pdf']\n",
    "    \n",
    "    string_for_title = \" - $\\mu = %.2f$, $\\sigma = %.2f$\" %(stats_dict['mu'], stats_dict['sigma'])\n",
    "    \n",
    "    if not (plot_title is None):\n",
    "        plot_title = plot_title + string_for_title\n",
    "        # %.2f: the number between . and f indicates the number of printed decimal cases\n",
    "        # the notation $\\ - Latex code for printing formatted equations and symbols.\n",
    "    \n",
    "    else:\n",
    "        # Set graphic title\n",
    "        plot_title = f\"histogram_of_{column_to_analyze}\" + string_for_title\n",
    "\n",
    "    if (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        horizontal_axis_title = column_to_analyze\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        vertical_axis_title = \"Counting/Frequency\"\n",
    "        \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    y_hist = DATASET[column_to_analyze]\n",
    "    \n",
    "    # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    ax.hist(y_hist, bins = total_of_bins, alpha = OPACITY, label = f'counting_of\\n{column_to_analyze}', color = 'darkblue')\n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    #ax.bar(x_hist, y_hist, label = f'counting_of\\n{column_to_analyze}', color = 'darkblue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    # Plot the probability density function for the data:\n",
    "    pdf_x = actual_pdf['x']\n",
    "    pdf_y = actual_pdf['y']\n",
    "    \n",
    "    ax.plot(pdf_x, pdf_y, color = 'darkgreen', linestyle = '-', alpha = OPACITY, label = 'probability\\ndensity')\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        # Check if a normal curve was obtained:\n",
    "        x_of_normal = fitted_normal['x']\n",
    "        y_normal = fitted_normal['y']\n",
    "\n",
    "        if (len(x_of_normal) > 0):\n",
    "            # Non-empty list, add the normal curve:\n",
    "            ax.plot(x_of_normal, y_normal, color = 'crimson', linestyle = 'dashed', alpha = OPACITY, label = 'expected\\nnormal_curve')\n",
    "\n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(horizontal_axis_title)\n",
    "    ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    ax.grid(grid) # show grid or not\n",
    "    ax.legend(loc = 'upper right')\n",
    "    # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "    # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "    # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "      \n",
    "    stats_dict = {\n",
    "                  'statistics': ['mean', 'median', 'standard_deviation', f'lowest_{column_to_analyze}', \n",
    "                                f'highest_{column_to_analyze}', 'count_of_values', 'number_of_bins', \n",
    "                                 'bin_size', 'bin_of_max_proba', 'count_on_bin_of_max_proba'],\n",
    "                  'value': [stats_dict['mu'], stats_dict['median'], stats_dict['sigma'], \n",
    "                            stats_dict['lowest'], stats_dict['highest'], stats_dict['sample_size'], \n",
    "                            histogram_dict['number_of_bins'], histogram_dict['bin_size'], \n",
    "                            histogram_dict['bin_of_max_proba'], histogram_dict['max_count']]\n",
    "                 }\n",
    "    \n",
    "    # Convert it to a Pandas dataframe setting the list 'statistics' as the index:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "    general_stats = pd.DataFrame(data = stats_dict)\n",
    "    \n",
    "    # Set the column 'statistics' as the index of the dataframe, using set_index method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\n",
    "    \n",
    "    # If inplace = True, modifies the DataFrame in place (do not create a new object).\n",
    "    # Then, we do not create an object equal to the expression. We simply apply the method (so,\n",
    "    # None is returned from the method):\n",
    "    general_stats.set_index(['statistics'], inplace = True)\n",
    "    \n",
    "    print(\"Check the general statistics from the analyzed variable:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(general_stats)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(general_stats)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the frequency table:\\n\")\n",
    "    \n",
    "    freq_table = histogram_dict['df']\n",
    "    \n",
    "    try:    \n",
    "        display(freq_table)    \n",
    "    except:\n",
    "        print(freq_table)\n",
    "\n",
    "    return general_stats, freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c0cd7017-5fb0-4705-b0f2-d7aa48534512"
   },
   "source": [
    "# **Function for testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "azdata_cell_guid": "4ab25ac2-5820-4ef8-877c-55805b45601d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def test_data_normality (df, column_to_analyze, column_with_labels_to_test_subgroups = None, alpha = 0.10, show_probability_plot = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "    # Check https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    # Check https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    # WARNING: The statistical tests require at least 20 samples\n",
    "    \n",
    "    # column_to_analyze: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "    # in quotes.\n",
    "    # e.g. column_to_analyze = 'col1' will analyze a column named 'col1'.\n",
    "    \n",
    "    # column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "    # subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "    # it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "    # will retrieve the labels from 'col2'.\n",
    "    # Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "    # will be tested.\n",
    "    \n",
    "    # Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "    # Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "    # results.\n",
    "    \n",
    "    print(\"WARNING: The statistical tests require at least 20 samples.\\n\")\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"p-value: probability that data is described by the normal distribution.\")\n",
    "    print(\"Criterion: the series is not described by normal if p < alpha = %.3f.\" %(alpha))\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Start a list to store the different Pandas series to test:\n",
    "    list_of_dicts = []\n",
    "    \n",
    "    if not (column_with_labels_to_test_subgroups is None):\n",
    "        \n",
    "        # 1. Get the unique values from column_with_labels_to_test_subgroups\n",
    "        # and save it as the list labels_list:\n",
    "        labels_list = list(DATASET[column_with_labels_to_test_subgroups].unique())\n",
    "        \n",
    "        # 2. Loop through each element from labels_list:\n",
    "        for label in labels_list:\n",
    "            \n",
    "            # 3. Create a copy of the DATASET, filtering for entries where \n",
    "            # column_with_labels_to_test_subgroups == label:\n",
    "            filtered_df = (DATASET[DATASET[column_with_labels_to_test_subgroups] == label]).copy(deep = True)\n",
    "            # 4. Reset index of the copied dataframe:\n",
    "            filtered_df = filtered_df.reset_index(drop = True)\n",
    "            # 5. Create a dictionary, with an identification of the series, and the series\n",
    "            # that will be tested:\n",
    "            series_dict = {'series_id': (column_to_analyze + \"_\" + label), \n",
    "                           'series': filtered_df[column_to_analyze],\n",
    "                           'total_elements_to_test': len(filtered_df[column_to_analyze])}\n",
    "            \n",
    "            # 6. Append this dictionary to the list of series:\n",
    "            list_of_dicts.append(series_dict)\n",
    "        \n",
    "    else:\n",
    "        # In this case, the only series is the column itself. So, let's create a dictionary with\n",
    "        # same structure:\n",
    "        series_dict = {'series_id': column_to_analyze, 'series': DATASET[column_to_analyze],\n",
    "                       'total_elements_to_test': len(DATASET[column_to_analyze])}\n",
    "        \n",
    "        # Append this dictionary to the list of series:\n",
    "        list_of_dicts.append(series_dict)\n",
    "    \n",
    "    \n",
    "    # Now, loop through each element from the list of series:\n",
    "    \n",
    "    for series_dict in list_of_dicts:\n",
    "        \n",
    "        # start a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Check if there are at least 20 samples to test:\n",
    "        series_id = series_dict['series_id']\n",
    "        total_elements_to_test = series_dict['total_elements_to_test']\n",
    "        \n",
    "        # Create an instance (object) from class capability_analysis:\n",
    "        capability_obj = capability_analysis(df = DATASET, column_with_variable_to_be_analyzed = series_id, specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}, alpha = alpha)\n",
    "        \n",
    "        # Check data normality:\n",
    "        capability_obj = capability_obj.check_data_normality()\n",
    "        # Attribute .normality_dict: dictionary with results from normality tests\n",
    "        \n",
    "        # Retrieve the normality dictionary:\n",
    "        normality_dict = capability_obj.normality_dict\n",
    "        # Nest it in series_dict:\n",
    "        series_dict['normality_dict'] = normality_dict\n",
    "        \n",
    "        # Finally, append the series dictionary to the support list:\n",
    "        support_list.append(series_dict)\n",
    "        \n",
    "        if ((total_elements_to_test >= 20) & (show_probability_plot == True)):\n",
    "            \n",
    "            y = series_dict['series']\n",
    "        \n",
    "            print(\"\\n\")\n",
    "            #Obtain the probability plot  \n",
    "            fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "            ax.set_title(f\"probability_plot_of_{series_id}_for_normal_distribution\")\n",
    "            \n",
    "            plot_results = stats.probplot(y, dist = 'norm', fit = True, plot = ax)\n",
    "            #This function resturns a tuple, so we must store it into res\n",
    "            \n",
    "            ax.grid(grid)\n",
    "            #ROTATE X AXIS IN XX DEGREES\n",
    "            plt.xticks(rotation = x_axis_rotation)\n",
    "            # XX = 70 DEGREES x_axis (Default)\n",
    "            #ROTATE Y AXIS IN XX DEGREES:\n",
    "            plt.yticks(rotation = y_axis_rotation)\n",
    "            # XX = 0 DEGREES y_axis (Default)   \n",
    "            \n",
    "            # Other distributions to check, see scipy Stats documentation. \n",
    "            # you could test dist=stats.loggamma, where stats was imported from scipy\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = \"probability_plot_normal\"\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 330 dpi\n",
    "                    png_resolution_dpi = 330\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            #plt.figure(figsize = (12, 8))\n",
    "            #fig.tight_layout()\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "                \n",
    "            print(\"\\n\")\n",
    "            \n",
    "    # Now we left the for loop, make the list of dicts support list itself:\n",
    "    list_of_dicts = support_list\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Finished normality tests. Returning a list of dictionaries, where each dictionary contains the series analyzed and the p-values obtained.\\n\")\n",
    "    print(\"Now, check general statistics of the data distribution:\\n\")\n",
    "    \n",
    "    # Now, let's obtain general statistics for all of the series, even those without the normality\n",
    "    # test results.\n",
    "    \n",
    "    # start a support list:\n",
    "    support_list = []\n",
    "    \n",
    "    for series_dict in list_of_dicts:\n",
    "        \n",
    "        y = series_dict['series']\n",
    "        # Guarantee it is still a pandas series:\n",
    "        y = pd.Series(y)\n",
    "        # Calculate data skewness and kurtosis\n",
    "    \n",
    "        # Skewness\n",
    "        data_skew = stats.skew(y)\n",
    "        # skewness = 0 : normally distributed.\n",
    "        # skewness > 0 : more weight in the left tail of the distribution.\n",
    "        # skewness < 0 : more weight in the right tail of the distribution.\n",
    "        # https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "\n",
    "        # Kurtosis\n",
    "        data_kurtosis = stats.kurtosis(y, fisher = True)\n",
    "        # scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function \n",
    "        # calculates the kurtosis (Fisher or Pearson) of a data set. It is the the fourth \n",
    "        # central moment divided by the square of the variance. \n",
    "        # It is a measure of the “tailedness” i.e. descriptor of shape of probability \n",
    "        # distribution of a real-valued random variable. \n",
    "        # In simple terms, one can say it is a measure of how heavy tail is compared \n",
    "        # to a normal distribution.\n",
    "        # fisher parameter: fisher : Bool; Fisher’s definition is used (normal 0.0) if True; \n",
    "        # else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "        # https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "        print(\"A normal distribution should present no skewness (distribution distortion); and no kurtosis (long-tail).\\n\")\n",
    "        print(\"For the data analyzed:\\n\")\n",
    "        print(f\"skewness = {data_skew}\")\n",
    "        print(f\"kurtosis = {data_kurtosis}\\n\")\n",
    "\n",
    "        if (data_skew < 0):\n",
    "\n",
    "            print(f\"Skewness = {data_skew} < 0: more weight in the left tail of the distribution.\")\n",
    "\n",
    "        elif (data_skew > 0):\n",
    "\n",
    "            print(f\"Skewness = {data_skew} > 0: more weight in the right tail of the distribution.\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"Skewness = {data_skew} = 0: no distortion of the distribution.\")\n",
    "                \n",
    "\n",
    "        if (data_kurtosis == 0):\n",
    "\n",
    "            print(\"Data kurtosis = 0. No long-tail effects detected.\\n\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"The kurtosis different from zero indicates long-tail effects on the distribution.\\n\")\n",
    "\n",
    "        #Calculate the mode of the distribution:\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "        data_mode = stats.mode(y, axis = None)[0][0]\n",
    "        # returns an array of arrays. The first array is called mode=array and contains the mode.\n",
    "        # Axis: Default is 0. If None, compute over the whole array.\n",
    "        # we set axis = None to compute the general mode.\n",
    "\n",
    "        #Create general statistics dictionary:\n",
    "        general_statistics_dict = {\n",
    "\n",
    "            \"series_mean\": y.mean(),\n",
    "            \"series_variance\": y.var(),\n",
    "            \"series_standard_deviation\": y.std(),\n",
    "            \"series_skewness\": data_skew,\n",
    "            \"series_kurtosis\": data_kurtosis,\n",
    "            \"series_mode\": data_mode\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Add this dictionary to the series dictionary:\n",
    "        series_dict['general_statistics'] = general_statistics_dict\n",
    "        \n",
    "        # Append the dictionary to support list:\n",
    "        support_list.append(series_dict)\n",
    "    \n",
    "    # Now, make the list of dictionaries support_list itself:\n",
    "    list_of_dicts = support_list\n",
    "\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bd01d165-b439-44ec-873b-50fb80e8b5fa"
   },
   "source": [
    "# **Function for testing different statistical distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f3f30a06-bb1b-4ef6-88e4-81d22979c9e6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def test_stat_distribution (df, column_to_analyze, column_with_labels_to_test_subgroups = None, statistical_distribution_to_test = 'lognormal'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "    # Check https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    # Check https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    # column_to_analyze: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "    # in quotes.\n",
    "    # e.g. column_to_analyze = 'col1' will analyze a column named 'col1'.\n",
    "    \n",
    "    # column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "    # subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "    # it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "    # will retrieve the labels from 'col2'.\n",
    "    # Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "    # will be tested.\n",
    "    \n",
    "    # Attention: if you want to test a normal distribution, use the function \n",
    "    # test_data_normality. Function test_data_normality tests normality through 4 methods \n",
    "    # and compare them: D’Agostino and Pearson’s; Shapiro-Wilk; Lilliefors; \n",
    "    # and Anderson-Darling tests.\n",
    "    # The calculus of the p-value from the Anderson-Darling statistic is available only \n",
    "    # for some distributions. The function specific for the normality calculates these \n",
    "    # probabilities of following the normal.\n",
    "    # Here, the function is destined to test a variety of distributions, and so only the \n",
    "    # Anderson-Darling test is performed.\n",
    "        \n",
    "    # statistical_distribution: string (inside quotes) containing the tested statistical \n",
    "    # distribution.\n",
    "    # Notice: if data Y follow a 'lognormal', log(Y) follow a normal\n",
    "    # Poisson is a special case from 'gamma' distribution.\n",
    "    ## There are 91 accepted statistical distributions:\n",
    "    # 'alpha', 'anglit', 'arcsine', 'beta', 'beta_prime', 'bradford', 'burr', 'burr12', \n",
    "    # 'cauchy', 'chi', 'chi-squared', 'cosine', 'double_gamma', \n",
    "    # 'double_weibull', 'erlang', 'exponential', 'exponentiated_weibull', 'exponential_power',\n",
    "    # 'fatigue_life_birnbaum-saunders', 'fisk_log_logistic', 'folded_cauchy', 'folded_normal',\n",
    "    # 'F', 'gamma', 'generalized_logistic', 'generalized_pareto', 'generalized_exponential', \n",
    "    # 'generalized_extreme_value', 'generalized_gamma', 'generalized_half-logistic', \n",
    "    # 'generalized_inverse_gaussian', 'generalized_normal', \n",
    "    # 'gilbrat', 'gompertz_truncated_gumbel', 'gumbel', 'gumbel_left-skewed', 'half-cauchy', \n",
    "    # 'half-normal', 'half-logistic', 'hyperbolic_secant', 'gauss_hypergeometric', \n",
    "    # 'inverted_gamma', 'inverse_normal', 'inverted_weibull', 'johnson_SB', 'johnson_SU', \n",
    "    # 'KSone', 'KStwobign', 'laplace', 'left-skewed_levy', \n",
    "    # 'levy', 'logistic', 'log_laplace', 'log_gamma', 'lognormal', 'log-uniform', 'maxwell', \n",
    "    # 'mielke_Beta-Kappa', 'nakagami', 'noncentral_chi-squared', 'noncentral_F', \n",
    "    # 'noncentral_t', 'normal', 'normal_inverse_gaussian', 'pareto', 'lomax', \n",
    "    # 'power_lognormal', 'power_normal', 'power-function', 'R', 'rayleigh', 'rice', \n",
    "    # 'reciprocal_inverse_gaussian', 'semicircular', 'student-t', \n",
    "    # 'triangular', 'truncated_exponential', 'truncated_normal', 'tukey-lambda',\n",
    "    # 'uniform', 'von_mises', 'wald', 'weibull_maximum_extreme_value', \n",
    "    # 'weibull_minimum_extreme_value', 'wrapped_cauchy'\n",
    "    \n",
    "    print(\"WARNING: The statistical tests require at least 20 samples.\\n\")\n",
    "    print(\"Attention: if you want to test a normal distribution, use the function test_data_normality.\")\n",
    "    print(\"Function test_data_normality tests normality through 4 methods and compare them: D’Agostino and Pearson’s; Shapiro-Wilk; Lilliefors; and Anderson-Darling tests.\")\n",
    "    print(\"The calculus of the p-value from the Anderson-Darling statistic is available only for some distributions.\")\n",
    "    print(\"The function which specifically tests the normality calculates these probabilities that data follows the normal.\")\n",
    "    print(\"Here, the function is destined to test a variety of distributions, and so only the Anderson-Darling test is performed.\\n\")\n",
    "    \n",
    "    print(\"If a compilation error is shown below, please update your Scipy version. Declare and run the following code into a separate cell:\")\n",
    "    print(\"! pip install scipy --upgrade\\n\")\n",
    "    \n",
    "    # Lets define the statistic distributions:\n",
    "    # This are the callable Scipy objects which can be tested through Anderson-Darling test:\n",
    "    # They are listed and explained in: \n",
    "    # https://docs.scipy.org/doc/scipy/tutorial/stats/continuous.html\n",
    "    \n",
    "    # This dictionary correlates the input name of the distribution to the correct scipy.stats\n",
    "    # callable object\n",
    "    # There are 91 possible statistical distributions:\n",
    "    \n",
    "    callable_statistical_distributions_dict = {\n",
    "        \n",
    "        'alpha': stats.alpha, 'anglit': stats.anglit, 'arcsine': stats.arcsine,\n",
    "        'beta': stats.beta, 'beta_prime': stats.betaprime, 'bradford': stats.bradford,\n",
    "        'burr': stats.burr, 'burr12': stats.burr12, 'cauchy': stats.cauchy,\n",
    "        'chi': stats.chi, 'chi-squared': stats.chi2,\n",
    "        'cosine': stats.cosine, 'double_gamma': stats.dgamma, 'double_weibull': stats.dweibull,\n",
    "        'erlang': stats.erlang, 'exponential': stats.expon, 'exponentiated_weibull': stats.exponweib,\n",
    "        'exponential_power': stats.exponpow, 'fatigue_life_birnbaum-saunders': stats.fatiguelife,\n",
    "        'fisk_log_logistic': stats.fisk, 'folded_cauchy': stats.foldcauchy,\n",
    "        'folded_normal': stats.foldnorm, 'F': stats.f, 'gamma': stats.gamma,\n",
    "        'generalized_logistic': stats.genlogistic, 'generalized_pareto': stats.genpareto,\n",
    "        'generalized_exponential': stats.genexpon, 'generalized_extreme_value': stats.genextreme,\n",
    "        'generalized_gamma': stats.gengamma, 'generalized_half-logistic': stats.genhalflogistic,\n",
    "        'generalized_inverse_gaussian': stats.geninvgauss,\n",
    "        'generalized_normal': stats.gennorm, 'gilbrat': stats.gilbrat,\n",
    "        'gompertz_truncated_gumbel': stats.gompertz, 'gumbel': stats.gumbel_r,\n",
    "        'gumbel_left-skewed': stats.gumbel_l, 'half-cauchy': stats.halfcauchy, 'half-normal': stats.halfnorm,\n",
    "        'half-logistic': stats.halflogistic, 'hyperbolic_secant': stats.hypsecant,\n",
    "        'gauss_hypergeometric': stats.gausshyper, 'inverted_gamma': stats.invgamma,\n",
    "        'inverse_normal': stats.invgauss, 'inverted_weibull': stats.invweibull,\n",
    "        'johnson_SB': stats.johnsonsb, 'johnson_SU': stats.johnsonsu, 'KSone': stats.ksone,\n",
    "        'KStwobign': stats.kstwobign, 'laplace': stats.laplace,\n",
    "        'left-skewed_levy': stats.levy_l,\n",
    "        'levy': stats.levy, 'logistic': stats.logistic, 'log_laplace': stats.loglaplace,\n",
    "        'log_gamma': stats.loggamma, 'lognormal': stats.lognorm, 'log-uniform': stats.loguniform,\n",
    "        'maxwell': stats.maxwell, 'mielke_Beta-Kappa': stats.mielke, 'nakagami': stats.nakagami,\n",
    "        'noncentral_chi-squared': stats.ncx2, 'noncentral_F': stats.ncf, 'noncentral_t': stats.nct,\n",
    "        'normal': stats.norm, 'normal_inverse_gaussian': stats.norminvgauss, 'pareto': stats.pareto,\n",
    "        'lomax': stats.lomax, 'power_lognormal': stats.powerlognorm, 'power_normal': stats.powernorm,\n",
    "        'power-function': stats.powerlaw, 'R': stats.rdist, 'rayleigh': stats.rayleigh,\n",
    "        'rice': stats.rayleigh, 'reciprocal_inverse_gaussian': stats.recipinvgauss,\n",
    "        'semicircular': stats.semicircular,\n",
    "        'student-t': stats.t, 'triangular': stats.triang,\n",
    "        'truncated_exponential': stats.truncexpon, 'truncated_normal': stats.truncnorm,\n",
    "        'tukey-lambda': stats.tukeylambda, 'uniform': stats.uniform, 'von_mises': stats.vonmises,\n",
    "        'wald': stats.wald, 'weibull_maximum_extreme_value': stats.weibull_max,\n",
    "        'weibull_minimum_extreme_value': stats.weibull_min, 'wrapped_cauchy': stats.wrapcauchy\n",
    "                \n",
    "    }\n",
    "    \n",
    "    # Get a list of keys from this dictionary, to compare with the selected string:\n",
    "    list_of_dictionary_keys = callable_statistical_distributions_dict.keys()\n",
    "    \n",
    "    #check if an invalid string was provided using the in method:\n",
    "    # The string must be in the list of dictionary keys\n",
    "    boolean_filter = statistical_distribution_to_test in list_of_dictionary_keys\n",
    "    # if it is the list, boolean_filter == True. If it is not, boolean_filter == False\n",
    "    \n",
    "    if (boolean_filter == False):\n",
    "        \n",
    "        print(f\"Please, select a valid statistical distribution to test: {list_of_dictionary_keys}\")\n",
    "        return \"error\"\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Start a list to store the different Pandas series to test:\n",
    "    list_of_dicts = []\n",
    "    \n",
    "    if not (column_with_labels_to_test_subgroups is None):\n",
    "        \n",
    "        # 1. Get the unique values from column_with_labels_to_test_subgroups\n",
    "        # and save it as the list labels_list:\n",
    "        labels_list = list(DATASET[column_with_labels_to_test_subgroups].unique())\n",
    "        \n",
    "        # 2. Loop through each element from labels_list:\n",
    "        for label in labels_list:\n",
    "            \n",
    "            # 3. Create a copy of the DATASET, filtering for entries where \n",
    "            # column_with_labels_to_test_subgroups == label:\n",
    "            filtered_df = (DATASET[DATASET[column_with_labels_to_test_subgroups] == label]).copy(deep = True)\n",
    "            # 4. Reset index of the copied dataframe:\n",
    "            filtered_df = filtered_df.reset_index(drop = True)\n",
    "            # 5. Create a dictionary, with an identification of the series, and the series\n",
    "            # that will be tested:\n",
    "            series_dict = {'series_id': (column_to_analyze + \"_\" + label), \n",
    "                           'series': filtered_df[column_to_analyze],\n",
    "                           'total_elements_to_test': filtered_df[column_to_analyze].count()}\n",
    "            \n",
    "            # 6. Append this dictionary to the list of series:\n",
    "            list_of_dicts.append(series_dict)\n",
    "        \n",
    "    else:\n",
    "        # In this case, the only series is the column itself. So, let's create a dictionary with\n",
    "        # same structure:\n",
    "        series_dict = {'series_id': column_to_analyze, 'series': DATASET[column_to_analyze],\n",
    "                       'total_elements_to_test': DATASET[column_to_analyze].count()}\n",
    "        \n",
    "        # Append this dictionary to the list of series:\n",
    "        list_of_dicts.append(series_dict)\n",
    "    \n",
    "    \n",
    "    # Now, loop through each element from the list of series:\n",
    "    \n",
    "    for series_dict in list_of_dicts:\n",
    "        \n",
    "        # start a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Check if there are at least 20 samples to test:\n",
    "        series_id = series_dict['series_id']\n",
    "        total_elements_to_test = series_dict['total_elements_to_test']\n",
    "        \n",
    "        if (total_elements_to_test < 20):\n",
    "            \n",
    "            print(f\"Unable to test series {series_id}: at least 20 samples are needed, but found only {total_elements_to_test} entries for this series.\\n\")\n",
    "            # Add a warning to the dictionary:\n",
    "            series_dict['WARNING'] = \"Series without the minimum number of elements (20) required to test the normality.\"\n",
    "            # Append it to the support list:\n",
    "            support_list.append(series_dict)\n",
    "            \n",
    "        else:\n",
    "            # Let's test the series.\n",
    "            y = series_dict['series']\n",
    "            \n",
    "            # Calculate data skewness and kurtosis\n",
    "            # Skewness\n",
    "            data_skew = stats.skew(y)\n",
    "            # skewness = 0 : normally distributed.\n",
    "            # skewness > 0 : more weight in the left tail of the distribution.\n",
    "            # skewness < 0 : more weight in the right tail of the distribution.\n",
    "            # https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "\n",
    "            # Kurtosis\n",
    "            data_kurtosis = stats.kurtosis(y, fisher = True)\n",
    "            # scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function \n",
    "            # calculates the kurtosis (Fisher or Pearson) of a data set. It is the the fourth \n",
    "            # central moment divided by the square of the variance. \n",
    "            # It is a measure of the “tailedness” i.e. descriptor of shape of probability \n",
    "            # distribution of a real-valued random variable. \n",
    "            # In simple terms, one can say it is a measure of how heavy tail is compared \n",
    "            # to a normal distribution.\n",
    "            # fisher parameter: fisher : Bool; Fisher’s definition is used (normal 0.0) if True; \n",
    "            # else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "            # https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "            print(\"A normal distribution should present no skewness (distribution distortion); and no kurtosis (long-tail).\\n\")\n",
    "            print(\"For the data analyzed:\\n\")\n",
    "            print(f\"skewness = {data_skew}\")\n",
    "            print(f\"kurtosis = {data_kurtosis}\\n\")\n",
    "\n",
    "            if (data_skew < 0):\n",
    "\n",
    "                print(f\"Skewness = {data_skew} < 0: more weight in the left tail of the distribution.\")\n",
    "\n",
    "            elif (data_skew > 0):\n",
    "\n",
    "                print(f\"Skewness = {data_skew} > 0: more weight in the right tail of the distribution.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"Skewness = {data_skew} = 0: no distortion of the distribution.\")\n",
    "                \n",
    "\n",
    "            if (data_kurtosis == 0):\n",
    "\n",
    "                print(\"Data kurtosis = 0. No long-tail effects detected.\\n\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"The kurtosis different from zero indicates long-tail effects on the distribution.\\n\")\n",
    "\n",
    "            #Calculate the mode of the distribution:\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "            data_mode = stats.mode(y, axis = None)[0][0]\n",
    "            # returns an array of arrays. The first array is called mode=array and contains the mode.\n",
    "            # Axis: Default is 0. If None, compute over the whole array.\n",
    "            # we set axis = None to compute the general mode.\n",
    "            \n",
    "            # Access the object correspondent to the distribution provided. To do so,\n",
    "            # simply access dict['key1'], where 'key1' is a key from a dictionary dict ={\"key1\": 'val1'}\n",
    "            # Access just as accessing a column from a dataframe.\n",
    "\n",
    "            anderson_darling_statistic = diagnostic.anderson_statistic(y, dist = (callable_statistical_distributions_dict[statistical_distribution_to_test]), fit = True)\n",
    "            \n",
    "            print(f\"Anderson-Darling statistic for the distribution {statistical_distribution_to_test} = {anderson_darling_statistic}\\n\")\n",
    "            print(\"The Anderson–Darling test assesses whether a sample comes from a specified distribution.\")\n",
    "            print(\"It makes use of the fact that, when given a hypothesized underlying distribution and assuming the data does arise from this distribution, the cumulative distribution function (CDF) of the data can be assumed to follow a uniform distribution.\")\n",
    "            print(\"Then, data can be tested for uniformity using an appropriate distance test (Shapiro 1980).\\n\")\n",
    "            # source: https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\n",
    "\n",
    "            # Fit the distribution and get its parameters\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html\n",
    "\n",
    "            distribution_parameters = callable_statistical_distributions_dict[statistical_distribution_to_test].fit(y)\n",
    "            \n",
    "            # With method=\"MLE\" (default), the fit is computed by minimizing the negative \n",
    "            # log-likelihood function. A large, finite penalty (rather than infinite negative \n",
    "            # log-likelihood) is applied for observations beyond the support of the distribution. \n",
    "            # With method=\"MM\", the fit is computed by minimizing the L2 norm of the relative errors \n",
    "            # between the first k raw (about zero) data moments and the corresponding distribution \n",
    "            # moments, where k is the number of non-fixed parameters. \n",
    "\n",
    "            # distribution_parameters: Estimates for any shape parameters (if applicable), \n",
    "            # followed by those for location and scale.\n",
    "            print(f\"Distribution shape parameters calculated for {statistical_distribution_to_test} = {distribution_parameters}\\n\")\n",
    "            \n",
    "            print(\"ATTENTION:\\n\")\n",
    "            print(\"The critical values for the Anderson-Darling test are dependent on the specific distribution that is being tested.\")\n",
    "            print(\"Tabulated values and formulas have been published (Stephens, 1974, 1976, 1977, 1979) for a few specific distributions: normal, lognormal, exponential, Weibull, logistic, extreme value type 1.\")\n",
    "            print(\"The test consists on an one-sided test of the hypothesis that the distribution is of a specific form.\")\n",
    "            print(\"The hypothesis is rejected if the test statistic, A, is greater than the critical value for that particular distribution.\")\n",
    "            print(\"Note that, for a given distribution, the Anderson-Darling statistic may be multiplied by a constant which usually depends on the sample size, n).\")\n",
    "            print(\"These constants are given in the various papers by Stephens, and may be simply referred as the \\'adjusted Anderson-Darling statistic\\'.\")\n",
    "            print(\"This adjusted statistic is what should be compared against the critical values.\")\n",
    "            print(\"Also, be aware that different constants (and therefore critical values) have been published.\")\n",
    "            print(\"Therefore, you just need to be aware of what constant was used for a given set of critical values (the needed constant is typically given with the correspondent critical values).\")\n",
    "            print(\"To learn more about the Anderson-Darling statistics and the check the full references of Stephens, go to the webpage from the National Institute of Standards and Technology (NIST):\\n\")\n",
    "            print(\"https://itl.nist.gov/div898/handbook/eda/section3/eda3e.htm\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            #Create general statistics dictionary:\n",
    "            general_statistics_dict = {\n",
    "\n",
    "                \"series_mean\": y.mean(),\n",
    "                \"series_variance\": y.var(),\n",
    "                \"series_standard_deviation\": y.std(),\n",
    "                \"series_skewness\": data_skew,\n",
    "                \"series_kurtosis\": data_kurtosis,\n",
    "                \"series_mode\": data_mode,\n",
    "                \"AndersonDarling_statistic_A\": anderson_darling_statistic,\n",
    "                \"distribution_parameters\": distribution_parameters\n",
    "\n",
    "            }\n",
    "\n",
    "            # Add this dictionary to the series dictionary:\n",
    "            series_dict['general_statistics'] = general_statistics_dict\n",
    " \n",
    "            # Now, append the series dictionary to the support list:\n",
    "            support_list.append(series_dict)\n",
    "        \n",
    "    # Now we left the for loop, make the list of dicts support list itself:\n",
    "    list_of_dicts = support_list\n",
    "    \n",
    "    print(\"General statistics successfully returned in the list \\'list_of_dicts\\'.\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(list_of_dicts)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(list_of_dicts)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Note: the obtention of the probability plot specific for each distribution requires shape parameters.\")\n",
    "    print(\"Check Scipy documentation for additional information:\")\n",
    "    print(\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html\")\n",
    "    \n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3422ebca-601b-4d56-b907-309605528865"
   },
   "source": [
    "# **Function for column filtering (selecting); ordering; or renaming all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "azdata_cell_guid": "2af59445-dfd7-4f3e-8b53-b0616e353822",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def select_order_or_rename_columns (df, columns_list, mode = 'select_or_order_columns'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # MODE = 'select_or_order_columns' for filtering only the list of columns passed as columns_list,\n",
    "    # and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "    # the order of elements on the list will be the new order of columns.\n",
    "\n",
    "    # MODE = 'rename_columns' for renaming the columns with the names passed as columns_list. In this\n",
    "    # mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "    # the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "    # will result into columns with incorrect names.\n",
    "    \n",
    "    # columns_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: columns_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{DATASET.columns}\\n\")\n",
    "    \n",
    "    if ((columns_list is None) | (columns_list == np.nan)):\n",
    "        # empty list\n",
    "        columns_list = []\n",
    "    \n",
    "    if (len(columns_list) == 0):\n",
    "        print(\"Please, input a valid list of columns.\\n\")\n",
    "        return DATASET\n",
    "    \n",
    "    if (mode == 'select_or_order_columns'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        DATASET = DATASET[columns_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\\n\")\n",
    "        print(\"Check the new dataframe:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET)\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET)\n",
    "        \n",
    "    elif (mode == 'rename_columns'):\n",
    "        \n",
    "        # Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(columns_list) == len(DATASET.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\\n\")\n",
    "            return DATASET\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            DATASET.columns = columns_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\\n\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\\n\")\n",
    "            print(\"Check the new dataframe:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(DATASET)\n",
    "\n",
    "            except: # regular mode\n",
    "                print(DATASET)\n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'select_or_order_columns\\' or \\'rename_columns\\'.\")\n",
    "        return DATASET\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3179f8b3-1e11-49fb-b1bc-10ff43fca76b"
   },
   "source": [
    "# **Function for renaming specific columns from the dataframe; or cleaning columns' labels**\n",
    "- The function `select_order_or_rename_columns` requires the user to pass a list containing the names from all columns.\n",
    "- Also, this list must contain the columns in the correct order (the order they appear in the dataframe).\n",
    "- This function may manipulate one or several columns by call, and is not dependent on their order.\n",
    "- This function can also be used for cleaning the columns' labels: capitalize (upper case) or lower cases of all columns' names; replace substrings on columns' names; or eliminating trailing and leading white spaces or characters from columns' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "azdata_cell_guid": "cc513d9e-835a-4a6f-a4dc-c4d810ef1b32",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def rename_or_clean_columns_labels (df, mode = 'set_new_names', substring_to_be_replaced = ' ', new_substring_for_replacement = '_', trailing_substring = None, list_of_columns_labels = [{'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    # Pandas .rename method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "    \n",
    "    # mode = 'set_new_names' will change the columns according to the specifications in\n",
    "    # list_of_columns_labels.\n",
    "    \n",
    "    # list_of_columns_labels = [{'column_name': None, 'new_column_name': None}]\n",
    "    # This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "    # the first one contains the original column name; and the second one contains the new name\n",
    "    # that will substitute the original one. The function will loop through all dictionaries in\n",
    "    # this list, access the values of the keys 'column_name', and it will be replaced (switched) \n",
    "    # by the correspondent value in key 'new_column_name'.\n",
    "    \n",
    "    # The object list_of_columns_labels must be declared as a list, \n",
    "    # in brackets, even if there is a single dictionary.\n",
    "    # Use always the same keys: 'column_name' for the original label; \n",
    "    # and 'new_column_name', for the correspondent new label.\n",
    "    # Notice that this function will not search substrings: it will substitute a value only when\n",
    "    # there is perfect correspondence between the string in 'column_name' and one of the columns\n",
    "    # labels. So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "    # values.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'column_name': original_col, 'new_column_name': new_col}, \n",
    "    # where original_col and new_col represent the strings for searching and replacement \n",
    "    # (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    # Example: list_of_columns_labels = [{'column_name': 'col1', 'new_column_name': 'col'}] will\n",
    "    # rename 'col1' as 'col'.\n",
    "    \n",
    "    \n",
    "    # mode = 'capitalize_columns' will capitalize all columns names (i.e., they will be put in\n",
    "    # upper case). e.g. a column named 'column' will be renamed as 'COLUMN'\n",
    "    \n",
    "    # mode = 'lowercase_columns' will lower the case of all columns names. e.g. a column named\n",
    "    # 'COLUMN' will be renamed as 'column'.\n",
    "    \n",
    "    # mode = 'replace_substring' will search on the columns names (strings) for the \n",
    "    # substring_to_be_replaced (which may be a character or a string); and will replace it by \n",
    "    # new_substring_for_replacement (which again may be either a character or a string). \n",
    "    # Numbers (integers or floats) will be automatically converted into strings.\n",
    "    # As an example, consider the default situation where we search for a whitespace ' ' \n",
    "    # and replace it by underscore '_': \n",
    "    # substring_to_be_replaced = ' ', new_substring_for_replacement = '_'  \n",
    "    # In this case, a column named 'new column' will be renamed as 'new_column'.\n",
    "    \n",
    "    # mode = 'trim' will remove all trailing or leading whitespaces from column names.\n",
    "    # e.g. a column named as ' col1 ' will be renamed as 'col1'; 'col2 ' will be renamed as\n",
    "    # 'col2'; and ' col3' will be renamed as 'col3'.\n",
    "    \n",
    "    # mode = 'eliminate_trailing_characters' will eliminate a defined trailing and leading \n",
    "    # substring from the columns' names. \n",
    "    # The substring must be indicated as trailing_substring, and its default, when no value\n",
    "    # is provided, is equivalent to mode = 'trim' (eliminate white spaces). \n",
    "    # e.g., if trailing_substring = '_test' and you have a column named 'col_test', it will be \n",
    "    # renamed as 'col'.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the columns were read as strings:\n",
    "    DATASET.columns = (DATASET.columns).astype(str)\n",
    "    # dataframe.columns is a Pandas Index object, so it has the dtype attribute as other Pandas\n",
    "    # objects. So, we can use the astype method to set its type as str or 'object' (or \"O\").\n",
    "    # Notice that there are situations with int Index, when integers are used as column names or\n",
    "    # as row indices. So, this portion guarantees that we can call the str attribute to apply string\n",
    "    # methods.\n",
    "    \n",
    "    if (mode == 'set_new_names'):\n",
    "        \n",
    "        # Start a mapping dictionary:\n",
    "        mapping_dict = {}\n",
    "        # This dictionary will be in the format required by .rename method: old column name as key,\n",
    "        # and new name as value.\n",
    "\n",
    "        # Loop through each element from list_of_columns_labels:\n",
    "        for dictionary in list_of_columns_labels:\n",
    "\n",
    "            # Access the values in keys:\n",
    "            column_name = dictionary['column_name']\n",
    "            new_column_name = dictionary['new_column_name']\n",
    "\n",
    "            # Check if neither is None:\n",
    "            if ((column_name is not None) & (new_column_name is not None)):\n",
    "                \n",
    "                # Guarantee that both were read as strings:\n",
    "                column_name = str(column_name)\n",
    "                new_column_name = str(new_column_name)\n",
    "\n",
    "                # Add it to the mapping dictionary setting column_name as key, and the new name as the\n",
    "                # value:\n",
    "                mapping_dict[column_name] = new_column_name\n",
    "\n",
    "        # Now, the dictionary is in the correct format for the method. Let's apply it:\n",
    "        DATASET.rename(columns = mapping_dict, inplace = True)\n",
    "    \n",
    "    elif (mode == 'capitalize_columns'):\n",
    "        \n",
    "        DATASET.rename(str.upper, axis = 'columns', inplace = True)\n",
    "    \n",
    "    elif (mode == 'lowercase_columns'):\n",
    "        \n",
    "        DATASET.rename(str.lower, axis = 'columns', inplace = True)\n",
    "    \n",
    "    elif (mode == 'replace_substring'):\n",
    "        \n",
    "        if (substring_to_be_replaced is None):\n",
    "            # set as the default (whitespace):\n",
    "            substring_to_be_replaced = ' '\n",
    "        \n",
    "        if (new_substring_for_replacement is None):\n",
    "            # set as the default (underscore):\n",
    "            new_substring_for_replacement = '_'\n",
    "        \n",
    "        # Apply the str attribute to guarantee that numbers were read as strings:\n",
    "        substring_to_be_replaced = str(substring_to_be_replaced)\n",
    "        new_substring_for_replacement = str(new_substring_for_replacement)\n",
    "        # Replace the substrings in the columns' names:\n",
    "        substring_replaced_series = (pd.Series(DATASET.columns)).str.replace(substring_to_be_replaced, new_substring_for_replacement)\n",
    "        # The Index object is not callable, and applying the str attribute to a np.array or to a list\n",
    "        # will result in a single string concatenating all elements from the array. So, we convert\n",
    "        # the columns index to a pandas series for performing a element-wise string replacement.\n",
    "        \n",
    "        # Now, convert the columns to the series with the replaced substrings:\n",
    "        DATASET.columns = substring_replaced_series\n",
    "        \n",
    "    elif (mode == 'trim'):\n",
    "        # Use the strip method from str attribute with no argument, correspondening to the\n",
    "        # Trim function.\n",
    "        DATASET.rename(str.strip, axis = 'columns', inplace = True)\n",
    "    \n",
    "    elif (mode == 'eliminate_trailing_characters'):\n",
    "        \n",
    "        if ((trailing_substring is None) | (trailing_substring == np.nan)):\n",
    "            # Apply the str.strip() with no arguments:\n",
    "            DATASET.rename(str.strip, axis = 'columns', inplace = True)\n",
    "        \n",
    "        else:\n",
    "            # Apply the str attribute to guarantee that numbers were read as strings:\n",
    "            trailing_substring = str(trailing_substring)\n",
    "\n",
    "            # Apply the strip method:\n",
    "            stripped_series = (pd.Series(DATASET.columns)).str.strip(trailing_substring)\n",
    "            # The Index object is not callable, and applying the str attribute to a np.array or to a list\n",
    "            # will result in a single string concatenating all elements from the array. So, we convert\n",
    "            # the columns index to a pandas series for performing a element-wise string replacement.\n",
    "\n",
    "            # Now, convert the columns to the series with the stripped strings:\n",
    "            DATASET.columns = stripped_series\n",
    "    \n",
    "    else:\n",
    "        print(\"Select a valid mode: \\'set_new_names\\', \\'capitalize_columns\\', \\'lowercase_columns\\', \\'replace_substrings\\', \\'trim\\', or \\'eliminate_trailing_characters\\'.\\n\")\n",
    "        return \"error\"\n",
    "    \n",
    "    print(\"Finished renaming dataframe columns.\\n\")\n",
    "    print(\"Check the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET)\n",
    "        \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e7542807-b6a5-482b-a6a7-024db246f837"
   },
   "source": [
    "# **Function for merging (joining) the data on a timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MERGE_ON_TIMESTAMP (df_left, df_right, left_key, right_key, how_to_join = \"inner\", merge_method = 'asof', merged_suffixes = ('_left', '_right'), asof_direction = 'nearest', ordered_filling = 'ffill'):\n",
    "    \n",
    "    #WARNING: Only two dataframes can be merged on each call of the function.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df_left: dataframe to be joined as the left one.\n",
    "    \n",
    "    # df_right: dataframe to be joined as the right one\n",
    "    \n",
    "    # left_key: (String) name of column of the left dataframe to be used as key for joining.\n",
    "    \n",
    "    # right_key: (String) name of column of the right dataframe to be used as key for joining.\n",
    "    \n",
    "    # how_to_join: joining method: \"inner\", \"outer\", \"left\", \"right\". The default is \"inner\".\n",
    "    \n",
    "    # merge_method: which pandas merging method will be applied:\n",
    "    # merge_method = 'ordered' for using the .merge_ordered method.\n",
    "    # merge_method = \"asof\" for using the .merge_asof method.\n",
    "    # WARNING: .merge_asof uses fuzzy matching, so the how_to_join parameter is not applicable.\n",
    "    \n",
    "    # merged_suffixes = ('_left', '_right') - tuple of the suffixes to be added to columns\n",
    "    # with equal names. Simply modify the strings inside quotes to modify the standard\n",
    "    # values. If no tuple is provided, the standard denomination will be used.\n",
    "    \n",
    "    # asof_direction: this parameter will only be used if the .merge_asof method is\n",
    "    # selected. The default is 'nearest' to merge the closest timestamps in both \n",
    "    # directions. The other options are: 'backward' or 'forward'.\n",
    "    \n",
    "    # ordered_filling: this parameter will only be used on the merge_ordered method.\n",
    "    # The default is None. Input ordered_filling = 'ffill' to fill missings with the\n",
    "    # previous value.\n",
    "    \n",
    "    # Create dataframe local copies to manipulate, avoiding that Pandas operates on\n",
    "    # the original objects; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DF_LEFT = df_left.copy(deep = True)\n",
    "    DF_RIGHT = df_right.copy(deep = True)\n",
    "    \n",
    "    # Firstly, let's guarantee that the keys were actually read as timestamps of the same type.\n",
    "    # We will do that by converting all values to np.datetime64. If fails, then\n",
    "    # try to convert to Pandas timestamps.\n",
    "    \n",
    "    # try parsing as np.datetime64:\n",
    "    try:\n",
    "        DF_LEFT[left_key] = DF_LEFT[left_key].astype(np.datetime64)\n",
    "        DF_RIGHT[right_key] = DF_RIGHT[right_key].astype(np.datetime64)\n",
    "    \n",
    "    except:\n",
    "        # Try conversion to pd.Timestamp (less efficient, involves looping)\n",
    "        # 1. Start lists to store the Pandas timestamps:\n",
    "        timestamp_list_left = []\n",
    "        timestamp_list_right = []\n",
    "\n",
    "        # 2. Loop through each element of the timestamp columns left_key and right_key, \n",
    "        # and apply the function to guarantee that all elements are Pandas timestamps\n",
    "\n",
    "        # left dataframe:\n",
    "        for timestamp in DF_LEFT[left_key]:\n",
    "            #Access each element 'timestamp' of the series df[timestamp_tag_column]\n",
    "            timestamp_list_left.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "\n",
    "        # right dataframe:\n",
    "        for timestamp in DF_RIGHT[right_key]:\n",
    "            #Access each element 'timestamp' of the series df[timestamp_tag_column]\n",
    "            timestamp_list_right.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "\n",
    "        # 3. Set the key columns as the lists of objects converted to Pandas dataframes:\n",
    "        DF_LEFT[left_key] = timestamp_list_left\n",
    "        DF_RIGHT[right_key] = timestamp_list_right\n",
    "    \n",
    "    \n",
    "    # Now, even if the dates were read as different types of variables (like string for one\n",
    "    # and datetime for the other), we converted them to a same type (Pandas timestamp), avoiding\n",
    "    # compatibility issues.\n",
    "    \n",
    "    # For performing merge 'asof', the timestamps must be previously sorted in ascending order.\n",
    "    # Pandas sort_values method: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    # Let's sort the dataframes in ascending order of timestamps before merging:\n",
    "    \n",
    "    DF_LEFT = DF_LEFT.sort_values(by = left_key, ascending = True)\n",
    "    DF_RIGHT = DF_RIGHT.sort_values(by = right_key, ascending = True)\n",
    "    \n",
    "    # Reset indices:\n",
    "    DF_LEFT = DF_LEFT.reset_index(drop = True)\n",
    "    DF_RIGHT = DF_RIGHT.reset_index(drop = True)\n",
    "        \n",
    "    \n",
    "    if (merge_method == 'ordered'):\n",
    "    \n",
    "        if (ordered_filling == 'ffill'):\n",
    "            \n",
    "            merged_df = pd.merge_ordered(DF_LEFT, DF_RIGHT, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes, fill_method='ffill')\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            merged_df = pd.merge_ordered(DF_LEFT, DF_RIGHT, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    elif (merge_method == 'asof'):\n",
    "        \n",
    "        merged_df = pd.merge_asof(DF_LEFT, DF_RIGHT, left_on = left_key, right_on = right_key, suffixes = merged_suffixes, direction = asof_direction)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"You did not enter a valid merge method for this function, \\'ordered\\' or \\'asof\\'.\")\n",
    "        print(\"Then, applying the conventional Pandas .merge method, followed by .sort_values method.\\n\")\n",
    "        \n",
    "        #Pandas sort_values method: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "        \n",
    "        merged_df = DF_LEFT.merge(DF_RIGHT, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "        merged_df = merged_df.sort_values(by = merged_df.columns[0], ascending = True)\n",
    "        #sort by the first column, with index 0.\n",
    "    \n",
    "    # Now, reset index positions of the merged dataframe:\n",
    "    merged_df = merged_df.reset_index(drop = True)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully merged. Check its 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(merged_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(merged_df.head(10))\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2146f034-2414-4933-90e7-95115d8f7ec0"
   },
   "source": [
    "# **Function for merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "3ecdb29d-a8d3-4c36-8854-fc1243f873bd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def MERGE_AND_SORT_DATAFRAMES (df_left, df_right, left_key, right_key, how_to_join = \"inner\", merged_suffixes = ('_left', '_right'), sort_merged_df = False, column_to_sort = None, ascending_sorting = True):\n",
    "    \n",
    "    #WARNING: Only two dataframes can be merged on each call of the function.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df_left: dataframe to be joined as the left one.\n",
    "    \n",
    "    # df_right: dataframe to be joined as the right one\n",
    "    \n",
    "    # left_key: (String) name of column of the left dataframe to be used as key for joining.\n",
    "    \n",
    "    # right_key: (String) name of column of the right dataframe to be used as key for joining.\n",
    "    \n",
    "    # how_to_join: joining method: \"inner\", \"outer\", \"left\", \"right\". The default is \"inner\".\n",
    "    \n",
    "    # merge_method: which pandas merging method will be applied:\n",
    "    # merge_method = 'ordered' for using the .merge_ordered method.\n",
    "    # merge_method = \"asof\" for using the .merge_asof method.\n",
    "    # WARNING: .merge_asof uses fuzzy matching, so the how_to_join parameter is not applicable.\n",
    "    \n",
    "    # merged_suffixes = ('_left', '_right') - tuple of the suffixes to be added to columns\n",
    "    # with equal names. Simply modify the strings inside quotes to modify the standard\n",
    "    # values. If no tuple is provided, the standard denomination will be used.\n",
    "    \n",
    "    # sort_merged_df = False not to sort the merged dataframe. If you want to sort it,\n",
    "    # set as True. If sort_merged_df = True and column_to_sort = None, the dataframe will\n",
    "    # be sorted by its first column.\n",
    "    \n",
    "    # column_to_sort = None. Keep it None if the dataframe should not be sorted.\n",
    "    # Alternatively, pass a string with a column name to sort, such as:\n",
    "    # column_to_sort = 'col1'; or a list of columns to use for sorting: column_to_sort = \n",
    "    # ['col1', 'col2']\n",
    "    \n",
    "    # ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "    # ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "    # you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "    # list of booleans like ascending_sorting = [False, True] - the first column of the list\n",
    "    # will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "    # the correspondence is element-wise: the boolean in list ascending_sorting will correspond \n",
    "    # to the sorting order of the column with the same position in list column_to_sort.\n",
    "    # If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "    # Create dataframe local copies to manipulate, avoiding that Pandas operates on\n",
    "    # the original objects; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DF_LEFT = df_left.copy(deep = True)\n",
    "    DF_RIGHT = df_right.copy(deep = True)\n",
    "    \n",
    "    # check if the keys are the same:\n",
    "    boolean_check = (left_key == right_key)\n",
    "    # if boolean_check is True, we will merge using the on parameter, instead of left_on and right_on:\n",
    "    \n",
    "    if (boolean_check): # runs if it is True:\n",
    "        \n",
    "        merged_df = DF_LEFT.merge(DF_RIGHT, on = left_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    else:\n",
    "        # use left_on and right_on\n",
    "        merged_df = DF_LEFT.merge(DF_RIGHT, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    # Check if the dataframe should be sorted:\n",
    "    if (sort_merged_df == True):\n",
    "        \n",
    "        # check if column_to_sort = None. If it is, set it as the first column (index 0):\n",
    "        if (column_to_sort is None):\n",
    "            \n",
    "            column_to_sort = merged_df.columns[0]\n",
    "            print(f\"Sorting merged dataframe by its first column = {column_to_sort}\\n\")\n",
    "        \n",
    "        # check if ascending_sorting is None. If it is, set it as True:\n",
    "        if (ascending_sorting is None):\n",
    "            \n",
    "            ascending_sorting = True\n",
    "            print(\"Sorting merged dataframe in ascending order.\\n\")\n",
    "        \n",
    "        # Now, sort the dataframe according to the parameters:\n",
    "        merged_df = merged_df.sort_values(by = column_to_sort, ascending = ascending_sorting)\n",
    "        #sort by the first column, with index 0.\n",
    "    \n",
    "        # Now, reset index positions:\n",
    "        merged_df = merged_df.reset_index(drop = True)\n",
    "        print(\"Merged dataframe successfully sorted.\\n\")\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully merged. Check its 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(merged_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(merged_df.head(10))\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fa008248-19ed-495e-9f0a-7983443f5cdf"
   },
   "source": [
    "# **Function for dropping specific columns or rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8a5e07e9-ebb4-4063-8607-6559b5afd7f9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def drop_columns_or_rows (df, what_to_drop = 'columns', cols_list = None, row_index_list = None, reset_index_after_drop = True):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # check https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html?highlight=drop\n",
    "    \n",
    "    # what_to_drop = 'columns' for removing the columns specified by their names (headers)\n",
    "    # in cols_list (a list of strings).\n",
    "    # what_to_drop = 'rows' for removing the rows specified by their indices in\n",
    "    # row_index_list (a list of integers). Remember that the indexing starts from zero, i.e.,\n",
    "    # the first row is row number zero.\n",
    "    \n",
    "    # cols_list = list of strings containing the names (headers) of the columns to be removed\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # remove columns 'col1', 'col2', and 'col3' from the dataframe.\n",
    "    # If a single column will be dropped, you can declare it as a string (outside a list)\n",
    "    # e.g. cols_list = 'col1'; or cols_list = ['col1']\n",
    "    \n",
    "    # row_index_list = a list of integers containing the indices of the rows that will be dropped.\n",
    "    # e.g. row_index_list = [0, 1, 2] will drop the rows with indices 0 (1st row), 1 (2nd row), and\n",
    "    # 2 (third row). Again, if a single row will be dropped, you can declare it as an integer (outside\n",
    "    # a list).\n",
    "    # e.g. row_index_list = 20 or row_index_list = [20] to drop the row with index 20 (21st row).\n",
    "    \n",
    "    # reset_index_after_drop = True. keep it True to restarting the indexing numeration after dropping.\n",
    "    # Alternatively, set reset_index_after_drop = False to keep the original numeration (the removed indices\n",
    "    # will be missing).\n",
    "    \n",
    "    # Create dataframe local copy to manipulate, avoiding that Pandas operates on\n",
    "    # the original object; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    if (what_to_drop == 'columns'):\n",
    "        \n",
    "        if (cols_list is None):\n",
    "            #check if a list was not input:\n",
    "            print(\"Input a list of columns cols_list to be dropped.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            #Drop the columns in cols_list:\n",
    "            DATASET = DATASET.drop(columns = cols_list)\n",
    "            print(f\"The columns in {cols_list} headers list were successfully removed.\\n\")\n",
    "    \n",
    "    elif (what_to_drop == 'rows'):\n",
    "        \n",
    "        if (row_index_list is None):\n",
    "            #check if a list was not input:\n",
    "            print(\"Input a list of rows indices row_index_list to be dropped.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            #Drop the rows in row_index_list:\n",
    "            DATASET = DATASET.drop(row_index_list)\n",
    "            print(f\"The rows in {row_index_list} indices list were successfully removed.\\n\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Input a valid string as what_to_drop, rows or columns.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    if (reset_index_after_drop == True):\n",
    "        \n",
    "        #restart the indexing\n",
    "        DATASET = DATASET.reset_index(drop = True)\n",
    "        print(\"The indices of the dataset were successfully restarted.\\n\")\n",
    "    \n",
    "    print(\"Check the 10 first rows from the returned dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "17c3a055-1d94-466c-9d24-55cb2c6300df"
   },
   "source": [
    "# **Function for removing duplicate rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2d4c0c56-86a9-4340-a066-2925072f62a9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_rows (df, list_of_columns_to_analyze = None, which_row_to_keep = 'first', reset_index_after_drop = True):\n",
    "    \n",
    "    import pandas as pd\n",
    "    # check https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\n",
    "    \n",
    "    # if list_of_columns_to_analyze = None, the whole dataset will be analyzed, i.e., rows\n",
    "    # will be removed only if they have same values for all columns from the dataset.\n",
    "    # Alternatively, pass a list of columns names (strings), if you want to remove rows with\n",
    "    # same values for that combination of columns. Pass it as a list, even if there is a single column\n",
    "    # being declared.\n",
    "    # e.g. list_of_columns_to_analyze = ['column1'] will check only 'column1'. Entries with same value\n",
    "    # on 'column1' will be considered duplicates and will be removed.\n",
    "    # list_of_columns_to_analyze = ['col1', 'col2',  'col3'] will analyze the combination of 3 columns:\n",
    "    # 'col1', 'col2', and 'col3'. Only rows with same value for these 3 columns will be considered\n",
    "    # duplicates and will be removed.\n",
    "    \n",
    "    # which_row_to_keep = 'first' will keep the first detected row and remove all other duplicates. If\n",
    "    # None or an invalid string is input, this method will be selected.\n",
    "    # which_row_to_keep = 'last' will keep only the last detected duplicate row, and remove all the others.\n",
    "    \n",
    "    # reset_index_after_drop = True. keep it True to restarting the indexing numeration after dropping.\n",
    "    # Alternatively, set reset_index_after_drop = False to keep the original numeration (the removed indices\n",
    "    # will be missing).\n",
    "    \n",
    "    # Create dataframe local copy to manipulate, avoiding that Pandas operates on\n",
    "    # the original object; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    if (which_row_to_keep == 'last'):\n",
    "        \n",
    "        #keep only the last duplicate.\n",
    "        if (list_of_columns_to_analyze is None):\n",
    "            # use the whole dataset\n",
    "            DATASET = DATASET.drop_duplicates(keep = 'last')\n",
    "            print(f\"The rows with duplicate entries were successfully removed.\")\n",
    "            print(\"Only the last one of the duplicate entries was kept in the dataset.\\n\")\n",
    "        \n",
    "        else:\n",
    "            #use the subset of columns\n",
    "            if (list_of_columns_to_analyze is None):\n",
    "                #check if a list was not input:\n",
    "                print(\"Input a list of columns list_of_columns_to_analyze to be analyzed.\")\n",
    "                return \"error\"\n",
    "        \n",
    "            else:\n",
    "                #Drop the columns in cols_list:\n",
    "                DATASET = DATASET.drop_duplicates(subset = list_of_columns_to_analyze, keep = 'last')\n",
    "                print(f\"The rows with duplicate values for the columns in {list_of_columns_to_analyze} headers list were successfully removed.\")\n",
    "                print(\"Only the last one of the duplicate entries was kept in the dataset.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #keep only the first duplicate.\n",
    "        if (list_of_columns_to_analyze is None):\n",
    "            # use the whole dataset\n",
    "            DATASET = DATASET.drop_duplicates()\n",
    "            print(f\"The rows with duplicate entries were successfully removed.\")\n",
    "            print(\"Only the first one of the duplicate entries was kept in the dataset.\\n\")\n",
    "        \n",
    "        else:\n",
    "            #use the subset of columns\n",
    "            if (list_of_columns_to_analyze is None):\n",
    "                #check if a list was not input:\n",
    "                print(\"Input a list of columns list_of_columns_to_analyze to be analyzed.\")\n",
    "                return \"error\"\n",
    "        \n",
    "            else:\n",
    "                #Drop the columns in cols_list:\n",
    "                DATASET = DATASET.drop_duplicates(subset = list_of_columns_to_analyze)\n",
    "                print(f\"The rows with duplicate values for the columns in {list_of_columns_to_analyze} headers list were successfully removed.\")\n",
    "                print(\"Only the first one of the duplicate entries was kept in the dataset.\\n\")\n",
    "    \n",
    "    if (reset_index_after_drop == True):\n",
    "        \n",
    "        #restart the indexing\n",
    "        DATASET = DATASET.reset_index(drop = True)\n",
    "        print(\"The indices of the dataset were successfully restarted.\\n\")\n",
    "    \n",
    "    print(\"Check the 10 first rows from the returned dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2040252f-0584-40ff-82a7-63832be658b9"
   },
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "18a70330-b470-49e7-9db2-388b8604296b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def export_pd_dataframe_as_csv (dataframe_obj_to_be_exported, new_file_name_without_extension, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_obj_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # new_file_name_without_extension - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "    # will export a file 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name_without_extension)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_obj_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_without_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "51aa738c-49ca-4367-bb52-2379be4c8ee7"
   },
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2a33d927-d39e-41da-85bc-08a419aa365b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2f28e624-5865-4f14-9f1d-adbcad1b102d"
   },
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "48c2e99c-f10f-4fa1-89fd-ebffe227185f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "32230c8d-f1cf-420e-be2e-f8cd95114776",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f0a2be5e-53db-4bfe-9c50-d2fbaa7e8679",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c14a67f9-23ea-4179-8869-319af38cbbb7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b59ae45c-5573-426d-91ba-ec0b08d266a4",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "# Also, html files and webpages may be also read.\n",
    "\n",
    "# You may input the path for an HTML file containing a table to be read; or \n",
    "# a string containing the address for a webpage containing the table. The address must start\n",
    "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
    "# or as FILE_NAME_WITH_EXTENSION.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ffe1bbda-3603-467c-b499-37c007fc70b8",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "45380112-6b2f-4809-9ea3-770492c99ed5"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "81455331-34ae-4454-b9e7-738c19fc5fa2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bc839c36-1ddf-4986-9f29-6de30274edbf"
   },
   "source": [
    "### **Characterizing the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0c0f5927-8bf4-46ec-9f75-7381c749dd64",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = None\n",
    "# TIMESTAMP_TAG_COLUMN: name (header) of the column containing the timestamps. \n",
    "# Keep TIMESTAMP_TAG_COLUMN = None if the dataframe do not contain timestamps.\n",
    "\n",
    "# Dataframe with summary from the categorical variables returned as cat_vars_summary. \n",
    "# Simply modify this object on the left of equality:\n",
    "cat_vars_summary = characterize_categorical_variables (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "88e4c93e-39e3-42b6-b85d-1971d456dade"
   },
   "source": [
    "### **Removing all columns and rows that contain only missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cdda68a5-dfbd-43a4-b373-b41e2dc6098d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_IGNORE = None\n",
    "# list_of_columns_to_ignore: if you do not want to check a specific column, pass its name\n",
    "# (header) as an element from this list. It should be declared as a list even if it contains\n",
    "# a single value.\n",
    "# e.g. list_of_columns_to_ignore = ['column1'] will not analyze missing values in column named\n",
    "# 'column1'; list_of_columns_to_ignore = ['col1', 'col2'] will ignore columns 'col1' and 'col2'\n",
    "\n",
    "# Cleaned dataframe returned as cleaned_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = remove_completely_blank_rows_and_columns (df = DATASET, list_of_columns_to_ignore = LIST_OF_COLUMNS_TO_IGNORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2e585a63-a745-4858-878c-5380756da2bd"
   },
   "source": [
    "### **Visualizing and characterizing distribution of missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9582b58c-86ba-4059-9679-414a7a145224",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SLICE_TIME_WINDOW_FROM = None\n",
    "SLICE_TIME_WINDOW_TO = None    \n",
    "# SLICE_TIME_WINDOW_FROM and SLICE_TIME_WINDOW_TO (timestamps). When analyzing time series,\n",
    "# use these parameters to observe only values in a given time range.\n",
    "    \n",
    "# SLICE_TIME_WINDOW_FROM: the inferior limit of the analyzed window. If you declare this value\n",
    "# and keep SLICE_TIME_WINDOW_TO = None, then you will analyze all values that comes after\n",
    "# SLICE_TIME_WINDOW_FROM.\n",
    "# SLICE_TIME_WINDOW_TO: the superior limit of the analyzed window. If you declare this value\n",
    "# and keep SLICE_TIME_WINDOW_FROM = None, then you will analyze all values until\n",
    "# SLICE_TIME_WINDOW_TO.\n",
    "# If SLICE_TIME_WINDOW_FROM = SLICE_TIME_WINDOW_TO = None, only the standard analysis with\n",
    "# the whole dataset will be performed. If both values are specified, then the specific time\n",
    "# window from 'SLICE_TIME_WINDOW_FROM' to 'SLICE_TIME_WINDOW_TO' will be analyzed.\n",
    "# e.g. SLICE_TIME_WINDOW_FROM = 'May-1976', and SLICE_TIME_WINDOW_TO = 'Jul-1976'\n",
    "# Notice that the timestamps must be declares in quotes, just as strings.\n",
    "\n",
    "AGGREGATE_TIME_IN_TERMS_OF = None    \n",
    "# AGGREGATE_TIME_IN_TERMS_OF = None. Keep it None if you do not want to aggregate the time\n",
    "# series. Alternatively, set AGGREGATE_TIME_IN_TERMS_OF = 'Y' or AGGREGATE_TIME_IN_TERMS_OF = \n",
    "# 'year' to aggregate the timestamps in years; set AGGREGATE_TIME_IN_TERMS_OF = 'M' or\n",
    "# 'month' to aggregate in terms of months; or set AGGREGATE_TIME_IN_TERMS_OF = 'D' or 'day'\n",
    "# to aggregate in terms of days.\n",
    "\n",
    "# Dataframes containing total of missing values and percent of missing values for each variable\n",
    "# returned as total_of_missing_values and percent_of_missing_values.\n",
    "# Simply modify these objects on the left of equality:\n",
    "df_missing_values = visualize_and_characterize_missing_values (df = DATASET, slice_time_window_from = SLICE_TIME_WINDOW_FROM, slice_time_window_to = SLICE_TIME_WINDOW_TO, aggregate_time_in_terms_of = AGGREGATE_TIME_IN_TERMS_OF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d424291b-e7ab-43b3-bfba-d64f267090ea"
   },
   "source": [
    "### **Visualizing missingness across a variable, and comparing it to another variable (both numeric)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a54cb498-b19c-43cc-a29e-c49c63ba1496",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column1'\n",
    "COLUMN_TO_COMPARE_WITH = 'column2'\n",
    "# COLUMN_TO_ANALYZE, COLUMN_TO_COMPARE_WITH: strings (in quotes).\n",
    "# COLUMN_TO_ANALYZE is the column from the dataframe df that will be analyzed in terms of\n",
    "# missingness; whereas COLUMN_TO_COMPARE_WITH is the column to which column_to_analyze will\n",
    "# be compared.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'column1' will analyze 'column1' from df.\n",
    "# COLUMN_TO_COMPARE_WITH = 'column2' will compare 'column1' against 'column2'\n",
    "\n",
    "SHOW_INTERPRETED_EXAMPLE = False\n",
    "# SHOW_INTERPRETED_EXAMPLE: set as True if you want to see an example of a graphic analyzed and\n",
    "# interpreted.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'comparison_of_missing_values.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "visualizing_and_comparing_missingness_across_numeric_vars (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_to_compare_with = COLUMN_TO_COMPARE_WITH, show_interpreted_example = SHOW_INTERPRETED_EXAMPLE, grid = GRID, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "899cf7bd-ad61-4278-8d96-eabedd83394b"
   },
   "source": [
    "### **Dealing with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9d584876-c022-4893-a584-201e4aae26ab",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be manipulated\n",
    "\n",
    "SUBSET_COLUMNS_LIST = None\n",
    "# SUBSET_COLUMNS_LIST = list of columns to look for missing values. Only missing values\n",
    "# in these columns will be considered for deciding which columns to remove.\n",
    "# Declare it as a list of strings inside quotes containing the columns' names to look at,\n",
    "# even if this list contains a single element. e.g. subset_columns_list = ['column1']\n",
    "# will check only 'column1'; whereas subset_columns_list = ['col1', 'col2', 'col3'] will\n",
    "# chek the columns named as 'col1', 'col2', and 'col3'.\n",
    "# ATTENTION: Subsets are considered only for dropping missing values, not for filling.\n",
    "    \n",
    "DROP_MISSING_VAL = True\n",
    "# DROP_MISSING_VAL = True to eliminate the rows containing missing values.\n",
    "# Alternatively: DROP_MISSING_VAL = False to use the filling method.\n",
    "\n",
    "FILL_MISSING_VAL = False\n",
    "# FILL_MISSING_VAL = False. Set this to True to activate the mode for filling the missing\n",
    "# values.\n",
    "\n",
    "ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = False\n",
    "# ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = False - This parameter shows effect only when\n",
    "# DROP_MISSING_VAL = True. If you set ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS = True, then\n",
    "# only the rows where all the columns are missing will be eliminated.\n",
    "# If you define a subset, then only the rows where all the subset columns are missing\n",
    "# will be eliminated.\n",
    "\n",
    "MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT = None\n",
    "# This parameter shows effect only when DROP_MISSING_VAL = True. \n",
    "# If you set MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT equals to an integer value,\n",
    "# then only the rows where at least this integer number of non-missing values will be kept\n",
    "# after dropping the NAs.\n",
    "# e.g. if MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT = 2, only rows containing at\n",
    "# least two columns without missing values will be kept.\n",
    "# If you define a subset, then the criterium is applied only to the subset.\n",
    "\n",
    "VALUE_TO_FILL = None\n",
    "# VALUE_TO_FILL = None - This parameter shows effect only when\n",
    "# FILL_MISSING_VAL = True. Set this parameter as a float value to fill all missing\n",
    "# values with this value. e.g. VALUE_TO_FILL = 0 will fill all missing values with\n",
    "# the number 0. You can also pass a function call like \n",
    "# VALUE_TO_FILL = np.sum(dataset['col1']). In this case, the missing values will be\n",
    "# filled with the sum of the series dataset['col1']\n",
    "# Alternatively, you can also input a string to fill the missing values. e.g.\n",
    "# VALUE_TO_FILL = 'text' will fill all the missing values with the string \"text\".\n",
    "\n",
    "# You can also input a dictionary containing the column(s) to be filled as key(s);\n",
    "# and the values to fill as the correspondent values. For instance:\n",
    "# VALUE_TO_FILL = {'col1': 10} will fill only 'col1' with value 10.\n",
    "# VALUE_TO_FILL = {'col1': 0, 'col2': 'text'} will fill 'col1' with zeros; and will\n",
    "# fill 'col2' with the value 'text'\n",
    "\n",
    "FILL_METHOD = \"fill_with_zeros\"\n",
    "# FILL_METHOD = \"fill_with_zeros\". - This parameter shows effect only \n",
    "# when FILL_MISSING_VAL = True.\n",
    "# Alternatively: FILL_METHOD = \"fill_with_zeros\" - fill all the missing values with 0\n",
    "    \n",
    "# FILL_METHOD = \"fill_with_value_to_fill\" - fill the missing values with the value\n",
    "# defined as the parameter value_to_fill\n",
    "    \n",
    "# FILL_METHOD = \"fill_with_avg_or_mode\" - fill the missing values with the average value for \n",
    "# each column, if the column is numeric; or fill with the mode, if the column is categorical.\n",
    "# The mode is the most commonly observed value.\n",
    "    \n",
    "# FILL_METHOD = \"ffill\" - Forward (pad) fill: propagate last valid observation forward \n",
    "# to next valid.\n",
    "# FILL_METHOD = 'bfill' - backfill: use next valid observation to fill gap.\n",
    "# FILL_METHOD = 'nearest' - 'ffill' or 'bfill', depending if the point is closest to the\n",
    "# next or to the previous non-missing value.\n",
    "\n",
    "# FILL_METHOD = \"fill_by_interpolating\" - fill by interpolating the previous and the \n",
    "# following value. For categorical columns, it fills the\n",
    "# missing with the previous value, just as like FILL_METHOD = 'ffill'\n",
    "\n",
    "INTERPOLATION_ORDER = 'linear'\n",
    "# INTERPOLATION_ORDER: order of the polynomial used for interpolating if fill_method =\n",
    "# \"fill_by_interpolating\". If INTERPOLATION_ORDER = None, INTERPOLATION_ORDER = 'linear',\n",
    "# or INTERPOLATION_ORDER = 1, a linear (1st-order polynomial) will be used.\n",
    "# If INTERPOLATION_ORDER is an integer > 1, then it will represent the polynomial order.\n",
    "# e.g. INTERPOLATION_ORDER = 2, for a 2nd-order polynomial; INTERPOLATION_ORDER = 3 for a\n",
    "# 3rd-order, and so on.\n",
    "    \n",
    "# WARNING: if the fillna method is selected (FILL_MISSING_VAL == True), but no filling\n",
    "# methodology is selected, the missing values of the dataset will be filled with 0.\n",
    "# The same applies when a non-valid fill methodology is selected.\n",
    "# Pandas fillna method does not allow us to fill only a selected subset.\n",
    "# WARNING: if FILL_METHOD == \"fill_with_value_to_fill\" but value_to_fill is None, the \n",
    "# missing values will be filled with the value 0.\n",
    "\n",
    "\n",
    "# New dataframe saved as cleaned_df\n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = handle_missing_values (df = DATASET, subset_columns_list = SUBSET_COLUMNS_LIST, drop_missing_val = DROP_MISSING_VAL, fill_missing_val = FILL_MISSING_VAL, eliminate_only_completely_empty_rows = ELIMINATE_ONLY_COMPLETELY_EMPTY_ROWS, min_number_of_non_missing_val_for_a_row_to_be_kept = MINIMUM_NUMBER_OF_NON_MISSING_VALUES_FOR_A_ROW_TO_BE_KEPT, value_to_fill = VALUE_TO_FILL, fill_method = FILL_METHOD, interpolation_order = INTERPOLATION_ORDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "44ebe8c8-d008-45ab-837b-d45eaad0f78c"
   },
   "source": [
    "### **Advanced imputation on time series data: finding the best imputation strategy for missing values on a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e80bb137-877f-4b4f-af6b-1e2755a788aa",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function handles only one column by call, whereas handle_missing_values can process the whole\n",
    "# dataframe at once.\n",
    "# The strategies used for handling missing values is different here. You can use the function to\n",
    "# process data that does not come from time series, but only plot the graphs for time series data.  \n",
    "# This function is more indicated for dealing with missing values on time series data than handle_missing_values.\n",
    "# This function will search for the best imputer for a given column.\n",
    "# It can process both numerical and categorical columns.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be manipulated\n",
    "\n",
    "COLUMN_TO_FILL = None\n",
    "# string (in quotes) indicating the column with missing values to fill.\n",
    "# e.g. if COLUMN_TO_FILL = 'col1', imputations will be performed on column 'col1'.\n",
    "    \n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp\"\n",
    "# TIMESTAMP_TAG_COLUMN = None. string containing the name of the column with the timestamp. \n",
    "# If TIMESTAMP_TAG_COLUMN is None, the index will be used for testing different imputations.\n",
    "# be the time series reference. declare as a string under quotes. This is the column from \n",
    "# which we will extract the timestamps or values with temporal information. e.g.\n",
    "# TIMESTAMP_TAG_COLUMN = 'timestamp' will consider the column 'timestamp' a time column.\n",
    "\n",
    "TEST_VALUE_TO_FILL = None\n",
    "# TEST_VALUE_TO_FILL: the function will test the imputation of a constant. Specify this constant here\n",
    "# or the tested constant will be zero. e.g. TEST_VALUE_TO_FILL = None will test the imputation of 0.\n",
    "# TEST_VALUE_TO_FILL = 10 will test the imputation of value zero.\n",
    "\n",
    "SHOW_IMPUTATION_COMPARISON_PLOTS = True\n",
    "# SHOW_IMPUTATION_COMPARISON_PLOTS = True. Keep it True to plot the scatter plot comparison\n",
    "# between imputed and original values, as well as the Kernel density estimate (KDE) plot.\n",
    "# Alternatively, set SHOW_IMPUTATION_COMPARISON_PLOTS = False to omit the plots.\n",
    "\n",
    "# The following imputation techniques will be tested, and the best one will be automatically\n",
    "# selected: mean_imputer, median_imputer, mode_imputer, constant_imputer, linear_interpolation,\n",
    "# quadratic_interpolation, cubic_interpolation, nearest_interpolation, bfill_imputation,\n",
    "# ffill_imputation, knn_imputer, mice_imputer (MICE = Multiple Imputations by Chained Equations).\n",
    "    \n",
    "# MICE: Performs multiple regressions over random samples of the data; \n",
    "# Takes the average of multiple regression values; and imputes the missing feature value for the \n",
    "# data point.\n",
    "# KNN (K-Nearest Neighbor): Selects K nearest or similar data points using all the \n",
    "# non-missing features. It takes the average of the selected data points to fill in the missing \n",
    "# feature.\n",
    "# These are Machine Learning techniques to impute missing values.\n",
    "# KNN finds most similar points for imputing.\n",
    "# MICE performs multiple regression for imputing. MICE is a very robust model for imputation.\n",
    "\n",
    "\n",
    "# New dataframe saved as cleaned_df\n",
    "# Simply modify this object on the left of equality:\n",
    "cleaned_df = adv_imputation_missing_values (df = DATASET, column_to_fill = COLUMN_TO_FILL, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, test_value_to_fill = TEST_VALUE_TO_FILL, show_imputation_comparison_plots = SHOW_IMPUTATION_COMPARISON_PLOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5edf08b5-2760-4d81-bc01-410a510233eb"
   },
   "source": [
    "### **Applying a list of row filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a4fac3f4-9911-4884-9bf4-024f5ec907cd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Warning: this function filter the rows and results into a smaller dataset, \n",
    "# since it removes the non-selected entries.\n",
    "# If you want to pass a filter to simply label the selected rows, use the function \n",
    "# LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 and (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 or (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "## ~ (not): inverts the boolean, i.e., True becomes False, and False becomes True. \n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "# The negative of this condition may be acessed with ~ operator:\n",
    "##  filter = ~(dataframe_column_series).isin([value1, value2, ...])\n",
    "## Also, you may use isna() method as filter for missing values:\n",
    "## filter = (dataframe_column_series).isna()\n",
    "## or, for not missing: ~(dataframe_column_series).isna()\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "# Filtered dataframe saved as filtered_df\n",
    "# Simply modify this object on the left of equality:\n",
    "filtered_df = APPLY_ROW_FILTERS_LIST (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "531848c0-9a11-463a-a07f-512759894e42"
   },
   "source": [
    "### **Obtaining the correlation plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2a82adc9-f09f-43e7-b353-de1c0353fceb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SHOW_MASKED_PLOT = True\n",
    "#SHOW_MASKED_PLOT = True - keep as True if you want to see a cleaned version of the plot\n",
    "# where a mask is applied. Alternatively, SHOW_MASKED_PLOT = True, or \n",
    "# SHOW_MASKED_PLOT = False\n",
    "\n",
    "RESPONSES_TO_RETURN_CORR = None\n",
    "#RESPONSES_TO_RETURN_CORR - keep as None to return the full correlation tensor.\n",
    "# If you want to display the correlations for a particular group of features, input them\n",
    "# as a list, even if this list contains a single element. Examples:\n",
    "# responses_to_return_corr = ['response1'] for a single response\n",
    "# responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "# responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "# of a column of the dataset that represents a response variable.\n",
    "# WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "# of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "# Alternatively: a list containing strings (inside quotes) with the names of the response\n",
    "# columns that you want to see the correlations. Declare as a list even if it contains a\n",
    "# single element.\n",
    "\n",
    "SET_RETURNED_LIMIT = None\n",
    "# SET_RETURNED_LIMIT = None - This variable will only present effects in case you have\n",
    "# provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "# to return all of the correlation coefficients; or, alternatively, \n",
    "# provide an integer number to limit the total of coefficients returned. \n",
    "# e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'correlation_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "#New dataframe saved as correlation_matrix. Simply modify this object on the left of equality:\n",
    "correlation_matrix = correlation_plot (df = DATASET, show_masked_plot = SHOW_MASKED_PLOT, responses_to_return_corr = RESPONSES_TO_RETURN_CORR, set_returned_limit = SET_RETURNED_LIMIT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "952a3fd0-afbf-4572-92d9-b5cce42550eb"
   },
   "source": [
    "### **Plotting a bar chart**\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`.\n",
    "- For obtaining the **data distribution of categorical variables**, select any numeric column as the response, and set `aggregate_function = 'count'`. You can also set `plot_cumulative_percent = True` to compare the frequencies of each possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'bar_chart.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a274c772-f4c2-4002-96f2-f88be33f215f"
   },
   "source": [
    "### **Calculating cumulative statistics**\n",
    "- Cumulative sum (cumsum); cumulative product (cumprod); cumulative maximum (cummax); cumulative minimum (cummin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "75c1a245-60a4-4cc2-94bd-0f31f716e4ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CUMULATIVE_STATISTIC = 'sum'\n",
    "# CUMULATIVE_STATISTIC: the statistic that will be calculated. The cumulative\n",
    "# statistics allowed are: 'sum' (for cumulative sum, cumsum); 'product' \n",
    "# (for cumulative product, cumprod); 'max' (for cumulative maximum, cummax);\n",
    "# and 'min' (for cumulative minimum, cummin).\n",
    "\n",
    "NEW_CUM_STATS_COL_NAME = None\n",
    "# NEW_CUM_STATS_COL_NAME = None or string (inside quotes), \n",
    "# containing the name of the new column created for storing the cumulative statistic\n",
    "# calculated. \n",
    "# e.g. NEW_CUM_STATS_COL_NAME = \"cum_stats\" will create a column named as 'cum_stats'.\n",
    "# If its None, the new column will be named as column_to_analyze + \"_\" + [selected\n",
    "# cumulative function] ('cumsum', 'cumprod', 'cummax', 'cummin')\n",
    "\n",
    "# New dataframe saved as new_df\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df = calculate_cumulative_stats (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, cumulative_statistic = CUMULATIVE_STATISTIC, new_cum_stats_col_name = NEW_CUM_STATS_COL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a49d1ed-08f7-4a57-9ef0-cb7f4cbb4f34"
   },
   "source": [
    "### **Obtaining scatter plots and simple linear regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "SHOW_LINEAR_REG = True\n",
    "#Alternatively: set SHOW_LINEAR_REG = True to plot the linear regressions graphics and show \n",
    "# the linear regressions calculated for each pair Y x X (i.e., each correlation \n",
    "# Y = aX + b, as well as the R² coefficient calculated). \n",
    "# Set SHOW_LINEAR_REG = False to omit both the linear regressions plots on the graphic, and\n",
    "# the correlations and R² coefficients obtained.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# JSON-formatted list containing all series converted to NumPy arrays, \n",
    "#  with timestamps parsed as datetimes, and all the information regarding the linear regressions, \n",
    "# including the predicted values for plotting, returned as list_of_dictionaries_with_series_and_predictions. \n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dictionaries_with_series_and_predictions = scatter_plot_lin_reg (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_linear_reg = SHOW_LINEAR_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "802b8032-50f7-4227-86c9-9f7257a64240"
   },
   "source": [
    "### **Performing the polynomial fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1ea91e86-357a-4626-950f-2faf49bf50cb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "POLYNOMIAL_DEGREE = 6\n",
    "# Integer value representing the degree of the fitted polynomial.\n",
    "CALCULATE_ROOTS = False\n",
    "# CALCULATE_ROOTS = False.  Alternatively, set as True to calculate the roots of the\n",
    "#  fitted polynomial and return them as a NumPy array.\n",
    "CALCULATE_DERIVATIVE = False\n",
    "# CALCULATE_DERIVATIVE = False. Alternatively, set as True to calculate the derivative of the\n",
    "#  fitted polynomial and add it as a column of the dataframe.\n",
    "CALCULATE_INTEGRAL = False\n",
    "# CALCULATE_INTEGRAL = False. Alternatively, set as True to calculate the integral of the\n",
    "#  fitted polynomial and add it as a column of the dataframe.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "SHOW_POLYNOMIAL_REG = True\n",
    "#Alternatively: set SHOW_POLYNOMIAL_REG = True to plot the polynomial regressions graphics\n",
    "# calculated for each pair Y x X. \n",
    "# Set SHOW_LINEAR_REG = False to omit these plots.\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'polynomial_fitting.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# JSON-formatted list containing all series converted to NumPy arrays, \n",
    "#  with timestamps parsed as datetimes, and all the information regarding the linear regressions, \n",
    "# including the predicted values for plotting, returned as list_of_dictionaries_with_series_and_predictions. \n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dictionaries_with_series_and_predictions = polynomial_fit (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, polynomial_degree = POLYNOMIAL_DEGREE, calculate_roots = CALCULATE_ROOTS, calculate_derivative = CALCULATE_DERIVATIVE, calculate_integral = CALCULATE_INTEGRAL, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_polynomial_reg = SHOW_POLYNOMIAL_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5b2a1a03-7736-45c9-8d65-5cf501159fa4"
   },
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fb1c468c-2ab7-46d1-8866-88764537d748"
   },
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "15d28d6f-b8d5-4554-98af-3f41bf4f7ea0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'analyzed_variable'\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# COLUMN_TO_ANALYZE = 'column1'\n",
    "\n",
    "TOTAL_OF_BINS = 10\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided.\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#New dataframes saved as general_stats and frequency_table.\n",
    "# Simply modify these objects on the left of equality:\n",
    "general_stats, frequency_table = histogram (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, total_of_bins = TOTAL_OF_BINS, normal_curve_overlay = NORMAL_CURVE_OVERLAY, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8c7f1908-6535-4326-84ad-b5dcbf456e93",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b4bb8f51-4265-4c84-afce-c6c33528e28a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze' \n",
    "# COLUMN_TO_ANALYZE: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "# in quotes.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'col1' will analyze a column named 'col1'.\n",
    "\n",
    "COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS = None\n",
    "# column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "# subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "# it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "# will retrieve the labels from 'col2'.\n",
    "# Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "# will be tested.\n",
    "    \n",
    "ALPHA = 0.10\n",
    "# Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "# Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "# results.\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'probability_plot_normal.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "# List of dictionaries containing the series, p-values, skewness and kurtosis returned as\n",
    "# list_of_dicts\n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dicts = test_data_normality (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_labels_to_test_subgroups = COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS, alpha = ALPHA, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f756b5b4-2120-4b2a-bdff-0b21aac0414b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing and visualizing probability plots for different statistical distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "language": "python",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "# Attention: if you want to test a normal distribution, use the function \n",
    "# test_data_normality.Function test_data_normality tests normality through 4 methods \n",
    "# and compare them: D’Agostino and Pearson’s; Shapiro-Wilk; Lilliefors; and Anderson-Darling tests.\n",
    "# The calculus of the p-value from the Anderson-Darling statistic is available only \n",
    "# for some distributions. The function specific for the normality calculates these \n",
    "# probabilities of following the normal.\n",
    "# Here, the function is destined to test a variety of distributions, and so only the \n",
    "# Anderson-Darling test is performed.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze' \n",
    "# COLUMN_TO_ANALYZE: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "# in quotes.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'col1' will analyze a column named 'col1'.\n",
    "\n",
    "COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS = None\n",
    "# column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "# subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "# it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "# will retrieve the labels from 'col2'.\n",
    "# Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "# will be tested.\n",
    "\n",
    "STATISTICAL_DISTRIBUTION_TO_TEST = 'lognormal'\n",
    "#STATISTICAL_DISTRIBUTION: string (inside quotes) containing the tested statistical \n",
    "# distribution. \n",
    "## Notice: if data Y follow a 'lognormal', log(Y) follow a normal\n",
    "## Poisson is a special case from 'gamma' distribution.\n",
    "## There are 91 accepted statistical distributions:\n",
    "# 'alpha', 'anglit', 'arcsine', 'beta', 'beta_prime', 'bradford', 'burr', 'burr12', 'cauchy',\n",
    "# 'chi', 'chi-squared', 'cosine', 'double_gamma', 'double_weibull', \n",
    "# 'erlang', 'exponential', 'exponentiated_weibull', 'exponential_power',\n",
    "# 'fatigue_life_birnbaum-saunders', 'fisk_log_logistic', 'folded_cauchy', 'folded_normal',\n",
    "# 'F', 'gamma', 'generalized_logistic', 'generalized_pareto', 'generalized_exponential', \n",
    "# 'generalized_extreme_value', 'generalized_gamma', 'generalized_half-logistic', \n",
    "# 'generalized_inverse_gaussian', 'generalized_normal', \n",
    "# 'gilbrat', 'gompertz_truncated_gumbel', 'gumbel', 'gumbel_left-skewed', 'half-cauchy', \n",
    "# 'half-normal', 'half-logistic', 'hyperbolic_secant', 'gauss_hypergeometric', \n",
    "# 'inverted_gamma', 'inverse_normal', 'inverted_weibull', 'johnson_SB', 'johnson_SU', \n",
    "# 'KSone','KStwobign', 'laplace', 'left-skewed_levy', \n",
    "# 'levy', 'logistic', 'log_laplace', 'log_gamma', 'lognormal', 'log-uniform', 'maxwell', \n",
    "# 'mielke_Beta-Kappa', 'nakagami', 'noncentral_chi-squared', 'noncentral_F', \n",
    "# 'noncentral_t', 'normal', 'normal_inverse_gaussian', 'pareto', 'lomax', 'power_lognormal',\n",
    "# 'power_normal', 'power-function', 'R', 'rayleigh', 'rice', 'reciprocal_inverse_gaussian', \n",
    "# 'semicircular', 'student-t', 'triangular', \n",
    "# 'truncated_exponential', 'truncated_normal', 'tukey-lambda', 'uniform', 'von_mises', \n",
    "# 'wald', 'weibull_maximum_extreme_value', 'weibull_minimum_extreme_value', 'wrapped_cauchy'\n",
    "\n",
    "\n",
    "# List of dictionaries containing the series, p-values, skewness and kurtosis returned as\n",
    "# list_of_dicts\n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dicts = test_stat_distribution (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_labels_to_test_subgroups = COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS, statistical_distribution_to_test = STATISTICAL_DISTRIBUTION_TO_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b852a4e3-ba17-44fb-b228-900b98cb1479"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "199f0f18-b8b3-423d-b163-d3b74a9c8811",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0919250e-0203-4417-96df-964a14919bba"
   },
   "source": [
    "### **Renaming specific columns from the dataframe; or cleaning columns' labels**\n",
    "- The function `select_order_or_rename_columns` requires the user to pass a list containing the names from all columns.\n",
    "- Also, this list must contain the columns in the correct order (the order they appear in the dataframe).\n",
    "- This function may manipulate one or several columns by call, and is not dependent on their order.\n",
    "- This function can also be used for cleaning the columns' labels: capitalize (upper case) or lower cases of all columns' names; replace substrings on columns' names; or eliminating trailing and leading white spaces or characters from columns' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b6f6da55-58b6-4f94-9fee-26f6622f70dd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'set_new_names'\n",
    "# MODE = 'set_new_names' will change the columns according to the specifications in\n",
    "# LIST_OF_COLUMNS_LABELS.\n",
    "\n",
    "# MODE = 'capitalize_columns' will capitalize all columns names (i.e., they will be put in\n",
    "# upper case). e.g. a column named 'column' will be renamed as 'COLUMN'\n",
    "\n",
    "# MODE = 'lowercase_columns' will lower the case of all columns names. e.g. a column named\n",
    "# 'COLUMN' will be renamed as 'column'.\n",
    "\n",
    "# MODE = 'replace_substring' will search on the columns names (strings) for the \n",
    "# SUBSTRING_TO_BE_REPLACED (which may be a character or a string); and will replace it by \n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT (which again may be either a character or a string). \n",
    "# Numbers (integers or floats) will be automatically converted into strings.\n",
    "# As an example, consider the default situation where we search for a whitespace ' ' and replace it\n",
    "# by underscore '_': SUBSTRING_TO_BE_REPLACED = ' ', NEW_SUBSTRING_FOR_REPLACEMENT = '_'  \n",
    "# In this case, a column named 'new column' will be renamed as 'new_column'.\n",
    "\n",
    "# MODE = 'trim' will remove all trailing or leading whitespaces from column names.\n",
    "# e.g. a column named as ' col1 ' will be renamed as 'col1'; 'col2 ' will be renamed as\n",
    "# 'col2'; and ' col3' will be renamed as 'col3'.\n",
    "\n",
    "# MODE = 'eliminate_trailing_characters' will eliminate a defined trailing and leading \n",
    "# substring from the columns' names. \n",
    "# The substring must be indicated as TRAILING_SUBSTRING, and its default, when no value\n",
    "# is provided, is equivalent to mode = 'trim' (eliminate white spaces). \n",
    "# e.g., if TRAILING_SUBSTRING = '_test' and you have a column named 'col_test', it will be \n",
    "# renamed as 'col'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = ' '\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = '_'\n",
    "\n",
    "TRAILING_SUBSTRING = None\n",
    "\n",
    "LIST_OF_COLUMNS_LABELS = [\n",
    "    \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_COLUMNS_LABELS = [{'column_name': None, 'new_column_name': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original column name; and the second one contains the new name\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'column_name', and it will be replaced (switched) \n",
    "# by the correspondent value in key 'new_column_name'.\n",
    "    \n",
    "# The object LIST_OF_COLUMNS_LABELS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'column_name' for the original label; \n",
    "# and 'new_column_name', for the correspondent new label.\n",
    "# Notice that this function will not search substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_name' and one of the columns\n",
    "# labels. So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'column_name': original_col, 'new_column_name': new_col}, \n",
    "# where original_col and new_col represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "# Example: LIST_OF_COLUMNS_LABELS = [{'column_name': 'col1', 'new_column_name': 'col'}] will\n",
    "# rename 'col1' as 'col'.\n",
    "\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = rename_or_clean_columns_labels (df = DATASET, mode = MODE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, trailing_substring = TRAILING_SUBSTRING, list_of_columns_labels = LIST_OF_COLUMNS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "824ec564-8540-4184-9a66-3ca8daf4892c"
   },
   "source": [
    "### **Merging (joining) the data on a timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5777fe37-4dab-41ef-bbdc-8b5077bb4741",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"DATE\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"DATE\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\". This option has no effect \n",
    "# if MERGE_METHOD = \"asof\". Keep inside quotes.\n",
    "\n",
    "MERGE_METHOD = \"asof\"\n",
    "# Alternatively: MERGE_METHOD = 'ordered' to use pandas .merge_ordered method, or\n",
    "# MERGE_METHOD = \"asof\" for using the .merge_asof method.\n",
    "# WARNING: .merge_asof uses fuzzy matching, so the HOW_TO_JOIN parameter is not applicable.\n",
    "# Keep inside quotes.\n",
    "\n",
    "## USE MERGE_METHOD = 'asof' to merge data collected asynchronously, i.e., data collected in\n",
    "# different moments, resulting in timestamps that do not perfectly match.\n",
    "# merge_asof method sorts the timestamps in ascending order and does not look for a perfect \n",
    "# combination of keys. Instead, it takes the timestamp from the right dataframe as key, and \n",
    "# searches for the closest dataframe on the left dataframe. So, it inputs the row from the right on \n",
    "# the correct position it should have in the left dataframe (in other words, it appends the rows from\n",
    "# one into the order, respecting the columns and the time order).\n",
    "# If a missing value would be generated, the 'ffill' parameter can be used to automatically \n",
    "# repeat the previous value (from the left dataframe) on the row that came from the right table,\n",
    "# filling the missing values.\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "ASOF_DIRECTION = \"nearest\"\n",
    "# Parameter of .merge_asof method. 'nearest' merge the closest timestamps in both directions.\n",
    "# Alternatively: 'backward' or 'forward'.\n",
    "# This option has no effect if MERGE_METHOD = \"ordered\". Keep inside quotes.\n",
    "\n",
    "ORDERED_FILLING = 'ffill'\n",
    "# Parameter or .merge_ordered method.\n",
    "# Alternatively: ORDERED_FILLING = 'ffill' (inside quotes) to fill missings \n",
    "# with the previous value.\n",
    "# This option has no effect if MERGE_METHOD = \"asof\", so you can keep it None\n",
    "\n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = MERGE_ON_TIMESTAMP (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merge_method = MERGE_METHOD, merged_suffixes = MERGED_SUFFIXES, asof_direction = ASOF_DIRECTION, ordered_filling = ORDERED_FILLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "86991fc9-144f-425e-a1f2-be16f08072cb"
   },
   "source": [
    "### **Merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cb774677-4355-4f48-9d6b-2cf06e180349",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"left_key_column\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"right_key_column\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\".\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "SORT_MERGED_DF = False\n",
    "# SORT_MERGED_DF = False not to sort the merged dataframe. If you want to sort it,\n",
    "# set as True. If SORT_MERGED_DF = True and COLUMN_TO_SORT = None, the dataframe will\n",
    "# be sorted by its first column.\n",
    "\n",
    "COLUMN_TO_SORT = None\n",
    "# COLUMN_TO_SORT = None. Keep it None if the dataframe should not be sorted.\n",
    "# Alternatively, pass a string with a column name to sort, such as:\n",
    "# COLUMN_TO_SORT = 'col1'; or a list of columns to use for sorting: COLUMN_TO_SORT = \n",
    "# ['col1', 'col2']\n",
    "\n",
    "ASCENDING_SORTING = True\n",
    "# ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "# ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "# you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "# list of booleans like ASCENDING_SORTING = [False, True] - the first column of the list\n",
    "# will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "# the correspondence is element-wise: the boolean in list ASCENDING_SORTING will correspond \n",
    "# to the sorting order of the column with the same position in list COLUMN_TO_SORT.\n",
    "# If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = MERGE_AND_SORT_DATAFRAMES (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merged_suffixes = MERGED_SUFFIXES, sort_merged_df = SORT_MERGED_DF, column_to_sort = COLUMN_TO_SORT, ascending_sorting = ASCENDING_SORTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8906ee22-5bad-4916-8a35-cbb03595a25c"
   },
   "source": [
    "### **Dropping specific columns or rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "17bedbfc-0a0e-4c88-bdbe-519bdaaab057",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "WHAT_TO_DROP = 'columns'\n",
    "# WHAT_TO_DROP = 'columns' for removing the columns specified by their names (headers)\n",
    "# in COLS_LIST (a list of strings).\n",
    "# WHAT_TO_DROP = 'rows' for removing the rows specified by their indices in\n",
    "# ROW_INDEX_LIST (a list of integers). Remember that the indexing starts from zero, i.e.,\n",
    "# the first row is row number zero.\n",
    "\n",
    "COLS_LIST = None\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to be removed\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# remove columns 'col1', 'col2', and 'col3' from the dataframe.\n",
    "# If a single column will be dropped, you can declare it as a string (outside a list)\n",
    "# e.g. COLS_LIST = 'col1'; or COLS_LIST = ['col1']\n",
    "\n",
    "ROW_INDEX_LIST = None\n",
    "# ROW_INDEX_LIST = a list of integers containing the indices of the rows that will be dropped.\n",
    "# e.g. ROW_INDEX_LIST = [0, 1, 2] will drop the rows with indices 0 (1st row), 1 (2nd row), and\n",
    "# 2 (third row). Again, if a single row will be dropped, you can declare it as an integer (outside\n",
    "# a list).\n",
    "# e.g. ROW_INDEX_LIST = 20 or ROW_INDEX_LIST = [20] to drop the row with index 20 (21st row).\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = drop_columns_or_rows (df = DATASET, what_to_drop = WHAT_TO_DROP, cols_list = COLS_LIST, row_index_list = ROW_INDEX_LIST, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "67295ed2-b106-4e24-8d3d-fcc4921fb1d7"
   },
   "source": [
    "### **Removing duplicate rows from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6636361d-82de-426a-8488-d1a3b601a4db",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_ANALYZE = None\n",
    "# if LIST_OF_COLUMNS_TO_ANALYZE = None, the whole dataset will be analyzed, i.e., rows\n",
    "# will be removed only if they have same values for all columns from the dataset.\n",
    "# Alternatively, pass a list of columns names (strings), if you want to remove rows with\n",
    "# same values for that combination of columns. Pass it as a list, even if there is a single column\n",
    "# being declared.\n",
    "# e.g. LIST_OF_COLUMNS_TO_ANALYZE = ['column1'] will check only 'column1'. Entries with same value\n",
    "# on 'column1' will be considered duplicates and will be removed.\n",
    "# LIST_OF_COLUMNS_TO_ANALYZE = ['col1', 'col2',  'col3'] will analyze the combination of 3 columns:\n",
    "# 'col1', 'col2', and 'col3'. Only rows with same value for these 3 columns will be considered\n",
    "# duplicates and will be removed.\n",
    "\n",
    "WHICH_ROW_TO_KEEP = 'first'\n",
    "# WHICH_ROW_TO_KEEP = 'first' will keep the first detected row and remove all other duplicates. If\n",
    "# None or an invalid string is input, this method will be selected.\n",
    "# WHICH_ROW_TO_KEEP = 'last' will keep only the last detected duplicate row, and remove all the others.\n",
    "    \n",
    "RESET_INDEX_AFTER_DROP = True\n",
    "# RESET_INDEX_AFTER_DROP = True. keep it True to restarting the indexing numeration after dropping.\n",
    "# Alternatively, set RESET_INDEX_AFTER_DROP = False to keep the original numeration (the removed indices\n",
    "# will be missing).\n",
    "\n",
    "# New dataframe saved as cleaned_df. Simply modify this object on the left of equality:\n",
    "cleaned_df = remove_duplicate_rows (df = DATASET, list_of_columns_to_analyze = LIST_OF_COLUMNS_TO_ANALYZE, which_row_to_keep = WHICH_ROW_TO_KEEP, reset_index_after_drop = RESET_INDEX_AFTER_DROP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "52556b7f-1d7b-454e-8f21-f842ca5c9b9c"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a13ad627-2b47-490e-96af-a7abb8591698",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9a4ada30-ab24-414b-aefa-1e0d34ae950b"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "24cc13bf-cf6b-48e0-b156-25fffa27eacd"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1ef010b4-178f-4328-ab69-07b654b525e3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cc7aa510-bb3b-4473-9da7-ccc252ab257d"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "4721ef19-6f7a-4b9f-ba9e-dd14064ac9f7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ea1239cd-f63a-405e-ad33-a57610203de3"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9e67c15d-6990-48de-b8ed-cf941bb57921",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "212c9850-18c8-4a3b-9213-2b37270132d4"
   },
   "source": [
    "# **Statiscal distribution skewness and kurtosis - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "11f01ee4-f306-46b8-b80f-ff9fefc2be5e"
   },
   "source": [
    "### scipy stats.skew()\n",
    "- scipy.stats.skew(array, axis=0, bias=True) function calculates the skewness of the data set.\n",
    "    - skewness = 0 : normally distributed.\n",
    "    - skewness > 0 : more weight in the left tail of the distribution.\n",
    "    - skewness < 0 : more weight in the right tail of the distribution. \n",
    "\n",
    "Its basic formula:\n",
    "\n",
    "`skewness = (3 *(Mean - Median))/(Standard deviation)`\n",
    "\n",
    "Parameters :\n",
    "- array : Input array or object having the elements.\n",
    "- axis : Axis along which the skewness value is to be measured. By default axis = 0.\n",
    "- bias : Bool; calculations are corrected for statistical bias, if set to False.\n",
    "- Returns : Skewness value of the data set, along the axis.\n",
    "\n",
    "https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "\n",
    "Scipy uses a more general and complex formula, shown in its documentation:\n",
    "https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.skew.html\n",
    "\n",
    "### scipy stats.kurtosis()\n",
    "- scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function calculates the kurtosis (Fisher or Pearson) of a data set. \n",
    "- It is the the fourth central moment divided by the square of the variance. \n",
    "- It is a measure of the “tailedness” i.e. descriptor of shape of probability distribution of a real-valued random variable. \n",
    "- In simple terms, one can say it is a measure of how heavy tail is compared to a normal distribution.\n",
    "\n",
    "Its formula:\n",
    "\n",
    "`Kurtosis(X) = E[((X - mu)/sigma)^4]`\n",
    "\n",
    "Parameters :\n",
    "- array : Input array or object having the elements.\n",
    "- axis : Axis along which the kurtosis value is to be measured. By default axis = 0.\n",
    "- fisher : Bool; Fisher’s definition is used (normal 0.0) if True; else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "- bias : Bool; calculations are corrected for statistical bias, if set to False.\n",
    "- Returns : Kurtosis value of the normal distribution for the data set.\n",
    "\n",
    "https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
