{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aggregation and Manipulation of Timestamps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _ETL Workflow Notebook 1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "1. Grouping by Timestamp; \n",
    "2. Merging on Timestamp;\n",
    "3. Merging (joining) dataframes on given keys; and sorting the merged table;\n",
    "4. Extracting Timestamp Information; \n",
    "5. Calculating Timedeltas; \n",
    "6. Calculating differences between successive timestamps (delays);\n",
    "7. Adding or subtracting Timedeltas;\n",
    "8. Slicing the dataframe (selecting a specific subset of rows);\n",
    "9. Concatenating (SQL Union/Stacking/Appending) dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '/', s3_bucket_name = None, s3_obj_key_preffix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = '/copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_key_preffix = None. Keep it None or as an empty string (s3_obj_key_preffix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_preffix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        \n",
    "        import sagemaker\n",
    "        # sagemaker is AWS SageMaker Python SDK\n",
    "        from sagemaker.session import Session\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead, to start the S3 client. boto3 is AWS S3 Python SDK:\n",
    "        \n",
    "        # import boto3\n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # ... [here, use the same following code until line new_session = Session()]\n",
    "        # [keep the line for session start. Substitute the line with the .download_data\n",
    "        # method by the following line:]\n",
    "        # s3_client.download_file(s3_bucket_name, s3_file_name_with_extension, path_to_store_imported_s3_bucket)\n",
    "        \n",
    "        # Check if the whole bucket will be downloaded (s3_obj_key_preffix = None):\n",
    "        if (s3_obj_key_preffix is None):\n",
    "            \n",
    "            s3_obj_key_preffix = ''\n",
    "        \n",
    "        # If the path to store is None, also import the bucket to the root path:\n",
    "        if (path_to_store_imported_s3_bucket is None):\n",
    "            \n",
    "            path_to_store_imported_s3_bucket = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start a new sagemaker session:\n",
    "\n",
    "            print(\"Starting a SageMaker session to be associated with the S3 bucket.\")\n",
    "\n",
    "            new_session = Session()\n",
    "            # Check sagemaker session class documentation:\n",
    "            # https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "            session.download_data(path = path_to_store_imported_s3_bucket, bucket = s3_bucket_name, key_prefix = s3_obj_key_preffix)\n",
    "\n",
    "            print(f\"S3 bucket contents successfully imported to path \\'{path_to_store_imported_s3_bucket}\\'.\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, has_header = True, txt_csv_col_sep = \"comma\", sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_preffix_list = None):\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # file_name_with_extension - (string, in quotes): input the name of the file with the extension\n",
    "    # e.g. file_name_with_extension = \"file.xlsx\", or, file_name_with_extension = \"file.csv\"\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    ## Parameters for loading txt or CSV files\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\" for columns separated by comma (\",\")\n",
    "    # txt_csv_col_sep = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_preffix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_preffix_list = ['name', 'last']\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_preffix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if (txt_csv_col_sep == \"comma\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path)\n",
    "\n",
    "                elif (txt_csv_col_sep == \"whitespace\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if (txt_csv_col_sep == \"comma\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None)\n",
    "\n",
    "                elif (txt_csv_col_sep == \"whitespace\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path) as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_preffix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\")\n",
    "            \n",
    "        if (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None)\n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None)\n",
    "    \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Dictionaries, possibly with nested dictionaries (JSON formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_obj_to_dataframe (json_obj_to_convert, json_record_path = None, json_field_separator = \"_\", json_metadata_preffix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_preffix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_preffix_list = ['name', 'last']\n",
    "    \n",
    "    json_file = json.loads(json_obj_to_convert)\n",
    "    # json.load() : This method is used to parse JSON from URL or file.\n",
    "    # json.loads(): This method is used to parse string with JSON content.\n",
    "    # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "    # like a dataframe.\n",
    "    # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_preffix_list)\n",
    "    \n",
    "    print(f\"JSON object {json_obj_to_convert} converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for grouping the data by a timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GROUP_BY_TIMESTAMP (df, timestamp_tag_column, grouping_frequency_unit = 'day', number_of_periods_to_group = 1, aggregate_function = 'mean', start_time = None, offset_time = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df - dataframe/table containing the data to be grouped\n",
    "    \n",
    "    #timestamp_tag_colum: name (header) of the column containing the\n",
    "    \n",
    "    #timestamps for grouping the data.\n",
    "    \n",
    "    #grouping_frequency_unit: the frequency of aggregation. The possible values are:\n",
    "    \n",
    "    grp_frq_unit_dict = {'year': \"Y\", 'month': \"M\", 'week': \"W\", \n",
    "                            'day': \"D\", 'hour': \"H\", 'minute': \"min\", 'second': 'S'}\n",
    "    \n",
    "    #Simply provide the key: 'year', 'month', 'week',..., 'second', and this dictionary\n",
    "    #will convert to the Pandas coding.\n",
    "    #The default is 'day', so this will be inferred frequency if no value is provided.\n",
    "    \n",
    "    #To access the value of a dictionary d = {key1: item1, ...}:\n",
    "    #d['key1'] = item1. - simply declare the key as a string (under quotes) inside brackets\n",
    "    #just as if you were accessing a column from the dataframe.\n",
    "    #Since grouping_frequency_unit is variable storing a string, it should not come under\n",
    "    #quotes:\n",
    "    \n",
    "    #Convert the input to Pandas encoding:\n",
    "    frq_unit = grp_frq_unit_dict[grouping_frequency_unit]\n",
    "    \n",
    "    #https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "    #To group by business day, check the example:\n",
    "    #https://stackoverflow.com/questions/13019719/get-business-days-between-start-and-end-date-using-pandas\n",
    "    \n",
    "    #number_of_periods_to_group: the bin size. The default is 1, so we will group by '1day'\n",
    "    #if number_of_periods_to_group = 2 we would be grouping by every 2 days.\n",
    "    #If the unit was minute and number_of_periods_to_group = 30, we would be grouping into\n",
    "    #30-min bins.\n",
    "    \n",
    "    if (number_of_periods_to_group <=0):\n",
    "        \n",
    "        print(\"Invalid number of periods to group. Changing to 1 period.\")\n",
    "        number_of_periods_to_group = 1\n",
    "    \n",
    "    if (number_of_periods_to_group == 1):\n",
    "        \n",
    "        #Do not put the number 1 prior to the frequency unit\n",
    "        FREQ =  frq_unit\n",
    "    \n",
    "    else:\n",
    "        #perform the string concatenation. Convert the number into a string:\n",
    "        number_of_periods_to_group = str(number_of_periods_to_group)\n",
    "        #Concatenate the strings:\n",
    "        FREQ = number_of_periods_to_group + frq_unit\n",
    "        #Expected output be like '2D' for a 2-days grouping\n",
    "        \n",
    "    #aggregate_function: Pandas aggregation method: 'mean', 'median', 'std', 'sum', 'min'\n",
    "    # 'max', etc. The default is 'mean'. Then, if no aggregate is provided, \n",
    "    # the mean will be calculated.\n",
    "    \n",
    "    #You can pass a list of multiple aggregations, like: \n",
    "    #aggregate_function = [mean, max, sum]\n",
    "    #You can also pass custom functions, like: pct30 (30-percentile), or np.mean\n",
    "    #aggregate_function = pct30\n",
    "    #aggregate_function = np.mean (numpy.mean)\n",
    "    \n",
    "    #ADJUST OF GROUPING BASED ON A FIXED TIMESTAMP\n",
    "    #This parameters are set to None as default.\n",
    "    #You can specify the origin (start_time) or the offset (offset_time), which are\n",
    "    #equivalent. The parameter should be declared as a timestamp.\n",
    "    #For instance: start_time = '2000-10-01 23:30:00'\n",
    "    \n",
    "    #WARNING: DECLARE ONLY ONE OF THESE PARAMETERS. DO NOT DECLARE AN OFFSET IF AN \n",
    "    #ORIGIN WAS SPECIFIED, AND VICE-VERSA.\n",
    "    \n",
    "    #Create a Pandas timestamp object from the timestamp_tag_column. It guarantees that\n",
    "    #the timestamp manipulation methods can be correctly applied.\n",
    "    #Let's create using nanoseconds resolution, so that the timestamps present the\n",
    "    #maximum possible resolution:\n",
    "    \n",
    "    # START: CONVERT ALL TIMESTAMPS/DATETIMES/STRINGS TO pandas.Timestamp OBJECTS.\n",
    "    # This will prevent any compatibility problems.\n",
    "    \n",
    "    #The pd.Timestamp function can handle a single timestamp per call. Then, we must\n",
    "    # loop trough the series, and apply the function to each element.\n",
    "    \n",
    "    #1. Start a list to store the Pandas timestamps:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    #2. Loop through each element of the timestamp column, and apply the function\n",
    "    # to guarantee that all elements are Pandas timestamps\n",
    "    \n",
    "    for timestamp in df[timestamp_tag_column]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    #3. Create a column in the dataframe that will be used as key for the Grouper class\n",
    "    # The grouper requires a column in the dataframe - it cannot use a list for that.\n",
    "    # Simply copy the list as the new column:\n",
    "    df['timestamp_obj'] = timestamp_list\n",
    "    \n",
    "    #Now we have a list correspondent to timestamp_tag_column, but only with\n",
    "    # Pandas timestamp objects\n",
    "    \n",
    "    #In this function, we do not convert the Timestamp to a datetime64 object.\n",
    "    #That is because the Grouper class specifically requires a Pandas Timestamp\n",
    "    #object to group the dataframes.\n",
    "    \n",
    "    if (start_time is not None):\n",
    "        \n",
    "        grouped_df = df.groupby(pd.Grouper(key = 'timestamp_obj' , freq = FREQ, origin = start_time)).agg(aggregate_function)\n",
    "    \n",
    "    elif (offset_time is not None):\n",
    "        \n",
    "        grouped_df = df.groupby(pd.Grouper(key = 'timestamp_obj' , freq = FREQ, offset = offset_time)).agg(aggregate_function)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Standard situation, when both start_time and offset_time are None\n",
    "        grouped_df = df.groupby(pd.Grouper(key = 'timestamp_obj' , freq = FREQ)).agg(aggregate_function)\n",
    "    \n",
    "    print (f\"Dataframe grouped by every {number_of_periods_to_group} {frq_unit}.\")\n",
    "    \n",
    "    #The parameter 'key' of the Grouper class must be the name (string) of a column\n",
    "    # of the dataframe\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html\n",
    "    \n",
    "    #The objects 'timestamp_obj' are now the index from grouped_df dataframe\n",
    "    #Let's store them as a column and restart the index:\n",
    "    #1. Copy the index to a new column:\n",
    "    grouped_df['Timestamp_grouped'] = grouped_df.index\n",
    "    \n",
    "    #2. Reset the index:\n",
    "    grouped_df = grouped_df.reset_index(drop = True)\n",
    "    \n",
    "    #3. 'pandas.Timestamp_grouped' is now the last column. Let's create a list of the\n",
    "    # reordered columns, starting from 'pandas.Timestamp_grouped'\n",
    "    \n",
    "    reordered_cols_list = ['Timestamp_grouped']\n",
    "    \n",
    "    for i in range((len(grouped_df.columns)-1)):\n",
    "        \n",
    "        #This loop goes from i = 0 to i = (len(grouped_df.columnns)-2)\n",
    "        # grouped_df.columnns is a list containing the columns names. Since indexing goes\n",
    "        # from 0, the last element is the index i = (len(grouped_df.columnns)-1).\n",
    "        # But this last element is 'pandas.Timestamp_grouped', which we used as the\n",
    "        # first element of the list reordered_cols_list. Then, we must loop from the\n",
    "        # first element of grouped_df.columnns to the element immediately before 'pandas.Timestamp_grouped'.\n",
    "        # Then, the last element to be read is (len(grouped_df.columnns)-2)\n",
    "        # range (i, j) goes from i to j-1. If only one value is specified, i = 0 and j =\n",
    "        # declared value. If you print all i values in range(10), numbers from 0 to 9\n",
    "        # will be shown.\n",
    "        \n",
    "        reordered_cols_list.append(grouped_df.columns[i])\n",
    "    \n",
    "    #4. Reorder the dataframe passing the list reordered_cols_list as the column filters\n",
    "    # / columns selection list.Notice that df[['col1', 'col2']] = df[list], where list =\n",
    "    # ['col1', 'col2']. To select or reorder columns, we pass the list of columns under\n",
    "    # brackets as parameter.\n",
    "    \n",
    "    grouped_df = grouped_df[reordered_cols_list]\n",
    "    \n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully grouped. Check its 10 first rows:\\n\")\n",
    "    print(grouped_df.head(10))\n",
    "    \n",
    "    #Now return the grouped dataframe with the timestamp as the first column:\n",
    "    \n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for merging (joining) the data on a timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MERGE_ON_TIMESTAMP (df_left, df_right, left_key, right_key, how_to_join = \"inner\", merge_method = 'ordered', merged_suffixes = ('_left', '_right'), asof_direction = 'nearest', ordered_filling = None):\n",
    "    \n",
    "    #WARNING: Only two dataframes can be merged on each call of the function.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df_left: dataframe to be joined as the left one.\n",
    "    \n",
    "    # df_right: dataframe to be joined as the right one\n",
    "    \n",
    "    # left_key: (String) name of column of the left dataframe to be used as key for joining.\n",
    "    \n",
    "    # right_key: (String) name of column of the right dataframe to be used as key for joining.\n",
    "    \n",
    "    # how_to_join: joining method: \"inner\", \"outer\", \"left\", \"right\". The default is \"inner\".\n",
    "    \n",
    "    # merge_method: which pandas merging method will be applied:\n",
    "    # merge_method = 'ordered' for using the .merge_ordered method.\n",
    "    # merge_method = \"asof\" for using the .merge_asof method.\n",
    "    # WARNING: .merge_asof uses fuzzy matching, so the how_to_join parameter is not applicable.\n",
    "    \n",
    "    # merged_suffixes = ('_left', '_right') - tuple of the suffixes to be added to columns\n",
    "    # with equal names. Simply modify the strings inside quotes to modify the standard\n",
    "    # values. If no tuple is provided, the standard denomination will be used.\n",
    "    \n",
    "    # asof_direction: this parameter will only be used if the .merge_asof method is\n",
    "    # selected. The default is 'nearest' to merge the closest timestamps in both \n",
    "    # directions. The other options are: 'backward' or 'forward'.\n",
    "    \n",
    "    # ordered_filling: this parameter will only be used on the merge_ordered method.\n",
    "    # The default is None. Input ordered_filling = 'ffill' to fill missings with the\n",
    "    # previous value.\n",
    "    \n",
    "    if (merge_method == 'ordered'):\n",
    "    \n",
    "        if (ordered_filling == 'ffill'):\n",
    "            \n",
    "            merged_df = pd.merge_ordered(df_left, df_right, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes, fill_method='ffill')\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            merged_df = pd.merge_ordered(df_left, df_right, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    elif (merge_method == 'asof'):\n",
    "        \n",
    "        merged_df = pd.merge_asof(df_left, df_right, left_on = left_key, right_on = right_key, suffixes = merged_suffixes, direction = asof_direction)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"You did not enter a valid merge method for this function, \\'ordered\\' or \\'asof\\'.\")\n",
    "        print(\"Then, applying the conventional Pandas .merge method, followed by .sort_values method.\")\n",
    "        \n",
    "        #Pandas sort_values method: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "        \n",
    "        merged_df = df_left.merge(df_right, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "        merged_df = merged_df.sort_values(by = merged_df.columns[0], ascending = True)\n",
    "        #sort by the first column, with index 0.\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    merged_df = merged_df.reset_index(drop = True)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully merged. Check its 10 first rows:\\n\")\n",
    "    print(merged_df.head(10))\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MERGE_AND_SORT_DATAFRAMES (df_left, df_right, left_key, right_key, how_to_join = \"inner\", merged_suffixes = ('_left', '_right'), sort_merged_df = False, column_to_sort = None, ascending_sorting = True):\n",
    "    \n",
    "    #WARNING: Only two dataframes can be merged on each call of the function.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df_left: dataframe to be joined as the left one.\n",
    "    \n",
    "    # df_right: dataframe to be joined as the right one\n",
    "    \n",
    "    # left_key: (String) name of column of the left dataframe to be used as key for joining.\n",
    "    \n",
    "    # right_key: (String) name of column of the right dataframe to be used as key for joining.\n",
    "    \n",
    "    # how_to_join: joining method: \"inner\", \"outer\", \"left\", \"right\". The default is \"inner\".\n",
    "    \n",
    "    # merge_method: which pandas merging method will be applied:\n",
    "    # merge_method = 'ordered' for using the .merge_ordered method.\n",
    "    # merge_method = \"asof\" for using the .merge_asof method.\n",
    "    # WARNING: .merge_asof uses fuzzy matching, so the how_to_join parameter is not applicable.\n",
    "    \n",
    "    # merged_suffixes = ('_left', '_right') - tuple of the suffixes to be added to columns\n",
    "    # with equal names. Simply modify the strings inside quotes to modify the standard\n",
    "    # values. If no tuple is provided, the standard denomination will be used.\n",
    "    \n",
    "    # sort_merged_df = False not to sort the merged dataframe. If you want to sort it,\n",
    "    # set as True. If sort_merged_df = True and column_to_sort = None, the dataframe will\n",
    "    # be sorted by its first column.\n",
    "    \n",
    "    # column_to_sort = None. Keep it None if the dataframe should not be sorted.\n",
    "    # Alternatively, pass a string with a column name to sort, such as:\n",
    "    # column_to_sort = 'col1'; or a list of columns to use for sorting: column_to_sort = \n",
    "    # ['col1', 'col2']\n",
    "    \n",
    "    # ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "    # ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "    # you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "    # list of booleans like ascending_sorting = [False, True] - the first column of the list\n",
    "    # will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "    # the correspondence is element-wise: the boolean in list ascending_sorting will correspond \n",
    "    # to the sorting order of the column with the same position in list column_to_sort.\n",
    "    # If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "    # check if the keys are the same:\n",
    "    boolean_check = (left_key == right_key)\n",
    "    # if boolean_check is True, we will merge using the on parameter, instead of left_on and right_on:\n",
    "    \n",
    "    if (boolean_check): # runs if it is True:\n",
    "        \n",
    "        merged_df = df_left.merge(df_right, on = left_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    else:\n",
    "        # use left_on and right_on\n",
    "        merged_df = df_left.merge(df_right, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    # Check if the dataframe should be sorted:\n",
    "    if (sort_merged_df == True):\n",
    "        \n",
    "        # check if column_to_sort = None. If it is, set it as the first column (index 0):\n",
    "        if (column_to_sort is None):\n",
    "            \n",
    "            column_to_sort = merged_df.columns[0]\n",
    "            print(f\"Sorting merged dataframe by its first column = {column_to_sort}\")\n",
    "        \n",
    "        # check if ascending_sorting is None. If it is, set it as True:\n",
    "        if (ascending_sorting is None):\n",
    "            \n",
    "            ascending_sorting = True\n",
    "            print(\"Sorting merged dataframe in ascending order.\")\n",
    "        \n",
    "        # Now, sort the dataframe according to the parameters:\n",
    "        merged_df = merged_df.sort_values(by = column_to_sort, ascending = ascending_sorting)\n",
    "        #sort by the first column, with index 0.\n",
    "    \n",
    "        # Now, reset index positions:\n",
    "        merged_df = merged_df.reset_index(drop = True)\n",
    "        print(\"Merged dataframe successfully sorted.\")\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully merged. Check its 10 first rows:\\n\")\n",
    "    print(merged_df.head(10))\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for creating a column with isolated informations from the timestamp**\n",
    "- Use this function for creating a column containing isolated information from the timestamp: \n",
    "    - Value of year,\n",
    "    - Value of month,\n",
    "    - Value of day,\n",
    "    - Value of hour,\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EXTRACT_TIMESTAMP_INFO (df, timestamp_tag_column, extracted_info, new_column_name = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe containing the timestamp.\n",
    "    \n",
    "    #timestamp_tag_column: declare as a string under quotes. This is the column from \n",
    "    #which we will extract the timestamp.\n",
    "    \n",
    "    #extracted_info: information to extract from the timestamp. The allowed values are:\n",
    "    #'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'\n",
    "    \n",
    "    #new_column_name: name (string)of the new created column. \n",
    "    #If no value is provided, it will be equals to extracted_info.\n",
    "    \n",
    "    if (new_column_name is None):\n",
    "        \n",
    "        new_column_name = extracted_info\n",
    "    \n",
    "    # START: CONVERT ALL TIMESTAMPS/DATETIMES/STRINGS TO pandas.Timestamp OBJECTS.\n",
    "    # This will prevent any compatibility problems.\n",
    "    \n",
    "    #The pd.Timestamp function can handle a single timestamp per call. Then, we must\n",
    "    # loop trough the series, and apply the function to each element.\n",
    "    \n",
    "    #1. Start a list to store the Pandas timestamps:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    #2. Loop through each element of the timestamp column, and apply the function\n",
    "    # to guarantee that all elements are Pandas timestamps\n",
    "    \n",
    "    for timestamp in df[timestamp_tag_column]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html\n",
    "    \n",
    "    #Use the extracted_info as key to access the correct command in the dictionary.\n",
    "    #To access an item from a dictionary d = {'key1': item1, ...}, declare d['key1'],\n",
    "    #as if you would do to access a column from a dataframe.\n",
    "    \n",
    "    #By doing so, you will select the extraction command from the dictionary:\n",
    "    # Loop through each element of the dataset, access the timestamp, \n",
    "    # extract the information and store it in the correspondent position of the \n",
    "    # new_column. Again. The methods can only be applied to a single Timestamp object,\n",
    "    # not to the series. That is why we must loop through each of them:\n",
    "    \n",
    "    #start a list to store the values of the new column\n",
    "    new_column_vals = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # i goes from zero to the index of the last element of the dataframe df\n",
    "        # This element has index len(df) - 1\n",
    "        # Append the values to the list according to the selected extracted_info\n",
    "        \n",
    "        if (extracted_info == 'year'):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).year)\n",
    "        \n",
    "        elif (extracted_info == \"month\"):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).month)\n",
    "        \n",
    "        elif (extracted_info == \"week\"):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).week)\n",
    "        \n",
    "        elif (extracted_info == \"day\"):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).day)\n",
    "        \n",
    "        elif (extracted_info == \"hour\"):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).hour)\n",
    "        \n",
    "        elif (extracted_info == \"minute\"):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).minute)\n",
    "        \n",
    "        elif (extracted_info == \"second\"):\n",
    "            \n",
    "            new_column_vals.append((timestamp_list[i]).second)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Invalid extracted information. Please select: year, month, week, day, hour, minute, or second.\")\n",
    "    \n",
    "    # Copy the list 'new_column_vals' to a new column of the dataframe:\n",
    "    \n",
    "    df[new_column_name] = new_column_vals\n",
    "     \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Timestamp information successfully extracted. Check dataset's 10 first rows:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    #Now that the information were retrieved from all Timestamps, return the new\n",
    "    #dataframe:\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating differences between timestamps (timedeltas)**\n",
    "- Use this function for creating a column containing differences between two or more timestamp columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CALCULATE_TIMEDELTA (df, timestamp_tag_column1, timestamp_tag_column2, timedelta_column_name  = None, returned_timedelta_unit = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #THIS FUNCTION PERFORMS THE OPERATION df[timestamp_tag_column1] - df[timestamp_tag_colum2]\n",
    "    #The declaration order will determine the sign of the output.\n",
    "    \n",
    "    #df: dataframe containing the two timestamp columns.\n",
    "    \n",
    "    #timestamp_tag_column1: string containing the name of the column with the timestamp\n",
    "    # on the left (from which the right timestamp will be subtracted).\n",
    "    \n",
    "    #timestamp_tag_column2: string containing the name of the column with the timestamp\n",
    "    # on the right, that will be substracted from the timestamp on the left.\n",
    "    \n",
    "    #timedelta_column_name: name of the new column. If no value is provided, the default\n",
    "    #name [timestamp_tag_column1]-[timestamp_tag_column2] will be given:\n",
    "    \n",
    "    if (timedelta_column_name is None):\n",
    "        \n",
    "        #apply the default name:\n",
    "        timedelta_column_name = \"[\" + timestamp_tag_column1 + \"]\" + \"-\" + \"[\" + timestamp_tag_column2 + \"]\"\n",
    "    \n",
    "    #Pandas Timedelta class: applicable to timedelta objects\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.html\n",
    "    #The delta method from the Timedelta class converts the timedelta to\n",
    "    #nanoseconds, guaranteeing the internal compatibility:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.delta.html#pandas.Timedelta.delta\n",
    "    \n",
    "    #returned_timedelta_unit: unit of the new column. If no value is provided, the unit will be\n",
    "    # considered as nanoseconds. \n",
    "    # POSSIBLE VALUES FOR THE TIMEDELTA UNIT:\n",
    "    #'year', 'month', 'day', 'hour', 'minute', 'second'.\n",
    "    \n",
    "    # START: CONVERT ALL TIMESTAMPS/DATETIMES/STRINGS TO pandas.Timestamp OBJECTS.\n",
    "    # This will prevent any compatibility problems.\n",
    "    \n",
    "    #The pd.Timestamp function can handle a single timestamp per call. Then, we must\n",
    "    # loop trough the series, and apply the function to each element.\n",
    "    \n",
    "    #1. Start a list to store the Pandas timestamps:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    #2. Loop through each element of the timestamp column, and apply the function\n",
    "    # to guarantee that all elements are Pandas timestamps\n",
    "    \n",
    "    for timestamp in df[timestamp_tag_column1]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column1]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    #3. Create a column in the dataframe that will store the timestamps.\n",
    "    # Simply copy the list as the column:\n",
    "    df[timestamp_tag_column1] = timestamp_list\n",
    "    \n",
    "    #Repeate these steps for the other column (timestamp_tag_column2):\n",
    "    # Restart the list, loop through all the column, and apply the pd.Timestamp function\n",
    "    # to each element, individually:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    for timestamp in df[timestamp_tag_column2]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column2]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    df[timestamp_tag_column2] = timestamp_list\n",
    "    \n",
    "    # Pandas Timestamps can be subtracted to result into a Pandas Timedelta.\n",
    "    # We will apply the delta method from Pandas Timedeltas.\n",
    "    \n",
    "    #4. Create a timedelta object as the difference between the timestamps:\n",
    "    \n",
    "    # NOTICE: Even though a list could not be submitted to direct operations like\n",
    "    # sum, subtraction and multiplication, the series and NumPy arrays can. When we\n",
    "    # copied the list as a new column on the dataframes, we converted the lists to series\n",
    "    # called df[timestamp_tag_column1] and df[timestamp_tag_column2]. These two series now\n",
    "    # can be submitted to direct operations.\n",
    "    \n",
    "    timedelta_obj = df[timestamp_tag_column1] - df[timestamp_tag_column2]\n",
    "    \n",
    "    #This timedelta_obj is a series of timedelta64 objects. The Pandas Timedelta function\n",
    "    # can process only one element of the series in each call. Then, we must loop through\n",
    "    # the series to obtain the float values in nanoseconds. Even though this loop may \n",
    "    # look unecessary, it uses the Delta method to guarantee the internal compatibility.\n",
    "    # Then, no errors due to manipulation of timestamps with different resolutions, or\n",
    "    # due to the presence of global variables, etc. will happen. This is the safest way\n",
    "    # to manipulate timedeltas.\n",
    "    \n",
    "    #5. Create an empty list to store the timedeltas in nanoseconds\n",
    "    TimedeltaList = []\n",
    "    \n",
    "    #6. Loop through each timedelta_obj and convert it to nanoseconds using the Delta\n",
    "    # method. Both pd.Timedelta function and the delta method can be applied to a \n",
    "    # a single object.\n",
    "    #len(timedelta_obj) is the total of timedeltas present.\n",
    "    \n",
    "    for i in range(len(timedelta_obj)):\n",
    "        \n",
    "        #This loop goes from i = 0 to i = [len(timedelta_obj) - 1], so that\n",
    "        #all indices are evaluated.\n",
    "        \n",
    "        #append the element resultant from the delta method application on the\n",
    "        # i-th element of the list timedelta_obj, i.e., timedelta_obj[i].\n",
    "        TimedeltaList.append(pd.Timedelta(timedelta_obj[i]).delta)\n",
    "    \n",
    "    #Notice that the loop is needed because Pandas cannot handle a series/list of\n",
    "    #Timedelta objects simultaneously. It can manipulate a single object\n",
    "    # in each call or iteration.\n",
    "    \n",
    "    #Now the list contains the timedeltas in nanoseconds and guarantees internal\n",
    "    #compatibility.\n",
    "    # The delta method converts the Timedelta object to an integer number equals to the\n",
    "    # value of the timedelta in nanoseconds. Then we are now dealing with numbers, not\n",
    "    # with timestamps.\n",
    "    # Even though some steps seem unecessary, they are added to avoid errors and bugs\n",
    "    # hard to identify, resultant from a timestamp assigned to the wrong type of\n",
    "    # object.\n",
    "    \n",
    "    #The list is not as the series (columns) and arrays: it cannot be directly submitted to \n",
    "    # operations like sum, division, and multiplication. For doing so, we can loop through \n",
    "    # each element, what would be the case for using the Pandas Timestamp and Timedelta \n",
    "    # functions, which can only manipulate one object per call.\n",
    "    # For simpler operations like division, we can convert the list to a NumPy array and\n",
    "    # submit the entire array to the operation at the same time, avoiding the use of \n",
    "    # memory consuminh iterative methods.\n",
    "    \n",
    "    #Convert the timedelta list to a NumPy array:\n",
    "    # Notice that we could have created a column with the Timedeltalist, so that it would\n",
    "    # be converted to a series. On the other hand, we still did not defined the name of the\n",
    "    # new column. So, it is easier to simply convert it to a NumPy array, and then copy\n",
    "    # the array as a new column.\n",
    "    TimedeltaList = np.array(TimedeltaList)\n",
    "    \n",
    "    #Convert the array to the desired unit by dividing it by the proper factor:\n",
    "    \n",
    "    if (returned_timedelta_unit == 'year'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #5. Convert it to years. 1 year = 365 days + 6 h = 365 days + 6/24 h/(h/day)\n",
    "        # = (365 + 1/4) days = 365.25 days\n",
    "        \n",
    "        TimedeltaList = TimedeltaList / (365.25) #in years\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in years. Considered 1 year = 365 days + 6 h.\")\n",
    "    \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'month'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #5. Convert it to months. Consider 1 month = 30 days\n",
    "        \n",
    "        TimedeltaList = TimedeltaList / (30.0) #in months\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in months. Considered 1 month = 30 days.\")\n",
    "        \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'day'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in days.\")\n",
    "        \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'hour'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in hours [h].\")\n",
    "    \n",
    "\n",
    "    elif (returned_timedelta_unit == 'minute'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in minutes [min].\")\n",
    "        \n",
    "        \n",
    "    elif (returned_timedelta_unit == 'second'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in seconds [s].\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        returned_timedelta_unit = 'ns'\n",
    "        print(\"No unit or invalid unit provided for timedelta. Then, returned timedelta in nanoseconds (1s = 10^9 ns).\")\n",
    "        \n",
    "        #In case None unit is provided or a non-valid value or string is provided,\n",
    "        #The calculus will be in nanoseconds.\n",
    "    \n",
    "    #Finally, create a column in the dataframe named as timedelta_column_name \n",
    "    # with the elements of TimedeltaList converted to the correct unit of time:\n",
    "    \n",
    "    #Append the selected unit as a suffix on the timedelta_column_name:\n",
    "    timedelta_column_name = timedelta_column_name + \"_\" + returned_timedelta_unit\n",
    "    \n",
    "    df[timedelta_column_name] = TimedeltaList\n",
    "      \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Timedeltas successfully calculated. Check dataset's 10 first rows:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    #Finally, return the dataframe with the new column:\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating differences between successive timestamps (delays)**\n",
    "- Use this function for creating a column containing differences between two successive timestamps from a same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CALCULATE_DELAY (df, timestamp_tag_column, new_timedelta_column_name  = None, returned_timedelta_unit = None, return_avg_delay = True):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #THIS FUNCTION CALCULATES THE DIFFERENCE (timedelta - delay) BETWEEN TWO SUCCESSIVE\n",
    "    # Timestamps from a same column\n",
    "    \n",
    "    #df: dataframe containing the two timestamp columns.\n",
    "    #timestamp_tag_column: string containing the name of the column with the timestamps\n",
    "    \n",
    "    #new_timedelta_column_name: name of the new column. If no value is provided, the default\n",
    "    #name [timestamp_tag_column1]-[timestamp_tag_column2] will be given:\n",
    "    \n",
    "    # return_avg_delay = True will print and return the value of the average delay.\n",
    "    # return_avg_delay = False will omit this information\n",
    "    \n",
    "    if (new_timedelta_column_name is None):\n",
    "        \n",
    "        #apply the default name:\n",
    "        new_timedelta_column_name = \"time_delay\"\n",
    "    \n",
    "    #Pandas Timedelta class: applicable to timedelta objects\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.html\n",
    "    #The delta method from the Timedelta class converts returns the timedelta in\n",
    "    #nanoseconds, guaranteeing the internal compatibility:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.delta.html#pandas.Timedelta.delta\n",
    "    \n",
    "    #returned_timedelta_unit: unit of the new column. If no value is provided, the unit will be\n",
    "    # considered as nanoseconds. \n",
    "    # POSSIBLE VALUES FOR THE TIMEDELTA UNIT:\n",
    "    #'year', 'month', 'day', 'hour', 'minute', 'second'.\n",
    "    \n",
    "    # START: CONVERT ALL TIMESTAMPS/DATETIMES/STRINGS TO pandas.Timestamp OBJECTS.\n",
    "    # This will prevent any compatibility problems.\n",
    "    \n",
    "    #The pd.Timestamp function can handle a single timestamp per call. Then, we must\n",
    "    # loop trough the series, and apply the function to each element.\n",
    "    \n",
    "    #1. Start a list to store the Pandas timestamps:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    #2. Loop through each element of the timestamp column, and apply the function\n",
    "    # to guarantee that all elements are Pandas timestamps\n",
    "    \n",
    "    for timestamp in df[timestamp_tag_column]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column1]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    #3. Create a column in the dataframe that will store the timestamps.\n",
    "    # Simply copy the list as the column:\n",
    "    timestamp_tag_column1 = timestamp_tag_column + \"_ts\"\n",
    "    df[timestamp_tag_column1] = timestamp_list\n",
    "    \n",
    "    # Now, let's create a list of the following timestamps\n",
    "    following_timestamp = []\n",
    "    # Let's skip the index 0, correspondent to the first timestamp:\n",
    "    \n",
    "    for i in range (1, len(timestamp_list)):\n",
    "        \n",
    "        # this loop goes from i = 1 to i = len(timestamp_list) - 1, the last index\n",
    "        # of the list. If we simply declared range (len(timestamp_list)), the loop\n",
    "        # will start from 0, the default\n",
    "        \n",
    "        #append the element from timestamp_list to following_timestamp:\n",
    "        following_timestamp.append(timestamp_list[i])\n",
    "    \n",
    "    # Notice that this list has one element less than the original list, because we started\n",
    "    # copying from index 1, not 0. Therefore, let's repeat the last element of timestamp_list:\n",
    "    following_timestamp.append(timestamp_list[i])\n",
    "    # Notice that, once we did not restarted the variable i, it keeps its last value obtained\n",
    "    # during the loop, correspondent to the index of the last element.\n",
    "    # Now, let's store it into a column (series) of the dataframe:\n",
    "    timestamp_tag_column2 = timestamp_tag_column + \"_ts_delayed\"\n",
    "    df[timestamp_tag_column2] = following_timestamp\n",
    "    \n",
    "    # Pandas Timestamps can be subtracted to result into a Pandas Timedelta.\n",
    "    # We will apply the delta method from Pandas Timedeltas.\n",
    "    \n",
    "    #4. Create a timedelta object as the difference between the timestamps:\n",
    "    \n",
    "    # NOTICE: Even though a list could not be submitted to direct operations like\n",
    "    # sum, subtraction and multiplication, the series and NumPy arrays can. When we\n",
    "    # copied the list as a new column on the dataframes, we converted the lists to series\n",
    "    # called df[timestamp_tag_column1] and df[timestamp_tag_column2]. These two series now\n",
    "    # can be submitted to direct operations.\n",
    "    \n",
    "    # Delay = next measurement (tag_column2, timestamp higher) - current measurement\n",
    "    # (tag_column2, timestamp lower). Since we repeated the last timestamp twice,\n",
    "    # in the last row it will be subtracted from itself, resulting in zero.\n",
    "    # This is the expected, since we do not have a delay yet\n",
    "    timedelta_obj = df[timestamp_tag_column2] - df[timestamp_tag_column1]\n",
    "    \n",
    "    #This timedelta_obj is a series of timedelta64 objects. The Pandas Timedelta function\n",
    "    # can process only one element of the series in each call. Then, we must loop through\n",
    "    # the series to obtain the float values in nanoseconds. Even though this loop may \n",
    "    # look unecessary, it uses the Delta method to guarantee the internal compatibility.\n",
    "    # Then, no errors due to manipulation of timestamps with different resolutions, or\n",
    "    # due to the presence of global variables, etc. will happen. This is the safest way\n",
    "    # to manipulate timedeltas.\n",
    "    \n",
    "    #5. Create an empty list to store the timedeltas in nanoseconds\n",
    "    TimedeltaList = []\n",
    "    \n",
    "    #6. Loop through each timedelta_obj and convert it to nanoseconds using the Delta\n",
    "    # method. Both pd.Timedelta function and the delta method can be applied to a \n",
    "    # a single object.\n",
    "    #len(timedelta_obj) is the total of timedeltas present.\n",
    "    \n",
    "    for i in range(len(timedelta_obj)):\n",
    "        \n",
    "        #This loop goes from i = 0 to i = [len(timedelta_obj) - 1], so that\n",
    "        #all indices are evaluated.\n",
    "        \n",
    "        #append the element resultant from the delta method application on the\n",
    "        # i-th element of the list timedelta_obj, i.e., timedelta_obj[i].\n",
    "        TimedeltaList.append(pd.Timedelta(timedelta_obj[i]).delta)\n",
    "    \n",
    "    #Notice that the loop is needed because Pandas cannot handle a series/list of\n",
    "    #Timedelta objects simultaneously. It can manipulate a single object\n",
    "    # in each call or iteration.\n",
    "    \n",
    "    #Now the list contains the timedeltas in nanoseconds and guarantees internal\n",
    "    #compatibility.\n",
    "    # The delta method converts the Timedelta object to an integer number equals to the\n",
    "    # value of the timedelta in nanoseconds. Then we are now dealing with numbers, not\n",
    "    # with timestamps.\n",
    "    # Even though some steps seem unecessary, they are added to avoid errors and bugs\n",
    "    # hard to identify, resultant from a timestamp assigned to the wrong type of\n",
    "    # object.\n",
    "    \n",
    "    #The list is not as the series (columns) and arrays: it cannot be directly submitted to \n",
    "    # operations like sum, division, and multiplication. For doing so, we can loop through \n",
    "    # each element, what would be the case for using the Pandas Timestamp and Timedelta \n",
    "    # functions, which can only manipulate one object per call.\n",
    "    # For simpler operations like division, we can convert the list to a NumPy array and\n",
    "    # submit the entire array to the operation at the same time, avoiding the use of \n",
    "    # memory consuminh iterative methods.\n",
    "    \n",
    "    #Convert the timedelta list to a NumPy array:\n",
    "    # Notice that we could have created a column with the Timedeltalist, so that it would\n",
    "    # be converted to a series. On the other hand, we still did not defined the name of the\n",
    "    # new column. So, it is easier to simply convert it to a NumPy array, and then copy\n",
    "    # the array as a new column.\n",
    "    TimedeltaList = np.array(TimedeltaList)\n",
    "    \n",
    "    #Convert the array to the desired unit by dividing it by the proper factor:\n",
    "    \n",
    "    if (returned_timedelta_unit == 'year'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #5. Convert it to years. 1 year = 365 days + 6 h = 365 days + 6/24 h/(h/day)\n",
    "        # = (365 + 1/4) days = 365.25 days\n",
    "        \n",
    "        TimedeltaList = TimedeltaList / (365.25) #in years\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in years. Considered 1 year = 365 days + 6 h.\")\n",
    "    \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'month'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #5. Convert it to months. Consider 1 month = 30 days\n",
    "        \n",
    "        TimedeltaList = TimedeltaList / (30.0) #in months\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in months. Considered 1 month = 30 days.\")\n",
    "        \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'day'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in days.\")\n",
    "        \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'hour'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in hours [h].\")\n",
    "    \n",
    "\n",
    "    elif (returned_timedelta_unit == 'minute'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in minutes [min].\")\n",
    "        \n",
    "        \n",
    "    elif (returned_timedelta_unit == 'second'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in seconds [s].\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        returned_timedelta_unit = 'ns'\n",
    "        print(\"No unit or invalid unit provided for timedelta. Then, returned timedelta in nanoseconds (1s = 10^9 ns).\")\n",
    "        \n",
    "        #In case None unit is provided or a non-valid value or string is provided,\n",
    "        #The calculus will be in nanoseconds.\n",
    "    \n",
    "    #Finally, create a column in the dataframe named as new_timedelta_column_name \n",
    "    # with the elements of TimedeltaList converted to the correct unit of time:\n",
    "    \n",
    "    #Append the selected unit as a suffix on the new_timedelta_column_name:\n",
    "    new_timedelta_column_name = new_timedelta_column_name + \"_\" + returned_timedelta_unit\n",
    "    \n",
    "    df[new_timedelta_column_name] = TimedeltaList\n",
    "      \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Time delays successfully calculated. Check dataset's 10 first rows:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    if (return_avg_delay == True):\n",
    "        \n",
    "        # Let's calculate the average delay, print and return it:\n",
    "        # Firstly, we must remove the last element of the TimedeltaList.\n",
    "        # Remember that this element is 0 because there is no delay. It was added to allow\n",
    "        # the element-wise operations between the series.\n",
    "        # Let's eliminate the last element from TimedeltaList. Since this list was already\n",
    "        # copied to the dataframe, there is no risk of losing information.\n",
    "        \n",
    "        # Index of the last element:\n",
    "        last_element_index = len(TimedeltaList) - 1\n",
    "        \n",
    "        # Delete the element:\n",
    "        del TimedeltaList[last_element_index]\n",
    "        # Deleted item at index last_element_index from TimedeltaList list\n",
    "        \n",
    "        # Now we calculate the average value:\n",
    "        avg_delay = np.average(TimedeltaList)\n",
    "        \n",
    "        print(f\"Average delay = {avg_delay} {returned_timedelta_unit}\")\n",
    "        \n",
    "        # Return the dataframe and the average value:\n",
    "        return df, avg_delay\n",
    "    \n",
    "    #Finally, return the dataframe with the new column:\n",
    "    \n",
    "    else: \n",
    "        # Return only the dataframe\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for adding or subtracting a timedelta from a timestamp**\n",
    "- Use this function for creating a column containing timestamps added or subtracted by a fixed timedelta value (offset).\n",
    "- Set `timedelta` as a negative value to subtract this timedelta from the timestamp (as explained in the comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADD_TIMEDELTA (df, timestamp_tag_column, timedelta, new_timestamp_col  = None, timedelta_unit = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #THIS FUNCTION PERFORMS THE OPERATION ADDING A FIXED TIMEDELTA (difference of time\n",
    "    # or offset) to a timestamp.\n",
    "    \n",
    "    #df: dataframe containing the timestamp column.\n",
    "    \n",
    "    #timestamp_tag_column: string containing the name of the column with the timestamp\n",
    "    # to which the timedelta will be added to.\n",
    "    \n",
    "    #timedelta: numeric value of the timedelta.\n",
    "    # WARNING: simply input a numeric value, not a string with unit. e.g. timedelta = 2.4\n",
    "    # If you want to subtract a timedelta, input a negative value. e.g. timedelta = - 2.4\n",
    "    \n",
    "    #new_timestamp_col: name of the new column containing the obtained timestamp. \n",
    "    # If no value is provided, the default name [timestamp_tag_column]+[timedelta] \n",
    "    # will be given (at the end of the code, after we created the timedelta object \n",
    "    # with correct units)\n",
    "    \n",
    "    #Pandas Timedelta class: applicable to timedelta objects\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.html\n",
    "    #The delta method from the Timedelta class converts returns the timedelta in\n",
    "    #nanoseconds, guaranteeing the internal compatibility:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.delta.html#pandas.Timedelta.delta\n",
    "    \n",
    "    #timedelta_unit: unit of the timedelta interval. If no value is provided, \n",
    "    # the unit will be considered 'ns' (default). Possible values are:\n",
    "    #'day', 'hour', 'minute', 'second', 'ns'.\n",
    "    \n",
    "    if (timedelta_unit is None):\n",
    "        \n",
    "        timedelta_unit = 'ns'\n",
    "    \n",
    "    # Pandas do not support timedeltas in years or months, since these values may\n",
    "    # be ambiguous (e.g. a month may have 30 or 31 days, so an approximation would\n",
    "    # be necessary).\n",
    "    \n",
    "    # START: CONVERT ALL TIMESTAMPS/DATETIMES/STRINGS TO pandas.Timestamp OBJECTS.\n",
    "    # This will prevent any compatibility problems.\n",
    "    \n",
    "    #The pd.Timestamp function can handle a single timestamp per call. Then, we must\n",
    "    # loop trough the series, and apply the function to each element.\n",
    "    \n",
    "    #1. Start a list to store the Pandas timestamps:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    #2. Loop through each element of the timestamp column, and apply the function\n",
    "    # to guarantee that all elements are Pandas timestamps\n",
    "    \n",
    "    for timestamp in df[timestamp_tag_column]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column1]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    #3. Create a column in the dataframe that will store the timestamps.\n",
    "    # Simply copy the list as the column:\n",
    "    df[timestamp_tag_column] = timestamp_list\n",
    "    \n",
    "    # The Pandas Timestamp can be directly added to a Pandas Timedelta.\n",
    " \n",
    "    #Dictionary for converting the timedelta_unit to Pandas encoding for the\n",
    "    # Timedelta method. to access the element of a dictionary d = {\"key\": element},\n",
    "    # simply declare d['key'], as if you were accessing the column of a dataframe. Here,\n",
    "    # the key is the argument of the function, whereas the element is the correspondent\n",
    "    # Pandas encoding for this method. With this dictionary we simplify the search for the\n",
    "    # proper time encoding: actually, depending on the Pandas method the encoding may be\n",
    "    # 'd', \"D\" or \"day\" for day, for instance. So, we avoid having to check the whole\n",
    "    # documentation by creating a simpler common encoding for the functions in this notebook.\n",
    "    \n",
    "    unit_dict = {\n",
    "        \n",
    "        'day': 'd',\n",
    "        'hour': 'h',\n",
    "        'minute': 'min',\n",
    "        'second': 's',\n",
    "        'ns': 'ns'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    #Create the Pandas timedelta object from the timedelta value and the selected\n",
    "    # time units:\n",
    "    timedelta = pd.Timedelta(timedelta, unit_dict[timedelta_unit])\n",
    "    \n",
    "    #A pandas Timedelta object has total compatibility with a pandas\n",
    "    #Timestamp, so we can simply add the Timedelta to the Timestamp to obtain a new \n",
    "    #corrected timestamp.\n",
    "    # Again, notice that the timedelta can be positive (sum of time), or negative\n",
    "    # (subtraction of time).\n",
    "    \n",
    "    #Now, add the timedelta to the timestamp, and store it into a proper list/series:\n",
    "    new_timestamps = df[timestamp_tag_column] + timedelta\n",
    "     \n",
    "    #Finally, create a column in the dataframe named as new_timestamp_col\n",
    "    #and store the new timestamps into it\n",
    "    \n",
    "    if (new_timestamp_col is None):\n",
    "        \n",
    "        #apply the default name:\n",
    "        new_timestamp_col = \"[\" + timestamp_tag_column + \"]\" + \"+\" + \"[\" + str(timedelta) + \"]\"\n",
    "        #The str function converts the timedelta object to a string, so it can be\n",
    "        #concatenated in this line of code.\n",
    "        #Notice that we defined the name of the new column at the end of the code so\n",
    "        #that we already converted the 'timedelta' to a Timedelta object containing\n",
    "        #the correct units.\n",
    "    \n",
    "    df[new_timestamp_col] = new_timestamps\n",
    "      \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Timedeltas successfully added. Check dataset's 10 first rows:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    #Finally, return the dataframe with the new column:\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for slicing the dataframe (selecting a specific subset of rows)**\n",
    "- This function maintains all columns from the original dataframe, returning a dataframe that is a subset of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLICE_DATAFRAME (df, from_row = 'first_only', to_row = 'only', restart_index_of_the_sliced_dataframe = False):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # restart_index_of_the_sliced_dataframe = False to keep the \n",
    "    # same row index of the original dataframe; or \n",
    "    # restart_index_of_the_sliced_dataframe = True to reset indices \n",
    "    # (start a new index, from 0 for the first row of the \n",
    "    # returned dataframe).\n",
    "    \n",
    "    # from_row and to_row: integer or strings:\n",
    "    \n",
    "    # from_row may be any integer from 0 to the last row of the dataset\n",
    "    # and the following strings: 'first' and 'first_only'\n",
    "    \n",
    "    # to_row may be any integer from 0 to the last row of the dataset\n",
    "    # and the following strings: 'last', 'last_only', and 'only'\n",
    "    \n",
    "    # the combination from_row = 'first', to_row = 'last' will\n",
    "    # return the original dataframe itself.\n",
    "    # The same is valid for the combination from_row = 'first_only', \n",
    "    # to_row = 'last_only'; or of combinations between from_row = 0\n",
    "    # (index of the first row) with 'last' or the index\n",
    "    # of the last row; or combinations between 'first' and the index\n",
    "    # of the last row.\n",
    "    \n",
    "    # These possibilities are the first checked by the code. If none\n",
    "    # of these special cases are present, then:\n",
    "    \n",
    "    # from_row = 'first_only' selects a dataframe containing only the\n",
    "    # first row, independently of the parameter passed as to_row;\n",
    "    \n",
    "    # to_row = 'last_only' selects a dataframe containing only the\n",
    "    # last row, independently of the parameter passed as from_row;\n",
    "    \n",
    "    # if to_row = 'only', the sliced dataframe will be formed by only the\n",
    "    # row passed as from_row (an integer representing the row index is\n",
    "    # passed) - explained in the following lines\n",
    "    \n",
    "    # These three special cases are dominant over the following ones\n",
    "    # (they are checked firstly, and force the modifying of slicing\n",
    "    # limits):\n",
    "    \n",
    "    # Other special cases:\n",
    "    \n",
    "    # from_row = 'first' starts slicing on the first row (index 0) -\n",
    "    # the 1st row from the dataframe will be the 1st row of the sliced\n",
    "    # dataframe too.\n",
    "    \n",
    "    # to_row = 'last' finishes slicing in the last row - the last row\n",
    "    # from the dataframe will be the last row of the sliced dataframe.\n",
    "    \n",
    "    # If i and j are integer numbers, they represent the indices of rows:\n",
    "    \n",
    "    # from_row = i starts the sliced dataframe from the row of index i\n",
    "    # of the original dataframe.\n",
    "    # e.g. from_row = 8 starts the slicing from row with index 8. Since\n",
    "    # slicing starts from 0, this is the 9th row of the original dataframe.\n",
    "    \n",
    "    # to_row = j finishes the sliced dataframe on the row of index j of\n",
    "    # the original dataframe. Attention: this row with index j is included,\n",
    "    # and will be the last_row of the sliced dataframe.\n",
    "    # e.g. if to_row = 21, the last row of the sliced dataframe will be the\n",
    "    # row with index 21 of the original dataframe. Since slicing starts\n",
    "    # from 0, this is the 22nd row of the original dataframe.\n",
    "    \n",
    "    # In summary, if from_row = 8, to_row = 21, the sliced dataframe\n",
    "    # will be formed from the row of index 8 to the row of index 21 of\n",
    "    # the original dataframe, including both the row of index 8 and the row\n",
    "    # index 21. \n",
    "    \n",
    "    # from_row is effectively the first row of the new dataframe;\n",
    "    # and to_row is effectively the last row of the new dataframe.\n",
    "    \n",
    "    # Notice that the use of to_row < from_row will raise an error.\n",
    "    \n",
    "    \n",
    "    # Store the total number of rows as num_rows:\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    if ((from_row == 'first') | (from_row == 0)):\n",
    "        # Set the first_row_index as the 0 (1st row index):\n",
    "        first_row_index = 0\n",
    "    \n",
    "    else:\n",
    "        # Pass the int attribute to guarantee that the value\n",
    "        # was read as an integer. This value is itself the index of\n",
    "        # the first row of the sliced dataframe:\n",
    "        first_row_index = int(from_row)\n",
    "    \n",
    "    if ((to_row == 'last') | (to_row == (num_rows - 1))):\n",
    "        # Set the index of the last sliced dataframe as the index of the last row:\n",
    "        last_row_index = (num_rows - 1)\n",
    "    \n",
    "    else:\n",
    "        # Pass the int attribute to guarantee that the value\n",
    "        # was read as an integer. This value is itself the index of\n",
    "        # the last row of the sliced dataframe:\n",
    "        last_row_index = int(to_row)\n",
    "    \n",
    "    # Slice a dataframe: df[i:j]\n",
    "    # Slice the dataframe, getting only row i to row (j-1)\n",
    "    # Indexing naturally starts from 0\n",
    "    # Notice that the slicer defined as df[i:j] takes all columns from\n",
    "    # the dataframe: it copies the dataframe structure (columns), but\n",
    "    # selects only the specified rows.\n",
    "    \n",
    "    # first_row = df[0:1]\n",
    "    # This is equivalent to df[:1] - if there is no start for the\n",
    "    # slicer, the start from 0 is implicit\n",
    "    # slice: get rows from row 0 to row (1-1) = 0\n",
    "    # Therefore, we will obtain a copy of the dataframe, but containing\n",
    "    # only the first row (row 0)\n",
    "    \n",
    "    # last_row = df[(num_rows - 1):(num_rows)] \n",
    "    # slice the dataframe from row (num_rows - 1), the index of the\n",
    "    # last row, to row (num_rows) - 1 = (num_rows - 1)\n",
    "    # Therefore, this slicer is a copy of the dataframe but containing\n",
    "    # only its last row.\n",
    "    \n",
    "    # Slices are (fractions of) pandas dataframes, so elements must be\n",
    "    # accessed through .iloc or .loc method\n",
    "    \n",
    "    \n",
    "    # Check the special combination from = 1st row to last row\n",
    "    # and return the original dataframe itself, without performing\n",
    "    # operations:\n",
    "    \n",
    "    if ((from_row == 'first_only') & (to_row == 'last_only')):\n",
    "        \n",
    "        #return the dataframe without performing any operation\n",
    "        print(\"Sliced dataframe is the original dataframe itself.\")\n",
    "        return df\n",
    "    \n",
    "    elif ((first_row_index == 0) & (last_row_index == (num_rows - 1))):\n",
    "        \n",
    "        #return the dataframe without performing any operation\n",
    "        print(\"Sliced dataframe is the original dataframe itself.\")\n",
    "        return df\n",
    "         \n",
    "    # The two special combinations were checked, now we can back to\n",
    "    # the main code\n",
    "    \n",
    "    # Special cases 'first_only' and 'last_only': force the index,\n",
    "    # independently on other parameters:\n",
    "    \n",
    "    elif (from_row == 'first_only'):\n",
    "        \n",
    "        first_row_index = 0\n",
    "        last_row_index = first_row_index\n",
    "    \n",
    "    elif (to_row == 'last_only'):\n",
    "        \n",
    "        last_row_index = (num_rows - 1)\n",
    "        first_row_index = last_row_index\n",
    "    \n",
    "    elif (to_row == 'only'):\n",
    "        \n",
    "        last_row_index = first_row_index\n",
    "    \n",
    "    # If none of these cases are present, use the integer input\n",
    "    # as parameters (already calculated)\n",
    "    \n",
    "    # Again:\n",
    "    # Slice a dataframe: df[i:j]\n",
    "    # Slice the dataframe, getting only row i to row (j-1)\n",
    "    \n",
    "    # Set slicing limits:\n",
    "    i = first_row_index # i is included\n",
    "    j = last_row_index + 1\n",
    "    # df[i:j] will include row i to row j - 1 = \n",
    "    # (last_row_index + 1) - 1 = last_row_index\n",
    "    # Then, by summing 1 we guarantee that the row passed as\n",
    "    # last_row_index will be actually included.\n",
    "    # notice that when last_row_index = first_row_index\n",
    "    # j will be the index of the next line.\n",
    "    # e.g. the slice of only the first line must be df[0:1]\n",
    "    # there must be a difference of 1 to include 1 line.\n",
    "    \n",
    "    # Now, slice the dataframe from line of index i to\n",
    "    # line j-1, where line (j-1) is the last one included:\n",
    "    \n",
    "    sliced_df = df[i:j]\n",
    "    \n",
    "    if (restart_index_after_slicing == True):\n",
    "        # Reset the index:\n",
    "        sliced_df = sliced_df.reset_index(drop = True)\n",
    "        print(\"Index of the returned dataframe was restarted.\")\n",
    "    \n",
    "    print(f\"Returning sliced dataframe, containing {sliced_df.shape[0]} rows and {sliced_df.shape[1]} columns.\")\n",
    "    # dataframe.shape is a tuple (N, M), where dataframe.shape[0] = N is\n",
    "    # the number of rows; and dataframe.shape[1] = M is the number of columns\n",
    "    # of the dataframe\n",
    "    \n",
    "    print(\"Check the dataframe below:\\n\")\n",
    "    print(sliced_df)\n",
    "    \n",
    "    return sliced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\")\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe (dataframe_to_be_exported, new_file_name_with_csv_extension, file_directory_path = None, export_to_s3_bucket = False, s3_bucket_name = None, desired_s3_file_name_with_csv_extension = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all file extensions should be .csv for this function\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # NEW_FILE_NAME_WITH_CSV_EXTENSION - (string, in quotes): input the name of the \n",
    "    # file with the  extension. e.g. FILE_NAME_WITH_CSV_EXTENSION = \"file.csv\"\n",
    "    \n",
    "    # export_to_s3_bucket = False. Alternatively, set as True to export the file to an\n",
    "    # AWS S3 Bucket.\n",
    "\n",
    "    ## The following parameters have effect only when export_to_s3_bucket == True:\n",
    "\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "\n",
    "    # The name desired for the object stored in S3 (string, in quotes). \n",
    "    # Keep it None to set it equals to new_file_name_with_csv_extension. \n",
    "    # Alternatively, set it as a string analogous to new_file_name_with_csv_extension. \n",
    "    # e.g. desired_s3_file_name_with_csv_extension = \"S3_file.csv\"\n",
    "    \n",
    "    if (export_to_s3_bucket == True):\n",
    "        \n",
    "        import boto3\n",
    "        #boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        \n",
    "        if (desired_s3_file_name_with_csv_extension is None):\n",
    "            #Repeat new_file_name_with_extension\n",
    "            desired_s3_file_name_with_csv_extension = new_file_name_with_csv_extension\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start S3 client:\n",
    "            print(\"Starting AWS S3 client.\")\n",
    "        \n",
    "            # Let's export the file to a AWS S3 (simple storage service) bucket\n",
    "            # instantiate S3 client and upload to s3\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            # Create a local copy of the file on the root.\n",
    "            local_copy_path = os.path.join(\"/\", new_file_name_with_csv_extension)\n",
    "            dataframe_to_be_exported.to_csv(local_copy_path, index = False)\n",
    "            \n",
    "            print(\"Local copy of the dataframe created on the root path to export to S3.\")\n",
    "            print(\"Simply delete this file from the root path if you only want to keep the S3 version.\")\n",
    "            \n",
    "            # Upload this local copy to S3:\n",
    "            try:\n",
    "                response = s3_client.meta.client.upload_file(local_copy_path, s3_bucket_name, desired_s3_file_name_with_extension)\n",
    "            \n",
    "            except ClientError as e:\n",
    "                logging.error(e)\n",
    "                return False\n",
    "            \n",
    "            print(f\"{desired_s3_file_name_with_csv_extension} successfully exported to {s3_bucket_name} AWS S3 bucket.\")\n",
    "            return True\n",
    "            # Check AWS Documentation:\n",
    "            # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "            \n",
    "            # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "            # the following code, instead:        \n",
    "            # ACCESS_KEY = 'access_key_ID'\n",
    "            # PASSWORD_KEY = 'password_key'\n",
    "            # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "            # s3_client.upload_file(local_copy_path, s3_bucket_name, desired_s3_file_name_with_extension)\n",
    "            \n",
    "    else :\n",
    "        # Do not export to AWS S3. Export to other path.\n",
    "        # Create the complete file path:\n",
    "        file_path = os.path.join(file_directory_path, new_file_name_with_csv_extension)\n",
    "\n",
    "        dataframe_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "        print(f\"Dataframe {new_file_name_with_csv_extension} exported as \\'{file_path}\\'.\")\n",
    "        print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_or_upload_file (source = 'aws', action = 'download', object_to_download_from_colab = None, s3_bucket_name = None, local_path_of_storage = '/', file_name_with_extension = None):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # source = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "    # source = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to AWS S3 or to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # object_to_download_from_colab = None. This option has effect only when\n",
    "    # source == 'google'. In this case, this parameter is obbligatory. \n",
    "    # Declare as object_to_download_from_colab the object that you want to download.\n",
    "    # Since it is an object and not a string, it should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = dict.\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = df.\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = keras_model\n",
    "    \n",
    "    ## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "    # to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "    # path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "    # If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "    # will be imported to the root path. Alternatively, input the path as a string \n",
    "    # (in quotes).\n",
    "    # Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "    # LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "    # Notice that only the directories should be declared: do not include the file name and\n",
    "    # its extension.\n",
    "    \n",
    "    # file_name_with_extension: string, in quotes, containing the file name which will be\n",
    "    # downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "    ## This parameter is obbligatory when source == 'aws'\n",
    "    # Examples:\n",
    "    # file_name_with_extension = 'Screen_Shot.png'; file_name_with_extension = 'dataset.csv',\n",
    "    # file_name_with_extension = \"dictionary.pkl\", file_name_with_extension = \"model.h5\",\n",
    "    # file_name_with_extension = 'doc.pdf', file_name_with_extension = 'model.dill'\n",
    "\n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import files\n",
    "        # google.colab library must be imported only in case \n",
    "        # it is going to be used, for avoiding \n",
    "        # AWS compatibility issues.\n",
    "        \n",
    "        if (action == 'upload'):\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            \n",
    "            colab_files_dict = files.upload()\n",
    "            \n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "                \n",
    "                print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "                print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "                print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "                print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "                print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "                print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "                print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "                print(\"df = pd.read_excel(uploaded_file)\")\n",
    "        \n",
    "        elif (action == 'download'):\n",
    "            \n",
    "            if (object_to_download_from_colab is None):\n",
    "                \n",
    "                #No object was declared\n",
    "                print(\"Please, inform an object to download. Since it is an object, not a string, it should not be declared in quotes.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "                files.download(object_to_download_from_colab)\n",
    "\n",
    "                print(f\"File {object_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(\"Please, select a valid action, download or upload.\")\n",
    "          \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead for starting the client:\n",
    "        \n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # Nextly, the code is the same.\n",
    "        \n",
    "        \n",
    "        # If the path to store is None, also import the bucket content to root path;\n",
    "        # or upload the file from root path to the bucket\n",
    "        if (local_path_of_storage is None):\n",
    "            \n",
    "            local_path_of_storage = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message. The same for the file name with extension:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name.\")\n",
    "        \n",
    "        elif (file_name_with_extension is None):\n",
    "            \n",
    "            print(\"Please, provide a valid file name with its extension. e.g. \\'dataset.csv\\'.\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Obtain the full file path from which the file will be uploaded to S3; or to\n",
    "            # which the file will be downloaded from S3:\n",
    "            file_path = os.path.join(local_path_of_storage, file_name_with_extension)\n",
    "            \n",
    "            # Start S3 client:\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            print(\"Starting AWS S3 client.\")\n",
    "            \n",
    "            if (action == 'upload'):\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).\\\n",
    "                    upload_file(Filename = file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully uploaded to AWS S3 {s3_bucket_name} bucket.\")\n",
    "            \n",
    "            elif (action == 'download'):\n",
    "\n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).download_file(file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully downloaded from AWS S3 {s3_bucket_name} bucket.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"Please, select a valid action, download or upload.\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = '/'\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None, or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'name_of_aws_s3_bucket_to_be_accessed'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_KEY_PREFFIX_FOLDER = None\n",
    "# S3_OBJECT_KEY_PREFFIX_FOLDER = None. Keep it None or as an empty string \n",
    "# (S3_OBJECT_KEY_PREFFIX_FOLDER = '') to import the whole bucket content, instead of a \n",
    "# single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, key_preffix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_key_preffix = S3_OBJECT_KEY_PREFFIX_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\"\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "## Parameters for loading txt or CSV files:\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# TXT_CSV_COL_SEP = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, TXT_CSV_COL_SEP = \"comma\" for columns separated by comma (\",\")\n",
    "# TXT_CSV_COL_SEP = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFFIX_LIST = None\n",
    "# JSON_METADATA_PREFFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, has_header = HAS_HEADER, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_preffix_list = JSON_METADATA_PREFFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFFIX_LIST = None\n",
    "# JSON_METADATA_PREFFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_preffix_list = JSON_METADATA_PREFFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grouping the data by a timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be grouped\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"DATE\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "\n",
    "GROUPING_FREQUENCY_UNIT = 'day'\n",
    "#Alternatively: 'year', 'month', 'week', 'hour', 'minute', 'day', or 'second'\n",
    "\n",
    "NUMBER_OF_PERIODS_TO_GROUP = 1 \n",
    "# Group by every NUMBER_OF_PERIODS_TO_GROUP = 1 periods (every day, if 'day' is selected).\n",
    "#Bin size. Alternatively: any integer number. Check the instructions in function comments.\n",
    "\n",
    "AGGREGATE_FUNCTION = 'mean'\n",
    "# Keep the method inside quotes.\n",
    "#Alternatively: any Pandas aggregation method: 'mean', 'median', 'std', 'sum', 'min' 'max',\n",
    "# etc. You can pass a list of multiple aggregations, like: AGGREGATE_FUNCTION  = \n",
    "# [mean, max, sum]. You can also pass custom functions, like: pct30 (30-percentile), \n",
    "# or np.mean: AGGREGATE_FUNCTION = pct30, AGGREGATE_FUNCTION = np.mean.\n",
    "\n",
    "#ADJUST OF GROUPING BASED ON A FIXED TIMESTAMP\n",
    "# You can specify the origin (start_time) or the offset (offset_time), which are equivalent.\n",
    "#WARNING: DECLARE ONLY ONE OF THESE PARAMETERS. DO NOT DECLARE AN OFFSET IF AN ORIGIN WAS \n",
    "# SPECIFIED, AND VICE-VERSA.\n",
    "START_TIME = None\n",
    "OFFSET_TIME = None\n",
    "# Alternatively, these parameters should be declared as a pandas Timestamp or in the\n",
    "# specific notation of Pandas offset_time for the Grouper class:\n",
    "# START_TIME = pd.Timestamp('2000-10-01 23:30:00', unit = 'ns')\n",
    "# Simply substitute the Timestamp inside quotes by the correct start timestamp.\n",
    "# This timestamp do not have to be complete, but must be interpretable by the Timestamp\n",
    "# function.\n",
    "# OFFSET_TIME = '23h30min', OFFSET_TIME = '2min', etc. Simply substitute the offset time\n",
    "# inside quotes by the correct value.\n",
    "# For examples on the notation for start and offset time, check Pandas grouper class\n",
    "# documentation, and Pandas timestamp class documentation:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html\n",
    "\n",
    "\n",
    "#New dataframe saved as grouped_df. Simply modify this object on the left of equality:\n",
    "grouped_df = GROUP_BY_TIMESTAMP (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, grouping_frequency_unit = GROUPING_FREQUENCY_UNIT, number_of_periods_to_group = NUMBER_OF_PERIODS_TO_GROUP, aggregate_function = AGGREGATE_FUNCTION, start_time = START_TIME, offset_time = OFFSET_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging (joining) the data on a timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"DATE\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"DATE\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\". This option has no effect \n",
    "# if MERGE_METHOD = \"asof\". Keep inside quotes.\n",
    "\n",
    "MERGE_METHOD = \"ordered\"\n",
    "#Alternatively: MERGE_METHOD = 'ordered' to use pandas .merge_ordered method, or\n",
    "# MERGE_METHOD = \"asof\" for using the .merge_asof method.\n",
    "# WARNING: .merge_asof uses fuzzy matching, so the HOW_TO_JOIN parameter is not applicable.\n",
    "# Keep inside quotes.\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "ASOF_DIRECTION = \"nearest\"\n",
    "# Parameter of .merge_asof method. 'nearest' merge the closest timestamps in both directions.\n",
    "# Alternatively: 'backward' or 'forward'.\n",
    "# This option has no effect if MERGE_METHOD = \"ordered\". Keep inside quotes.\n",
    "\n",
    "ORDERED_FILLING = None\n",
    "# Parameter or .merge_ordered method.\n",
    "# Alternatively: ORDERED_FILLING = 'ffill' (inside quotes) to fill missings \n",
    "# with the previous value.\n",
    "# This option has no effect if MERGE_METHOD = \"asof\".\n",
    "\n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = MERGE_ON_TIMESTAMP (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merge_method = MERGE_METHOD, merged_suffixes = MERGED_SUFFIXES, asof_direction = ASOF_DIRECTION, ordered_filling = ORDERED_FILLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"left_key_column\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"right_key_column\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\".\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "SORT_MERGED_DF = False\n",
    "# SORT_MERGED_DF = False not to sort the merged dataframe. If you want to sort it,\n",
    "# set as True. If SORT_MERGED_DF = True and COLUMN_TO_SORT = None, the dataframe will\n",
    "# be sorted by its first column.\n",
    "\n",
    "COLUMN_TO_SORT = None\n",
    "# COLUMN_TO_SORT = None. Keep it None if the dataframe should not be sorted.\n",
    "# Alternatively, pass a string with a column name to sort, such as:\n",
    "# COLUMN_TO_SORT = 'col1'; or a list of columns to use for sorting: COLUMN_TO_SORT = \n",
    "# ['col1', 'col2']\n",
    "\n",
    "ASCENDING_SORTING = True\n",
    "# ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "# ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "# you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "# list of booleans like ASCENDING_SORTING = [False, True] - the first column of the list\n",
    "# will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "# the correspondence is element-wise: the boolean in list ASCENDING_SORTING will correspond \n",
    "# to the sorting order of the column with the same position in list COLUMN_TO_SORT.\n",
    "# If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = MERGE_AND_SORT_DATAFRAMES (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merged_suffixes = MERGED_SUFFIXES, sort_merged_df = SORT_MERGED_DF, column_to_sort = COLUMN_TO_SORT, ascending_sorting = ASCENDING_SORTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a column with isolated informations from the timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"DATE\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "#Keep inside quotes.\n",
    "\n",
    "EXTRACTED_INFO = \"year\"\n",
    "#information to extract from the timestamp. The allowed values are:\n",
    "#Alternatively: 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'\n",
    "\n",
    "NEW_COLUMN_NAME = None\n",
    "# Name of the new created column. If no value is provided, it will be equals to \n",
    "# extracted_info. Alternatively: keep it as None, or input a name (string) for the new\n",
    "# column, inside quotes (e.g. NEW_COLUMN_NAME = \"extracted_information\")\n",
    "\n",
    "\n",
    "#New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = EXTRACT_TIMESTAMP_INFO (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, extracted_info = EXTRACTED_INFO, new_column_name = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating differences between timestamps (timedeltas)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN1 = \"DATE\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "#Keep inside quotes.\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN2 = \"TIMESTAMP2\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the right, that will be substracted from the timestamp on the left.\n",
    "#Keep inside quotes.\n",
    "\n",
    "TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = None\n",
    "#Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = CALCULATE_TIMEDELTA (df = DATASET, timestamp_tag_column1 = TIMESTAMP_TAG_COLUMN1, timestamp_tag_column2 = TIMESTAMP_TAG_COLUMN2, timedelta_column_name  = TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating differences between successive timestamps (delays)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: return average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"DATE\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column.\n",
    "#Keep inside quotes.\n",
    "\n",
    "NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. NEW_TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = None\n",
    "#Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "RETURN_AVG_DELAY = True\n",
    "# RETURN_AVG_DELAY = True will print and return the value of the average delay.\n",
    "# RETURN_AVG_DELAY = False will omit this information\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality.\n",
    "# Average delay float value istored into variable avg_delay. \n",
    "# Simply modify this object on the left of equality.\n",
    "new_df, avg_delay = CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: do not return average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"DATE\"\n",
    "#Alternatively: string (inside quotes) containing the name (header) of the timestamp column.\n",
    "#Keep inside quotes.\n",
    "\n",
    "NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = None\n",
    "#Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "RETURN_AVG_DELAY = False\n",
    "# RETURN_AVG_DELAY = True will print and return the value of the average delay.\n",
    "# RETURN_AVG_DELAY = False will omit this information\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adding or subtracting a timedelta from a timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"DATE\"\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "\n",
    "TIMEDELTA = 2\n",
    "# Numeric value of the timedelta.\n",
    "# WARNING: simply input a numeric value, not a string with unit. e.g. timedelta = 2.4\n",
    "# If you want to subtract a timedelta, input a negative value. e.g. timedelta = - 2.4\n",
    "# Alternatively, input any desired real number.\n",
    "\n",
    "NEW_TIMESTAMP_COL = None\n",
    "# Name of the new column containing the obtained timestamp.  If no value is provided, the \n",
    "# default name [timestamp_tag_column]+[timedelta] will be given.\n",
    "# Alternatively, input a string value inside quotes with the name of this new column.\n",
    "# e.g. NEW_TIMESTAMP_COL = \"new_timestamp\"\n",
    "\n",
    "TIMEDELTA_UNIT = None\n",
    "# Unit of the timedelta interval. If no value is provided, the unit will be considered 'ns' \n",
    "# (default). \n",
    "# Possible values are: TIMEDELTA_UNIT = None, 'day', 'hour', 'minute', 'second', or 'ns'.\n",
    "# Keep the unit inside quotes. \n",
    "\n",
    "#New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = ADD_TIMEDELTA (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, timedelta = TIMEDELTA, new_timestamp_col  = NEW_TIMESTAMP_COL, timedelta_unit = TIMEDELTA_UNIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Slicing the dataframe (selecting a specific subset of rows)**\n",
    "- This function maintains all columns from the original dataframe, returning a dataframe that is a subset of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "FROM_ROW = 'first_only'\n",
    "TO_ROW = 'only'\n",
    "# FROM_ROW and to_row: integer or strings:\n",
    "# FROM_ROW may be any integer from 0 to the last row of the dataset\n",
    "# and the following strings: 'first' and 'first_only'\n",
    "# TO_ROW may be any integer from 0 to the last row of the dataset\n",
    "# and the following strings: 'last', 'last_only', and 'only'\n",
    "    \n",
    "# the combination FROM_ROW = 'first', TO_ROW = 'last' will\n",
    "# return the original dataframe itself.\n",
    "# The same is valid for the combination FROM_ROW = 'first_only', \n",
    "# TO_ROW = 'last_only'; or of combinations between FROM_ROW = 0\n",
    "# (index of the first row) with 'last' or the index\n",
    "# of the last row; or combinations between 'first' and the index\n",
    "# of the last row.\n",
    "    \n",
    "# These possibilities are the first checked by the code. If none\n",
    "# of these special cases are present, then:\n",
    "    \n",
    "# FROM_ROW = 'first_only' selects a dataframe containing only the\n",
    "# first row, independently of the parameter passed as TO_ROW;\n",
    "\n",
    "# TO_ROW = 'last_only' selects a dataframe containing only the\n",
    "# last row, independently of the parameter passed as FROM_ROW;\n",
    "    \n",
    "# if TO_ROW = 'only', the sliced dataframe will be formed by only the\n",
    "# row passed as FROM_ROW (an integer representing the row index is\n",
    "# passed) - explained in the following lines\n",
    "    \n",
    "# These three special cases are dominant over the following ones\n",
    "# (they are checked firstly, and force the modifying of slicing limits):\n",
    "# Other special cases:\n",
    "    \n",
    "# FROM_ROW = 'first' starts slicing on the first row (index 0) -\n",
    "# the 1st row from the dataframe will be the 1st row of the sliced\n",
    "# dataframe too.\n",
    "    \n",
    "# TO_ROW = 'last' finishes slicing in the last row - the last row\n",
    "# from the dataframe will be the last row of the sliced dataframe.\n",
    "    \n",
    "# If i and j are integer numbers, they represent the indices of rows:\n",
    "# FROM_ROW = i starts the sliced dataframe from the row of index i\n",
    "# of the original dataframe.\n",
    "# e.g. FROM_ROW = 8 starts the slicing from row with index 8. Since\n",
    "# slicing starts from 0, this is the 9th row of the original dataframe.\n",
    "# TO_ROW = j finishes the sliced dataframe on the row of index j of\n",
    "# the original dataframe. Attention: this row with index j is included,\n",
    "# and will be the last_row of the sliced dataframe.\n",
    "# e.g. if TO_ROW = 21, the last row of the sliced dataframe will be the\n",
    "# row with index 21 of the original dataframe. Since slicing starts\n",
    "# from 0, this is the 22nd row of the original dataframe.\n",
    "    \n",
    "# In summary, if FROM_ROW = 8, TO_ROW = 21, the sliced dataframe\n",
    "# will be formed from the row of index 8 to the row of index 21 of\n",
    "# the original dataframe, including both the row of index 8 and the row\n",
    "# index 21. \n",
    "# FROM_ROW is effectively the first row of the new dataframe;\n",
    "# and TO_ROW is effectively the last row of the new dataframe.\n",
    "# Notice that the use of TO_ROW < FROM_ROW will raise an error.\n",
    "\n",
    "RESTART_INDEX_OF_THE_SLICED_DATAFRAME = False\n",
    "# RESTART_INDEX_OF_THE_SLICED_DATAFRAME = False to keep the \n",
    "# same row index of the original dataframe; or \n",
    "# RESTART_INDEX_OF_THE_SLICED_DATAFRAME = True to reset indices \n",
    "# (start a new index, from 0 for the first row of the \n",
    "# returned dataframe).\n",
    "    \n",
    "# New dataframe saved as sliced_df. Simply modify this object on the left of equality:\n",
    "sliced_df = SLICE_DATAFRAME (df = DATASET, from_row = FROM_ROW, to_row = TO_ROW, restart_index_of_the_sliced_dataframe = RESTART_INDEX_OF_THE_SLICED_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting the dataframe as CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all file extensions should be .csv for this function\n",
    "\n",
    "DATAFRAME_TO_BE_EXPORTED = dataset\n",
    "#Alternatively: object containing the dataset to be exported.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITH_CSV_EXTENSION = \"dataset.csv\"\n",
    "# NEW_FILE_NAME_WITH_CSV_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_CSV_EXTENSION = \"file.csv\"\n",
    "\n",
    "EXPORT_TO_S3_BUCKET = False\n",
    "# export_to_s3_bucket = False. Alternatively, set as True to export the file to an\n",
    "# AWS S3 Bucket.\n",
    "    \n",
    "## The following parameters have effect only when export_to_s3_bucket == True:\n",
    "\n",
    "S3_BUCKET_NAME = None   \n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION = None\n",
    "# The name desired for the object stored in S3 (string, in quotes). \n",
    "# Keep it None to set it equals to NEW_FILE_NAME_WITH_CSV_EXTENSION. \n",
    "# Alternatively, set it as a string analogous to NEW_FILE_NAME_WITH_CSV_EXTENSION.\n",
    "# e.g. DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION = \"S3_file.csv\"\n",
    "\n",
    "export_dataframe(dataframe_to_be_exported = DATAFRAME_TO_BE_EXPORTED, new_file_name_with_csv_extension = NEW_FILE_NAME_WITH_CSV_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH, export_to_s3_bucket = EXPORT_TO_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, desired_s3_file_name_with_csv_extension = DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "# SOURCE = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "\n",
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to AWS S3 or to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "OBJECT_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# OBJECT_TO_DOWNLOAD_FROM_COLAB = None. This option has effect only when\n",
    "# SOURCE == 'google'. In this case, this parameter is obbligatory. \n",
    "# Declare as OBJECT_TO_DOWNLOAD_FROM_COLAB the object that you want to download.\n",
    "# Since it is an object and not a string, it should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, OBJECT_TO_DOWNLOAD_FROM_COLAB = dict.\n",
    "# To download a dataframe named df, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = df.\n",
    "# To export a model named keras_model, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = keras_model\n",
    "    \n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "\n",
    "S3_BUCKET_NAME = None\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "LOCAL_PATH_OF_STORAGE = '/'\n",
    "# LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "# to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "# path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "# If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "# will be imported to the root path. Alternatively, input the path as a string (in quotes). \n",
    "# Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "# LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "# Notice that only the directories should be declared: do not include the file name and\n",
    "# its extension.\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = None\n",
    "# FILE_NAME_WITH_EXTENSION: string, in quotes, containing the file name which will be\n",
    "# downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "## This parameter is obbligatory when SOURCE == 'aws'\n",
    "# Examples:\n",
    "# FILE_NAME_WITH_EXTENSION = 'Screen_Shot.png'; FILE_NAME_WITH_EXTENSION = 'dataset.csv',\n",
    "# FILE_NAME_WITH_EXTENSION = \"dictionary.pkl\", FILE_NAME_WITH_EXTENSION = \"model.h5\",\n",
    "# FILE_NAME_WITH_EXTENSION = 'doc.pdf', FILE_NAME_WITH_EXTENSION = 'model.dill'\n",
    "\n",
    "download_or_upload_file (source = SOURCE, action = ACTION, object_to_download_from_colab = OBJECT_TO_DOWNLOAD_FROM_COLAB, s3_bucket_name = S3_BUCKET_NAME, local_path_of_storage = LOCAL_PATH_OF_STORAGE, file_name_with_extension = FILE_NAME_WITH_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Grouping by Date in Pandas - Background and Documentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we have timestamps with the datetime objects stored in column 'Date' of the dataframe df.\n",
    "\n",
    "## In the examples below, we aggregate the dataframes by date (year, month, day, min) in terms of the mean values over the set time interval.\n",
    "- The time interval is the aggregation bin.\n",
    "- To aggregate in terms of sum, simply substitute .mean() by .sum().\n",
    "- The same is applied to the other possible aggregate functions: median, var, std, min, max, etc.\n",
    "- **There are many use cases where we want the total sum over a given period of time. In those cases, we apply the .sum() aggregate** function of Pandas, instead of the .mean() used in the next examples.\n",
    "\n",
    "### WARNING: Before grouping, make sure that the 'Date' column stores a pandas Timestamp object, with resolution of at least seconds. For that, use:\n",
    "`timestamp_object = pd.Timestamp(datetime_object, unit = 's')`\n",
    "- For a resolution in other scale, simply modify this parameter. For instance, unit = 'ns' for nanoseconds.\n",
    "- Check the pandas.Timestamp class documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html\n",
    "\n",
    "## Calling Grouper class\n",
    "- Firstly, convert all datetime objects into pandas.Timestamps.\n",
    "- To group by dates, we must call the Grouper class:\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```\n",
    "pandas.Grouper(key=None, level=None, freq=None, axis=0, sort=False)\n",
    "```\n",
    "- Notice that setting sort = True will sort the grouped values. We do not need to specify axis = 0, since it is the default.\n",
    "\n",
    "## Group by Year\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='1Y')).mean()\n",
    "```\n",
    "\n",
    "In this case, we grouped by intervals of 1 year. We could group by different values of years, though. For instance:\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='2Y')).mean()\n",
    "```\n",
    "Groups by intervals of 2 years.\n",
    "\n",
    "## Group by Month\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='1M')).mean()\n",
    "```\n",
    "- Again, we could modify the number of months. For instance, the aggregation by trimesters is done as:\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='3M')).mean()\n",
    "```\n",
    "\n",
    "## Group by Week\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='1W')).mean()\n",
    "```\n",
    "- As usual, simply modify the number before 'W' to change the number of weeks in the grouping.\n",
    "- The substitution of '1W' by '2W' results in the aggregation every 2 weeks.\n",
    "\n",
    "## Group by Day\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='1D')).mean()\n",
    "```\n",
    "\n",
    "- If you want to group by a different number of days, simply modify the number before 'D'.\n",
    "- The group by every two days, so, is performed as `df.groupby(pd.Grouper(key='Date', freq='2D')).mean()`; whereas `df.groupby(pd.Grouper(key='Date', freq='5D')).mean()` groups by every five days.\n",
    "\n",
    "## Group by Hour\n",
    "\n",
    "```\n",
    "grouper = df.groupby([pd.Grouper(freq='1H'), 'Location'])\n",
    "```\n",
    "\n",
    "## Group by Minute\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='1min')).mean()\n",
    "```\n",
    "- To group by every 15 mins: `df.groupby(pd.Grouper(key='Date', freq='15min')).mean()`\n",
    "- To group by every 2 mins: `df.groupby(pd.Grouper(key='Date', freq='2min')).mean()`\n",
    "\n",
    "## Group by Second\n",
    "\n",
    "The next example upsample the time series into 30 second bins.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.asfreq.html\n",
    "\n",
    "```\n",
    "df.asfreq(freq='30S')\n",
    "```\n",
    "\n",
    "### Adjusting the time bins based on a fixed timestamp:\n",
    "- Suppose a grouping by every 17 mins.\n",
    "- You can specify an origin or specify an offset (equivalent):\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='17min', origin='2000-01-01')).mean()\n",
    "```\n",
    "\n",
    "If the resolution of the timestamps is in days, the grouping will consider the first instant as 00:00:00. So, the following lines are completely equivalent: in the second one, we simply specified the offset in hours and minutes to not start the grouping by 00:00:00 of a given day (we specifically set the first day to start from '23h30min' after 00:00:00:\n",
    "\n",
    "```\n",
    "df.groupby(pd.Grouper(key='Date', freq='17min', origin='2000-10-01 23:30:00')).mean()\n",
    "df.groupby(pd.Grouper(key='Date', freq='17min', offset='23h30min')).mean()\n",
    "```\n",
    "The same output can be obtained by defining a string or timestamp and passing it as argument:\n",
    "\n",
    "```\n",
    "start = '2000-10-01 23:30:00'\n",
    "df.groupby(pd.Grouper(key='Date', freq='17min', origin= start)).mean()\n",
    "```\n",
    "\n",
    "Now, suppose the timestamps contain the hour information (e.g.: 01:10:20). Now, the **'offset' parameter will represent a moment for starting after the first timestamp.**\n",
    "- That is because our timestamp is not necessarily 00:00:00, as before. \n",
    "- When the hours are not declare, Python gives the time 00:00:00 to each timestamp.\n",
    "- So, if we have `offset='2min'` the first timestamp of the grouping bins will be 2 min after the first timestamp of the dataframe df.\n",
    "- Therefore, the `offset = 'XXhYYmin'` indicates to the `Grouper` class that the first bin should start with an offset of XX h and YY min in relation to the first timestamp, i.e., XX h and YY min after the first timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Merging (joining) the data by a timestamp with Pandas - Background and Documentation**\n",
    "- We could use the .merge method, but this will not return an ordered dataframe.\n",
    "- Let's use the .merge_ordered instead.\n",
    "- If the data is not synchronous, we can perform the fuzzy merging using the .merge_asof method.\n",
    "\n",
    "## Methods comparison\n",
    "_From Datacamp course: Joining Data with pandas, chapter 4 - Merging Ordered and Time-Series Data_\n",
    "\n",
    "### .merge() method:\n",
    "- Column(s) to join on: on , left_on , and right_on\n",
    "- Type of join: how (left, right, inner, outer) {{@}}\n",
    "    - Default: 'inner'.\n",
    "- Overlapping column names: suffixes\n",
    "- Calling the method: df1.merge(df2)\n",
    "\n",
    "### .merge_ordered() method:\n",
    "- Column(s) to join on: on , left_on , and right_on\n",
    "- Type of join: how (left, right, inner, outer)\n",
    "    - Default: 'outer'.\n",
    "- Overlapping column names: suffixes\n",
    "- Calling the method: pd.merge_ordered(df1, df2)\n",
    "\n",
    "Examples:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "pd.merge_ordered(appl, mcd, on='date', suffixes=('_aapl','_mcd'))\n",
    "```\n",
    "#### Forward fill: fills missing with previous value\n",
    "\n",
    "```\n",
    "pd.merge_ordered(appl, mcd, on='date', suffixes=('_aapl','_mcd'), fill_method='ffill')\n",
    "```\n",
    "- When to use merge_ordered()?\n",
    "    - Ordered data / time series.\n",
    "    - Filling in missing values.\n",
    "\n",
    "### .merge_asof() method:\n",
    "- Similar to a merge_ordered() left join.\n",
    "    - Similar features as merge_ordered().\n",
    "- Match on the nearest key column and not exact matches.\n",
    "    - Merged \"on\" columns must be sorted.\n",
    "\n",
    "```\n",
    "pd.merge_asof(visa, ibm, on='date_time', suffixes=('_visa','_ibm'))\n",
    "```\n",
    "#### merge_asof() example with direction\n",
    "```\n",
    "pd.merge_asof(visa, ibm, on=['date_time'], suffixes=('_visa','_ibm'), direction='forward')\n",
    "```\n",
    "\n",
    "direction: ‘backward’ (default), ‘forward’, or ‘nearest’.\n",
    "-'nearest' allows both directions.\n",
    "- merge_asof does not allow filling. Check: \n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_asof.html\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_ordered.html#pandas.merge_ordered\n",
    "\n",
    "\n",
    "- When to use merge_asof()\n",
    "    - Data sampled from a process.\n",
    "    - Developing a training set (no data leakage).\n",
    "    - .merge_asof uses fuzzy matching, so the HOW parameter is not applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
