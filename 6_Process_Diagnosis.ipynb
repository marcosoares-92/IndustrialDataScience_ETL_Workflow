{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "59d25268-788f-4687-b87d-43ccfa129f08"
   },
   "source": [
    "# **Process Diagnosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2d0f0b78-49c5-4a6e-83c9-987941d850a3"
   },
   "source": [
    "## _ETL Workflow Notebook 6_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "99a9d9ec-4717-4303-8356-604db6c8a4b8"
   },
   "source": [
    "## Content:\n",
    "1. Calculating general statistics from a given column; \n",
    "2. Getting data quantiles from a given column; \n",
    "3. Getting a particular P-percent quantile limit; \n",
    "4. Applying a list of row filters to a dataframe; \n",
    "5. Selecting subsets from a dataframe (using row filters) and labelling these subsets; \n",
    "6. Performing Analysis of Variance (ANOVA) and obtaining boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To install a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow\n",
    "# to update a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow --upgrade\n",
    "# to update pip, unmark and run:\n",
    "# ! pip install pip --upgrade\n",
    "# to show if a library is installed and visualize its information, unmark and run\n",
    "# (e.g. tensorflow):\n",
    "# ! pip show tensorflow\n",
    "# To run a Python file (e.g idsw_etl.py) saved in the notebook's workspace directory,\n",
    "# unmark and run:\n",
    "# import idsw_etl\n",
    "# or:\n",
    "# import idsw_etl as etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "478ab50d-150a-4635-9cc2-b20e489ae05f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "97f3cef5-7061-4148-9f9c-e8b25ec58381"
   },
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b2651705-5adc-4812-9917-c718958c7d5d"
   },
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "048c260e-fec3-4978-ab79-86b2624f2abb"
   },
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "efe09659-8b77-46ac-aacc-1684b89e447c"
   },
   "source": [
    "# **Function for obtaining Statistical Process Control (SPC) charts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "azdata_cell_guid": "5e27f181-e411-4a71-bb03-91d8981e8980"
   },
   "outputs": [],
   "source": [
    "class spc_chart_assistant:\n",
    "            \n",
    "    # Initialize instance attributes.\n",
    "    # define the Class constructor, i.e., how are its objects:\n",
    "    def __init__(self, assistant_startup = True, keep_assistant_on = True):\n",
    "                \n",
    "        import os\n",
    "        \n",
    "        # If the user passes the argument, use them. Otherwise, use the standard values.\n",
    "        # Set the class objects' attributes.\n",
    "        # Suppose the object is named assistant. We can access the attribute as:\n",
    "        # assistant.assistant_startup, for instance.\n",
    "        # So, we can save the variables as objects' attributes.\n",
    "        self.assistant_startup = assistant_startup\n",
    "        self.keep_assistant_on = keep_assistant_on\n",
    "        # Base Github directory containing the assistant images to be downloaded:\n",
    "        self.base_git_dir = \"https://github.com/marcosoares-92/img_examples_guides/raw/main\"\n",
    "        # Create a new folder to store the images in local environment, \n",
    "        # if the folder do not exists:\n",
    "        self.new_dir = \"tmp\"\n",
    "        \n",
    "        os.makedirs(self.new_dir, exist_ok = True)\n",
    "        # exist_ok = True creates the directory only if it does not exist.\n",
    "        \n",
    "        self.last_img_number = 18 # number of the last image on the assistant\n",
    "        self.numbers_to_end_assistant = (3, 4, 7, 9, 10, 13, 15, 16, 19, 20, 21, 22)\n",
    "        # tuple: cannot be modified\n",
    "        # 3: 'g', 4: 't', 7: 'i_mr', 9: 'std_error', 10: '3s', 13: 'x_bar_s'\n",
    "        # 15: 'std_error' (grouped), 16: '3s' (grouped), 19: 'p', 20: 'np',\n",
    "        # 21: 'c', 22: 'u'\n",
    "        self.screen_number = 0 # start as zero\n",
    "        self.file_to_fetch = ''\n",
    "        self.img_url = ''\n",
    "        self.img_local_path = ''\n",
    "        # to check the class attributes, use the __dict__ method. Examples:\n",
    "        ## object.__dict__ will show all attributes from object\n",
    "                \n",
    "    # Define the class methods.\n",
    "    # All methods must take an object from the class (self) as one of the parameters\n",
    "    \n",
    "    def download_assistant_imgs (self):\n",
    "                \n",
    "        import os\n",
    "        import shutil # component of the standard library to move or copy files.\n",
    "        from html2image import Html2Image\n",
    "                \n",
    "        # Start the html object\n",
    "        html_img = Html2Image()\n",
    "                \n",
    "        for screen_number in range(0, (self.last_img_number + 1)):\n",
    "                \n",
    "            # ranges from 0 to (last_img_number + 1) - 1 = last_img_number\n",
    "            # convert the screen number to string to create the file name:\n",
    "            \n",
    "            # Update the attributes:\n",
    "            self.file_to_fetch = \"cc_s\" + str(screen_number) + \".png\"\n",
    "            self.img_url = os.path.join(self.base_git_dir, self.file_to_fetch)\n",
    "            \n",
    "            # Download the image:\n",
    "            # pypi.org/project/html2image/\n",
    "            img = html_img.screenshot(url = self.img_url, save_as = self.file_to_fetch, size = (500, 500))\n",
    "            # If size is omitted, the image is downloaded in the low-resolution default.\n",
    "            # save_as must be a file name, a path is not accepted.\n",
    "            # Make the output from the method equals to a variable eliminates its verbosity\n",
    "                    \n",
    "            # Create the new path for the image (local environment):\n",
    "            self.img_local_path = os.path.join(self.new_dir, self.file_to_fetch)\n",
    "            # Move the image files to the new paths:\n",
    "            # use shutil.move(source, destination) method to move the files:\n",
    "            # pynative.com/python-move-files\n",
    "            # docs.python.org/3/library/shutil.html\n",
    "            shutil.move(self.file_to_fetch, self.img_local_path)\n",
    "            # Notice that file_to_fetch attribute still stores a file name like 'cc_s0.png'\n",
    "        \n",
    "        # Now, all images for the assistant were downloaded and stored in the temporary\n",
    "        # folder. So, let's start the two boolean variables to initiate it and run it:\n",
    "        self.assistant_startup = True \n",
    "        # attribute to start the assistant in the first screen\n",
    "        self.keep_assistant_on = True\n",
    "        # attribute to maintain the assistant working\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def delete_assistant_imgs (self):\n",
    "                \n",
    "        import os\n",
    "        # Now, that the user closed the assistant, we can remove the downloaded files \n",
    "        # (delete them) from the notebook's workspace.\n",
    "                \n",
    "        # The os.remove function deletes a file or directory specified.\n",
    "        for screen_number in range(0, (self.last_img_number + 1)):\n",
    "                    \n",
    "            self.file_to_fetch = \"cc_s\" + str(screen_number) + \".png\"\n",
    "            self.img_local_path = os.path.join(self.new_dir, self.file_to_fetch)\n",
    "            os.remove(self.img_local_path)\n",
    "                \n",
    "        # Now that the files were removed, check if the tmp folder is empty:\n",
    "        size = os.path.getsize(self.new_dir)\n",
    "        # os.path.getsize returns the total size in Bytes from a folder or a file.\n",
    "                \n",
    "        # Get the list of sub-folders, files or subdirectories (the content) from the folder:\n",
    "        list_of_contents = os.listdir(self.new_dir)\n",
    "        # doc.python.org/3/library/os.html\n",
    "        # It returns a list of strings representing the paths of each file or directory \n",
    "        # in the analyzed folder.\n",
    "                \n",
    "        # If the size is 0 and the length of the list_of_contents is also zero (i.e., there is no\n",
    "        # previous sub-directory created), then remove the directory:\n",
    "        if ((size == 0) & (len(list_of_contents) == 0)):\n",
    "            \n",
    "            os.rmdir(self.new_dir)\n",
    "\n",
    "    def print_screen_legend (self):\n",
    "        \n",
    "        if (self.screen_number == 0):\n",
    "            \n",
    "            print(\"The control chart is a line graph showing a measure (y-axis) over time (x-axis).\")\n",
    "            \n",
    "            print(\"In contrast to the run chart, the central line of the control chart represents the (weighted) mean, rather than the median.\")\n",
    "            print(\"Additionally, two lines representing the upper and lower control limits are shown.\\n\")\n",
    "            print(\"The control limits represent the boundaries of the so-called common cause variation, which is inherent to the process.\")\n",
    "            print(\"Walther A. Shewhart, who invented the control chart, described two types of variation: chance-cause variation and assignable-cause variation.\")\n",
    "            print(\"These were later renamed to common-cause and special-cause variation.\\n\")\n",
    "            \n",
    "            print(\"Common-cause variation:\")\n",
    "            print(\"Is present in any process.\")\n",
    "            print(\"It is caused by phenomena that are always present within the system.\")\n",
    "            print(\"It makes the process predictable (within limits).\")\n",
    "            print(\"Common-cause variation is also called random variation or noise.\\n\")\n",
    "                    \n",
    "            print(\"Special-cause variation:\")\n",
    "            print(\"Is present in some processes.\")\n",
    "            print(\"It is caused by phenomena that are not normally present in the system.\")\n",
    "            print(\"It makes the process unpredictable.\")\n",
    "            print(\"Special-cause variation is also called non-random variation or signal.\\n\")\n",
    "                    \n",
    "            print(\"It is important to notice that neither common, nor special-cause variation is in itself 'good' or 'bad'.\")\n",
    "            print(\"A stable process may function at an unsatisfactory level; and an unstable process may be moving in the right direction.\")\n",
    "            print(\"On the other hand, the end goal of improvement is always to achieve a stable process functioning at a satisfactory level.\\n\")\n",
    "                    \n",
    "            print(\"Control chart limits:\")\n",
    "            print(\"The control limits, also called sigma limits, are usually placed at ±3 standard deviations from the central line.\")\n",
    "            print(\"So, the standard deviation is estimated as the common variation of the process of interest.\")\n",
    "            print(\"This variation depends on the theoretical distribution of data.\")\n",
    "            print(\"It is a beginner's mistake to simply calculate the standard deviation of all the data points.\")\n",
    "            print(\"This procedure would include both the common and special-cause variation in the calculus.\")\n",
    "            print(\"Since the calculations of control limits depend on the type of data (distribution), many types of control charts have been developed for specific purposes.\")\n",
    "        \n",
    "        elif (self.screen_number == 1):\n",
    "                    \n",
    "            print(\"CHARTS FOR RARE EVENTS\\n\")\n",
    "            print(\"ATTENTION: Due not previously group data in this case. Since events are rare, they are likely to be eliminated during aggregation.\\n\")\n",
    "                    \n",
    "            print(\"G-chart for units produced between (rare) defectives or defects;\")\n",
    "            print(\"or total events between successive rare occurrences:\\n\")\n",
    "            print(\"When defects or defectives are rare and the subgroups are small, C, U, and P-charts become useless.\")\n",
    "            print(\"That is because most subgroups will have no defects.\")\n",
    "                    \n",
    "            print(\"Example: if 8% of discharged patients have a hospitals-acquired pressure ulcer, and the average weekly number of discharges in a small department is 10, we would, on average, expect to have less than one pressure ulcer per week.\")\n",
    "            print(\"Instead, we could plot the number of discharges between each discharge of a patient with one or more pressure ulcers.\\n\")\n",
    "            print(\"The number of units between defectives is modelled by the geometric distribution.\")\n",
    "            print(\"So, the G-control chart plots counting of occurrence by number; time unit; or timestamp.\\n\")\n",
    "                    \n",
    "            print(\"In the example of discharged patients: the indicator is the number of discharges between each of these rare cases.\")\n",
    "            print(\"Note that the first patient with pressure ulcer is missing from the chart.\")\n",
    "            print(\"It is due to the fact that we do not know how many discharges there had been before the first patient with detected pressure ulcer.\\n\")\n",
    "            print(\"The central line of the G-chart is the theoretical median of the distribution\")\n",
    "            print(\"median = mean × 0.693\")\n",
    "                    \n",
    "            print(\"Since the geometric distribution is highly skewed, the median is a better representation of the process center.\")\n",
    "            print(\"Also, notice that the G-chart rarely has a lower control limit.\\n\")\n",
    "                    \n",
    "            print(\"T-chart for time between successive rare events:\\n\")\n",
    "            print(\"Like the G-chart, the T-chart is a rare event chart.\")\n",
    "            print(\"Instead of displaying the number of cases between events (defectives), this chart represents the time between successive rare events.\\n\")\n",
    "            print(\"Since time is a continuous variable, the T-chart belongs with the other charts for measure numeric data.\")\n",
    "            print(\"Then, T-chart plots the timedelta (e.g. number of days between occurrences) by the measurement, time unit, or timestamp.\")\n",
    "                \n",
    "        elif (self.screen_number == 2):\n",
    "            \n",
    "            print(\"A quality characteristic that is measured on a numerical scale is called a variable.\")\n",
    "            print(\"Examples: length or width, temperature, and volume.\\n\")\n",
    "            \n",
    "            print(\"The Shewhart control charts are widely used to monitor the mean and variability of variables.\")\n",
    "            print(\"On the other hand, many quality characteristics can be expressed in terms of a numerical measurement.\")\n",
    "                    \n",
    "            print(\"For example: the diameter of a bearing could be measured with a micrometer and expressed in millimeters.\\n\")\n",
    "            print(\"A single measurable quality characteristic, such as a dimension, weight, or volume, is a variable.\")\n",
    "            print(\"Control charts for variables are used extensively, and are one of the primary tools used in the analize and control steps of DMAIC.\")\n",
    "            \n",
    "            print(\"Many quality characteristics cannot be conveniently represented numerically, though.\")\n",
    "            print(\"In such cases, we usually classify each item inspected as either conforming or nonconforming to the specifications on that quality characteristic.\")\n",
    "            print(\"The terminology defective or nondefective is often used to identify these two classifications of product.\")\n",
    "            print(\"More recently, this terminology conforming and nonconforming has become popular.\")        \n",
    "            print(\"Quality characteristics of this type are called attributes.\\n\")\n",
    "            \n",
    "            print(\"Control Charts for Nonconformities (defects):\")\n",
    "            print(\"A nonconforming item is a unit of product that does not satisfy one or more of the specifications of that product.\")\n",
    "            print(\"Each specific point at which a specification is not satisfied results in a defect or nonconformity.\")\n",
    "            print(\"Consequently, a nonconforming item will contain at least one nonconformity.\")\n",
    "            print(\"However, depending on their nature and severity, it is quite possible for a unit to contain several nonconformities and not be classified as nonconforming.\")\n",
    "                    \n",
    "            print(\"Example: suppose we are manufacturing personal computers. Each unit could have one or more very minor flaws in the cabinet finish,\")\n",
    "            print(\"but since these flaws do not seriously affect the unit's functional operation, it could be classified as conforming.\")\n",
    "            print(\"However, if there are too many of these flaws, the personal computer should be classified as nonconforming,\")\n",
    "            print(\"because the flaws would be very noticeable to the customer and might affect the sale of the unit.\\n\")\n",
    "                    \n",
    "            print(\"There are many practical situations in which we prefer to work directly with the number of defects or nonconformities,\")\n",
    "            print(\"rather than the fraction nonconforming.\")\n",
    "            print(\"These include:\")\n",
    "            print(\"1. Number of defective welds in 100 m of oil pipeline.\")\n",
    "            print(\"2. Number of broken rivets in an aircraft wing.\")\n",
    "            print(\"3. Number of functional defects in an electronic logic device.\")\n",
    "            print(\"4. Number of errors on a document, etc.\\n\")\n",
    "                    \n",
    "            print(\"It is possible to develop control charts for either the total number of nonconformities in a unit,\")\n",
    "            print(\"or for the average number of nonconformities per unit.\\n\")\n",
    "            print(\"These control charts usually assume that the occurrence of nonconformities in samples of constant size is well modeled by the Poisson distribution.\\n\")\n",
    "                    \n",
    "            print(\"Essentially, this requires that the number of opportunities or potential locations for nonconformities be infinitely large;\")\n",
    "            print(\"and that the probability of occurrence of a nonconformity at any location be small and constant.\")\n",
    "            print(\"Furthermore, the inspection unit must be the same for each sample.\")\n",
    "            print(\"That is, each inspection unit must always represent an identical area of opportunity for the occurrence of nonconformities.\")\n",
    "                    \n",
    "            print(\"In addition, we can count nonconformities of several different types on one unit, as long as the above conditions are satisfied for each class of nonconformity.\\n\")\n",
    "            print(\"In most practical situations, these conditions will not be perfectly satisfied.\")\n",
    "            print(\"The number of opportunities for the occurrence of nonconformities may be finite,\")\n",
    "            print(\"or the probability of occurrence of nonconformities may not be constant.\\n\")\n",
    "                    \n",
    "            print(\"As long as these departures from the assumptions are not severe,\")\n",
    "            print(\"the Poisson model will usually work reasonably well.\")\n",
    "            print(\"There are cases, however, in which the Poisson model is completely inappropriate.\")\n",
    "            print(\"So, always check carefully the distributions.\")\n",
    "                    \n",
    "            print(\"If you are not sure, use the estimatives based on more general assumptions, i.e.,\")\n",
    "            print(\"The estimative of the natural variation as 3 times the standard deviation;\")\n",
    "            print(\"or as 3 times the standard error.\\n\")\n",
    "                    \n",
    "            print(\"Individual samples x Grouped data\")\n",
    "            print(\"Often, we collect a batch of samples corresponding to the same conditions, and use aggregation measurements such as mean, sum, or standard deviation to represent them.\")\n",
    "            print(\"In this case, we are grouping our data, and not working with individual measurements.\")\n",
    "            print(\"In turns, we can collect individual samples: there are no repetitions, only individual measurements corresponding to different conditions.\\n\")\n",
    "            print(\"Usually, time series data is collected individually: each measurement corresponds to an instant, so it is not possible to collect multiple samples corresponding to the same conditions for further grouping.\")\n",
    "            print(\"Example: instant assessment of pH, temperature, pressure, etc.\\n\")\n",
    "            print(\"Naturally, we can define a time window like a day, and group values on that window.\")\n",
    "            print(\"The dynamic of the phenomena should not create significant differences between samples collected for a same window, though.\")\n",
    "        \n",
    "        elif (self.screen_number == 5):\n",
    "            \n",
    "            print(\"CHARTS FOR NUMERICAL VARIABLES\\n\")\n",
    "            print(\"When dealing with a quality characteristic that is a variable, it is usually necessary to monitor both the mean value of the quality characteristic and its variability.\")\n",
    "            print(\"The control of the process average or mean quality level is usually done with the control chart for means, or the X-bar control chart.\")\n",
    "            print(\"The process variability can be monitored with either a control chart for the standard deviation, called the s control chart, or with a control chart for the range, called an R control chart.\\n\")\n",
    "            \n",
    "            print(\"I and MR charts for individual measurements:\")\n",
    "            print(\"ATTENTION: The I-MR chart can only be used for data that follows the normal distribution.\")\n",
    "            print(\"That is because the calculus of the control limits are based on the strong hypothesis of normality.\")\n",
    "            print(\"If you have individual samples that do not follow the normal curve (like skewed data, or data with high kurtosis);\")\n",
    "            print(\"or data with an unknown distribution, select number 8 for using less restrictive hypotheses for the estimative of the natural variation.\\n\")\n",
    "                    \n",
    "            print(\"Example: in healthcare, most quality data are count data.\")\n",
    "            print(\"However, from time to time, there are measurement data present.\")\n",
    "            print(\"These data are often in the form of physiological parameters or waiting times.\")\n",
    "            print(\"e.g. a chart of birth weights from 24 babies.\")\n",
    "            print(\"If the birth weights follow the normal, you can use the individuals chart.\\n\")\n",
    "                    \n",
    "            print(\"Actually, there are many situations in which the sample size used for process monitoring is n = 1; that is, the sample consists of an individual unit.\")\n",
    "            print(\"Some other examples of these situations are as follows:\")\n",
    "            print(\"1. Automated inspection and measurement technology is used, and every unit manufactured is analyzed.\")\n",
    "            print(\"So, there is no basis for rational subgrouping.\")\n",
    "            print(\"2. Data comes available relatively slowly, and it is inconvenient to allow sample sizes of n > 1 to accumulate before analysis.\") \n",
    "            print(\"The long interval between observations will cause problems with rational subgrouping.\")\n",
    "            print(\"This occurs frequently in both manufacturing and non-manufacturing situations.\")\n",
    "            print(\"3. Repeat measurements on the process differ only because of laboratory or analysis error, as in many chemical processes.\")\n",
    "            print(\"4. Multiple measurements are taken on the same unit of product, such as measuring oxide thickness at several different locations on a wafer in semiconductor manufacturing.\")\n",
    "            print(\"5. In process plants, such as papermaking, measurements on some parameter (such as coating thickness across the roll) will differ very little and produce a standard deviation that is much too small if the objective is to control coating thickness along the roll.\")\n",
    "                    \n",
    "            print(\"In such situations, the control chart for individual units is useful.\")\n",
    "            print(\"In many applications of the individuals control chart, we use the moving range two successive observations as the basis of estimating the process variability.\\n\")\n",
    "            print(\"I-charts are often accompanied by moving range (MR) charts, which show the absolute difference between neighbouring data points.\")\n",
    "            print(\"The purpose of the MR chart is to identify sudden changes in the (estimated) within-subgroup variation.\")\n",
    "            print(\"If any data point in the MR is above the upper control limit, one should interpret the I-chart very cautiously.\\n\")\n",
    "        \n",
    "        elif(self.screen_number == 6):\n",
    "                    \n",
    "            print(\"One important difference: numeric variables are representative of continuous data, usually in the form of real numbers (float values).\")\n",
    "            print(\"It means that its possible values cannot be counted: there is an infinite number of possible real values.\")\n",
    "            \n",
    "            print(\"Categoric variables, in turn, are discrete.\")        \n",
    "            print(\"It means they can be counted, since there is a finite number of possibilities.\")\n",
    "            print(\"Such variables are usually present as strings (texts), or as ordinal (integer) numbers.\")\n",
    "                    \n",
    "            print(\"If there are only two categories, we have a binary classification.\")\n",
    "            print(\"Each category can be reduced to a binary system: or the category is present, or it is not.\")\n",
    "            print(\"This is the idea for the One-Hot Encoding.\")\n",
    "            print(\"Usually, values in a binary classification are 1 or 0, so that a probability can be easily associated through the sigmoid function.\\n\")\n",
    "                    \n",
    "            print(\"Some examples of quality characteristics that are based on the analysis of attributes:\")\n",
    "            print(\"1. Proportion of warped automobile engine connecting rods in a day's production.\")\n",
    "            print(\"2. Number of nonfunctional semiconductor chips on a wafer.\")\n",
    "            print(\"3. Number of errors or mistakes made in completing a loan application.\")\n",
    "            print(\"4. Number of medical errors made in a hospital.\\n\")\n",
    "        \n",
    "        elif((self.screen_number == 8) | (self.screen_number == 14)):\n",
    "            \n",
    "            print(\"If you have a distribution that is not normal, like distributions with high skewness or high kurtosis,\")\n",
    "            print(\"use less restrictive methodologies to estimate the natural variation.\\n\")\n",
    "                    \n",
    "            print(\"You may estimate the natural variation as 3 times the standard error; or as 3 times the standard deviation.\")\n",
    "            print(\"The interval will be symmetric around the mean value.\\n\")\n",
    "            print(\"Recommended: standard error, which normalizes by the total of values.\")\n",
    "                \n",
    "        elif(self.screen_number == 11):\n",
    "            \n",
    "            print(\"CHARTS FOR NUMERICAL VARIABLES\\n\")\n",
    "            print(\"When dealing with a quality characteristic that is a variable, it is usually necessary to monitor both the mean value of the quality characteristic and its variability.\")\n",
    "            print(\"The control of the process average or mean quality level is usually done with the control chart for means, or the X-bar control chart.\")\n",
    "            print(\"The process variability can be monitored with either a control chart for the standard deviation, called the s control chart, or with a control chart for the range, called an R control chart.\\n\")\n",
    "                    \n",
    "            print(\"X-bar and S charts for average measurements:\")\n",
    "            print(\"If there is more than one measurement of a numeric variable in each subgroup,\")\n",
    "            print(\"the Xbar and S charts will display the average and the within-subgroup standard deviation, respectively.\")\n",
    "            print(\"e.g. a chart of average birth weights per month, for babies born over last year.\")\n",
    "        \n",
    "        elif(self.screen_number == 12):\n",
    "            \n",
    "            print(\"CHARTS FOR CATEGORICAL VARIABLES\\n\")\n",
    "            print(\"There are 4 widely used attributes control charts: P, nP, U, and C.\\n\")\n",
    "                    \n",
    "            print(\"To illustrate them, consider a dataset containing the weekly number of hospital acquired pressure ulcers at a hospital\")\n",
    "            print(\"The hospital has 300 patients, with an average length of stay of four days.\") \n",
    "            print(\"Each of the dataframe's 24 rows contains information for one week on: the number of discharges,\")\n",
    "            print(\"patient days; pressure ulcers; and number of discharged patients with one or more pressure ulcers.\")\n",
    "            print(\"On average, 8% of discharged patients have 1.5 hospital acquired pressure ulcers.\\n\")\n",
    "                    \n",
    "            print(\"Some of the charts for categorical variables are based on the definition of the fraction nonconforming.\")\n",
    "            print(\"The fraction nonconforming is defined as the ratio between:\")\n",
    "            print(\"the number of nonconforming items in a population; by the total number of items in that population.\")\n",
    "            print(\"The items may have several quality characteristics that are examined simultaneously by the inspector.\")\n",
    "            print(\"If the item does not conform to the standards of one or more of these characteristics, it is classified as nonconforming.\\n\")\n",
    "                    \n",
    "            print(\"ATTENTION: Although it is customary to work with fraction nonconforming,\")\n",
    "            print(\"we could also analyze the fraction conforming just as easily, resulting in a control chart of process yield.\")\n",
    "            print(\"Many manufacturing organizations operate a yield-management system at each stage of their manufacturing process,\")\n",
    "            print(\"with the first-pass yield tracked on a control chart.\\n\")\n",
    "            \n",
    "            print(\"Traditionally, the term 'defect' has been used to name whatever it is being analyzed through counting with control charts.\\n\")\n",
    "            print(\"There is a subtle, but important, distinction between:\")\n",
    "            print(\"counting defects, e.g. number of pressure ulcers;\")\n",
    "            print(\"and counting defectives, e.g. number of patients with one or more pressure ulcers.\\n\")\n",
    "                    \n",
    "            print(\"Defects are expected to reflect the Poisson distribution,\")\n",
    "            print(\"while defectives reflect the binomial distribution.\\n\")\n",
    "        \n",
    "        elif(self.screen_number == 17):\n",
    "                    \n",
    "            print(\"P-charts for proportion of defective units:\")\n",
    "            print(\"The first of these relates to the fraction of nonconforming or defective product produced by a manufacturing process, and is called the control chart for fraction nonconforming, or P-chart.\")\n",
    "            \n",
    "            print(\"The P chart is probably the most common control chart in healthcare.\")\n",
    "            print(\"It is used to plot the proportion (or percent) of defective units.\")\n",
    "            print(\"e.g. the proportion of patients with one or more pressure ulcers.\")\n",
    "                    \n",
    "            print(\"As mentioned, defectives are modelled by the binomial distribution.\")\n",
    "            print(\"In theory, the P chart is less sensitive to special cause variation than the U chart.\")\n",
    "            print(\"That is because it discards information by dichotomising inspection units (patients) in defectives and non-defectives ignoring the fact that a unit may have more than one defect (pressure ulcers).\")\n",
    "            print(\"On the other hand, the P chart often communicates better.\")\n",
    "                    \n",
    "            print(\"For most people, not to mention the press, the percent of harmed patients is easier to grasp than the the rate of pressure ulcers expressed in counts per 1000 patient days.\\n\")\n",
    "            print(\"The sample fraction nonconforming is defined as the ratio of the number of nonconforming units in the sample D to the sample size n:\")\n",
    "            print(\"p = D/n\")\n",
    "            print(\"From the binomial distribution, the mean should be estimated as p, and the variance s² as p(1-p)/n.\")\n",
    "                    \n",
    "            print(\"nP-Charts for number nonconforming:\")\n",
    "            print(\"It is also possible to base a control chart on the number nonconforming,\")\n",
    "            print(\"rather than on the fraction nonconforming.\")\n",
    "            print(\"This is often called as number nonconforming (nP) control chart.\\n\")\n",
    "            \n",
    "        elif(self.screen_number == 18):\n",
    "            \n",
    "            print(\"C-charts for count of defects:\")\n",
    "            print(\"In some situations, it is more convenient to deal with the number of defects or nonconformities observed,\")\n",
    "            print(\"rather than the fraction nonconforming.\\n\")\n",
    "            \n",
    "            print(\"So, another type of control chart, called the control chart for nonconformities, or the C chart,\")\n",
    "            print(\"is designed to deal with this case.\\n\")\n",
    "                    \n",
    "            print(\"In the hospital example:\")\n",
    "            print(\"The correct control chart for the number of pressure ulcers is the C-chart,\")\n",
    "            print(\"which is based on the Poisson distribution.\\n\")\n",
    "                    \n",
    "            print(\"As mentioned, DEFECTIVES are modelled by the BINOMIAL distribution, whereas DEFECTS are are modelled by POISSON distribution.\\n\")\n",
    "            \n",
    "            print(\"U-charts for rate of defects:\")\n",
    "            print(\"The control chart for nonconformities per unit, or the U-chart, is useful in situations\")\n",
    "            print(\"where the average number of nonconformities per unit is a more convenient basis for process control.\\n\")\n",
    "                \n",
    "            print(\"The U-chart is different from the C-chart in that it accounts for variation in the area of opportunity.\")\n",
    "            print(\"Examples:\")\n",
    "            print(\"1. Number of patients over time.\")\n",
    "            print(\"2. Number of patients between units one wishes to compare.\")\n",
    "            print(\"3. Number of patient days over time.\")\n",
    "            print(\"4. Number of patient days between units one wishes to compare.\\n\")\n",
    "                 \n",
    "            print(\"If there are many more patients in the hospital in the winter than in the summer,\")\n",
    "            print(\"the C-chart may falsely detect special cause variation in the raw number of pressure ulcers.\\n\")\n",
    "            \n",
    "            print(\"The U-chart plots the rate of defects.\")\n",
    "            print(\"A rate differs from a proportion in that the numerator and the denominator need not be of the same kind,\")\n",
    "            print(\"and that the numerator may exceed the denominator.\\n\")\n",
    "                \n",
    "            print(\"For example: the rate of pressure ulcers may be expressed as the number of pressure ulcers per 1000 patient days.\\n\")\n",
    "            print(\"The larger the numerator, the narrower the control limits.\\n\")\n",
    "            print(\"So, the main difference between U and C-charts is that U is based on the average number of nonconformities per inspection unit.\\n\")\n",
    "            \n",
    "            print(\"If we find x total nonconformities in a sample of n inspection units,\")\n",
    "            print(\"then the average number of nonconformities per inspection unit will be:\")\n",
    "            print(\"u = x/n\")\n",
    "            print(\"\\n\")\n",
    "               \n",
    "    def open_chart_assistant_screen (self):\n",
    "                \n",
    "        import os\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        from html2image import Html2Image\n",
    "        from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "        # img_to_array: convert the image into its numpy array representation\n",
    "                \n",
    "        if (self.assistant_startup): #run if it is True:\n",
    "            \n",
    "            self.screen_number = 0 # first screen\n",
    "        \n",
    "        if (self.screen_number not in self.numbers_to_end_assistant):\n",
    "            \n",
    "            self.print_screen_legend()\n",
    "            # Use its own method\n",
    "            \n",
    "            # Update attributes:\n",
    "            self.file_to_fetch = \"cc_s\" + str(self.screen_number) + \".png\"\n",
    "            # Obtain the path of the image (local environment):\n",
    "            self.img_local_path = os.path.join(self.new_dir, self.file_to_fetch)\n",
    "                    \n",
    "            # Load the image and save it on variables:\n",
    "            assistant_screen = load_img(self.img_local_path)\n",
    "                    \n",
    "            # show image with plt.imshow function:\n",
    "            fig = plt.figure(figsize = (12, 8))\n",
    "            plt.imshow(assistant_screen)\n",
    "            # If the image is black and white, you can color it with a cmap as fig.set_cmap('hot')\n",
    "            \n",
    "            #set axis off:\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Run again the assistant for next screen (keep assistant on):\n",
    "            self.keep_assistant_on = True\n",
    "            # In the next round, the assistant should not be restarted:\n",
    "            self.assistant_startup = False\n",
    "            \n",
    "            screen_number = input(\"Enter the number you wish here (in the right), according to the shown in the image above: \")\n",
    "            #convert the screen number to string:\n",
    "            screen_number = str(screen_number)        \n",
    "            # Strip spaces and format characters (trim):\n",
    "            screen_number = screen_number.strip()        \n",
    "            # We do not call the str attribute for string variables (only for iterables)\n",
    "            # Convert to integer\n",
    "            screen_number = int(screen_number)\n",
    "            # Update the attribute:\n",
    "            self.screen_number = screen_number\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # user selected a value that ends the assistant:\n",
    "            self.keep_assistant_on = False\n",
    "            self.assistant_startup = False\n",
    "        \n",
    "        # Return the booleans to the main function:\n",
    "        return self\n",
    "        \n",
    "    def chart_selection (self):\n",
    "                \n",
    "        # Only if the screen is in the tuple numbers_to_end_assistant:\n",
    "        if (self.screen_number in self.numbers_to_end_assistant):\n",
    "                    \n",
    "            # Variables are created only when requested:\n",
    "            rare_events_tuple = (3, 4) # g, t\n",
    "            continuous_dist_not_defined_tuple = (9, 10) # std_error, 3std\n",
    "            grouped_dist_not_defined_tuple = (15, 16) # std_error, 3std\n",
    "            grouped_tuple = (13, 19, 20, 21, 22) # x, p, np, c, u\n",
    "            charts_map_dict = {3:'g', 4:'t', 7:'i_mr', 9:'std_error', 10:'3s_as_natural_variation',\n",
    "                                13:'xbar_s', 15:'std_error', 16:'3s_as_natural_variation',\n",
    "                                19:'p', 20:'np', 21:'c', 22:'u'}\n",
    "                    \n",
    "            chart_to_use = charts_map_dict[self.screen_number]\n",
    "                    \n",
    "            # Variable with subgroups, which will be updated if needed:\n",
    "            column_with_labels_or_subgroups = None\n",
    "                    \n",
    "            # Variable for skewed distribution, which will be updated if needed:\n",
    "            consider_skewed_dist_when_estimating_with_std = False\n",
    "                    \n",
    "            column_with_variable_to_be_analyzed = str(input(\"Enter here (in the right) the name or number of the column (its header) that will be analyzed with the control chart.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "            # Try to convert it to integer, if it is a number:\n",
    "            try:\n",
    "                # Clean the string:\n",
    "                column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed.strip()\n",
    "                column_with_variable_to_be_analyzed = int(column_with_variable_to_be_analyzed)\n",
    "                    \n",
    "            except: # simply pass\n",
    "                pass\n",
    "                    \n",
    "            print(\"\\n\")\n",
    "            \n",
    "            yes_no = str(input(\"Do your data have a column containing timestamps or time indication (event order)?\\nType yes or no, here (in the right).\\nDo not type it in quotes: \"))\n",
    "            yes_no = yes_no.strip()        \n",
    "            # convert to full lower case, independently of the user:\n",
    "            yes_no = yes_no.lower()\n",
    "                    \n",
    "            if (yes_no == 'yes'):\n",
    "                    \n",
    "                print(\"\\n\")\n",
    "                timestamp_tag_column = str(input(\"Enter here (in the right) the name or number of the column containing timestamps or time indication (event order).\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "                \n",
    "                # Try to convert it to integer, if it is a number:\n",
    "                try:\n",
    "                    timestamp_tag_column = timestamp_tag_column.strip()\n",
    "                    timestamp_tag_column = int(timestamp_tag_column)\n",
    "                        \n",
    "                except: # simply pass\n",
    "                    pass\n",
    "                    \n",
    "            else:\n",
    "                timestamp_tag_column = None\n",
    "                    \n",
    "            yes_no = str(input(\"Do your data have a column containing event frame indication; indication for separating time windows for comparison analysis;\\nstages; events to be analyzed separately; or any other indication for slicing the time axis for comparison of different means, variations, etc?\\nType yes or no, here (in the right).\\nDo not type it in quotes: \"))\n",
    "            yes_no = yes_no.strip()\n",
    "            yes_no = yes_no.lower()\n",
    "                    \n",
    "            if (yes_no == 'yes'):\n",
    "                        \n",
    "                print(\"\\n\")\n",
    "                column_with_event_frame_indication = str(input(\"Enter here (in the right) the name or number of the column containing the event frame indication.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "                        \n",
    "                # Try to convert it to integer, if it is a number:\n",
    "                try:\n",
    "                    column_with_event_frame_indication = column_with_event_frame_indication.strip()\n",
    "                    column_with_event_frame_indication = int(column_with_event_frame_indication)\n",
    "                        \n",
    "                except: # simply pass\n",
    "                    pass\n",
    "            \n",
    "            else:\n",
    "                column_with_event_frame_indication = None\n",
    "                    \n",
    "            if (self.screen_number in rare_events_tuple):\n",
    "                        \n",
    "                print(\"\\n\")\n",
    "                print(f\"How are the rare events represented in the column {column_with_variable_to_be_analyzed}?\")\n",
    "                print(f\"Before obtaining the chart, you must have modified the {column_with_variable_to_be_analyzed} to labe these data.\")\n",
    "                print(\"The function cannot work with boolean filters. So, if a value corresponds to a rare event occurrence, modify its value to properly labelling it.\")\n",
    "                print(\"You can set a special string or a special numeric value for indicating that a particular row corresponds to a rare event.\")\n",
    "                print(\"That is because rare events occurrences must be compared against all other 'regular' events.\")\n",
    "                print(f\"For instance, {column_with_variable_to_be_analyzed} may show a value like 'rare_event', or 'ulcer' (in our example) if it is a rare occurrence.\")\n",
    "                print(\"Also, you could input a value extremely high, like 1000000000, or extremely low, like -10000000 for marking the rare events in the column.\")\n",
    "                print(\"The chart will be obtained after finding these rare events marks on the column.\\n\")\n",
    "                        \n",
    "                rare_event_indication = str(input(f\"How are the rare events represented in the column {column_with_variable_to_be_analyzed}?\\nEnter here (in the right) the text or number representing a rare event.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or the rare events will not be localized in the dataset): \"))\n",
    "                        \n",
    "                # Try to convert it to float, if it is a number:\n",
    "                try:\n",
    "                    column_with_event_frame_indication = column_with_event_frame_indication.strip()\n",
    "                    column_with_event_frame_indication = float(column_with_event_frame_indication)\n",
    "                \n",
    "                except: # simply pass\n",
    "                    pass\n",
    "                        \n",
    "                rare_event_timedelta_unit = str(input(f\"What is the usual order of magnitude for the intervals (timedeltas) between rare events?\\nEnter here (in the right).\\nYou may type: year, month, day, hour, minute, or second.\\nDo not type it in quotes: \"))\n",
    "                rare_event_timedelta_unit = rare_event_timedelta_unit.strip()\n",
    "                rare_event_timedelta_unit = rare_event_timedelta_unit.lower()\n",
    "                \n",
    "                while (rare_event_timedelta_unit not in ['year', 'month', 'day', 'hour', 'minute', 'second']):\n",
    "                    \n",
    "                    rare_event_timedelta_unit = str(input(\"Please, enter a valid timedelta unit: year, month, day, hour, minute, or second.\\nDo not type it in quotes: \"))\n",
    "                    rare_event_timedelta_unit = rare_event_timedelta_unit.strip()\n",
    "                    rare_event_timedelta_unit = rare_event_timedelta_unit.lower()\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                rare_event_timedelta_unit = None\n",
    "                rare_event_indication = None\n",
    "                        \n",
    "                if ((self.screen_number in grouped_dist_not_defined_tuple) | (self.screen_number in grouped_tuple)):\n",
    "                            \n",
    "                    print(\"\\n\")\n",
    "                    column_with_labels_or_subgroups = str(input(\"Enter here (in the right) the name or number of the column containing the subgroups or samples for aggregating the measurements in terms of mean, standard deviation, etc.\\nIt may be a column with indications like 'A', 'B', or 'C'; 'subgroup1',..., 'sample1',..., or an integer like 1, 2, 3,...\\nThis column will allow grouping of rows in terms of the correspondent samples.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "                            \n",
    "                    # Try to convert it to integer, if it is a number:\n",
    "                    try:\n",
    "                        column_with_labels_or_subgroups = column_with_labels_or_subgroups.strip()\n",
    "                        column_with_labels_or_subgroups = int(column_with_labels_or_subgroups)\n",
    "                    \n",
    "                    except: # simply pass\n",
    "                        pass\n",
    "                \n",
    "                if ((self.screen_number in grouped_dist_not_defined_tuple) | (self.screen_number in continuous_dist_not_defined_tuple)):\n",
    "                            \n",
    "                    print(\"\\n\")\n",
    "                    print(\"Is data skewed or with high kurtosis? If it is, the median will be used as the central line estimative.\")\n",
    "                    print(\"median = mean × 0.693\\n\")\n",
    "                            \n",
    "                    yes_no = str(input(\"Do you want to assume a skewed (or with considerable kurtosis) distribution?\\nType yes or no, here (in the right).\\nDo not type it in quotes: \"))\n",
    "                    yes_no = yes_no.strip()\n",
    "                    yes_no = yes_no.lower()\n",
    "                            \n",
    "                    if (yes_no == 'yes'):\n",
    "                        \n",
    "                        # update the boolean variable\n",
    "                        consider_skewed_dist_when_estimating_with_std = True\n",
    "                \n",
    "                \n",
    "            print(\"Finished mapping the variables for obtaining the control chart plots.\")\n",
    "            print(\"If an error is raised; or if the chart is not complete, check if the columns' names inputs are strictly correct.\\n\")\n",
    "            \n",
    "            return chart_to_use, column_with_labels_or_subgroups, consider_skewed_dist_when_estimating_with_std, column_with_variable_to_be_analyzed, timestamp_tag_column, column_with_event_frame_indication, rare_event_timedelta_unit, rare_event_indication\n",
    "        \n",
    "\n",
    "def statistical_process_control_chart (df, column_with_variable_to_be_analyzed, timestamp_tag_column = None, column_with_labels_or_subgroups = None, column_with_event_frame_indication = None, specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}, use_spc_chart_assistant = False, chart_to_use = 'std_error', consider_skewed_dist_when_estimating_with_std = False, rare_event_indication = None, rare_event_timedelta_unit = 'day', x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from scipy import stats\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "    # df: dataframe to be analyzed.\n",
    "    \n",
    "    # timestamp_tag_column: column containing the timescale, that can be expressed as a timestamp, or\n",
    "    # as a series of floats or integers, correspondent to the sequence order. \n",
    "    # If timestamp_tag_column = None, the index of the dataframe will be used as timestamp_tag_column.\n",
    "    \n",
    "    # column_with_variable_to_be_analyzed:  \n",
    "    # column_with_labels_or_subgroups = None\n",
    "    # column_with_event_frame_indication = None\n",
    "    \n",
    "    # specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "    # If there are specification limits, input them in this dictionary. Do not modify the keys,\n",
    "    # simply substitute None by the lower and/or the upper specification.\n",
    "    # e.g. Suppose you have a tank that cannot have more than 10 L. So:\n",
    "    # specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': 10}, there is only\n",
    "    # an upper specification equals to 10 (do not add units);\n",
    "    # Suppose a temperature cannot be lower than 10 ºC, but there is no upper specification. So,\n",
    "    # specification_limits = {'lower_spec_lim': 10, 'upper_spec_lim': None}. Finally, suppose\n",
    "    # a liquid which pH must be between 6.8 and 7.2:\n",
    "    # specification_limits = {'lower_spec_lim': 6.8, 'upper_spec_lim': 7.2}\n",
    "    \n",
    "    # use_spc_chart_assistant = False. Set as True to open the visual flow chart assistant\n",
    "    # that will help you select the appropriate parameters; as well as passing the data in the\n",
    "    # correct format. If the assistant is open, many of the arguments of the function will be\n",
    "    # filled when using it.\n",
    "    \n",
    "    # chart_to_use = '3s_as_natural_variation', 'std_error', 'i_mr', 'xbar_s', 'np', 'p', \n",
    "    # 'u', 'c', 'g', 't'\n",
    "    # 'std_error' stands for standard error = s/(n**0.5), where n is the number of samples (that may\n",
    "    # be the number of individual data samples collected). Here, the natural variation will be\n",
    "    # calculated as 3 times the standard error.\n",
    "    # https://en.wikipedia.org/wiki/Standard_error\n",
    "    \n",
    "    # consider_skewed_dist_when_estimating_with_std. If False, the central lines will be estimated\n",
    "    # as the mean values. If True, they will be estimated with the median, which is a better alternative\n",
    "    # for skewed data such as the ones that follow geometric or lognormal distributions\n",
    "    # (median = mean × 0.693).\n",
    "    \n",
    "    # rare_event_indication = None. String (in quotes), float or integer. If you want to analyze a\n",
    "    # rare event through 'g' or 't' control charts, this parameter is obbligatory. Also, notice that:\n",
    "    # column_with_variable_to_be_analyzed must be the column which contain an indication of the rare\n",
    "    # event occurrence, and the rare_event_indication is the value of the column column_with_variable_to_be_analyzed\n",
    "    # when a rare event takes place.\n",
    "    # For instance, suppose rare_event_indication = 'shutdown'. It means that column column_with_variable_to_be_analyzed\n",
    "    # has the value 'shutdown' when the rare event occurs, i.e., for timestamps when the system\n",
    "    # system stopped. Other possibilities are rare_event_indication = 0, or rare_event_indication = -1,\n",
    "    # indicating that when column_with_variable_to_be_analyzed = 0 (or -1), we know that\n",
    "    # a rare event occurred. The most important thing here is that the value given to the rare event\n",
    "    # should be assigned only to the rare events.\n",
    "    # You do not need to assign values for the other timestamps when no rare event took place. But it is\n",
    "    # important to keep all timestamps in the dataframe. That is because the rare events charts will\n",
    "    # compare the rare event occurrence against all other dataframes.\n",
    "    # If you are not analyzing rare events with g or t charts, keep rare_event_indication = None.\n",
    "    \n",
    "    # rare_event_timedelta_unit: 'day', 'second', 'nanosecond', 'minute', 'hour',\n",
    "    # 'month', 'year' - This is the unit of time that will be used to plot the time interval\n",
    "    # (timedelta) between each successive rare event. If None or invalid value used, timedelta\n",
    "    # will be given in days.\n",
    "    # Notice that this parameter is referrent only to the rare events analysis with G or T charts.\n",
    "    # Also, it is valid only the timetag column effectively stores a timestamp.\n",
    "    \n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "\n",
    "    ## CONTROL CHARTS CALCULATION\n",
    "    \n",
    "    # References: \n",
    "    # Douglas C. Montgomery. Introduction to Statistical Quality Control. 6th Edition. \n",
    "    # John Wiley and Sons, 2009.\n",
    "    # Jacob Anhoej. Control Charts with qicharts for R. 2021-04-20. In: https://cran.r-project.org/web/packages/qicharts/vignettes/controlcharts.html\n",
    "    \n",
    "    # Define a dictionary of constants.\n",
    "    # Each key in the dictionary corresponds to a number of samples in a subgroup.\n",
    "    # number_of_labels - This variable represents the total of labels or subgroups n. \n",
    "    # If there are multiple labels, this variable will be updated later.\n",
    "    def get_constants (number_of_labels):\n",
    "        if (number_of_labels < 2):\n",
    "            number_of_labels = 2\n",
    "        if (number_of_labels <= 25):\n",
    "            dict_of_constants = {\n",
    "                2: {'A':2.121, 'A2':1.880, 'A3':2.659, 'c4':0.7979, '1/c4':1.2533, 'B3':0, 'B4':3.267, 'B5':0, 'B6':2.606, 'd2':1.128, '1/d2':0.8865, 'd3':0.853, 'D1':0, 'D2':3.686, 'D3':0, 'D4':3.267},\n",
    "                3: {'A':1.732, 'A2':1.023, 'A3':1.954, 'c4':0.8862, '1/c4':1.1284, 'B3':0, 'B4':2.568, 'B5':0, 'B6':2.276, 'd2':1.693, '1/d2':0.5907, 'd3':0.888, 'D1':0, 'D2':4.358, 'D3':0, 'D4':2.574},\n",
    "                4: {'A':1.500, 'A2':0.729, 'A3':1.628, 'c4':0.9213, '1/c4':1.0854, 'B3':0, 'B4':2.266, 'B5':0, 'B6':2.088, 'd2':2.059, '1/d2':0.4857, 'd3':0.880, 'D1':0, 'D2':4.698, 'D3':0, 'D4':2.282},\n",
    "                5: {'A':1.342, 'A2':0.577, 'A3':1.427, 'c4':0.9400, '1/c4':1.0638, 'B3':0, 'B4':2.089, 'B5':0, 'B6':1.964, 'd2':2.326, '1/d2':0.4299, 'd3':0.864, 'D1':0, 'D2':4.918, 'D3':0, 'D4':2.114},\n",
    "                6: {'A':1.225, 'A2':0.483, 'A3':1.287, 'c4':0.9515, '1/c4':1.0510, 'B3':0.030, 'B4':1.970, 'B5':0.029, 'B6':1.874, 'd2':2.534, '1/d2':0.3946, 'd3':0.848, 'D1':0, 'D2':5.078, 'D3':0, 'D4':2.004},\n",
    "                7: {'A':1.134, 'A2':0.419, 'A3':1.182, 'c4':0.9594, '1/c4':1.0423, 'B3':0.118, 'B4':1.882, 'B5':0.113, 'B6':1.806, 'd2':2.704, '1/d2':0.3698, 'd3':0.833, 'D1':0.204, 'D2':5.204, 'D3':0.076, 'D4':1.924},\n",
    "                8: {'A':1.061, 'A2':0.373, 'A3':1.099, 'c4':0.9650, '1/c4':1.0363, 'B3':0.185, 'B4':1.815, 'B5':0.179, 'B6':1.751, 'd2':2.847, '1/d2':0.3512, 'd3':0.820, 'D1':0.388, 'D2':5.306, 'D3':0.136, 'D4':1.864},\n",
    "                9: {'A':1.000, 'A2':0.337, 'A3':1.032, 'c4':0.9693, '1/c4':1.0317, 'B3':0.239, 'B4':1.761, 'B5':0.232, 'B6':1.707, 'd2':2.970, '1/d2':0.3367, 'd3':0.808, 'D1':0.547, 'D2':5.393, 'D3':0.184, 'D4':1.816},\n",
    "                10: {'A':0.949, 'A2':0.308, 'A3':0.975, 'c4':0.9727, '1/c4':1.0281, 'B3':0.284, 'B4':1.716, 'B5':0.276, 'B6':1.669, 'd2':3.078, '1/d2':0.3249, 'd3':0.797, 'D1':0.687, 'D2':5.469, 'D3':0.223, 'D4':1.777},\n",
    "                11: {'A':0.905, 'A2':0.285, 'A3':0.927, 'c4':0.9754, '1/c4':1.0252, 'B3':0.321, 'B4':1.679, 'B5':0.313, 'B6':1.637, 'd2':3.173, '1/d2':0.3152, 'd3':0.787, 'D1':0.811, 'D2':5.535, 'D3':0.256, 'D4':1.744},\n",
    "                12: {'A':0.866, 'A2':0.266, 'A3':0.886, 'c4':0.9776, '1/c4':1.0229, 'B3':0.354, 'B4':1.646, 'B5':0.346, 'B6':1.610, 'd2':3.258, '1/d2':0.3069, 'd3':0.778, 'D1':0.922, 'D2':5.594, 'D3':0.283, 'D4':1.717},\n",
    "                13: {'A':0.832, 'A2':0.249, 'A3':0.850, 'c4':0.9794, '1/c4':1.0210, 'B3':0.382, 'B4':1.618, 'B5':0.374, 'B6':1.585, 'd2':3.336, '1/d2':0.2998, 'd3':0.770, 'D1':1.025, 'D2':5.647, 'D3':0.307, 'D4':1.693},\n",
    "                14: {'A':0.802, 'A2':0.235, 'A3':0.817, 'c4':0.9810, '1/c4':1.0194, 'B3':0.406, 'B4':1.594, 'B5':0.399, 'B6':1.563, 'd2':3.407, '1/d2':0.2935, 'd3':0.763, 'D1':1.118, 'D2':5.696, 'D3':0.328, 'D4':1.672},\n",
    "                15: {'A':0.775, 'A2':0.223, 'A3':0.789, 'c4':0.9823, '1/c4':1.0180, 'B3':0.428, 'B4':1.572, 'B5':0.421, 'B6':1.544, 'd2':3.472, '1/d2':0.2880, 'd3':0.756, 'D1':1.203, 'D2':5.741, 'D3':0.347, 'D4':1.653},\n",
    "                16: {'A':0.750, 'A2':0.212, 'A3':0.763, 'c4':0.9835, '1/c4':1.0168, 'B3':0.448, 'B4':1.552, 'B5':0.440, 'B6':1.526, 'd2':3.532, '1/d2':0.2831, 'd3':0.750, 'D1':1.282, 'D2':5.782, 'D3':0.363, 'D4':1.637},\n",
    "                17: {'A':0.728, 'A2':0.203, 'A3':0.739, 'c4':0.9845, '1/c4':1.0157, 'B3':0.466, 'B4':1.534, 'B5':0.458, 'B6':1.511, 'd2':3.588, '1/d2':0.2787, 'd3':0.744, 'D1':1.356, 'D2':5.820, 'D3':0.378, 'D4':1.622},\n",
    "                18: {'A':0.707, 'A2':0.194, 'A3':0.718, 'c4':0.9854, '1/c4':1.0148, 'B3':0.482, 'B4':1.518, 'B5':0.475, 'B6':1.496, 'd2':3.640, '1/d2':0.2747, 'd3':0.739, 'D1':1.424, 'D2':5.856, 'D3':0.391, 'D4':1.608},\n",
    "                19: {'A':0.688, 'A2':0.187, 'A3':0.698, 'c4':0.9862, '1/c4':1.0140, 'B3':0.497, 'B4':1.503, 'B5':0.490, 'B6':1.483, 'd2':3.689, '1/d2':0.2711, 'd3':0.734, 'D1':1.487, 'D2':5.891, 'D3':0.403, 'D4':1.597},\n",
    "                20: {'A':0.671, 'A2':0.180, 'A3':0.680, 'c4':0.9869, '1/c4':1.0133, 'B3':0.510, 'B4':1.490, 'B5':0.504, 'B6':1.470, 'd2':3.735, '1/d2':0.2677, 'd3':0.729, 'D1':1.549, 'D2':5.921, 'D3':0.415, 'D4':1.585},\n",
    "                21: {'A':0.655, 'A2':0.173, 'A3':0.663, 'c4':0.9876, '1/c4':1.0126, 'B3':0.523, 'B4':1.477, 'B5':0.516, 'B6':1.459, 'd2':3.778, '1/d2':0.2647, 'd3':0.724, 'D1':1.605, 'D2':5.951, 'D3':0.425, 'D4':1.575},\n",
    "                22: {'A':0.640, 'A2':0.167, 'A3':0.647, 'c4':0.9882, '1/c4':1.0119, 'B3':0.534, 'B4':1.466, 'B5':0.528, 'B6':1.448, 'd2':3.819, '1/d2':0.2618, 'd3':0.720, 'D1':1.659, 'D2':5.979, 'D3':0.434, 'D4':1.566},\n",
    "                23: {'A':0.626, 'A2':0.162, 'A3':0.633, 'c4':0.9887, '1/c4':1.0114, 'B3':0.545, 'B4':1.455, 'B5':0.539, 'B6':1.438, 'd2':3.858, '1/d2':0.2592, 'd3':0.716, 'D1':1.710, 'D2':6.006, 'D3':0.443, 'D4':1.557},\n",
    "                24: {'A':0.612, 'A2':0.157, 'A3':0.619, 'c4':0.9892, '1/c4':1.0109, 'B3':0.555, 'B4':1.445, 'B5':0.549, 'B6':1.429, 'd2':3.895, '1/d2':0.2567, 'd3':0.712, 'D1':1.759, 'D2':6.031, 'D3':0.451, 'D4':1.548},\n",
    "                25: {'A':0.600, 'A2':0.153, 'A3':0.606, 'c4':0.9896, '1/c4':1.0105, 'B3':0.565, 'B4':1.435, 'B5':0.559, 'B6':1.420, 'd2':3.931, '1/d2':0.2544, 'd3':0.708, 'D1':1.806, 'D2':6.056, 'D3':0.459, 'D4':1.541},\n",
    "            }\n",
    "            # Access the key:\n",
    "            dict_of_constants = dict_of_constants[number_of_labels]\n",
    "        else: #>= 26\n",
    "            dict_of_constants = {'A':(3/(number_of_labels**(0.5))), 'A2':0.153, 'A3':3/((4*(number_of_labels-1)/(4*number_of_labels-3))*(number_of_labels**(0.5))), 'c4':(4*(number_of_labels-1)/(4*number_of_labels-3)), '1/c4':1/((4*(number_of_labels-1)/(4*number_of_labels-3))), 'B3':(1-3/(((4*(number_of_labels-1)/(4*number_of_labels-3)))*((2*(number_of_labels-1))**(0.5)))), 'B4':(1+3/(((4*(number_of_labels-1)/(4*number_of_labels-3)))*((2*(number_of_labels-1))**(0.5)))), 'B5':(((4*(number_of_labels-1)/(4*number_of_labels-3)))-3/((2*(number_of_labels-1))**(0.5))), 'B6':(((4*(number_of_labels-1)/(4*number_of_labels-3)))+3/((2*(number_of_labels-1))**(0.5))), 'd2':3.931, '1/d2':0.2544, 'd3':0.708, 'D1':1.806, 'D2':6.056, 'D3':0.459, 'D4':1.541}\n",
    "        return dict_of_constants\n",
    "\n",
    "    # CENTER LINE = m (mean)\n",
    "    # s = std deviation\n",
    "    # General equation of the control limits:\n",
    "    # UCL = upper control limit = m + L*s\n",
    "    # LCL = lower control limit = m - L*s\n",
    "    # where L is a measurement of distance from central line.\n",
    "    \n",
    "    # for a subgroup of data collected for a continuous variable X:\n",
    "    # x_bar_1 = mean value for subgroup 1,..., x_bar_n = mean for subgroup n.\n",
    "    # x_bar_bar = (x_bar_1 + ... + x_bar_n)/n\n",
    "    \n",
    "    # On the other hand, the range R between two collected values is defined as: R = x_max - x_min\n",
    "    # If R1, ... Rm are the ranges for m subgroups (which may be of size 2),\n",
    "    # we have R_bar = (R1 + R2 + ... + Rm)/m\n",
    "    \n",
    "    # Analogously, if s_i is the standard deviation for a subgroup i and there are m subgroups:\n",
    "    # s_bar = (s_1 +... +s_m)/m\n",
    "    \n",
    "    ## INDIVIDUAL MEASUREMENTS (Montgomery, pg.259, section 6.4)\n",
    "    \n",
    "    # For individual measurements, we consider subgroups formed by two consecutive measurements, so\n",
    "    # m = 2, and R = abs(x_i - x_(i-1)), where abs function calculates the absolute value of the\n",
    "    # difference between two successive subgroups.\n",
    "    # UCL =  x_bar_bar + 3*(1/d2)*R_bar\n",
    "    # LCL =  x_bar_bar - 3*(1/d2)*R_bar\n",
    "    # Center line = x_bar_bar\n",
    "    \n",
    "    def chart_i_mr (dictionary, column_with_variable_to_be_analyzed):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # CONTROL LIMIT EQUATIONS:\n",
    "        # X-bar = (sum of measurements)/(number of measurements)\n",
    "        # R = Absolute value of [(largest in subgroup) - (lowest in subgroup)]\n",
    "        # Individual chart: subgroup = 1\n",
    "        # R = Absolute value of [(data) - (next data)]\n",
    "        # R-bar = (sum of ranges R)/(number of R values calculated)\n",
    "        # Lower control limit (LCL) = X-bar - (2.66)R-bar\n",
    "        # Upper control limit (UCL) = X-bar + (2.66)R-bar\n",
    "        # loop through each row from df, starting from the second (row 1):    \n",
    "        # calculate mR as the difference (Xmax - Xmin) of the difference between\n",
    "        # df[column_with_variable_to_be_analyzed] on row i and the row\n",
    "        # i-1. Since we do not know, in principle, which one is the maximum, we can use\n",
    "        # the max and min functions from Python:\n",
    "        # https://www.w3schools.com/python/ref_func_max.asp\n",
    "        # https://www.w3schools.com/python/ref_func_min.asp\n",
    "        # Also, the moving range here must be calculated as an absolute value\n",
    "        # https://www.w3schools.com/python/ref_func_abs.asp\n",
    "        moving_range = [abs(max((df[column_with_variable_to_be_analyzed][i]), (df[column_with_variable_to_be_analyzed][(i-1)])) - min((df[column_with_variable_to_be_analyzed][i]), (df[column_with_variable_to_be_analyzed][(i-1)]))) for i in range (1, len(df))]\n",
    "        x_bar_list = [(df[column_with_variable_to_be_analyzed][i] + df[column_with_variable_to_be_analyzed][(i-1)])/2 for i in range (1, len(df))]\n",
    "        # These lists were created from index 1. We must add a initial element to\n",
    "        # make their sizes equal to the original dataset length\n",
    "        # Start the list to store the moving ranges, containing only the number 0\n",
    "        # for the moving range (by simple concatenation):\n",
    "        moving_range = [0] + moving_range\n",
    "        # Start the list that stores the mean values of the 2-elements subgroups\n",
    "        # with the first element itself (index 0):\n",
    "        x_bar_list = [df[column_with_variable_to_be_analyzed][0]] + x_bar_list\n",
    "        # Save the moving ranges as a new column from df (it may be interesting to check it):\n",
    "        df['moving_range'] = moving_range\n",
    "        # Save x_bar_list as the column to be analyzed:\n",
    "        df[column_with_variable_to_be_analyzed] = x_bar_list\n",
    "        # Get the mean values from x_bar:\n",
    "        x_bar_bar = df[column_with_variable_to_be_analyzed].mean()\n",
    "        # Calculate the mean value of the column moving_range, and save it as r_bar:\n",
    "        r_bar = df['moving_range'].mean()       \n",
    "        # Get the control chart constant A2 from the dictionary, considering n = 2 the\n",
    "        # number of elements of each subgroup:\n",
    "        dict_to_access = get_constants (number_of_labels = 2)\n",
    "        control_chart_constant = dict_to_access['1/d2']\n",
    "        control_chart_constant = control_chart_constant * 3\n",
    "        # calculate the upper control limit as x_bar + (3/d2)r_bar:\n",
    "        upper_cl = x_bar_bar + (control_chart_constant) * (r_bar)\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl \n",
    "        # calculate the lower control limit as x_bar - (3/d2)r_bar:\n",
    "        lower_cl = x_bar_bar - (control_chart_constant) * (r_bar)\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = x_bar_bar\n",
    "        # Update the dataframe in the dictionary and return it:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "\n",
    "    def chart_3s (dictionary, column_with_variable_to_be_analyzed, consider_skewed_dist_when_estimating_with_std):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        if(consider_skewed_dist_when_estimating_with_std):            \n",
    "            # Skewed data. Use the median:\n",
    "            center = df[column_with_variable_to_be_analyzed].median()    \n",
    "        else:\n",
    "            center = dictionary['center']    \n",
    "        # calculate the upper control limit as the mean + 3s\n",
    "        upper_cl = center + 3 * (dictionary['std'])\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl        \n",
    "        # calculate the lower control limit as the mean - 3s:\n",
    "        lower_cl = center - 3 * (dictionary['std'])\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl        \n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = center        \n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "    \n",
    "    def chart_std_error (dictionary, column_with_variable_to_be_analyzed, consider_skewed_dist_when_estimating_with_std):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        n_samples = df[column_with_variable_to_be_analyzed].count()\n",
    "        s = dictionary['std']\n",
    "        std_error = s/(n_samples**(0.5))        \n",
    "        if(consider_skewed_dist_when_estimating_with_std):      \n",
    "            # Skewed data. Use the median:\n",
    "            center = df[column_with_variable_to_be_analyzed].median()    \n",
    "        else:\n",
    "            center = dictionary['center']    \n",
    "        # calculate the upper control limit as the mean + 3 std_error\n",
    "        upper_cl = center + 3 * (std_error)\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl        \n",
    "        # calculate the lower control limit as the mean - 3 std_error:\n",
    "        lower_cl = center - 3 * (std_error)\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = center        \n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "        \n",
    "    # CONTROL CHARTS FOR SUBGROUPS \n",
    "    \n",
    "    def create_grouped_df (dictionary):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']    \n",
    "        # We need to group each dataframe in terms of the subgroups stored in the variable\n",
    "        # column_with_labels_or_subgroups.\n",
    "        # The catehgorical or datetime columns must be aggregated in terms of mode.\n",
    "        # The numeric variables must be aggregated both in terms of mean and in terms of count\n",
    "        # (subgroup size)   \n",
    "        # 1. Start a list for categorical columns and other for numeric columns:\n",
    "        categorical_cols = []\n",
    "        numeric_cols = []\n",
    "        # Variables to map if there are categorical or numeric variables:\n",
    "        is_categorical = 0\n",
    "        is_numeric = 0\n",
    "        # 2. Loop through each column from the list of columns of the dataframe:\n",
    "        for column in list(df.columns):        \n",
    "            # check the type of column:\n",
    "            column_data_type = df[column].dtype\n",
    "            if (column_data_type not in numeric_dtypes):        \n",
    "                # If the Pandas series was defined as an object, it means it is categorical\n",
    "                # (string, date, etc). Also, this if captures the variables converted to datetime64\n",
    "                # Append the column to the list of categorical columns:\n",
    "                categorical_cols.append(column)\n",
    "            else:\n",
    "                # append the column to the list of numeric columns:\n",
    "                numeric_cols.append(column)\n",
    "            # 3. Check if column_with_labels_or_subgroups is in both lists. \n",
    "            # If it is missing, append it. We need that this column in all subsets for grouping.\n",
    "            if not (column_with_labels_or_subgroups in categorical_cols):\n",
    "                categorical_cols.append(column_with_labels_or_subgroups)\n",
    "            if not (column_with_labels_or_subgroups in numeric_cols):\n",
    "                numeric_cols.append(column_with_labels_or_subgroups)\n",
    "            if (len(categorical_cols) > 1):\n",
    "                # There is at least one column plus column_with_labels_or_subgroups:\n",
    "                is_categorical = 1\n",
    "            if (len(numeric_cols) > 1):\n",
    "                # There is at least one column plus column_with_labels_or_subgroups:\n",
    "                is_numeric = 1\n",
    "        # 4. Create copies of df, subsetting by type of column\n",
    "        # 5. Group the dataframes by column_with_labels_or_subgroups, \n",
    "        # according to the aggregate function:\n",
    "        if (is_categorical == 1):\n",
    "            df_agg_mode = df.copy(deep = True)\n",
    "            df_agg_mode = df_agg_mode[categorical_cols]\n",
    "            df_agg_mode = df_agg_mode.groupby(by = column_with_labels_or_subgroups, as_index = False, sort = True).agg(stats.mode)\n",
    "            # 6. df_agg_mode processing:\n",
    "            # Loop through each column from this dataframe:\n",
    "            for col_mode in list(df_agg_mode.columns):        \n",
    "                # start a list of modes:\n",
    "                list_of_modes = []    \n",
    "                # Now, loop through each row from the dataset:\n",
    "                for i in range(0, len(df_agg_mode)):\n",
    "                    # i = 0 to i = len(df_agg_mode) - 1    \n",
    "                    mode_array = df_agg_mode[col_mode][i]    \n",
    "                    try:\n",
    "                        # try accessing the mode\n",
    "                        # mode array is like:\n",
    "                        # ModeResult(mode=array([calculated_mode]), count=array([counting_of_occurrences]))\n",
    "                        # To retrieve only the mode, we must access the element [0][0] from this array:\n",
    "                        mode = mode_array[0][0]\n",
    "                    except:\n",
    "                        mode = np.nan\n",
    "                    # Append it to the list of modes:\n",
    "                    list_of_modes.append(mode)\n",
    "                # Finally, make the column the list of modes itself:\n",
    "                df_agg_mode[col_mode] = list_of_modes    \n",
    "                # try to convert to datetime64 (case it is not anymore):\n",
    "                try:\n",
    "                    df_agg_mode[col_mode] = df_agg_mode[col_mode].astype(np.datetime64)    \n",
    "                except:\n",
    "                    # simply ignore this step in case it is not possible to parse\n",
    "                    # because it is a string:\n",
    "                    pass\n",
    "        if (is_numeric == 1):\n",
    "            df_agg_mean = df.copy(deep = True)\n",
    "            df_agg_sum = df.copy(deep = True)\n",
    "            df_agg_std = df.copy(deep = True)\n",
    "            df_agg_count = df.copy(deep = True)\n",
    "            df_agg_mean = df_agg_mean[numeric_cols]\n",
    "            df_agg_sum = df_agg_sum[numeric_cols]\n",
    "            df_agg_std = df_agg_std[numeric_cols]\n",
    "            df_agg_count = df_agg_count[numeric_cols]    \n",
    "            df_agg_mean = df_agg_mean.groupby(by = column_with_labels_or_subgroups, as_index = False, sort = True).mean()\n",
    "            df_agg_sum = df_agg_sum.groupby(by = column_with_labels_or_subgroups, as_index = False, sort = True).sum()\n",
    "            df_agg_std = df_agg_sum.groupby(by = column_with_labels_or_subgroups, as_index = False, sort = True).std()\n",
    "            df_agg_count = df_agg_count.groupby(by = column_with_labels_or_subgroups, as_index = False, sort = True).count()\n",
    "            # argument as_index = False: prevents the grouper variable to be set as index of the new dataframe.\n",
    "            # (default: as_index = True).\n",
    "            # 7. df_agg_count processing:\n",
    "            # Here, all original columns contain only the counting of elements in each\n",
    "            # label. So, let's select only the columns 'key_for_merging' and column_with_variable_to_be_analyzed:\n",
    "            df_agg_count = df_agg_count[[column_with_variable_to_be_analyzed]]\n",
    "            # Rename the columns:\n",
    "            df_agg_count.columns = ['count_of_elements_by_label']    \n",
    "            # Analogously, let's keep only the colums column_with_variable_to_be_analyzed and\n",
    "            # 'key_for_merging' from the dataframes df_agg_sum and df_agg_std, and rename them:\n",
    "            df_agg_sum = df_agg_sum[[column_with_variable_to_be_analyzed]]\n",
    "            df_agg_std = df_agg_std[[column_with_variable_to_be_analyzed]]    \n",
    "            df_agg_sum.columns = ['sum_of_values_by_label']\n",
    "            df_agg_std.columns = ['std_of_values_by_label']\n",
    "        if ((is_categorical + is_numeric) == 2):\n",
    "            # Both subsets are present and the column column_with_labels_or_subgroups\n",
    "            # is duplicated.\n",
    "            # Remove this column from df_agg_mean:\n",
    "            df_agg_mean = df_agg_mean.drop(columns = column_with_labels_or_subgroups)\n",
    "            # Concatenate all dataframes:\n",
    "            df = pd.concat([df_agg_mode, df_agg_mean, df_agg_sum, df_agg_std, df_agg_count], axis = 1, join = \"inner\")      \n",
    "        elif (is_numeric == 1):\n",
    "            # Only the numeric dataframes are present. So, concatenate them:\n",
    "            df = pd.concat([df_agg_mean, df_agg_sum, df_agg_std, df_agg_count], axis = 1, join = \"inner\")\n",
    "        elif (is_categorical == 1):\n",
    "            # There is only the categorical dataframe:\n",
    "            df = df_agg_mode\n",
    "        df = df.reset_index(drop = True)\n",
    "        # Notice that now we have a different mean value: we have a mean value\n",
    "        # of the means calculated for each subgroup. So, we must update the\n",
    "        # dictionary information:    \n",
    "        dictionary['center'] = df[column_with_variable_to_be_analyzed].mean()\n",
    "        dictionary['sum'] = df[column_with_variable_to_be_analyzed].sum()\n",
    "        dictionary['std'] = df[column_with_variable_to_be_analyzed].std()\n",
    "        dictionary['var'] = df[column_with_variable_to_be_analyzed].var()\n",
    "        dictionary['count'] = df[column_with_variable_to_be_analyzed].count()\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "            \n",
    "    # X-bar-S (continuous variables) - (Montgomery, pg.253, section 6.3.1):\n",
    "    # For subgroups with m elements:\n",
    "    # UCL = x_bar_bar + A3*s_bar\n",
    "    # Center line = x_bar_bar\n",
    "    # LCL = x_bar_bar - A3*s_bar\n",
    "    \n",
    "    def chart_x_bar_s (dictionary):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # CONTROL LIMIT EQUATIONS:\n",
    "        # X-bar = mean =  (sum of measurements)/(subgroup size)\n",
    "        # s = standard deviation in each subgroup\n",
    "        # s-bar = mean (s) = (sum of all s values)/(number of subgroups)\n",
    "        # x-bar-bar = mean (x-bar) = (sum of all x-bar)/(number of subgroups)\n",
    "        # Lower control limit (LCL) = X-bar-bar - (A3)(s-bar)\n",
    "        # Upper control limit (UCL) = X-bar-bar + (A3)(s-bar)        \n",
    "        s = df['std_of_values_by_label']\n",
    "        number_of_labels = dictionary['count']\n",
    "        s_bar = (s.sum())/(number_of_labels)\n",
    "        x_bar_bar = dictionary['center']\n",
    "        # Retrieve A3\n",
    "        dict_to_access = get_constants (number_of_labels = number_of_labels)\n",
    "        control_chart_constant = dict_to_access['A3']\n",
    "        # calculate the upper control limit as X-bar-bar + (A3)(s-bar):\n",
    "        upper_cl = x_bar_bar + (control_chart_constant) * (s_bar)\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl\n",
    "        # calculate the lower control limit as X-bar-bar - (A3)(s-bar):\n",
    "        lower_cl = x_bar_bar - (control_chart_constant) * (s_bar)\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = x_bar_bar\n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "    \n",
    "    # CONTROL CHARTS FOR CATEGORICAL VARIABLES\n",
    "    \n",
    "    # P-chart - (Montgomery, pg.291, section 7.2.1):\n",
    "    # The sample fraction nonconforming is defined as the ratio of the number of nonconforming units \n",
    "    # in the sample D to the sample size n:\n",
    "    # p = D/n. For a subgroup i containing n elements: p_i = D_i/n, i = 1,... m subgroups\n",
    "    # From the binomial distribution, the mean should be estimated as p, and the variance s² \n",
    "    # as p(1-p)/n. The average of p is:\n",
    "    # p_bar = (p_1 + ... + p_m)/m\n",
    "    \n",
    "    # UCL = p_bar + 3((p_bar(1-p_bar)/n)**(0.5))\n",
    "    # Center line = p_bar\n",
    "    # LCL = p_bar - 3((p_bar(1-p_bar)/n)**(0.5)), whereas n is the size of a given subgroup\n",
    "    # Therefore, the width of control limits here vary with the size of the subgroups (it is\n",
    "    # a function of the size of each subgroup)\n",
    "    \n",
    "    # np-chart - (Montgomery, pg.300, section 7.2.1):\n",
    "    # UCL = np + 3((np(1-p))**(0.5))\n",
    "    # Center line = np_bar\n",
    "    # LCL = np - 3((np(1-p))**(0.5))\n",
    "    # where np is the total sum of nonconforming, considering all subgroups, and p is the\n",
    "    # propoertion for each individual subgroup, as previously defined.\n",
    "    # Then, again the control limits depend on the subgroup (label) size.\n",
    "    \n",
    "    def chart_p (dictionary):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # CONTROL LIMIT EQUATIONS:\n",
    "        # p-chart: control chart for proportion of defectives\n",
    "        # p = mean =  (sum of measurements)/(subgroup size)\n",
    "        # pbar = (sum of subgroup defective counts)/(sum of subgroups sizes)\n",
    "        # n = subgroup size\n",
    "        # Lower control limit (LCL) = pbar - 3.sqrt((pbar)*(1-pbar)/n)\n",
    "        # Upper control limit (UCL) = pbar + 3.sqrt((pbar)*(1-pbar)/n)\n",
    "        number_of_labels = dictionary['count']\n",
    "        count_per_label = df['count_of_elements_by_label']\n",
    "        p_bar = dictionary['center']        \n",
    "        # calculate the upper control limit as pbar + 3.sqrt((pbar)*(1-pbar)/n):\n",
    "        upper_cl = p_bar + 3 * (((p_bar)*(1 - p_bar)/(count_per_label))**(0.5))\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl\n",
    "        # calculate the lower control limit as pbar - 3.sqrt((pbar)*(1-pbar)/n):\n",
    "        lower_cl = p_bar - 3 * (((p_bar)*(1 - p_bar)/(count_per_label))**(0.5))\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = p_bar\n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "\n",
    "    def chart_np (dictionary, column_with_variable_to_be_analyzed):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # CONTROL LIMIT EQUATIONS:\n",
    "        # np-chart: control chart for count of defectives\n",
    "        # np = sum = subgroup defective count\n",
    "        # npbar = (sum of subgroup defective counts)/(number of subgroups)\n",
    "        # n = subgroup size\n",
    "        # pbar = npbar/n\n",
    "        # Lower control limit (LCL) = np - 3.sqrt((npbar)*(1-p))\n",
    "        # Upper control limit (UCL) = np + 3.sqrt((npbar)*(1-p))\n",
    "        # available function: **(0.5) - 0.5 power        \n",
    "        # Here, the column that we want to evaluate is not the mean, but the sum.\n",
    "        # Since the graphics will be plotted using the column column_with_variable_to_be_analyzed\n",
    "        # Let's make this column equals to the column of sums:\n",
    "        df[column_with_variable_to_be_analyzed] = df['sum_of_values_by_label']\n",
    "        number_of_labels = dictionary['count']\n",
    "        count_per_label = df['count_of_elements_by_label']\n",
    "        sum_p = df['sum_of_values_by_label'].sum() # It is the np, would is already used for NumPy\n",
    "        p = (df['sum_of_values_by_label'])/(count_per_label)        \n",
    "        # calculate the upper control limit as np + 3.sqrt((np)*(1-p)):\n",
    "        upper_cl = np + 3 * (((sum_p)*(1 - p))**(0.5))\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl\n",
    "        # calculate the lower control limit as np - 3.sqrt((np)*(1-p)):\n",
    "        lower_cl = np - 3 * (((sum_p)*(1 - p))**(0.5))\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = sum_p\n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "    \n",
    "    # C-chart - (Montgomery, pg.309, section 7.3.1):\n",
    "    # In Poisson distribution, mean and variance are both equal to c.\n",
    "    # UCL = c_bar + 3(c_bar**(0.5))\n",
    "    # Center line = c_bar\n",
    "    # LCL = c_bar + 3(c_bar**(0.5))\n",
    "    # where c_bar is the mean c among all subgroups (labels)\n",
    "    \n",
    "    def chart_c (dictionary, column_with_variable_to_be_analyzed):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # CONTROL LIMIT EQUATIONS:\n",
    "        # c-chart: control chart for counts of occurrences per unit\n",
    "        # c = sum = sum of subgroup occurrences\n",
    "        # cbar = (sum of subgroup occurrences)/(number of subgroups)\n",
    "        # n = subgroup size\n",
    "        # Lower control limit (LCL) = cbar - 3.sqrt(cbar)\n",
    "        # Upper control limit (UCL) = cbar + 3.sqrt(cbar)\n",
    "        # Here, the column that we want to evaluate is not the mean, but the sum.\n",
    "        # Since the graphics will be plotted using the column column_with_variable_to_be_analyzed\n",
    "        # Let's make this column equals to the column of sums:        \n",
    "        df[column_with_variable_to_be_analyzed] = df['sum_of_values_by_label']\n",
    "        number_of_labels = dictionary['count']\n",
    "        c_bar = (df['sum_of_values_by_label'].sum())/(number_of_labels)        \n",
    "        # calculate the upper control limit as cbar + 3.sqrt(cbar):\n",
    "        upper_cl = c_bar + 3 * ((c_bar)**(0.5))\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl\n",
    "        # calculate the lower control limit as cbar - 3.sqrt(cbar):\n",
    "        lower_cl = c_bar - 3 * ((c_bar)**(0.5))\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = c_bar\n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "    \n",
    "    # U-chart - (Montgomery, pg.314, section 7.3.1):\n",
    "    # If we find x total nonconformities in a sample of n inspection units, then the average \n",
    "    # number of nonconformities per inspection unit is: u = x/n\n",
    "    # UCL = u_bar + 3((u_bar/n)**(0.5))\n",
    "    # Center line = u_bar\n",
    "    # LCL = u_bar + 3((u_bar/n)**(0.5))\n",
    "    # where u_bar is the mean u among all subgroups (labels), but n is the individual subgroup size,\n",
    "    # making the chart limit not-constant depending on the subgroups' sizes\n",
    "    \n",
    "    def chart_u (dictionary, column_with_variable_to_be_analyzed):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # CONTROL LIMIT EQUATIONS:\n",
    "        # u-chart: control chart for average occurrence per unit\n",
    "        # u = mean =  (subgroup count of occurrences)/(subgroup size, in units)\n",
    "        # ubar = mean value of u\n",
    "        # n = subgroup size\n",
    "        # Lower control limit (LCL) = ubar - 3.sqrt(ubar/n)\n",
    "        # Upper control limit (UCL) = ubar + 3.sqrt(ubar/n)        \n",
    "        number_of_labels = dictionary['count']\n",
    "        count_per_label = df['count_of_elements_by_label']\n",
    "        u_bar = (df[column_with_variable_to_be_analyzed])/(number_of_labels)        \n",
    "        # calculate the upper control limit as ubar + 3.sqrt(ubar/n):\n",
    "        upper_cl = u_bar + 3 * ((u_bar/count_per_label)**(0.5))\n",
    "        # add a column 'upper_cl' on the dataframe with this value:\n",
    "        df['upper_cl'] = upper_cl\n",
    "        # calculate the lower control limit as ubar - 3.sqrt(ubar/n):\n",
    "        lower_cl = u_bar - 3 * ((u_bar/count_per_label)**(0.5))\n",
    "        # add a column 'lower_cl' on the dataframe with this value:\n",
    "        df['lower_cl'] = lower_cl\n",
    "        # Add a column with the mean value of the considered interval:\n",
    "        df['center'] = u_bar\n",
    "        # Update the dataframe in the dictionary:\n",
    "        dictionary['df'] = df\n",
    "        return dictionary\n",
    "\n",
    "    # RARE EVENTS\n",
    "    # ATTENTION: Due not group data in this case. Since events are rare, they are likely to be \n",
    "    # eliminated during aggregation.\n",
    "    # Usually, similarly to what is observed for the log-normal, data that follow the geometric\n",
    "    # distribution is highly skewed, so the mean is not a good estimator for the central line.\n",
    "    # Usually, it is better to apply the median = 0.693 x mean.\n",
    "    \n",
    "    # G-Chart: counts of total occurences between successive rare events occurences.\n",
    "    # https://www.spcforexcel.com/knowledge/attribute-control-charts/g-control-chart\n",
    "    # (Montgomery, pg.317, section 7.3.1)\n",
    "    \n",
    "    # e.g. total of patients between patient with ulcer.\n",
    "    # The probability model that they use for the geometric distribution is p(x) = p(1-p)**(x-a)\n",
    "    # where a is the known minimum possible number of events.\n",
    "    # The number of units between defectives is modelled by the geometric distribution.\n",
    "    # So, the G-control chart plots counting of occurrence by number, time unit, or timestamp.\n",
    "    # Geometric distribution is highly skewed, thus the median is a better representation of the \n",
    "    # process centre to be used with the runs analysis.\n",
    "\n",
    "    # Let y = total count of events between successive occurences of the rare event.\n",
    "    \n",
    "    # g_bar = median(y) = 0.693 x mean(y)\n",
    "    # One can estimate the control limits as g_bar + 3(((g_bar)*(g_bar+1))**(0.5)) and\n",
    "    # g_bar - 3(((g_bar)*(g_bar+1))**(0.5)), and central line = g_bar.\n",
    "    # A better approach takes into account the probabilities associated to the geometric (g)\n",
    "    # distribution.\n",
    "    \n",
    "    # In the probability approach, we start by calculating the value of p, which is an event\n",
    "    # probability:\n",
    "    # p = (1/(g_bar + 1))*((N-1)/N), where N is the total of individual values of counting between\n",
    "    # successive rare events. So, if we have 3 rare events, A, B, and C, we have two values of \n",
    "    # counting between rare events, from A to B, and from B to C. In this case, since we have two\n",
    "    # values that follow the G distribution, N = 2.\n",
    "    # Let alpha_UCL a constant dependent on the number of sigma limits for the control limits. For\n",
    "    # the usual 3 sigma limits, the value of alpha_UCL = 0.00135. With this constant, we can\n",
    "    # calculate the control limits and central line\n",
    "    \n",
    "    # UCL = ((ln(0.00135))/(ln(1 - p))) - 1\n",
    "    # Center line = ((ln(0.5))/(ln(1 - p))) - 1\n",
    "    # LCL = max(0, (((ln(1 - 0.00135))/(ln(1 - p))) - 1))\n",
    "    # where max represent the maximum among the two values in parentheses; and ln is the natural\n",
    "    # logarithm (base e), sometimes referred as log (inverse of the exponential): ln(exp(x)) = x\n",
    "    # = log(exp(x)) = x\n",
    "    \n",
    "    # t-chart: timedelta between rare events.\n",
    "    # (Montgomery, pg.324, section 7.3.5):\n",
    "    # https://www.qimacros.com/control-chart-formulas/t-chart-formula/\n",
    "    \n",
    "    # But instead of displaying the number of cases between events (defectives) it displays \n",
    "    # the time between events.\n",
    "    \n",
    "    # Nelson (1994) has suggested solving this problem by transforming the exponential random\n",
    "    # variable to a Weibull random variable such that the resulting Weibull distribution is well\n",
    "    # approximated by the normal distribution. \n",
    "    # If y represents the original exponential random variable (timedelta between successive rare\n",
    "    # event occurence), the appropriate transformation is:\n",
    "    # x = y**(1/3.6)= y**(0.2777)\n",
    "    # where x is treated as a normal distributed variable, and the control chart for x is the I-MR.\n",
    "    # So: 1. transform y into x;\n",
    "    # 2. Calculate the control limits for x as if it was a chart for individuals;\n",
    "    # reconvert the values to the original scale doing: y = x**(3.6)\n",
    "    \n",
    "    # We calculate for the transformed values x the moving ranges R and the mean of the \n",
    "    # mean values calculated for the 2-elements consecutive subgroups x_bar_bar, in the exact same way\n",
    "    # we would do for any I-MR chart\n",
    "    # UCL_transf =  x_bar_bar + 3*(1/d2)*R_bar\n",
    "    # LCL_transf =  x_bar_bar - 3*(1/d2)*R_bar\n",
    "    # Center line_transf = x_bar_bar\n",
    "    \n",
    "    # These values are in the transformed scale. So, we must reconvert them to the original scale for\n",
    "    # obtaining the control limits and central line:\n",
    "    # UCL = (UCL_transf)**(3.6)\n",
    "    # LCL = (LCL_transf)**(3.6)\n",
    "    # Center line = (Center line_transf)**(3.6)\n",
    "    \n",
    "    # Notice that this procedure naturally corrects the deviations caused by the skewness of the \n",
    "    # distribution. Actually, log and exponential transforms tend to reduce the skewness and to \n",
    "    # normalize the data.\n",
    "    \n",
    "    def rare_events_chart (dictionary, column_with_variable_to_be_analyzed, rare_event_indication, rare_event_timedelta_unit, chart_to_use):\n",
    "        # access the dataframe:\n",
    "        df = dictionary['df']\n",
    "        # Filter df to the rare events:\n",
    "        rare_events_df = df.copy(deep = True)\n",
    "        rare_events_df = rare_events_df[rare_events_df[column_with_variable_to_be_analyzed] == rare_event_indication]\n",
    "        # rare_events_df stores only the entries for rare events.\n",
    "        # Let's get a list of the indices of these entries (we did not reset the index):\n",
    "        rare_events_indices = list(rare_events_df.index)        \n",
    "        # Start lists for storing the count of events between the rares and the time between\n",
    "        # the rare events. Start both lists from np.nan, since we do not have information of\n",
    "        # any rare event before the first one registered (np.nan is float).\n",
    "        count_between_rares = [np.nan]\n",
    "        timedelta_between_rares = [np.nan]\n",
    "        # Check if the times are datetimes or not:\n",
    "        column_data_type = df[timestamp_tag_column].dtype        \n",
    "        if (column_data_type not in numeric_dtypes):            \n",
    "            # It is a datetime. Let's loop between successive indices:\n",
    "            if (len(rare_events_indices) > 1):            \n",
    "                for i in range(0, (len(rare_events_indices)-1)):\n",
    "                    # get the timedelta:\n",
    "                    index_i = rare_events_indices[i]\n",
    "                    index_i_plus = rare_events_indices[i + 1]        \n",
    "                    t_i = pd.Timestamp((df[timestamp_tag_column])[index_i], unit = 'ns')\n",
    "                    t_i_plus = pd.Timestamp((df[timestamp_tag_column])[index_i_plus], unit = 'ns')        \n",
    "                    # to slice a dataframe from row i to row j (including j): df[i:(j+1)]\n",
    "                    count_between_rares = (df[(index_i + 1): index_i_plus]).count()\n",
    "                    # We sliced the dataframe from index_i + 1 not to include the rare\n",
    "                    # event, so we started from the next one. Also, the last element is\n",
    "                    # of index index_i_plus - 1, the element before the next rare.\n",
    "                    count_between_rares.append(count_between_rares)        \n",
    "                    # Calculate the timedelta:\n",
    "                    # Convert to an integer representing the total of nanoseconds:\n",
    "                    timedelta = pd.Timedelta(t_i_plus - t_i).delta\n",
    "                    if (rare_event_timedelta_unit == 'year'):\n",
    "                        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "                        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "                        timedelta = timedelta / (10**9) #in seconds\n",
    "                        #2. Convert it to minutes (1 min = 60 s):\n",
    "                        timedelta = timedelta / 60.0 #in minutes\n",
    "                        #3. Convert it to hours (1 h = 60 min):\n",
    "                        timedelta = timedelta / 60.0 #in hours\n",
    "                        #4. Convert it to days (1 day = 24 h):\n",
    "                        timedelta = timedelta / 24.0 #in days\n",
    "                        #5. Convert it to years. 1 year = 365 days + 6 h = 365 days + 6/24 h/(h/day)\n",
    "                        # = (365 + 1/4) days = 365.25 days\n",
    "                        timedelta = timedelta / (365.25) #in years\n",
    "                        #The .0 after the numbers guarantees a float division.\n",
    "                    elif (rare_event_timedelta_unit == 'month'):\n",
    "                        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "                        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "                        timedelta = timedelta / (10**9) #in seconds\n",
    "                        #2. Convert it to minutes (1 min = 60 s):\n",
    "                        timedelta = timedelta / 60.0 #in minutes\n",
    "                        #3. Convert it to hours (1 h = 60 min):\n",
    "                        timedelta = timedelta / 60.0 #in hours\n",
    "                        #4. Convert it to days (1 day = 24 h):\n",
    "                        timedelta = timedelta / 24.0 #in days\n",
    "                        #5. Convert it to months. Consider 1 month = 30 days\n",
    "                        timedelta = timedelta / (30.0) #in months\n",
    "                        #The .0 after the numbers guarantees a float division.\n",
    "                    elif (rare_event_timedelta_unit == 'day'):\n",
    "                        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "                        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "                        timedelta = timedelta / (10**9) #in seconds\n",
    "                        #2. Convert it to minutes (1 min = 60 s):\n",
    "                        timedelta = timedelta / 60.0 #in minutes\n",
    "                        #3. Convert it to hours (1 h = 60 min):\n",
    "                        timedelta = timedelta / 60.0 #in hours\n",
    "                        #4. Convert it to days (1 day = 24 h):\n",
    "                        timedelta = timedelta / 24.0 #in days\n",
    "                    elif (rare_event_timedelta_unit == 'hour'):\n",
    "                        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "                        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "                        timedelta = timedelta / (10**9) #in seconds\n",
    "                        #2. Convert it to minutes (1 min = 60 s):\n",
    "                        timedelta = timedelta / 60.0 #in minutes\n",
    "                        #3. Convert it to hours (1 h = 60 min):\n",
    "                        timedelta = timedelta / 60.0 #in hours\n",
    "                    elif (rare_event_timedelta_unit == 'minute'):\n",
    "                        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "                        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "                        timedelta = timedelta / (10**9) #in seconds\n",
    "                        #2. Convert it to minutes (1 min = 60 s):\n",
    "                        timedelta = timedelta / 60.0 #in minutes\n",
    "                    elif (rare_event_timedelta_unit == 'second'):\n",
    "                        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "                        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "                        timedelta = timedelta / (10**9) #in seconds\n",
    "                    else:\n",
    "                        timedelta = timedelta # nanoseconds.  \n",
    "                    # Append the timedelta to the list:\n",
    "                    timedelta_between_rares.append(timedelta)\n",
    "            else:\n",
    "                # There is a single rare event.\n",
    "                print(\"There is a single rare event. Impossible to calculate timedeltas and counting between rare events.\\n\")\n",
    "                return {}\n",
    "        else: \n",
    "            # The column is not a timestamp. Simply subtract the values to calculate the\n",
    "            # timedeltas:\n",
    "            # It is a datetime. Let's loop between successive indices:\n",
    "            if (len(rare_events_indices) > 1):            \n",
    "                for i in range(0, (len(rare_events_indices)-1)):\n",
    "                    # get the timedelta:\n",
    "                    index_i = rare_events_indices[i]\n",
    "                    index_i_plus = rare_events_indices[i + 1]\n",
    "                    t_i = (df[timestamp_tag_column])[index_i]\n",
    "                    t_i_plus = (df[timestamp_tag_column])[index_i_plus]\n",
    "                    timedelta = (t_i_plus - t_i)\n",
    "                    # to slice a dataframe from row i to row j (including j): df[i:(j+1)] \n",
    "                    count_between_rares = (df[(index_i + 1): index_i_plus]).count()\n",
    "                    count_between_rares.append(count_between_rares)\n",
    "                    timedelta_between_rares.append(timedelta)\n",
    "            else:\n",
    "                # There is a single rare event.\n",
    "                print(\"There is a single rare event. Impossible to calculate timedeltas and counting between rare events.\\n\")\n",
    "                return {}\n",
    "        # Notice that the lists still have one element less than the dataframe of rares.\n",
    "        # That is because we do not have data for accounting for the next rare event. So,\n",
    "        # append np.nan to both lists:\n",
    "        count_between_rares.append(np.nan)\n",
    "        timedelta_between_rares.append(np.nan)\n",
    "        # Now, lists have the same total elements of the rare_events_df, and can be\n",
    "        # added as columns:        \n",
    "        # firstly, reset the index:\n",
    "        rare_events_df = rare_events_df.reset_index(drop = True)\n",
    "        # Add the columns:\n",
    "        rare_events_df['count_between_rares'] = count_between_rares\n",
    "        rare_events_df['timedelta_between_rares'] = timedelta_between_rares            \n",
    "        # Now, make the rares dataframe the df itself:\n",
    "        df = rare_events_df        \n",
    "        if (chart_to_use == 'g'):            \n",
    "            # Here, the column that we want to evaluate is not the mean, but the 'count_between_rares'.\n",
    "            # Since the graphics will be plotted using the column column_with_variable_to_be_analyzed\n",
    "            # Let's make this column equals to the column 'count_between_rares':\n",
    "            df[column_with_variable_to_be_analyzed] = df['count_between_rares']        \n",
    "            g_bar = df['count_between_rares'].median()\n",
    "            n_samples = df['count_between_rares'].count()\n",
    "            p = (1/(g_bar + 1))*((n_samples - 1)/n_samples)\n",
    "            # np.log = natural logarithm\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.log.html\n",
    "            center = ((np.log(0.5))/(np.log(1 - p))) - 1        \n",
    "            # calculate the upper control limit as log(0.00135)/log(1-p)-1:\n",
    "            upper_cl = ((np.log(0.00135))/(np.log(1 - p))) - 1\n",
    "            # add a column 'upper_cl' on the dataframe with this value:\n",
    "            df['upper_cl'] = upper_cl        \n",
    "            # calculate the lower control limit as Max(0, log(1-0.00135)/log(1-p)-1):\n",
    "            lower_cl = max(0, ((np.log(1 - 0.00135))/(log(1 - p)) - 1))\n",
    "            # add a column 'lower_cl' on the dataframe with this value:\n",
    "            df['lower_cl'] = lower_cl        \n",
    "            # Add a column with the mean value of the considered interval:\n",
    "            df['center'] = center\n",
    "            # Update the dataframe in the dictionary:\n",
    "            dictionary['df'] = df \n",
    "        elif (chart_to_use == 't'):\n",
    "            # Here, the column that we want to evaluate is not the mean, but the 'timedelta_between_rares'.\n",
    "            # Since the graphics will be plotted using the column column_with_variable_to_be_analyzed\n",
    "            # Let's make this column equals to the column 'timedelta_between_rares':\n",
    "            df[column_with_variable_to_be_analyzed] = df['timedelta_between_rares']        \n",
    "            # Create the transformed series:\n",
    "            # y = df['timedelta_between_rares']\n",
    "            # y_transf = y**(1/3.6)\n",
    "            y_transf = (df['timedelta_between_rares'])**(1/(3.6))\n",
    "            # Now, let's create an I-MR chart for y_transf        \n",
    "            moving_range = [abs(max((y_transf[i]), (y_transf[(i-1)])) - min((y_transf[i]), (y_transf[(i-1)]))) for i in range (1, len(y_transf))]\n",
    "            y_bar = [(y_transf[i] + y_transf[(i-1)])/2 for i in range (1, len(y_transf))]\n",
    "            # These lists were created from index 1. We must add a initial element to\n",
    "            # make their sizes equal to the original dataset length\n",
    "            # Start the list to store the moving ranges, containing only the number 0\n",
    "            # for the moving range (by simple concatenation):\n",
    "            moving_range = [0] + moving_range\n",
    "            # Start the list that stores the mean values of the 2-elements subgroups\n",
    "            # with the first element itself (index 0):  - list y_bar:\n",
    "            y_bar = [y_transf[0]] + y_bar\n",
    "            # Convert the lists to NumPy arrays, for performing vectorial (element-wise)\n",
    "            # operations:\n",
    "            moving_range = np.array(moving_range)\n",
    "            y_bar = np.array(y_bar)        \n",
    "            # Get the mean values from y_bar list and moving_range:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.average.html\n",
    "            y_bar_bar = np.average(y_bar)\n",
    "            r_bar = np.average(moving_range)\n",
    "            # Get the control chart constant A2 from the dictionary, considering n = 2 the\n",
    "            # number of elements of each subgroup:\n",
    "            dict_to_access = get_constants (number_of_labels = 2)\n",
    "            control_chart_constant = dict_to_access['1/d2']\n",
    "            control_chart_constant = control_chart_constant * 3\n",
    "            # calculate the upper control limit as y_bar_bar + (3/d2)r_bar:\n",
    "            upper_cl_transf = y_bar_bar + (control_chart_constant) * (r_bar)                \n",
    "            # calculate the lower control limit as y_bar_bar - (3/d2)r_bar:\n",
    "            lower_cl_transf = y_bar_bar - (control_chart_constant) * (r_bar)        \n",
    "            # Notice that these values are for the transformed variables:\n",
    "            # y_transf = (df['timedelta_between_rares'])**(1/(3.6))\n",
    "            # To reconvert to the correct time scale, we reverse this transform as:\n",
    "            # (y_transf)**(3.6)        \n",
    "            # add a column 'upper_cl' on the dataframe with upper_cl_transf\n",
    "            # converted to the original scale:\n",
    "            df['upper_cl'] = (upper_cl_transf)**(3.6)\n",
    "            # add a column 'lower_cl' on the dataframe with lower_cl_transf\n",
    "            # converted to the original scale:\n",
    "            df['lower_cl'] = (lower_cl_transf)**(3.6)\n",
    "            # Finally, add the central line by reconverting y_bar_bar to the\n",
    "            # original scale:\n",
    "            df['center'] = (y_bar_bar)**(3.6)\n",
    "            # Notice that this procedure naturally corrects the deviations caused by\n",
    "            # the skewness of the distribution. Actually, log and exponential transforms\n",
    "            # tend to reduce the skewness and to normalize the data.\n",
    "            # Update the dataframe in the dictionary:\n",
    "            dictionary['df'] = df\n",
    "        return dictionary\n",
    "        \n",
    "    \n",
    "    if (use_spc_chart_assistant == True):\n",
    "        \n",
    "        # Run if it is True. Requires TensorFlow to load. Load the extra library only\n",
    "        # if necessary:\n",
    "        # To show the Python class attributes, use the __dict__ method:\n",
    "        # http://www.learningaboutelectronics.com/Articles/How-to-display-all-attributes-of-a-class-or-instance-of-a-class-in-Python.php#:~:text=So%20the%20__dict__%20method%20is%20a%20very%20useful,other%20data%20type%20such%20as%20a%20class%20itself.\n",
    "\n",
    "        # instantiate the object\n",
    "        assistant = spc_chart_assistant()\n",
    "        # Download the images:\n",
    "        assistant = assistant.download_assistant_imgs()\n",
    "\n",
    "        # Run the assistant:\n",
    "        while (assistant.keep_assistant_on == True):\n",
    "\n",
    "            # Run the wrapped function until the user tells you to stop:\n",
    "            # Notice that both variables are True for starting the first loop:\n",
    "            assistant = assistant.open_chart_assistant_screen()\n",
    "\n",
    "        # Delete the images\n",
    "        assistant.delete_assistant_imgs()\n",
    "        # Select the chart and the parameters:\n",
    "        chart_to_use, column_with_labels_or_subgroups, consider_skewed_dist_when_estimating_with_std, column_with_variable_to_be_analyzed, timestamp_tag_column, column_with_event_frame_indication, rare_event_timedelta_unit, rare_event_indication = assistant.chart_selection()\n",
    "\n",
    "    # Back to the main code, independently on the use of the assistant:    \n",
    "    # set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    # Start a list unique_labels containing only element 0:\n",
    "    unique_labels = [0]\n",
    "    # If there is a column of labels or subgroups, this list will be updated. So we can control\n",
    "    # the chart selection using the length of this list: if it is higher than 1, we have subgroups,\n",
    "    # not individual values.\n",
    "\n",
    "    if (timestamp_tag_column is None):\n",
    "\n",
    "        # use the index itself:\n",
    "        timestamp_tag_column = 'index'\n",
    "        DATASET[timestamp_tag_column] = DATASET.index\n",
    "\n",
    "    elif (DATASET[timestamp_tag_column].dtype not in numeric_dtypes):\n",
    "\n",
    "        # The timestamp_tag_column was read as an object, indicating that it is probably a timestamp.\n",
    "        # Try to convert it to datetime64:\n",
    "        try:\n",
    "            DATASET[timestamp_tag_column] = (DATASET[timestamp_tag_column]).astype(np.datetime64)\n",
    "            print(f\"Variable {timestamp_tag_column} successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "        except:\n",
    "            # Simply ignore it\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    # Check if there are subgroups or if values are individuals. Also, check if there is no selected\n",
    "    # chart:\n",
    "    if ((column_with_labels_or_subgroups is None) & (chart_to_use is None)):\n",
    "            \n",
    "        print(\"There are only individual observations and no valid chart selected, so using standard error as natural variation (control limits).\\n\")\n",
    "        # values are individual, so set it as so:\n",
    "        chart_to_use = 'std_error'\n",
    "        \n",
    "    elif ((chart_to_use is None) | (chart_to_use not in ['std_error', '3s_as_natural_variation', 'i_mr', 'xbar_s', 'np', 'p', 'u', 'c', 'g', 't'])):\n",
    "            \n",
    "        print(\"No valid chart selected, so using standard error as natural variation (control limits).\\n\")\n",
    "        # values are individual, so set it as so:\n",
    "        chart_to_use = 'std_error'\n",
    "        \n",
    "    elif (chart_to_use in ['g', 't']):\n",
    "            \n",
    "        if (rare_event_timedelta_unit is None):\n",
    "            print(\"No valid timedelta unit provided, so selecting \\'day\\' for analysis of rare events.\\n\")\n",
    "            rare_event_timedelta_unit = 'day'\n",
    "        elif (rare_event_timedelta_unit not in ['day', 'second', 'nanosecond', 'milisecond', 'hour', 'week', 'month', 'year']):\n",
    "            print(\"No valid timedelta unit provided, so selecting \\'day\\' for analysis of rare events.\\n\")\n",
    "            rare_event_timedelta_unit = 'day'\n",
    "            \n",
    "        if (rare_event_indication is None):\n",
    "            print(\"No rare event indication provided, so changing the plot to \\'std_error\\'.\\n\")\n",
    "            chart_to_use = 'std_error'\n",
    "                \n",
    "    # Now, a valid chart_to_use was selected and is saved as chart_to_use.\n",
    "    # We can sort the dataframe according to the columns present:\n",
    "          \n",
    "    if ((column_with_labels_or_subgroups is not None) & (column_with_event_frame_indication is not None)):\n",
    "        # There are time windows to consider and labels.\n",
    "        # Update the list of unique labels:\n",
    "        unique_labels = list(DATASET[column_with_labels_or_subgroups].unique())\n",
    "        \n",
    "        # sort DATASET by timestamp_tag_column, column_with_labels_or_subgroups, \n",
    "        # column_with_event_frame_indication, and column_with_variable_to_be_analyzed,\n",
    "        # all in Ascending order:\n",
    "        DATASET = DATASET.sort_values(by = [timestamp_tag_column, column_with_labels_or_subgroups, column_with_event_frame_indication, column_with_variable_to_be_analyzed], ascending = [True, True, True, True])\n",
    "        \n",
    "    elif (column_with_event_frame_indication is not None):\n",
    "        # We already tested the simultaneous presence of both. So, to reach this condition, \n",
    "        # there is no column_with_labels_or_subgroups, but there is column_with_event_frame_indication\n",
    "            \n",
    "        # sort DATASET by timestamp_tag_column, column_with_event_frame_indication, \n",
    "        # and column_with_variable_to_be_analyzed, all in Ascending order:\n",
    "        DATASET = DATASET.sort_values(by = [timestamp_tag_column, column_with_event_frame_indication, column_with_variable_to_be_analyzed], ascending = [True, True, True])\n",
    "        \n",
    "    elif (column_with_labels_or_subgroups is not None):\n",
    "        # We already tested the simultaneous presence of both. So, to reach this condition, \n",
    "        # there is no column_with_event_frame_indication, but there is column_with_labels_or_subgroups\n",
    "        # Update the list of unique labels:\n",
    "        unique_labels = list(DATASET[column_with_labels_or_subgroups].unique())\n",
    "        \n",
    "        # sort DATASET by timestamp_tag_column, column_with_labels_or_subgroups, \n",
    "        # and column_with_variable_to_be_analyzed, all in Ascending order:\n",
    "        DATASET = DATASET.sort_values(by = [timestamp_tag_column, column_with_labels_or_subgroups, column_with_variable_to_be_analyzed], ascending = [True, True, True])\n",
    "        \n",
    "    else:\n",
    "        # There is neither column_with_labels_or_subgroups, nor column_with_event_frame_indication\n",
    "        # sort DATASET by timestamp_tag_column, and column_with_variable_to_be_analyzed, \n",
    "        # both in Ascending order:\n",
    "        DATASET = DATASET.sort_values(by = [timestamp_tag_column, column_with_variable_to_be_analyzed], ascending = [True, True])\n",
    "        \n",
    "    # Finally, reset indices:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # Start a list of dictionaries to store the dataframes and subdataframes that will be analyzed:\n",
    "    list_of_dictionaries_with_dfs = []\n",
    "    # By now, dictionaries will contain a key 'event_frame' storing an integer identifier for the\n",
    "    # event frame, starting from 0 (the index of the event in the list of unique event frames), or\n",
    "    # zero, for the cases where there is a single event; a key 'df' storing the dataframe object, \n",
    "    # a key 'mean', storing the mean value of column_with_variable_to_be_analyzed; keys 'std' and\n",
    "    # 'var', storing the standard deviation and the variance for column_with_variable_to_be_analyzed;\n",
    "    # and column 'count', storing the counting of rows. \n",
    "    # After obtaining the control limits, they will also get a key 'list_of_points_out_of_cl'. \n",
    "    # This key will store a list of dictionaries (nested JSON), where each dictionary \n",
    "    # will have two keys, 'x', and 'y', with the coordinates of the point outside control \n",
    "    # limits as the values.\n",
    "    \n",
    "    if ((column_with_event_frame_indication is not None) & (chart_to_use not in ['g', 't'])):\n",
    "        \n",
    "        # Get a series of unique values of the event frame indications, and save it as a list \n",
    "        # using the list attribute:\n",
    "        unique_event_indication = list(DATASET[column_with_event_frame_indication].unique())\n",
    "        \n",
    "        print(f\"{len(unique_event_indication)} different values of event frame indication detected: {unique_event_indication}.\\n\")\n",
    "        \n",
    "        if (len(unique_event_indication) > 0):\n",
    "            \n",
    "            # There are more than one event frame to consider. Let's loop through the list of\n",
    "            # event frame indication, where each element is referred as 'event_frame':\n",
    "            for event_frame in unique_event_indication:\n",
    "                \n",
    "                # Define a boolean filter to select only the rows correspondent to event_frame:\n",
    "                boolean_filter = (DATASET[column_with_event_frame_indication] == event_frame)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by timestamp_tag_column and column_with_variable_to_be_analyzed, \n",
    "                # to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [timestamp_tag_column, column_with_variable_to_be_analyzed], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Store ds_copy in a dictionary of values. Put the index of event_frame\n",
    "                # in the list unique_event_indication as the value in key 'event_frame', to numerically\n",
    "                # identify the dictionary according to the event frame.\n",
    "                # Also, store the 'mean', 'sum,' 'std', 'var', and 'count' aggregates for the column\n",
    "                # column_with_variable_to_be_analyzed from ds_copy:\n",
    "                \n",
    "                dict_of_values = {\n",
    "                                    'event_frame': unique_event_indication.index(event_frame),\n",
    "                                    'df': ds_copy, \n",
    "                                    'center': ds_copy[column_with_variable_to_be_analyzed].mean(),\n",
    "                                    'sum': ds_copy[column_with_variable_to_be_analyzed].sum(),\n",
    "                                    'std': ds_copy[column_with_variable_to_be_analyzed].std(),\n",
    "                                    'var': ds_copy[column_with_variable_to_be_analyzed].var(),\n",
    "                                    'count': ds_copy[column_with_variable_to_be_analyzed].count()\n",
    "                                 }\n",
    "                \n",
    "                # append dict_of_values to the list:\n",
    "                list_of_dictionaries_with_dfs.append(dict_of_values)\n",
    "                \n",
    "                # Now, the loop will pick the next event frame.\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # There is actually a single time window. The only dataframe \n",
    "            # stored in the dictionary is DATASET itself, which is stored with \n",
    "            # the aggregate statistics calculated for the whole dataframe. Since\n",
    "            # there is a single dataframe, the single value in 'event_frame' is 0.\n",
    "            dict_of_values = {\n",
    "                                'event_frame': 0,\n",
    "                                'df': DATASET, \n",
    "                                'center': DATASET[column_with_variable_to_be_analyzed].mean(),\n",
    "                                'sum': DATASET[column_with_variable_to_be_analyzed].sum(),\n",
    "                                'std': DATASET[column_with_variable_to_be_analyzed].std(),\n",
    "                                'var': DATASET[column_with_variable_to_be_analyzed].var(),\n",
    "                                'count': DATASET[column_with_variable_to_be_analyzed].count()\n",
    "                            }\n",
    "            \n",
    "            # append dict_of_values to the list:\n",
    "            list_of_dictionaries_with_dfs.append(dict_of_values)\n",
    "    \n",
    "    else:\n",
    "        # The other case where there is a single time window or 'g' or 't' charts. \n",
    "        # So, the only dataframe stored in the dictionary is DATASET itself, which is stored with \n",
    "        # the aggregate statistics calculated for the whole dataframe. Since\n",
    "        # there is a single dataframe, the single value in 'event_frame' is 0.\n",
    "        dict_of_values = {\n",
    "                            'event_frame': 0,\n",
    "                            'df': DATASET, \n",
    "                            'center': DATASET[column_with_variable_to_be_analyzed].mean(),\n",
    "                            'sum': DATASET[column_with_variable_to_be_analyzed].sum(),\n",
    "                            'std': DATASET[column_with_variable_to_be_analyzed].std(),\n",
    "                            'var': DATASET[column_with_variable_to_be_analyzed].var(),\n",
    "                            'count': DATASET[column_with_variable_to_be_analyzed].count()\n",
    "                        }\n",
    "            \n",
    "        # append dict_of_values to the list:\n",
    "        list_of_dictionaries_with_dfs.append(dict_of_values)\n",
    "    \n",
    "    # Now, data is sorted, timestamps were converted to datetime objects, and values collected\n",
    "    # for different timestamps were separated into dictionaries (elements) from the list\n",
    "    # list_of_dictionaries_with_dfs. Each dictionary contains a key 'df' used to access the\n",
    "    # dataframe, as well as keys storing the aggregate statistics: 'mean', 'std', 'var', and\n",
    "    # 'count'.\n",
    "    \n",
    "    # Now, we can process the different control limits calculations.\n",
    "    \n",
    "    # Start a support list:\n",
    "    support_list = []\n",
    "    \n",
    "    ## INDIVIDUAL VALUES\n",
    "    # Use the unique_labels list to guarantee that there have to be more than 1 subgroup\n",
    "    # to not treat data as individual values. If there is a single subgroup, unique_labels\n",
    "    # will have a single element.\n",
    "    if ((column_with_labels_or_subgroups is None) | (len(unique_labels) <= 1)):\n",
    "        \n",
    "        if (chart_to_use == 'i_mr'):\n",
    "            \n",
    "            print(\"WARNING: the I-MR control limits are based on the strong hypothesis that data follows a normal distribution. If it is not the case, do not use this chart.\")\n",
    "            print(\"If you are not confident about the statistical distribution, select chart_to_use = \\'3s_as_natural_variation\\' to use 3 times the standard deviation as estimator for the natural variation (the control limits); or chart_to_use = \\'std_error\\' to use 3 times the standard error as control limits.\\n\")\n",
    "            \n",
    "            for dictionary in list_of_dictionaries_with_dfs:\n",
    "                \n",
    "                dictionary = chart_i_mr (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed)\n",
    "                # Append the updated dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "            \n",
    "            # Now that we finished looping through dictionaries, make list_of_dictionaries_with_dfs\n",
    "            # the support_list itself:\n",
    "            list_of_dictionaries_with_dfs = support_list\n",
    "        \n",
    "        elif (chart_to_use in ['g', 't']):\n",
    "            \n",
    "            print(f\"Analyzing occurence of rare event {rare_event_indication} through chart {chart_to_use}.\\n\")\n",
    "            \n",
    "            for dictionary in list_of_dictionaries_with_dfs:\n",
    "                \n",
    "                dictionary = rare_events_chart (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed, rare_event_indication = rare_event_indication, rare_event_timedelta_unit = rare_event_timedelta_unit, chart_to_use = chart_to_use)\n",
    "                # Finally, append the dictionary to the list and go to next iteration\n",
    "                support_list.append(dictionary)\n",
    "            \n",
    "            # Now that we finished looping through dictionaries, make list_of_dictionaries_with_dfs\n",
    "            # the support_list itself:\n",
    "            list_of_dictionaries_with_dfs = support_list\n",
    "            \n",
    "        elif (chart_to_use == '3s_as_natural_variation'):\n",
    "            \n",
    "            print(\"Using 3s (3 times the standard deviation) as estimator of the natural variation (control limits).\\n\")\n",
    "            \n",
    "            for dictionary in list_of_dictionaries_with_dfs:\n",
    "                \n",
    "                dictionary = chart_3s (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed, consider_skewed_dist_when_estimating_with_std = consider_skewed_dist_when_estimating_with_std)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "            \n",
    "            # Now that we finished looping through dictionaries, make list_of_dictionaries_with_dfs\n",
    "            # the support_list itself:\n",
    "            list_of_dictionaries_with_dfs = support_list\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Using 3 times the standard error as estimator of the natural variation (control limits).\\n\")\n",
    "            \n",
    "            for dictionary in list_of_dictionaries_with_dfs:\n",
    "                \n",
    "                dictionary = chart_std_error (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed, consider_skewed_dist_when_estimating_with_std = consider_skewed_dist_when_estimating_with_std)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "            \n",
    "            # Now that we finished looping through dictionaries, make list_of_dictionaries_with_dfs\n",
    "            # the support_list itself:\n",
    "            list_of_dictionaries_with_dfs = support_list\n",
    "    \n",
    "    ## DATA IN SUBGROUPS\n",
    "    else:\n",
    "        \n",
    "        # Loop through each dataframe:\n",
    "        for dictionary in list_of_dictionaries_with_dfs:\n",
    "            \n",
    "            dictionary = create_grouped_df (dictionary = dictionary)\n",
    "            # Now, dataframe is ready for the calculation of control limits.\n",
    "            # Let's select the appropriate chart:\n",
    "        \n",
    "            if (chart_to_use == '3s_as_natural_variation'):\n",
    "\n",
    "                dictionary = chart_3s (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed, consider_skewed_dist_when_estimating_with_std = consider_skewed_dist_when_estimating_with_std)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "            \n",
    "            elif (chart_to_use == 'std_error'):\n",
    "                \n",
    "                dictionary = chart_std_error (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed, consider_skewed_dist_when_estimating_with_std = consider_skewed_dist_when_estimating_with_std)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "                \n",
    "            elif (chart_to_use == 'xbar_s'):\n",
    "                \n",
    "                dictionary = chart_x_bar_s (dictionary = dictionary)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "            \n",
    "            elif (chart_to_use == 'p'):\n",
    "                \n",
    "                dictionary = chart_p (dictionary = dictionary)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "\n",
    "            elif (chart_to_use == 'np'):\n",
    "            \n",
    "                dictionary = chart_np (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "\n",
    "            elif (chart_to_use == 'c'):\n",
    "                \n",
    "                dictionary = chart_c (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "                \n",
    "            elif (chart_to_use == 'u'):\n",
    "                \n",
    "                dictionary = chart_u (dictionary = dictionary, column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed)\n",
    "                # Append the dictionary to the support_list:\n",
    "                support_list.append(dictionary)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"Select a valid control chart: {['3s_as_natural_variation', 'std_error', 'i_mr', 'xbar_s', 'np', 'p', 'u', 'c', 'g', 't']}.\\n\")\n",
    "                return \"error\", \"error\" # Two, since two dataframes are returned\n",
    "            \n",
    "            # Go to the next element (dictionary) from the list list_of_dictionaries_with_dfs\n",
    "        \n",
    "        # Now that we finished looping through dictionaries, make list_of_dictionaries_with_dfs\n",
    "        # the support_list itself:\n",
    "        list_of_dictionaries_with_dfs = support_list\n",
    "        \n",
    "        # Now we finished looping, we can print the warnings\n",
    "        if (chart_to_use == '3s_as_natural_variation'):\n",
    "    \n",
    "            print(\"Using 3s (3 times the standard deviation) as estimator of the natural variation (control limits). Remember that we are taking the standard deviation from the subgroup (label) aggregates.\\n\")\n",
    "        \n",
    "        elif (chart_to_use == 'xbar_s'):\n",
    "            \n",
    "            print(\"WARNING: the X-bar-S control limits are based on the strong hypothesis that the mean values from the subgroups follow a normal distribution. If it is not the case, do not use this chart.\")\n",
    "            print(\"If you are not confident about the statistical distribution, select chart_to_use = \\'3s_as_natural_variation\\' to use 3 times the standard deviation as estimator for the natural variation (the control limits).\\n\")\n",
    "            print(\"Use this chart for analyzing mean values from multiple data collected together in groups (or specific labels), usually in close moments.\\n\")\n",
    "        \n",
    "        elif (chart_to_use == 'np'):\n",
    "            print(\"WARNING: the U control limits are based on the strong hypothesis that the counting of values from the subgroups follow a Poisson distribution (Poisson is a case from the Gamma distribution). If it is not the case, do not use this chart.\")\n",
    "            \n",
    "        \n",
    "        elif (chart_to_use == 'p'):\n",
    "            print(\"WARNING: the U control limits are based on the strong hypothesis that the counting of values from the subgroups follow a Poisson distribution (Poisson is a case from the Gamma distribution). If it is not the case, do not use this chart.\")\n",
    "            \n",
    "        \n",
    "        elif (chart_to_use == 'u'):\n",
    "            \n",
    "            print(\"WARNING: the U control limits are based on the strong hypothesis that the counting of values from the subgroups follow a Poisson distribution (Poisson is a case from the Gamma distribution). If it is not the case, do not use this chart.\")\n",
    "            print(\"If you are not confident about the statistical distribution, select chart_to_use = \\'3s_as_natural_variation\\' to use 3 times the standard deviation as estimator for the natural variation (the control limits).\\n\")\n",
    "        \n",
    "        else:\n",
    "            # chart_to_use == 'c'\n",
    "            print(\"WARNING: the C control limits are based on the strong hypothesis that the counting of values from the subgroups follow a Poisson distribution (Poisson is a case from the Gamma distribution). If it is not the case, do not use this chart.\")\n",
    "            print(\"If you are not confident about the statistical distribution, select chart_to_use = \\'3s_as_natural_variation\\' to use 3 times the standard deviation as estimator for the natural variation (the control limits).\\n\")\n",
    "        \n",
    "        \n",
    "    # Now, we have all the control limits calculated, and data aggregated when it is the case.\n",
    "    # Let's merge (append - SQL UNION) all the dataframes stored in the dictionaries (elements)\n",
    "    # from the list list_of_dictionaries_with_dfs. Pick the first dictionary, i.e., element of\n",
    "    # index 0 from the list:\n",
    "    \n",
    "    dictionary = list_of_dictionaries_with_dfs[0]\n",
    "    # access the dataframe:\n",
    "    df = dictionary['df']\n",
    "    \n",
    "    # Also, start a list for storing the timestamps where the event frames start:\n",
    "    time_of_event_frame_start = []\n",
    "    \n",
    "    # Now, let's loop through each one of the other dictionaries from the list:\n",
    "    \n",
    "    for i in range (1, len(list_of_dictionaries_with_dfs)):\n",
    "        \n",
    "        # start from i = 1, index of the second element (we already picked the first one),\n",
    "        # and goes to the index of the last dictionary, len(list_of_dictionaries_with_dfs) - 1:\n",
    "        \n",
    "        # access element (dictionary) i from the list:\n",
    "        dictionary_i = list_of_dictionaries_with_dfs[i]\n",
    "        # access the dataframe i:\n",
    "        df_i = dictionary_i['df']\n",
    "        \n",
    "        # Pick the first element (index 0) of the series timestamp_tag_column:\n",
    "        time_tag_value = (df_i[timestamp_tag_column])[0]\n",
    "        # Append this value on the list time_of_event_frame_start:\n",
    "        time_of_event_frame_start.append(time_tag_value)\n",
    "        \n",
    "        # Append df_i to df (SQL UNION - concatenate or append rows):\n",
    "        # Save in df itself:\n",
    "        df = pd.concat([df, df_i], axis = 0, ignore_index = True, sort = True, join = 'inner')\n",
    "        # axis = 0 to append rows; 'inner' join removes missing values\n",
    "    \n",
    "    # Now, reset the index of the final concatenated dataframe, containing all the event frames\n",
    "    # separately processed, and containing the control limits and mean values for each event:\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    # Now, add a column to inform if each point is in or out of the control limits.\n",
    "    # apply a filter to select each situation:\n",
    "    # (syntax: dataset.loc[dataset['column_filtered'] <= 0.87, 'labelled_column'] = 1)\n",
    "    # Start the new column as 'in_control_lim' (default case):\n",
    "    df['control_limits_check'] = 'in_control_lim'\n",
    "    \n",
    "    # Now modify only points which are out of the control ranges:\n",
    "    df.loc[(df[column_with_variable_to_be_analyzed] < lower_cl), 'control_limits_check'] = 'below_lower_control_lim'\n",
    "    df.loc[(df[column_with_variable_to_be_analyzed] > upper_cl), 'control_limits_check'] = 'above_upper_control_lim'\n",
    "                \n",
    "    # Let's also create the 'red_df' containing only the values outside of the control limits.\n",
    "    # This dataframe will be used to highlight the values outside the control limits\n",
    "    \n",
    "    # copy the dataframe:\n",
    "    red_df = df.copy(deep = True)\n",
    "    # Filter red_df to contain only the values outside the control limits:\n",
    "    boolean_filter = ((red_df[column_with_variable_to_be_analyzed] < lower_cl) | (red_df[column_with_variable_to_be_analyzed] > upper_cl))\n",
    "    red_df = red_df[boolean_filter]\n",
    "    # Reset the index of this dataframe:\n",
    "    red_df = red_df.reset_index(drop = True)\n",
    "    \n",
    "    if (len(red_df) > 0):\n",
    "        \n",
    "        # There is at least one row outside the control limits.\n",
    "        print(\"Attention! Point outside of natural variation (control limits).\")\n",
    "        print(\"Check the red_df dataframe returned for details on values outside the control limits. They occur at the following time values:\\n\")\n",
    "        print(list(red_df[timestamp_tag_column]))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # specification_limits = {'lower_spec_lim': value1, 'upper_spec_lim': value2}\n",
    "    \n",
    "    # Check if there is a lower specification limit:\n",
    "    if (specification_limits['lower_spec_lim'] is not None):\n",
    "        \n",
    "        lower_spec_lim = specification_limits['lower_spec_lim']\n",
    "        \n",
    "        # Now, add a column to inform if each point is in or out of the specification limits.\n",
    "        # apply a filter to select each situation:\n",
    "\n",
    "        # Start the new column as 'in_spec_lim' (default case):\n",
    "        df['spec_limits_check'] = 'in_spec_lim'\n",
    "        # Now modify only points which are below the specification limit:\n",
    "        df.loc[(df[column_with_variable_to_be_analyzed] < lower_spec_lim), 'spec_limits_check'] = 'below_lower_spec_lim'\n",
    "    \n",
    "        if (len(df[(df[column_with_variable_to_be_analyzed] < lower_spec_lim)]) > 0):\n",
    "            \n",
    "            print(\"Attention! Point below lower specification limit.\")\n",
    "            print(\"Check the returned dataframe df to obtain more details.\\n\")\n",
    "        \n",
    "        # Check if there is an upper specification limit too:\n",
    "        if (specification_limits['upper_spec_lim'] is not None):\n",
    "\n",
    "            upper_spec_lim = specification_limits['upper_spec_lim']\n",
    "\n",
    "            # Now modify only points which are above the specification limit:\n",
    "            df.loc[(df[column_with_variable_to_be_analyzed] > upper_spec_lim), 'spec_limits_check'] = 'above_upper_spec_lim'\n",
    "\n",
    "            if (len(df[(df[column_with_variable_to_be_analyzed] > upper_spec_lim)]) > 0):\n",
    "\n",
    "                print(\"Attention! Point above upper specification limit.\")\n",
    "                print(\"Check the returned dataframe df to obtain more details.\\n\")\n",
    "    \n",
    "    # Check the case where there is no lower, but there is a specification limit:\n",
    "    elif (specification_limits['upper_spec_lim'] is not None):\n",
    "        \n",
    "        upper_spec_lim = specification_limits['upper_spec_lim']\n",
    "        \n",
    "        # Start the new column as 'in_spec_lim' (default case):\n",
    "        df['spec_limits_check'] = 'in_spec_lim'\n",
    "        \n",
    "        # Now modify only points which are above the specification limit:\n",
    "        df.loc[(df[column_with_variable_to_be_analyzed] > upper_spec_lim), 'spec_limits_check'] = 'above_upper_spec_lim'\n",
    "\n",
    "        if (len(df[(df[column_with_variable_to_be_analyzed] > upper_spec_lim)]) > 0):\n",
    "\n",
    "            print(\"Attention! Point above upper specification limit.\")\n",
    "            print(\"Check the returned dataframe df to obtain more details.\\n\")\n",
    "    \n",
    "    \n",
    "    # Now, we have the final dataframe containing analysis regarding the control and specification\n",
    "    # limits, and the red_df, that will be used to plot red points for values outside the control\n",
    "    # range. Then, we can plot the graph.\n",
    "    \n",
    "    # Now, we can plot the figure.\n",
    "    # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "    # so that one series do not completely block the visualization of the other.\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        LINE_STYLE = '-'\n",
    "\n",
    "    else:\n",
    "        LINE_STYLE = ''\n",
    "        \n",
    "    if (add_scatter_dots == True):\n",
    "        MARKER = 'o'\n",
    "            \n",
    "    else:\n",
    "        MARKER = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "    if (plot_title is None):\n",
    "        \n",
    "        if (chart_to_use == 'g'):\n",
    "            plot_title = f\"{chart_to_use}_for_count_of_events_between_rare_occurence\"\n",
    "        \n",
    "        elif (chart_to_use == 't'):\n",
    "            plot_title = f\"{chart_to_use}_for_timedelta_between_rare_occurence\"\n",
    "        \n",
    "        else:\n",
    "            plot_title = f\"{chart_to_use}_for_{column_with_variable_to_be_analyzed}\"\n",
    "    \n",
    "    if ((column_with_labels_or_subgroups is None) | (len(unique_labels) <= 1)):\n",
    "        \n",
    "        # 'i_mr' or, 'std_error', or '3s_as_natural_variation' (individual measurements)\n",
    "        \n",
    "        LABEL = column_with_variable_to_be_analyzed\n",
    "        \n",
    "        if (chart_to_use == 'i_mr'):\n",
    "            LABEL_MEAN = 'mean'\n",
    "            \n",
    "        elif ((chart_to_use == '3s_as_natural_variation')|(chart_to_use == 'std_error')):\n",
    "            if(consider_skewed_dist_when_estimating_with_std):\n",
    "                LABEL_MEAN = 'median'\n",
    "\n",
    "            else:\n",
    "                LABEL_MEAN = 'mean'\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if (chart_to_use == 'g'):\n",
    "                LABEL = 'count_between\\nrare_events'\n",
    "                LABEL_MEAN = 'median'\n",
    "            \n",
    "            elif (chart_to_use == 't'):\n",
    "                LABEL = 'timedelta_between\\nrare_events'\n",
    "                LABEL_MEAN = 'central_line'\n",
    "                \n",
    "    else:\n",
    "        if ((chart_to_use == '3s_as_natural_variation')|(chart_to_use == 'std_error')):\n",
    "            \n",
    "            LABEL = \"mean_value\\nby_label\"\n",
    "            \n",
    "            if(consider_skewed_dist_when_estimating_with_std):\n",
    "                LABEL_MEAN = 'median'\n",
    "            else:\n",
    "                LABEL_MEAN = 'mean'\n",
    "        \n",
    "        elif (chart_to_use == 'xbar_s'):\n",
    "            \n",
    "            LABEL = \"mean_value\\nby_label\"\n",
    "            LABEL_MEAN = 'mean'\n",
    "            \n",
    "        elif (chart_to_use == 'np'):\n",
    "            \n",
    "            LABEL = \"total_occurences\\nby_label\"\n",
    "            LABEL_MEAN = 'sum_of_ocurrences'\n",
    "            \n",
    "        elif (chart_to_use == 'p'):\n",
    "            \n",
    "            LABEL = \"mean_value\\ngrouped_by_label\"\n",
    "            LABEL_MEAN = 'mean'\n",
    "        \n",
    "        elif (chart_to_use == 'u'):\n",
    "            \n",
    "            LABEL = \"mean_value\\ngrouped_by_label\"\n",
    "            LABEL_MEAN = 'mean'\n",
    "            \n",
    "        else:\n",
    "            # chart_to_use == 'c'\n",
    "            LABEL = \"total_occurences\\nby_label\"\n",
    "            LABEL_MEAN = 'average_sum\\nof_ocurrences'\n",
    "    \n",
    "    x = df[timestamp_tag_column]\n",
    "    \n",
    "    y = df[column_with_variable_to_be_analyzed]  \n",
    "    upper_control_lim = df['upper_cl']\n",
    "    lower_control_lim = df['lower_cl']\n",
    "    mean_line = df['center']\n",
    "    \n",
    "    if (specification_limits['lower_spec_lim'] is not None):\n",
    "        \n",
    "        lower_spec_lim = specification_limits['lower_spec_lim']\n",
    "    \n",
    "    else:\n",
    "        lower_spec_lim = None\n",
    "    \n",
    "    if (specification_limits['upper_spec_lim'] is not None):\n",
    "        \n",
    "        upper_spec_lim = specification_limits['upper_spec_lim']\n",
    "    \n",
    "    else:\n",
    "        upper_spec_lim = None\n",
    "    \n",
    "    if (len(red_df) > 0):\n",
    "        \n",
    "        red_x = red_df[timestamp_tag_column]\n",
    "        red_y = red_df[column_with_variable_to_be_analyzed]\n",
    "        \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "        \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    # Set graphic title\n",
    "    ax.set_title(plot_title) \n",
    "\n",
    "    if not (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "\n",
    "    if not (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    # Scatter plot of time series:\n",
    "    ax.plot(x, y, linestyle = LINE_STYLE, marker = MARKER, color = 'darkblue', alpha = OPACITY, label = LABEL)\n",
    "    # Axes.plot documentation:\n",
    "    # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "    # x and y are positional arguments: they are specified by their position in function\n",
    "    # call, not by an argument name like 'marker'.\n",
    "            \n",
    "    # Matplotlib markers:\n",
    "    # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "    \n",
    "    # Plot the mean line as a step function (values connected by straight splines, forming steps):\n",
    "    ax.step(x, y = mean_line, color = 'black', label = LABEL_MEAN)\n",
    "    \n",
    "    # Plot the control limits as step functions too:\n",
    "    ax.step(x, y = upper_control_lim, color = 'red', linestyle = 'dashed', label = 'control\\nlimit')\n",
    "    ax.step(x, y = lower_control_lim, color = 'red', linestyle = 'dashed')\n",
    "    \n",
    "    # If there are specifications, plot as horizontal constant lines (axhlines):\n",
    "    if (lower_spec_lim is not None):\n",
    "        \n",
    "        ax.axhline(lower_spec_lim, color = 'darkgreen', linestyle = 'dashed', label = 'specification\\nlimit')\n",
    "    \n",
    "    if (upper_spec_lim is not None):\n",
    "        \n",
    "        ax.axhline(upper_spec_lim, color = 'darkgreen', linestyle = 'dashed', label = 'specification\\nlimit')\n",
    "    \n",
    "    # If there are red points outside of control limits to highlight, plot them above the graph\n",
    "    # (plot as scatter plot, with no spline, and 100% opacity = 1.0):\n",
    "    if (len(red_df) > 0):\n",
    "        \n",
    "        ax.plot(red_x, red_y, linestyle = '', marker = 'o', color = 'firebrick', alpha = 1.0)\n",
    "    \n",
    "    # If the length of list time_of_event_frame_start is higher than zero,\n",
    "    # loop through each element on the list and add a vertical constant line for the timestamp\n",
    "    # correspondent to the beginning of an event frame:\n",
    "    \n",
    "    if (len(time_of_event_frame_start) > 0):\n",
    "        \n",
    "        for timestamp in time_of_event_frame_start:\n",
    "            # add timestamp as a vertical line (axvline):\n",
    "            ax.axvline(timestamp, color = 'darkblue', linestyle = 'dashed', label = 'event_frame\\nchange')\n",
    "            \n",
    "            \n",
    "    # Now we finished plotting all of the series, we can set the general configuration:\n",
    "    ax.grid(grid) # show grid or not\n",
    "    ax.legend()\n",
    "\n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = f\"control_chart_{chart_to_use}\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return df, red_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spc_chart_assistant:\n",
    "            \n",
    "    # Initialize instance attributes.\n",
    "    # define the Class constructor, i.e., how are its objects:\n",
    "    def __init__(self, assistant_startup = True, keep_assistant_on = True):\n",
    "                \n",
    "        import os\n",
    "        \n",
    "        # If the user passes the argument, use them. Otherwise, use the standard values.\n",
    "        # Set the class objects' attributes.\n",
    "        # Suppose the object is named assistant. We can access the attribute as:\n",
    "        # assistant.assistant_startup, for instance.\n",
    "        # So, we can save the variables as objects' attributes.\n",
    "        self.assistant_startup = assistant_startup\n",
    "        self.keep_assistant_on = keep_assistant_on\n",
    "        # Base Github directory containing the assistant images to be downloaded:\n",
    "        self.base_git_dir = \"https://github.com/marcosoares-92/img_examples_guides/raw/main\"\n",
    "        # Create a new folder to store the images in local environment, \n",
    "        # if the folder do not exists:\n",
    "        self.new_dir = \"tmp\"\n",
    "        \n",
    "        os.makedirs(self.new_dir, exist_ok = True)\n",
    "        # exist_ok = True creates the directory only if it does not exist.\n",
    "        \n",
    "        self.last_img_number = 18 # number of the last image on the assistant\n",
    "        self.numbers_to_end_assistant = (3, 4, 7, 9, 10, 13, 15, 16, 19, 20, 21, 22)\n",
    "        # tuple: cannot be modified\n",
    "        # 3: 'g', 4: 't', 7: 'i_mr', 9: 'std_error', 10: '3s', 13: 'x_bar_s'\n",
    "        # 15: 'std_error' (grouped), 16: '3s' (grouped), 19: 'p', 20: 'np',\n",
    "        # 21: 'c', 22: 'u'\n",
    "        self.screen_number = 0 # start as zero\n",
    "        self.file_to_fetch = ''\n",
    "        self.img_url = ''\n",
    "        self.img_local_path = ''\n",
    "        # to check the class attributes, use the __dict__ method. Examples:\n",
    "        ## object.__dict__ will show all attributes from object\n",
    "                \n",
    "    # Define the class methods.\n",
    "    # All methods must take an object from the class (self) as one of the parameters\n",
    "    \n",
    "    def download_assistant_imgs (self):\n",
    "                \n",
    "        import os\n",
    "        import shutil # component of the standard library to move or copy files.\n",
    "        from html2image import Html2Image\n",
    "                \n",
    "        # Start the html object\n",
    "        html_img = Html2Image()\n",
    "                \n",
    "        for screen_number in range(0, (self.last_img_number + 1)):\n",
    "                \n",
    "            # ranges from 0 to (last_img_number + 1) - 1 = last_img_number\n",
    "            # convert the screen number to string to create the file name:\n",
    "            \n",
    "            # Update the attributes:\n",
    "            self.file_to_fetch = \"cc_s\" + str(screen_number) + \".png\"\n",
    "            self.img_url = os.path.join(self.base_git_dir, self.file_to_fetch)\n",
    "            \n",
    "            # Download the image:\n",
    "            # pypi.org/project/html2image/\n",
    "            img = html_img.screenshot(url = self.img_url, save_as = self.file_to_fetch, size = (500, 500))\n",
    "            # If size is omitted, the image is downloaded in the low-resolution default.\n",
    "            # save_as must be a file name, a path is not accepted.\n",
    "            # Make the output from the method equals to a variable eliminates its verbosity\n",
    "                    \n",
    "            # Create the new path for the image (local environment):\n",
    "            self.img_local_path = os.path.join(self.new_dir, self.file_to_fetch)\n",
    "            # Move the image files to the new paths:\n",
    "            # use shutil.move(source, destination) method to move the files:\n",
    "            # pynative.com/python-move-files\n",
    "            # docs.python.org/3/library/shutil.html\n",
    "            shutil.move(self.file_to_fetch, self.img_local_path)\n",
    "            # Notice that file_to_fetch attribute still stores a file name like 'cc_s0.png'\n",
    "        \n",
    "        # Now, all images for the assistant were downloaded and stored in the temporary\n",
    "        # folder. So, let's start the two boolean variables to initiate it and run it:\n",
    "        self.assistant_startup = True \n",
    "        # attribute to start the assistant in the first screen\n",
    "        self.keep_assistant_on = True\n",
    "        # attribute to maintain the assistant working\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def delete_assistant_imgs (self):\n",
    "                \n",
    "        import os\n",
    "        # Now, that the user closed the assistant, we can remove the downloaded files \n",
    "        # (delete them) from the notebook's workspace.\n",
    "                \n",
    "        # The os.remove function deletes a file or directory specified.\n",
    "        for screen_number in range(0, (self.last_img_number + 1)):\n",
    "                    \n",
    "            self.file_to_fetch = \"cc_s\" + str(screen_number) + \".png\"\n",
    "            self.img_local_path = os.path.join(self.new_dir, self.file_to_fetch)\n",
    "            os.remove(self.img_local_path)\n",
    "                \n",
    "        # Now that the files were removed, check if the tmp folder is empty:\n",
    "        size = os.path.getsize(self.new_dir)\n",
    "        # os.path.getsize returns the total size in Bytes from a folder or a file.\n",
    "                \n",
    "        # Get the list of sub-folders, files or subdirectories (the content) from the folder:\n",
    "        list_of_contents = os.listdir(self.new_dir)\n",
    "        # doc.python.org/3/library/os.html\n",
    "        # It returns a list of strings representing the paths of each file or directory \n",
    "        # in the analyzed folder.\n",
    "                \n",
    "        # If the size is 0 and the length of the list_of_contents is also zero (i.e., there is no\n",
    "        # previous sub-directory created), then remove the directory:\n",
    "        if ((size == 0) & (len(list_of_contents) == 0)):\n",
    "            \n",
    "            os.rmdir(self.new_dir)\n",
    "\n",
    "    def print_screen_legend (self):\n",
    "        \n",
    "        if (self.screen_number == 0):\n",
    "            \n",
    "            print(\"The control chart is a line graph showing a measure (y-axis) over time (x-axis).\")\n",
    "            \n",
    "            print(\"In contrast to the run chart, the central line of the control chart represents the (weighted) mean, rather than the median.\")\n",
    "            print(\"Additionally, two lines representing the upper and lower control limits are shown.\\n\")\n",
    "            print(\"The control limits represent the boundaries of the so-called common cause variation, which is inherent to the process.\")\n",
    "            print(\"Walther A. Shewhart, who invented the control chart, described two types of variation: chance-cause variation and assignable-cause variation.\")\n",
    "            print(\"These were later renamed to common-cause and special-cause variation.\\n\")\n",
    "            \n",
    "            print(\"Common-cause variation:\")\n",
    "            print(\"Is present in any process.\")\n",
    "            print(\"It is caused by phenomena that are always present within the system.\")\n",
    "            print(\"It makes the process predictable (within limits).\")\n",
    "            print(\"Common-cause variation is also called random variation or noise.\\n\")\n",
    "                    \n",
    "            print(\"Special-cause variation:\")\n",
    "            print(\"Is present in some processes.\")\n",
    "            print(\"It is caused by phenomena that are not normally present in the system.\")\n",
    "            print(\"It makes the process unpredictable.\")\n",
    "            print(\"Special-cause variation is also called non-random variation or signal.\\n\")\n",
    "                    \n",
    "            print(\"It is important to notice that neither common, nor special-cause variation is in itself 'good' or 'bad'.\")\n",
    "            print(\"A stable process may function at an unsatisfactory level; and an unstable process may be moving in the right direction.\")\n",
    "            print(\"On the other hand, the end goal of improvement is always to achieve a stable process functioning at a satisfactory level.\\n\")\n",
    "                    \n",
    "            print(\"Control chart limits:\")\n",
    "            print(\"The control limits, also called sigma limits, are usually placed at ±3 standard deviations from the central line.\")\n",
    "            print(\"So, the standard deviation is estimated as the common variation of the process of interest.\")\n",
    "            print(\"This variation depends on the theoretical distribution of data.\")\n",
    "            print(\"It is a beginner's mistake to simply calculate the standard deviation of all the data points.\")\n",
    "            print(\"This procedure would include both the common and special-cause variation in the calculus.\")\n",
    "            print(\"Since the calculations of control limits depend on the type of data (distribution), many types of control charts have been developed for specific purposes.\")\n",
    "        \n",
    "        elif (self.screen_number == 1):\n",
    "                    \n",
    "            print(\"CHARTS FOR RARE EVENTS\\n\")\n",
    "            print(\"ATTENTION: Due not previously group data in this case. Since events are rare, they are likely to be eliminated during aggregation.\\n\")\n",
    "                    \n",
    "            print(\"G-chart for units produced between (rare) defectives or defects;\")\n",
    "            print(\"or total events between successive rare occurrences:\\n\")\n",
    "            print(\"When defects or defectives are rare and the subgroups are small, C, U, and P-charts become useless.\")\n",
    "            print(\"That is because most subgroups will have no defects.\")\n",
    "                    \n",
    "            print(\"Example: if 8% of discharged patients have a hospitals-acquired pressure ulcer, and the average weekly number of discharges in a small department is 10, we would, on average, expect to have less than one pressure ulcer per week.\")\n",
    "            print(\"Instead, we could plot the number of discharges between each discharge of a patient with one or more pressure ulcers.\\n\")\n",
    "            print(\"The number of units between defectives is modelled by the geometric distribution.\")\n",
    "            print(\"So, the G-control chart plots counting of occurrence by number; time unit; or timestamp.\\n\")\n",
    "                    \n",
    "            print(\"In the example of discharged patients: the indicator is the number of discharges between each of these rare cases.\")\n",
    "            print(\"Note that the first patient with pressure ulcer is missing from the chart.\")\n",
    "            print(\"It is due to the fact that we do not know how many discharges there had been before the first patient with detected pressure ulcer.\\n\")\n",
    "            print(\"The central line of the G-chart is the theoretical median of the distribution\")\n",
    "            print(\"median = mean × 0.693\")\n",
    "                    \n",
    "            print(\"Since the geometric distribution is highly skewed, the median is a better representation of the process center.\")\n",
    "            print(\"Also, notice that the G-chart rarely has a lower control limit.\\n\")\n",
    "                    \n",
    "            print(\"T-chart for time between successive rare events:\\n\")\n",
    "            print(\"Like the G-chart, the T-chart is a rare event chart.\")\n",
    "            print(\"Instead of displaying the number of cases between events (defectives), this chart represents the time between successive rare events.\\n\")\n",
    "            print(\"Since time is a continuous variable, the T-chart belongs with the other charts for measure numeric data.\")\n",
    "            print(\"Then, T-chart plots the timedelta (e.g. number of days between occurrences) by the measurement, time unit, or timestamp.\")\n",
    "                \n",
    "        elif (self.screen_number == 2):\n",
    "            \n",
    "            print(\"A quality characteristic that is measured on a numerical scale is called a variable.\")\n",
    "            print(\"Examples: length or width, temperature, and volume.\\n\")\n",
    "            \n",
    "            print(\"The Shewhart control charts are widely used to monitor the mean and variability of variables.\")\n",
    "            print(\"On the other hand, many quality characteristics can be expressed in terms of a numerical measurement.\")\n",
    "                    \n",
    "            print(\"For example: the diameter of a bearing could be measured with a micrometer and expressed in millimeters.\\n\")\n",
    "            print(\"A single measurable quality characteristic, such as a dimension, weight, or volume, is a variable.\")\n",
    "            print(\"Control charts for variables are used extensively, and are one of the primary tools used in the analize and control steps of DMAIC.\")\n",
    "            \n",
    "            print(\"Many quality characteristics cannot be conveniently represented numerically, though.\")\n",
    "            print(\"In such cases, we usually classify each item inspected as either conforming or nonconforming to the specifications on that quality characteristic.\")\n",
    "            print(\"The terminology defective or nondefective is often used to identify these two classifications of product.\")\n",
    "            print(\"More recently, this terminology conforming and nonconforming has become popular.\")        \n",
    "            print(\"Quality characteristics of this type are called attributes.\\n\")\n",
    "            \n",
    "            print(\"Control Charts for Nonconformities (defects):\")\n",
    "            print(\"A nonconforming item is a unit of product that does not satisfy one or more of the specifications of that product.\")\n",
    "            print(\"Each specific point at which a specification is not satisfied results in a defect or nonconformity.\")\n",
    "            print(\"Consequently, a nonconforming item will contain at least one nonconformity.\")\n",
    "            print(\"However, depending on their nature and severity, it is quite possible for a unit to contain several nonconformities and not be classified as nonconforming.\")\n",
    "                    \n",
    "            print(\"Example: suppose we are manufacturing personal computers. Each unit could have one or more very minor flaws in the cabinet finish,\")\n",
    "            print(\"but since these flaws do not seriously affect the unit's functional operation, it could be classified as conforming.\")\n",
    "            print(\"However, if there are too many of these flaws, the personal computer should be classified as nonconforming,\")\n",
    "            print(\"because the flaws would be very noticeable to the customer and might affect the sale of the unit.\\n\")\n",
    "                    \n",
    "            print(\"There are many practical situations in which we prefer to work directly with the number of defects or nonconformities,\")\n",
    "            print(\"rather than the fraction nonconforming.\")\n",
    "            print(\"These include:\")\n",
    "            print(\"1. Number of defective welds in 100 m of oil pipeline.\")\n",
    "            print(\"2. Number of broken rivets in an aircraft wing.\")\n",
    "            print(\"3. Number of functional defects in an electronic logic device.\")\n",
    "            print(\"4. Number of errors on a document, etc.\\n\")\n",
    "                    \n",
    "            print(\"It is possible to develop control charts for either the total number of nonconformities in a unit,\")\n",
    "            print(\"or for the average number of nonconformities per unit.\\n\")\n",
    "            print(\"These control charts usually assume that the occurrence of nonconformities in samples of constant size is well modeled by the Poisson distribution.\\n\")\n",
    "                    \n",
    "            print(\"Essentially, this requires that the number of opportunities or potential locations for nonconformities be infinitely large;\")\n",
    "            print(\"and that the probability of occurrence of a nonconformity at any location be small and constant.\")\n",
    "            print(\"Furthermore, the inspection unit must be the same for each sample.\")\n",
    "            print(\"That is, each inspection unit must always represent an identical area of opportunity for the occurrence of nonconformities.\")\n",
    "                    \n",
    "            print(\"In addition, we can count nonconformities of several different types on one unit, as long as the above conditions are satisfied for each class of nonconformity.\\n\")\n",
    "            print(\"In most practical situations, these conditions will not be perfectly satisfied.\")\n",
    "            print(\"The number of opportunities for the occurrence of nonconformities may be finite,\")\n",
    "            print(\"or the probability of occurrence of nonconformities may not be constant.\\n\")\n",
    "                    \n",
    "            print(\"As long as these departures from the assumptions are not severe,\")\n",
    "            print(\"the Poisson model will usually work reasonably well.\")\n",
    "            print(\"There are cases, however, in which the Poisson model is completely inappropriate.\")\n",
    "            print(\"So, always check carefully the distributions.\")\n",
    "                    \n",
    "            print(\"If you are not sure, use the estimatives based on more general assumptions, i.e.,\")\n",
    "            print(\"The estimative of the natural variation as 3 times the standard deviation;\")\n",
    "            print(\"or as 3 times the standard error.\\n\")\n",
    "                    \n",
    "            print(\"Individual samples x Grouped data\")\n",
    "            print(\"Often, we collect a batch of samples corresponding to the same conditions, and use aggregation measurements such as mean, sum, or standard deviation to represent them.\")\n",
    "            print(\"In this case, we are grouping our data, and not working with individual measurements.\")\n",
    "            print(\"In turns, we can collect individual samples: there are no repetitions, only individual measurements corresponding to different conditions.\\n\")\n",
    "            print(\"Usually, time series data is collected individually: each measurement corresponds to an instant, so it is not possible to collect multiple samples corresponding to the same conditions for further grouping.\")\n",
    "            print(\"Example: instant assessment of pH, temperature, pressure, etc.\\n\")\n",
    "            print(\"Naturally, we can define a time window like a day, and group values on that window.\")\n",
    "            print(\"The dynamic of the phenomena should not create significant differences between samples collected for a same window, though.\")\n",
    "        \n",
    "        elif (self.screen_number == 5):\n",
    "            \n",
    "            print(\"CHARTS FOR NUMERICAL VARIABLES\\n\")\n",
    "            print(\"When dealing with a quality characteristic that is a variable, it is usually necessary to monitor both the mean value of the quality characteristic and its variability.\")\n",
    "            print(\"The control of the process average or mean quality level is usually done with the control chart for means, or the X-bar control chart.\")\n",
    "            print(\"The process variability can be monitored with either a control chart for the standard deviation, called the s control chart, or with a control chart for the range, called an R control chart.\\n\")\n",
    "            \n",
    "            print(\"I and MR charts for individual measurements:\")\n",
    "            print(\"ATTENTION: The I-MR chart can only be used for data that follows the normal distribution.\")\n",
    "            print(\"That is because the calculus of the control limits are based on the strong hypothesis of normality.\")\n",
    "            print(\"If you have individual samples that do not follow the normal curve (like skewed data, or data with high kurtosis);\")\n",
    "            print(\"or data with an unknown distribution, select number 8 for using less restrictive hypotheses for the estimative of the natural variation.\\n\")\n",
    "                    \n",
    "            print(\"Example: in healthcare, most quality data are count data.\")\n",
    "            print(\"However, from time to time, there are measurement data present.\")\n",
    "            print(\"These data are often in the form of physiological parameters or waiting times.\")\n",
    "            print(\"e.g. a chart of birth weights from 24 babies.\")\n",
    "            print(\"If the birth weights follow the normal, you can use the individuals chart.\\n\")\n",
    "                    \n",
    "            print(\"Actually, there are many situations in which the sample size used for process monitoring is n = 1; that is, the sample consists of an individual unit.\")\n",
    "            print(\"Some other examples of these situations are as follows:\")\n",
    "            print(\"1. Automated inspection and measurement technology is used, and every unit manufactured is analyzed.\")\n",
    "            print(\"So, there is no basis for rational subgrouping.\")\n",
    "            print(\"2. Data comes available relatively slowly, and it is inconvenient to allow sample sizes of n > 1 to accumulate before analysis.\") \n",
    "            print(\"The long interval between observations will cause problems with rational subgrouping.\")\n",
    "            print(\"This occurs frequently in both manufacturing and non-manufacturing situations.\")\n",
    "            print(\"3. Repeat measurements on the process differ only because of laboratory or analysis error, as in many chemical processes.\")\n",
    "            print(\"4. Multiple measurements are taken on the same unit of product, such as measuring oxide thickness at several different locations on a wafer in semiconductor manufacturing.\")\n",
    "            print(\"5. In process plants, such as papermaking, measurements on some parameter (such as coating thickness across the roll) will differ very little and produce a standard deviation that is much too small if the objective is to control coating thickness along the roll.\")\n",
    "                    \n",
    "            print(\"In such situations, the control chart for individual units is useful.\")\n",
    "            print(\"In many applications of the individuals control chart, we use the moving range two successive observations as the basis of estimating the process variability.\\n\")\n",
    "            print(\"I-charts are often accompanied by moving range (MR) charts, which show the absolute difference between neighbouring data points.\")\n",
    "            print(\"The purpose of the MR chart is to identify sudden changes in the (estimated) within-subgroup variation.\")\n",
    "            print(\"If any data point in the MR is above the upper control limit, one should interpret the I-chart very cautiously.\\n\")\n",
    "        \n",
    "        elif(self.screen_number == 6):\n",
    "                    \n",
    "            print(\"One important difference: numeric variables are representative of continuous data, usually in the form of real numbers (float values).\")\n",
    "            print(\"It means that its possible values cannot be counted: there is an infinite number of possible real values.\")\n",
    "            \n",
    "            print(\"Categoric variables, in turn, are discrete.\")        \n",
    "            print(\"It means they can be counted, since there is a finite number of possibilities.\")\n",
    "            print(\"Such variables are usually present as strings (texts), or as ordinal (integer) numbers.\")\n",
    "                    \n",
    "            print(\"If there are only two categories, we have a binary classification.\")\n",
    "            print(\"Each category can be reduced to a binary system: or the category is present, or it is not.\")\n",
    "            print(\"This is the idea for the One-Hot Encoding.\")\n",
    "            print(\"Usually, values in a binary classification are 1 or 0, so that a probability can be easily associated through the sigmoid function.\\n\")\n",
    "                    \n",
    "            print(\"Some examples of quality characteristics that are based on the analysis of attributes:\")\n",
    "            print(\"1. Proportion of warped automobile engine connecting rods in a day's production.\")\n",
    "            print(\"2. Number of nonfunctional semiconductor chips on a wafer.\")\n",
    "            print(\"3. Number of errors or mistakes made in completing a loan application.\")\n",
    "            print(\"4. Number of medical errors made in a hospital.\\n\")\n",
    "        \n",
    "        elif((self.screen_number == 8) | (self.screen_number == 14)):\n",
    "            \n",
    "            print(\"If you have a distribution that is not normal, like distributions with high skewness or high kurtosis,\")\n",
    "            print(\"use less restrictive methodologies to estimate the natural variation.\\n\")\n",
    "                    \n",
    "            print(\"You may estimate the natural variation as 3 times the standard error; or as 3 times the standard deviation.\")\n",
    "            print(\"The interval will be symmetric around the mean value.\\n\")\n",
    "            print(\"Recommended: standard error, which normalizes by the total of values.\")\n",
    "                \n",
    "        elif(self.screen_number == 11):\n",
    "            \n",
    "            print(\"CHARTS FOR NUMERICAL VARIABLES\\n\")\n",
    "            print(\"When dealing with a quality characteristic that is a variable, it is usually necessary to monitor both the mean value of the quality characteristic and its variability.\")\n",
    "            print(\"The control of the process average or mean quality level is usually done with the control chart for means, or the X-bar control chart.\")\n",
    "            print(\"The process variability can be monitored with either a control chart for the standard deviation, called the s control chart, or with a control chart for the range, called an R control chart.\\n\")\n",
    "                    \n",
    "            print(\"X-bar and S charts for average measurements:\")\n",
    "            print(\"If there is more than one measurement of a numeric variable in each subgroup,\")\n",
    "            print(\"the Xbar and S charts will display the average and the within-subgroup standard deviation, respectively.\")\n",
    "            print(\"e.g. a chart of average birth weights per month, for babies born over last year.\")\n",
    "        \n",
    "        elif(self.screen_number == 12):\n",
    "            \n",
    "            print(\"CHARTS FOR CATEGORICAL VARIABLES\\n\")\n",
    "            print(\"There are 4 widely used attributes control charts: P, nP, U, and C.\\n\")\n",
    "                    \n",
    "            print(\"To illustrate them, consider a dataset containing the weekly number of hospital acquired pressure ulcers at a hospital\")\n",
    "            print(\"The hospital has 300 patients, with an average length of stay of four days.\") \n",
    "            print(\"Each of the dataframe's 24 rows contains information for one week on: the number of discharges,\")\n",
    "            print(\"patient days; pressure ulcers; and number of discharged patients with one or more pressure ulcers.\")\n",
    "            print(\"On average, 8% of discharged patients have 1.5 hospital acquired pressure ulcers.\\n\")\n",
    "                    \n",
    "            print(\"Some of the charts for categorical variables are based on the definition of the fraction nonconforming.\")\n",
    "            print(\"The fraction nonconforming is defined as the ratio between:\")\n",
    "            print(\"the number of nonconforming items in a population; by the total number of items in that population.\")\n",
    "            print(\"The items may have several quality characteristics that are examined simultaneously by the inspector.\")\n",
    "            print(\"If the item does not conform to the standards of one or more of these characteristics, it is classified as nonconforming.\\n\")\n",
    "                    \n",
    "            print(\"ATTENTION: Although it is customary to work with fraction nonconforming,\")\n",
    "            print(\"we could also analyze the fraction conforming just as easily, resulting in a control chart of process yield.\")\n",
    "            print(\"Many manufacturing organizations operate a yield-management system at each stage of their manufacturing process,\")\n",
    "            print(\"with the first-pass yield tracked on a control chart.\\n\")\n",
    "            \n",
    "            print(\"Traditionally, the term 'defect' has been used to name whatever it is being analyzed through counting with control charts.\\n\")\n",
    "            print(\"There is a subtle, but important, distinction between:\")\n",
    "            print(\"counting defects, e.g. number of pressure ulcers;\")\n",
    "            print(\"and counting defectives, e.g. number of patients with one or more pressure ulcers.\\n\")\n",
    "                    \n",
    "            print(\"Defects are expected to reflect the Poisson distribution,\")\n",
    "            print(\"while defectives reflect the binomial distribution.\\n\")\n",
    "        \n",
    "        elif(self.screen_number == 17):\n",
    "                    \n",
    "            print(\"P-charts for proportion of defective units:\")\n",
    "            print(\"The first of these relates to the fraction of nonconforming or defective product produced by a manufacturing process, and is called the control chart for fraction nonconforming, or P-chart.\")\n",
    "            \n",
    "            print(\"The P chart is probably the most common control chart in healthcare.\")\n",
    "            print(\"It is used to plot the proportion (or percent) of defective units.\")\n",
    "            print(\"e.g. the proportion of patients with one or more pressure ulcers.\")\n",
    "                    \n",
    "            print(\"As mentioned, defectives are modelled by the binomial distribution.\")\n",
    "            print(\"In theory, the P chart is less sensitive to special cause variation than the U chart.\")\n",
    "            print(\"That is because it discards information by dichotomising inspection units (patients) in defectives and non-defectives ignoring the fact that a unit may have more than one defect (pressure ulcers).\")\n",
    "            print(\"On the other hand, the P chart often communicates better.\")\n",
    "                    \n",
    "            print(\"For most people, not to mention the press, the percent of harmed patients is easier to grasp than the the rate of pressure ulcers expressed in counts per 1000 patient days.\\n\")\n",
    "            print(\"The sample fraction nonconforming is defined as the ratio of the number of nonconforming units in the sample D to the sample size n:\")\n",
    "            print(\"p = D/n\")\n",
    "            print(\"From the binomial distribution, the mean should be estimated as p, and the variance s² as p(1-p)/n.\")\n",
    "                    \n",
    "            print(\"nP-Charts for number nonconforming:\")\n",
    "            print(\"It is also possible to base a control chart on the number nonconforming,\")\n",
    "            print(\"rather than on the fraction nonconforming.\")\n",
    "            print(\"This is often called as number nonconforming (nP) control chart.\\n\")\n",
    "            \n",
    "        elif(self.screen_number == 18):\n",
    "            \n",
    "            print(\"C-charts for count of defects:\")\n",
    "            print(\"In some situations, it is more convenient to deal with the number of defects or nonconformities observed,\")\n",
    "            print(\"rather than the fraction nonconforming.\\n\")\n",
    "            \n",
    "            print(\"So, another type of control chart, called the control chart for nonconformities, or the C chart,\")\n",
    "            print(\"is designed to deal with this case.\\n\")\n",
    "                    \n",
    "            print(\"In the hospital example:\")\n",
    "            print(\"The correct control chart for the number of pressure ulcers is the C-chart,\")\n",
    "            print(\"which is based on the Poisson distribution.\\n\")\n",
    "                    \n",
    "            print(\"As mentioned, DEFECTIVES are modelled by the BINOMIAL distribution, whereas DEFECTS are are modelled by POISSON distribution.\\n\")\n",
    "            \n",
    "            print(\"U-charts for rate of defects:\")\n",
    "            print(\"The control chart for nonconformities per unit, or the U-chart, is useful in situations\")\n",
    "            print(\"where the average number of nonconformities per unit is a more convenient basis for process control.\\n\")\n",
    "                \n",
    "            print(\"The U-chart is different from the C-chart in that it accounts for variation in the area of opportunity.\")\n",
    "            print(\"Examples:\")\n",
    "            print(\"1. Number of patients over time.\")\n",
    "            print(\"2. Number of patients between units one wishes to compare.\")\n",
    "            print(\"3. Number of patient days over time.\")\n",
    "            print(\"4. Number of patient days between units one wishes to compare.\\n\")\n",
    "                 \n",
    "            print(\"If there are many more patients in the hospital in the winter than in the summer,\")\n",
    "            print(\"the C-chart may falsely detect special cause variation in the raw number of pressure ulcers.\\n\")\n",
    "            \n",
    "            print(\"The U-chart plots the rate of defects.\")\n",
    "            print(\"A rate differs from a proportion in that the numerator and the denominator need not be of the same kind,\")\n",
    "            print(\"and that the numerator may exceed the denominator.\\n\")\n",
    "                \n",
    "            print(\"For example: the rate of pressure ulcers may be expressed as the number of pressure ulcers per 1000 patient days.\\n\")\n",
    "            print(\"The larger the numerator, the narrower the control limits.\\n\")\n",
    "            print(\"So, the main difference between U and C-charts is that U is based on the average number of nonconformities per inspection unit.\\n\")\n",
    "            \n",
    "            print(\"If we find x total nonconformities in a sample of n inspection units,\")\n",
    "            print(\"then the average number of nonconformities per inspection unit will be:\")\n",
    "            print(\"u = x/n\")\n",
    "            print(\"\\n\")\n",
    "               \n",
    "    def open_chart_assistant_screen (self):\n",
    "                \n",
    "        import os\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        from html2image import Html2Image\n",
    "        from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "        # img_to_array: convert the image into its numpy array representation\n",
    "                \n",
    "        if (self.assistant_startup): #run if it is True:\n",
    "            \n",
    "            self.screen_number = 0 # first screen\n",
    "        \n",
    "        if (self.screen_number not in self.numbers_to_end_assistant):\n",
    "            \n",
    "            self.print_screen_legend()\n",
    "            # Use its own method\n",
    "            \n",
    "            # Update attributes:\n",
    "            self.file_to_fetch = \"cc_s\" + str(self.screen_number) + \".png\"\n",
    "            # Obtain the path of the image (local environment):\n",
    "            self.img_local_path = os.path.join(self.new_dir, self.file_to_fetch)\n",
    "                    \n",
    "            # Load the image and save it on variables:\n",
    "            assistant_screen = load_img(self.img_local_path)\n",
    "                    \n",
    "            # show image with plt.imshow function:\n",
    "            fig = plt.figure(figsize = (12, 8))\n",
    "            plt.imshow(assistant_screen)\n",
    "            # If the image is black and white, you can color it with a cmap as fig.set_cmap('hot')\n",
    "            \n",
    "            #set axis off:\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Run again the assistant for next screen (keep assistant on):\n",
    "            self.keep_assistant_on = True\n",
    "            # In the next round, the assistant should not be restarted:\n",
    "            self.assistant_startup = False\n",
    "            \n",
    "            screen_number = input(\"Enter the number you wish here (in the right), according to the shown in the image above: \")\n",
    "            #convert the screen number to string:\n",
    "            screen_number = str(screen_number)        \n",
    "            # Strip spaces and format characters (trim):\n",
    "            screen_number = screen_number.strip()        \n",
    "            # We do not call the str attribute for string variables (only for iterables)\n",
    "            # Convert to integer\n",
    "            screen_number = int(screen_number)\n",
    "            # Update the attribute:\n",
    "            self.screen_number = screen_number\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # user selected a value that ends the assistant:\n",
    "            self.keep_assistant_on = False\n",
    "            self.assistant_startup = False\n",
    "        \n",
    "        # Return the booleans to the main function:\n",
    "        return self\n",
    "        \n",
    "    def chart_selection (self):\n",
    "                \n",
    "        # Only if the screen is in the tuple numbers_to_end_assistant:\n",
    "        if (self.screen_number in self.numbers_to_end_assistant):\n",
    "                    \n",
    "            # Variables are created only when requested:\n",
    "            rare_events_tuple = (3, 4) # g, t\n",
    "            continuous_dist_not_defined_tuple = (9, 10) # std_error, 3std\n",
    "            grouped_dist_not_defined_tuple = (15, 16) # std_error, 3std\n",
    "            grouped_tuple = (13, 19, 20, 21, 22) # x, p, np, c, u\n",
    "            charts_map_dict = {3:'g', 4:'t', 7:'i_mr', 9:'std_error', 10:'3s_as_natural_variation',\n",
    "                                13:'xbar_s', 15:'std_error', 16:'3s_as_natural_variation',\n",
    "                                19:'p', 20:'np', 21:'c', 22:'u'}\n",
    "                    \n",
    "            chart_to_use = charts_map_dict[self.screen_number]\n",
    "                    \n",
    "            # Variable with subgroups, which will be updated if needed:\n",
    "            column_with_labels_or_subgroups = None\n",
    "                    \n",
    "            # Variable for skewed distribution, which will be updated if needed:\n",
    "            consider_skewed_dist_when_estimating_with_std = False\n",
    "                    \n",
    "            column_with_variable_to_be_analyzed = str(input(\"Enter here (in the right) the name or number of the column (its header) that will be analyzed with the control chart.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "            # Try to convert it to integer, if it is a number:\n",
    "            try:\n",
    "                # Clean the string:\n",
    "                column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed.strip()\n",
    "                column_with_variable_to_be_analyzed = int(column_with_variable_to_be_analyzed)\n",
    "                    \n",
    "            except: # simply pass\n",
    "                pass\n",
    "                    \n",
    "            print(\"\\n\")\n",
    "            \n",
    "            yes_no = str(input(\"Do your data have a column containing timestamps or time indication (event order)?\\nType yes or no, here (in the right).\\nDo not type it in quotes: \"))\n",
    "            yes_no = yes_no.strip()        \n",
    "            # convert to full lower case, independently of the user:\n",
    "            yes_no = yes_no.lower()\n",
    "                    \n",
    "            if (yes_no == 'yes'):\n",
    "                    \n",
    "                print(\"\\n\")\n",
    "                timestamp_tag_column = str(input(\"Enter here (in the right) the name or number of the column containing timestamps or time indication (event order).\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "                \n",
    "                # Try to convert it to integer, if it is a number:\n",
    "                try:\n",
    "                    timestamp_tag_column = timestamp_tag_column.strip()\n",
    "                    timestamp_tag_column = int(timestamp_tag_column)\n",
    "                        \n",
    "                except: # simply pass\n",
    "                    pass\n",
    "                    \n",
    "            else:\n",
    "                timestamp_tag_column = None\n",
    "                    \n",
    "            yes_no = str(input(\"Do your data have a column containing event frame indication; indication for separating time windows for comparison analysis;\\nstages; events to be analyzed separately; or any other indication for slicing the time axis for comparison of different means, variations, etc?\\nType yes or no, here (in the right).\\nDo not type it in quotes: \"))\n",
    "            yes_no = yes_no.strip()\n",
    "            yes_no = yes_no.lower()\n",
    "                    \n",
    "            if (yes_no == 'yes'):\n",
    "                        \n",
    "                print(\"\\n\")\n",
    "                column_with_event_frame_indication = str(input(\"Enter here (in the right) the name or number of the column containing the event frame indication.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "                        \n",
    "                # Try to convert it to integer, if it is a number:\n",
    "                try:\n",
    "                    column_with_event_frame_indication = column_with_event_frame_indication.strip()\n",
    "                    column_with_event_frame_indication = int(column_with_event_frame_indication)\n",
    "                        \n",
    "                except: # simply pass\n",
    "                    pass\n",
    "            \n",
    "            else:\n",
    "                column_with_event_frame_indication = None\n",
    "                    \n",
    "            if (self.screen_number in rare_events_tuple):\n",
    "                        \n",
    "                print(\"\\n\")\n",
    "                print(f\"How are the rare events represented in the column {column_with_variable_to_be_analyzed}?\")\n",
    "                print(f\"Before obtaining the chart, you must have modified the {column_with_variable_to_be_analyzed} to labe these data.\")\n",
    "                print(\"The function cannot work with boolean filters. So, if a value corresponds to a rare event occurrence, modify its value to properly labelling it.\")\n",
    "                print(\"You can set a special string or a special numeric value for indicating that a particular row corresponds to a rare event.\")\n",
    "                print(\"That is because rare events occurrences must be compared against all other 'regular' events.\")\n",
    "                print(f\"For instance, {column_with_variable_to_be_analyzed} may show a value like 'rare_event', or 'ulcer' (in our example) if it is a rare occurrence.\")\n",
    "                print(\"Also, you could input a value extremely high, like 1000000000, or extremely low, like -10000000 for marking the rare events in the column.\")\n",
    "                print(\"The chart will be obtained after finding these rare events marks on the column.\\n\")\n",
    "                        \n",
    "                rare_event_indication = str(input(f\"How are the rare events represented in the column {column_with_variable_to_be_analyzed}?\\nEnter here (in the right) the text or number representing a rare event.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or the rare events will not be localized in the dataset): \"))\n",
    "                        \n",
    "                # Try to convert it to float, if it is a number:\n",
    "                try:\n",
    "                    column_with_event_frame_indication = column_with_event_frame_indication.strip()\n",
    "                    column_with_event_frame_indication = float(column_with_event_frame_indication)\n",
    "                \n",
    "                except: # simply pass\n",
    "                    pass\n",
    "                        \n",
    "                rare_event_timedelta_unit = str(input(f\"What is the usual order of magnitude for the intervals (timedeltas) between rare events?\\nEnter here (in the right).\\nYou may type: year, month, day, hour, minute, or second.\\nDo not type it in quotes: \"))\n",
    "                rare_event_timedelta_unit = rare_event_timedelta_unit.strip()\n",
    "                rare_event_timedelta_unit = rare_event_timedelta_unit.lower()\n",
    "                \n",
    "                while (rare_event_timedelta_unit not in ['year', 'month', 'day', 'hour', 'minute', 'second']):\n",
    "                    \n",
    "                    rare_event_timedelta_unit = str(input(\"Please, enter a valid timedelta unit: year, month, day, hour, minute, or second.\\nDo not type it in quotes: \"))\n",
    "                    rare_event_timedelta_unit = rare_event_timedelta_unit.strip()\n",
    "                    rare_event_timedelta_unit = rare_event_timedelta_unit.lower()\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                rare_event_timedelta_unit = None\n",
    "                rare_event_indication = None\n",
    "                        \n",
    "                if ((self.screen_number in grouped_dist_not_defined_tuple) | (self.screen_number in grouped_tuple)):\n",
    "                            \n",
    "                    print(\"\\n\")\n",
    "                    column_with_labels_or_subgroups = str(input(\"Enter here (in the right) the name or number of the column containing the subgroups or samples for aggregating the measurements in terms of mean, standard deviation, etc.\\nIt may be a column with indications like 'A', 'B', or 'C'; 'subgroup1',..., 'sample1',..., or an integer like 1, 2, 3,...\\nThis column will allow grouping of rows in terms of the correspondent samples.\\nDo not type it in quotes.\\nKeep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): \"))\n",
    "                            \n",
    "                    # Try to convert it to integer, if it is a number:\n",
    "                    try:\n",
    "                        column_with_labels_or_subgroups = column_with_labels_or_subgroups.strip()\n",
    "                        column_with_labels_or_subgroups = int(column_with_labels_or_subgroups)\n",
    "                    \n",
    "                    except: # simply pass\n",
    "                        pass\n",
    "                \n",
    "                if ((self.screen_number in grouped_dist_not_defined_tuple) | (self.screen_number in continuous_dist_not_defined_tuple)):\n",
    "                            \n",
    "                    print(\"\\n\")\n",
    "                    print(\"Is data skewed or with high kurtosis? If it is, the median will be used as the central line estimative.\")\n",
    "                    print(\"median = mean × 0.693\\n\")\n",
    "                            \n",
    "                    yes_no = str(input(\"Do you want to assume a skewed (or with considerable kurtosis) distribution?\\nType yes or no, here (in the right).\\nDo not type it in quotes: \"))\n",
    "                    yes_no = yes_no.strip()\n",
    "                    yes_no = yes_no.lower()\n",
    "                            \n",
    "                    if (yes_no == 'yes'):\n",
    "                        \n",
    "                        # update the boolean variable\n",
    "                        consider_skewed_dist_when_estimating_with_std = True\n",
    "                \n",
    "                \n",
    "            print(\"Finished mapping the variables for obtaining the control chart plots.\")\n",
    "            print(\"If an error is raised; or if the chart is not complete, check if the columns' names inputs are strictly correct.\\n\")\n",
    "            \n",
    "            return chart_to_use, column_with_labels_or_subgroups, consider_skewed_dist_when_estimating_with_std, column_with_variable_to_be_analyzed, timestamp_tag_column, column_with_event_frame_indication, rare_event_timedelta_unit, rare_event_indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assistant_startup': True, 'keep_assistant_on': True, 'base_git_dir': 'https://github.com/marcosoares-92/img_examples_guides/raw/main', 'new_dir': 'tmp', 'last_img_number': 18, 'numbers_to_end_assistant': (3, 4, 7, 9, 10, 13, 15, 16, 19, 20, 21, 22), 'screen_number': 0, 'file_to_fetch': '', 'img_url': '', 'img_local_path': ''}\n",
      "{'assistant_startup': True, 'keep_assistant_on': True, 'base_git_dir': 'https://github.com/marcosoares-92/img_examples_guides/raw/main', 'new_dir': 'tmp', 'last_img_number': 18, 'numbers_to_end_assistant': (3, 4, 7, 9, 10, 13, 15, 16, 19, 20, 21, 22), 'screen_number': 0, 'file_to_fetch': '', 'img_url': '', 'img_local_path': ''}\n",
      "The control chart is a line graph showing a measure (y-axis) over time (x-axis).\n",
      "In contrast to the run chart, the central line of the control chart represents the (weighted) mean, rather than the median.\n",
      "Additionally, two lines representing the upper and lower control limits are shown.\n",
      "\n",
      "The control limits represent the boundaries of the so-called common cause variation, which is inherent to the process.\n",
      "Walther A. Shewhart, who invented the control chart, described two types of variation: chance-cause variation and assignable-cause variation.\n",
      "These were later renamed to common-cause and special-cause variation.\n",
      "\n",
      "Common-cause variation:\n",
      "Is present in any process.\n",
      "It is caused by phenomena that are always present within the system.\n",
      "It makes the process predictable (within limits).\n",
      "Common-cause variation is also called random variation or noise.\n",
      "\n",
      "Special-cause variation:\n",
      "Is present in some processes.\n",
      "It is caused by phenomena that are not normally present in the system.\n",
      "It makes the process unpredictable.\n",
      "Special-cause variation is also called non-random variation or signal.\n",
      "\n",
      "It is important to notice that neither common, nor special-cause variation is in itself 'good' or 'bad'.\n",
      "A stable process may function at an unsatisfactory level; and an unstable process may be moving in the right direction.\n",
      "On the other hand, the end goal of improvement is always to achieve a stable process functioning at a satisfactory level.\n",
      "\n",
      "Control chart limits:\n",
      "The control limits, also called sigma limits, are usually placed at ±3 standard deviations from the central line.\n",
      "So, the standard deviation is estimated as the common variation of the process of interest.\n",
      "This variation depends on the theoretical distribution of data.\n",
      "It is a beginner's mistake to simply calculate the standard deviation of all the data points.\n",
      "This procedure would include both the common and special-cause variation in the calculus.\n",
      "Since the calculations of control limits depend on the type of data (distribution), many types of control charts have been developed for specific purposes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Enter the number you wish here (in the right), according to the shown in the image above: 7\n",
      "Enter here (in the right) the name or number of the column (its header) that will be analyzed with the control chart.\n",
      "Do not type it in quotes.\n",
      "Keep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): values\n",
      "\n",
      "\n",
      "Do your data have a column containing timestamps or time indication (event order)?\n",
      "Type yes or no, here (in the right).\n",
      "Do not type it in quotes: timestamp\n",
      "Do your data have a column containing event frame indication; indication for separating time windows for comparison analysis;\n",
      "stages; events to be analyzed separately; or any other indication for slicing the time axis for comparison of different means, variations, etc?\n",
      "Type yes or no, here (in the right).\n",
      "Do not type it in quotes: yes\n",
      "\n",
      "\n",
      "Enter here (in the right) the name or number of the column containing the event frame indication.\n",
      "Do not type it in quotes.\n",
      "Keep the exact same format of the dataset, with spaces, characters, upper and lower cases, etc (or an error will be raised): events\n",
      "Finished mapping the variables for obtaining the control chart plots.\n",
      "If an error is raised; or if the chart is not complete, check if the columns' names inputs are strictly correct.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To show the Python class attributes, use the __dict__ method:\n",
    "# http://www.learningaboutelectronics.com/Articles/How-to-display-all-attributes-of-a-class-or-instance-of-a-class-in-Python.php#:~:text=So%20the%20__dict__%20method%20is%20a%20very%20useful,other%20data%20type%20such%20as%20a%20class%20itself.\n",
    "                \n",
    "# instantiate the object\n",
    "assistant = spc_chart_assistant()\n",
    "\n",
    "# Download the images:\n",
    "assistant = assistant.download_assistant_imgs()\n",
    "\n",
    "# Run the assistant:\n",
    "while (assistant.keep_assistant_on == True):\n",
    "    \n",
    "    # Run the wrapped function until the user tells you to stop:\n",
    "    # Notice that both variables are True for starting the first loop:\n",
    "    assistant = assistant.open_chart_assistant_screen ()\n",
    "        \n",
    "# Delete the images\n",
    "assistant.delete_assistant_imgs()\n",
    "        \n",
    "# Select the chart and the parameters:\n",
    "chart_to_use, column_with_labels_or_subgroups, consider_skewed_dist_when_estimating_with_std, column_with_variable_to_be_analyzed, timestamp_tag_column, column_with_event_frame_indication, rare_event_timedelta_unit, rare_event_indication = assistant.chart_selection()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the images\n",
    "assistant.delete_assistant_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_mr None False values None events None None\n"
     ]
    }
   ],
   "source": [
    "print(chart_to_use, column_with_labels_or_subgroups, consider_skewed_dist_when_estimating_with_std, column_with_variable_to_be_analyzed, timestamp_tag_column, column_with_event_frame_indication, rare_event_timedelta_unit, rare_event_indication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assistant_startup': True, 'keep_assistant_on': True, 'base_git_dir': 'https://github.com/marcosoares-92/img_examples_guides/raw/main', 'new_dir': 'tmp', 'last_img_number': 18, 'numbers_to_end_assistant': (3, 4, 7, 9, 10, 13, 15, 16, 19, 20, 21, 22), 'screen_number': 0, 'file_to_fetch': '', 'img_url': '', 'img_local_path': ''}\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    assistant = spc_chart_assistant()\n",
    "    print(assistant.__dict__)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "01ecd79e-5b89-46bd-8671-7622a949139b"
   },
   "source": [
    "# **Function for column filtering (selecting) or column renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "azdata_cell_guid": "4e483322-9d78-46b2-9f22-226b5d8bfc54"
   },
   "outputs": [],
   "source": [
    "def col_filter_rename (df, cols_list, mode = 'filter'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #mode = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "    #mode = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "    \n",
    "    #cols_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{df.columns}\")\n",
    "    \n",
    "    if (mode == 'filter'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        df = df[cols_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\")\n",
    "        \n",
    "    elif (mode == 'rename'):\n",
    "        \n",
    "        #Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(cols_list) == len(df.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\")\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            df.columns = cols_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'filter\\' or \\'rename\\'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "77cc0633-e954-4296-ac33-09ef67155e28"
   },
   "source": [
    "# **Function for calculating general statistics from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "9b43c013-b881-4b17-a350-4a69b4e6931e"
   },
   "outputs": [],
   "source": [
    "def GENERAL_STATISTICS (df, column_to_analyze):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe to be analyzed.\n",
    "    \n",
    "    #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "    # will analyze column named as 'col1'\n",
    "    analyzed_series = df[column_to_analyze]\n",
    "    \n",
    "    general_stats = analyzed_series.describe()\n",
    "    print(f\"General descriptive statistics from variable {column_to_analyze}, ignoring missing values:\\n\")\n",
    "    print(general_stats)\n",
    "    print(\"\\n\") #line break\n",
    "    print(\"Interpretation: count = total of values evaluated. Missing values are ignored; mean = mean value of the series, ignoring missing values; std = standard deviation of the series, ignoring missing values; min = minimum observed value for the analyzed series; 25\\% = 0.25 quantile (1st-quartile): 25\\% of data fall below (<=) this value; 50\\% = 2nd-quartile: 50\\% <= this value; 75\\% = 3rd-quartile - 75\\% of data <= this value; max = maximum observed value for the analyzed series.\")\n",
    "    print(\"This function shows the general statistics only for numerical variables. The results were returned as the dataframe general_stats.\")\n",
    "    \n",
    "    # Return only the dataframe\n",
    "    return general_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ed6f47d1-374f-46e4-84ab-d6296ecf9089"
   },
   "source": [
    "# **Function for getting data quantiles from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "75e7abb8-973a-4a8a-a261-f342d7f59a67"
   },
   "outputs": [],
   "source": [
    "def GET_QUANTILES (df, column_to_analyze):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe to be analyzed.\n",
    "    \n",
    "    #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "    # will analyze column named as 'col1'\n",
    "    analyzed_series = df[column_to_analyze]\n",
    "    \n",
    "    list_of_quantiles = []\n",
    "    list_of_values = []\n",
    "    list_of_interpretation = []\n",
    "    \n",
    "    #First element: minimum\n",
    "    list_of_quantiles.append(0)\n",
    "    list_of_values.append(analyzed_series.min())\n",
    "    list_of_interpretation.append(f\"Minimum value of {column_to_analyze}\")\n",
    "    \n",
    "    i = 0.05\n",
    "    #Start from the 5% quantile\n",
    "    while (i <= 0.95):\n",
    "        \n",
    "        list_of_quantiles.append(i)\n",
    "        list_of_values.append(analyzed_series.quantile(i))\n",
    "        list_of_interpretation.append(f\"{i}-quantile: {i*100}\\% of data fall below this value\")\n",
    "        \n",
    "        i = i + 0.05\n",
    "    \n",
    "    # Last element: maximum value\n",
    "    list_of_quantiles.append(1)\n",
    "    list_of_values.append(analyzed_series.max())\n",
    "    list_of_interpretation.append(f\"Maximum value of {column_to_analyze}\")\n",
    "    \n",
    "    # Summarize the lists as a dictionary and convert the dictionary to a dataframe:\n",
    "    quantiles_dict = {\"quantile\": list_of_quantiles, f\"value_of_{column_to_analyze}\": list_of_values,\n",
    "                     \"interpretation\": list_of_interpretation}\n",
    "    \n",
    "    quantiles_summ_df = pd.DataFrame(data = quantiles_dict)\n",
    "    \n",
    "    print(\"Quantiles returned as dataframe quantiles_summ_df. Check it below:\\n\")\n",
    "    print(quantiles_summ_df)\n",
    "    \n",
    "    return quantiles_summ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1f556313-d7e2-43bb-8c4a-f1991dfc74e4"
   },
   "source": [
    "# **Function for getting a particular P-percent quantile limit**\n",
    "- input P as a percent, from 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "49c83bcf-7e4d-462a-a6ff-f13edaa6e65b"
   },
   "outputs": [],
   "source": [
    "def GET_P_PERCENT_QUANTILE_LIM (df, column_to_analyze, p_percent = 100):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #df: dataframe to be analyzed.\n",
    "    \n",
    "    #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "    # will analyze column named as 'col1'\n",
    "    \n",
    "    # p_percent: float value from 0 to 100 representing the percent of the quantile\n",
    "    # if p_percent = 31.2, then 31.2% of the data will fall below the returned value\n",
    "    # if p_percent = 75, then 75% of the data will fall below the returned value\n",
    "    # if p_percent = 0, the minimum value is returned.\n",
    "    # if p_percent = 100, the maximum value is returned.\n",
    "    \n",
    "    analyzed_series = df[column_to_analyze]\n",
    "    \n",
    "    #convert the quantile to fraction\n",
    "    quantile_fraction = p_percent/100.0 #.0 to guarantee a float result\n",
    "    \n",
    "    if (quantile_fraction < 0):\n",
    "        print(\"Invalid percent value - it cannot be lower than zero.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    elif (quantile_fraction == 0):\n",
    "        #get the minimum value\n",
    "        quantile_lim = analyzed_series.min()\n",
    "        print(f\"Minimum value of {column_to_analyze} = {quantile_lim}\")\n",
    "    \n",
    "    elif (quantile_fraction == 1):\n",
    "        #get the maximum value\n",
    "        quantile_lim = analyzed_series.max()\n",
    "        print(f\"Maximum value of {column_to_analyze} = {quantile_lim}\")\n",
    "        \n",
    "    else:\n",
    "        #get the quantile\n",
    "        quantile_lim = analyzed_series.quantile(quantile_fraction)\n",
    "        print(f\"{quantile_fraction}-quantile: {p_percent}\\% of data fall below this value\")\n",
    "    \n",
    "    return quantile_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d4a94c3d-251f-4c56-8eb5-8f36d85393c0"
   },
   "source": [
    "# **Function for applying a list of row filters to a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0fc5b7b7-75f4-4619-bc08-3c51144765d8"
   },
   "outputs": [],
   "source": [
    "def APPLY_ROW_FILTERS_LIST (df, list_of_row_filters):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Warning: this function filter the rows and results into a smaller dataset, since it removes the non-selected entries.\")\n",
    "    print(\"If you want to pass a filter to simply label the selected rows, use the function LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\")\n",
    "    \n",
    "    # This function applies filters to the dataframe and remove the non-selected entries.\n",
    "    \n",
    "    # df: dataframe to be analyzed.\n",
    "    \n",
    "    ## define the filters and only them define the filters list\n",
    "    # EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "    # boolean_filter1 = ((None) & (None)) \n",
    "    # (condition1 AND (&) condition2)\n",
    "    # boolean_filter2 = ((None) | (None)) \n",
    "    # condition1 OR (|) condition2\n",
    "    \n",
    "    # boolean filters result into boolean values True or False.\n",
    "\n",
    "    ## Examples of filters:\n",
    "    ## filter1 = (condition 1) & (condition 2)\n",
    "    ## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "    ## filter2 = (condition)\n",
    "    ## filter2 = (df['column3'] <= 2.5)\n",
    "    ## filter3 = (df['column4'] > 10.7)\n",
    "    ## filter3 = (condition 1) | (condition 2)\n",
    "    ## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "\n",
    "    ## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "    ## <= (lower or equal); == (equal); != (different)\n",
    "\n",
    "    ## concatenation operators: & (and): the filter is True only if the \n",
    "    ## two conditions concatenated through & are True\n",
    "    ## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "    ## through | are True.\n",
    "    \n",
    "    ## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "    ## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "    ## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "    ## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "\n",
    "    ## separate conditions with parentheses. Use parentheses to define a order\n",
    "    ## of definition of the conditions:\n",
    "    ## filter = ((condition1) & (condition2)) | (condition3)\n",
    "    ## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "    ## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "    \n",
    "    # list_of_row_filters: list of boolean filters to be applied to the dataframe\n",
    "    # e.g. list_of_row_filters = [filter1]\n",
    "    # applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "    # boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "    # That is because the function will loop through the list of filters.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3, filter4]\n",
    "    # will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "    # Notice that the filters must be declared in the order you want to apply them.\n",
    "    \n",
    "    \n",
    "    #Loop through the filters list, applying the filters sequentially:\n",
    "    # Each element of the list is identified as 'boolean_filter'\n",
    "    for boolean_filter in list_of_row_filters:\n",
    "        \n",
    "        #Apply the filter:\n",
    "        df = df[boolean_filter]\n",
    "    \n",
    "    print(\"Successfully filtered the dataframe. Check the 10 first rows of the filtered and returned dataframe:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "354f6b12-a83a-47fc-a9a7-03249d92b7ee"
   },
   "source": [
    "# **Function for selecting subsets from a dataframe (using row filters) and labelling these subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9894acba-26ed-486c-915c-c04079fa6ee3"
   },
   "outputs": [],
   "source": [
    "def LABEL_DATAFRAME_SUBSETS (df, list_of_row_filters, new_labels_list = None, new_labelled_column_name = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Attention: this function selects subsets from the dataframe and label them, allowing the seggregation of the data.\")\n",
    "    print(\"If you want to filter the dataframe to eliminate non-selected rows, use the function APPLY_ROW_FILTERS_LIST\")\n",
    "    \n",
    "    ## This function selects subsets of the dataframe by applying a list\n",
    "    ## of row filters, and then it labels each one of the filtered subsets.\n",
    "    \n",
    "    # df: dataframe to be analyzed.\n",
    "    \n",
    "    ## define the filters and only them define the filters list\n",
    "    # EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "    # boolean_filter1 = ((None) & (None)) \n",
    "    # (condition1 AND (&) condition2)\n",
    "    # boolean_filter2 = ((None) | (None)) \n",
    "    # condition1 OR (|) condition2\n",
    "    \n",
    "    # boolean filters result into boolean values True or False.\n",
    "\n",
    "    ## Examples of filters:\n",
    "    ## filter1 = (condition 1) & (condition 2)\n",
    "    ## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "    ## filter2 = (condition)\n",
    "    ## filter2 = (df['column3'] <= 2.5)\n",
    "    ## filter3 = (df['column4'] > 10.7)\n",
    "    ## filter3 = (condition 1) | (condition 2)\n",
    "    ## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "\n",
    "    ## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "    ## <= (lower or equal); == (equal); != (different)\n",
    "\n",
    "    ## concatenation operators: & (and): the filter is True only if the \n",
    "    ## two conditions concatenated through & are True\n",
    "    ## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "    ## through | are True.\n",
    "    \n",
    "    ## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "    ## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "    ## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "    ## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "\n",
    "    ## separate conditions with parentheses. Use parentheses to define a order\n",
    "    ## of definition of the conditions:\n",
    "    ## filter = ((condition1) & (condition2)) | (condition3)\n",
    "    ## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "    ## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "    \n",
    "    # list_of_row_filters: list of boolean filters to be applied to the dataframe\n",
    "    # e.g. list_of_row_filters = [filter1]\n",
    "    # applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "    # boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "    # That is because the function will loop through the list of filters.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3, filter4]\n",
    "    # will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "    # Notice that the filters must be declared in the order you want to apply them.\n",
    "        \n",
    "    # Here, each filter will be associated with one label. Therefore, if you want to apply \n",
    "    # a filter with several conditions, combine them using the & and | operators.\n",
    "    \n",
    "    # The labels are input as a list of strings or of discrete integer values. There must be\n",
    "    # exactly one filter to each label: the filter will be used to obtain a subset from\n",
    "    # the dataframe, and the label will be applied to all the rows of this subset.\n",
    "    # e.g. list_of_row_filters = [filter1, filter2], new_labels_list = ['label1', 'label2']\n",
    "    # In this case, the rows of the dataset obtained when applying filter1 will receive the\n",
    "    # string 'label1' as label. In turns, \n",
    "    # rows selected from filter2 will be labelled as 'label2'.\n",
    "    # list_of_row_filters = [filter1, filter2, filter3], new_labels_list = [1, 2, 3]\n",
    "    # Here, the labels are integers: rows filtered through filter1 are marked as 1;\n",
    "    # rows selected by filter2 are labelled as 2; and rows selected from filter3 are marked\n",
    "    # as 3.\n",
    "    \n",
    "    # Warning: the sequence of filters must be correspondent to the sequence of labels. \n",
    "    # Rows selected from the first filter are labelled with the first item from the labels\n",
    "    # list; rows selected by the 2nd filter are labelled with the 2nd element, and so on.\n",
    "    \n",
    "    # If no list of labels is provided, a list of integers starting from zero and with the\n",
    "    # same total of elements from list_of_row_filters will be created. For instance,\n",
    "    # if list_of_row_filters = [filter1, filter2, filter3, filter4], the list of labels\n",
    "    # [0, 1, 2, 3] would be created. Again, the rows selected from filter1 would be labelled\n",
    "    # as 0; rows selected from filter 2 would be labelled as 1; and so on.\n",
    "    \n",
    "    # new_labelled_column_name = None. Alternatively, declare a string (inside quotes), with\n",
    "    # the name of the column containing the applied labels. For example, if\n",
    "    # new_labelled_column_name = 'labels', then the new labels are saved in the column 'labels'\n",
    "    # If no new_labelled_column_name is provided, the default name = \"df_label\" is provided.\n",
    "    \n",
    "    if (new_labelled_column_name is None):\n",
    "        \n",
    "        new_labelled_column_name = \"df_label\"\n",
    "    \n",
    "    if (new_labels_list is None):\n",
    "        \n",
    "        #Create a list of integers with the same length as list_of_row_filters\n",
    "        # start the list:\n",
    "        new_labels_list = []\n",
    "        # Append the value zero:\n",
    "        new_labels_list.append(0)\n",
    "        \n",
    "        #check the total of elements of list list_of_row_filters:\n",
    "        total_of_filters = len(list_of_row_filters)\n",
    "        \n",
    "        #loop from i = 1 to i = (total_of_filters - 1), index of the last element from\n",
    "        #list total_of_filters. Append these values to the list of new labels:\n",
    "        for i in range (1, total_of_filters):\n",
    "            #i goes from 1 to (total_of_filters - 1).\n",
    "            # the element 0 was already added\n",
    "            new_labels_list.append(i)\n",
    "            # now we have a list of integers from 0 to (total_of_filters - 1), where each\n",
    "            # number is a label to be applied to the rows selected from one of the filters.\n",
    "        \n",
    "    # Let's select rows applying the filters, and label them according    \n",
    "    \n",
    "    #Loop through the filters list, applying the filters sequentially:\n",
    "    # Each element of the list is identified as 'boolean_filter'\n",
    "    for i in range (0, total_of_filters):\n",
    "        # Here, i ranges from 0 to total_of_filters - 1, index of the last element\n",
    "        # We specifically declare 0 as the first element to avoid problems derived\n",
    "        # from the fact that a counting variable i was already used and could have a\n",
    "        # value in the memory. The value 0 is the default when only one value is declared\n",
    "        # as argument from range\n",
    "        \n",
    "        #select the boolean filter from list_of_row_filters. It is the i-th indexed element \n",
    "        boolean_filter = list_of_row_filters[i]\n",
    "        # Select the correspondent label from new_labels_list:\n",
    "        applied_label = new_labels_list[i]\n",
    "        \n",
    "        #Apply the filter to select a group of rows, and apply the correspondent label\n",
    "        # to the selected rows\n",
    "        \n",
    "        # syntax: dataset.loc[dataset['column_filtered'] <= 0.87, 'labelled_column'] = 1\n",
    "        # which is equivalent to dataset.loc[(filter), 'labelled_column'] = label\n",
    "        df.loc[(boolean_filter), new_labelled_column_name] = applied_label\n",
    "    \n",
    "    print(\"Successfully labelled the dataframe. Check the 10 first rows of the labelled and returned dataframe:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d63008c0-72cd-4eab-ab30-499c733a2a69"
   },
   "source": [
    "# **Function for performing Analysis of Variance (ANOVA) and obtaining boxplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "azdata_cell_guid": "8ca7e1cf-e19f-4eae-98e6-780392820baa",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def anova_boxplot (df, column_to_analyze, column_with_group_labels = None, confidence_level = 0.95, boxplot_notch = False, boxplot_vertically_oriented = True, boxplot_patch_artist = False,  reference_value = None, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "        \n",
    "        print (\"If an error message is shown, update statsmodels. Declare and run a cell as:\")\n",
    "        print (\"!pip install statsmodels --upgrade\")\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        from statsmodels.stats.oneway import anova_oneway\n",
    "        \n",
    "        # df: dataframe to be analyzed.\n",
    "    \n",
    "        #column_to_analyze: name of the new column. e.g. column_to_analyze = 'col1'\n",
    "        # will analyze column named as 'col1'\n",
    "        \n",
    "        # column_with_group_labels: name of the column registering groups, labels, or\n",
    "        # factors correspondent to each row. e.g. column_with_group_labels = \"group\"\n",
    "        # if the correspondent group is saved as column named 'group'.\n",
    "        # If no name is provided, then the default name is used:\n",
    "        # COLUMN_WITH_GROUP_LABELS = COLUMN_TO_ANALYZE + \"_label\"\n",
    "        \n",
    "        if (column_with_group_labels is None):\n",
    "            \n",
    "            column_with_group_labels = column_to_analyze + \"_label\"\n",
    "    \n",
    "        # CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "        # Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "        # Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "        # to get less restrictive results.\n",
    "        alpha = 1 - confidence_level\n",
    "        \n",
    "        # reference_value: keep it as None or add a float value.\n",
    "        # This reference value will be shown as a red constant line on the plot to be compared\n",
    "        # with the boxes. e.g. reference_value = 1.0 will plot a red line passing through\n",
    "        # column_to_analyze = 1.0\n",
    "        \n",
    "        # boxplot_notch = False\n",
    "        # Manipulate parameter notch (boolean, default: False) from the boxplot object\n",
    "        # Whether to draw a notched boxplot (True), or a rectangular boxplot (False). \n",
    "        # The notches represent the confidence interval (CI) around the median. \n",
    "        # The documentation for bootstrap describes how the locations of the notches are \n",
    "        # computed by default, but their locations may also be overridden by setting the \n",
    "        # conf_intervals parameter.\n",
    "        \n",
    "        # boxplot_vertically_oriented = True\n",
    "        # Manipulate the parameter vert (boolean, default: True)\n",
    "        # If True, draws vertical boxes. If False, draw horizontal boxes.\n",
    "        \n",
    "        # boxplot_patch_artist = False\n",
    "        # Manipulate parameter patch_artist (boolean, default: False)\n",
    "        # If False produces boxes with the Line2D artist. Otherwise, boxes are drawn \n",
    "        # with Patch artists.\n",
    "        # Check documentation:\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html\n",
    "        \n",
    "        # Create a local copy of the dataframe to be manipulated:\n",
    "        DATASET = df\n",
    "        \n",
    "        # Sort the dataframe by the groups/factors column (column_with_group_labels):\n",
    "        DATASET = DATASET.sort_values(column_with_group_labels, ascending = True)\n",
    "        \n",
    "        # Reset the index of the new dataframe:\n",
    "        DATASET = DATASET.reset_index(drop = True)\n",
    "        \n",
    "        # Get a series for the analyzed values and for the \n",
    "        # column with the correspondent labels:\n",
    "        analyzed_series = DATASET[column_to_analyze]\n",
    "        groups_series = DATASET[column_with_group_labels]\n",
    "        \n",
    "        # Get a series of unique values of groups\n",
    "        # Convert the series of columns into a NumPy array\n",
    "        array_of_groups = np.array(groups_series)\n",
    "        \n",
    "        #Keep only the unique values into this array:\n",
    "        array_of_groups = np.unique(array_of_groups)\n",
    "        \n",
    "        #Get the total of groups\n",
    "        total_of_groups = len(array_of_groups)\n",
    "        \n",
    "        \n",
    "        # Create a dictionary of NumPy arrays. This dictionary will store NumPy arrays\n",
    "        # correspondent to the elements of each group:\n",
    "        values_for_each_group_dict = {}\n",
    "        \n",
    "        # Loop through each element of the array_of_groups (array with unique values).\n",
    "        # The elements of array_of_groups are identified as 'group'\n",
    "        for group in array_of_groups:\n",
    "            \n",
    "            #start a list to store the values:\n",
    "            analyzed_vals_list = []\n",
    "            \n",
    "            # Now loop through each row of the dataframe. The dataframe row index go from\n",
    "            # zero to len(DATASET) - 1:\n",
    "            for i in range((len(DATASET) - 1)):\n",
    "                #loop go from i = 0 to i = len(DATASET) - 1, index of the last row\n",
    "                \n",
    "                # if the row in index is correspondent to the grouping/label 'group',\n",
    "                # then store the analyzed value into list analyzed_vals_list:\n",
    "                if (groups_series[i] == group):\n",
    "                    \n",
    "                    analyzed_vals_list.append(analyzed_series[i])\n",
    "            \n",
    "            # lets convert the lists into NumPy arrays:\n",
    "            analyzed_vals_array = np.array(analyzed_vals_list)\n",
    "            \n",
    "            # Now let's update the dictionary values_for_each_group_dict\n",
    "            # 1. Set the 'key' of the dictionary as the name of the group\n",
    "            dict_key = group\n",
    "            \n",
    "            # Use the update method to add the key 'dict_key', and the \n",
    "            # array analyzed_vals_array as the value correspondent to the key.\n",
    "            \n",
    "            # syntax: dict.update({\"key\": value})\n",
    "            # check: https://www.w3schools.com/python/ref_dictionary_update.asp\n",
    "            \n",
    "            values_for_each_group_dict.update({dict_key: analyzed_vals_array})\n",
    "        \n",
    "        # Now, the dictionary values_for_each_group_dict stores each group name as its keys\n",
    "        # and the arrays of analyzed values as the correspondent object.\n",
    "        \n",
    "        # Notice that the keys of the dictionary were created with the same order of\n",
    "        # array_of_groups, so there is no risk of losing the correspondence\n",
    "        \n",
    "        # Create a humongous list of arrays, containing the arrays stored into the\n",
    "        # dictionary. It is necessary for the statsmodels API:\n",
    "        humongous_list = []\n",
    "        \n",
    "        # Loop through each key of the dictionary:\n",
    "        for group in values_for_each_group_dict.keys():\n",
    "            # loop through each element of the array of keys from the dictionary.\n",
    "            # each element is named 'group'.\n",
    "            # append the correspondent array as an element from humongous_list\n",
    "            humongous_list.append(values_for_each_group_dict[group])\n",
    "        \n",
    "        #Now, we can pass the humongous_list as argument for the one-way Anova:\n",
    "        anova_one_way_summary = anova_oneway(humongous_list, groups = array_of_groups, use_var = 'bf', welch_correction=True, trim_frac=0)\n",
    "        # When use_var = 'bf', variances are not assumed to be equal across samples.\n",
    "        # Check documentation: \n",
    "        # https://www.statsmodels.org/stable/generated/statsmodels.stats.oneway.anova_oneway.html\n",
    "        \n",
    "        # The information is stored in a tuple (f_statistic, p-value)\n",
    "        # f_statistic: Test statistic for k-sample mean comparison which is approximately \n",
    "        # F-distributed.\n",
    "        # p-value: If use_var=\"bf\", then the p-value is based on corrected degrees of freedom following Mehrotra 1997.\n",
    "        f_statistic = anova_one_way_summary[0]\n",
    "        p_value = anova_one_way_summary[1]\n",
    "        \n",
    "        print(f\"Probability that the means of the groups are the same = {100*p_value}\\% (p-value = {p_value})\")\n",
    "        print(f\"Calculated F-statistic for the variances = {f_statistic}\")\n",
    "        \n",
    "        if (p_value <= alpha):\n",
    "            print(f\"For a confidence level of {confidence_level*100}\\%, we can reject the null hypothesis.\")\n",
    "            print(f\"The means are different for a {confidence_level*100}\\% confidence level.\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"For a confidence level of {confidence_level*100}\\%, we can accept the null hypothesis.\")\n",
    "            print(f\"The means are equal for a {confidence_level*100}\\% confidence level.\")\n",
    "        \n",
    "        print(\"Check ANOVA summary:\\n\")\n",
    "        print(anova_one_way_summary)\n",
    "        # When printing the summary, the set of supporting information of the Tuple are shown.\n",
    "        \n",
    "        anova_summary_dict = {'F_statistic': f_statistic, 'p_value': p_value}\n",
    "        \n",
    "        #Now, let's obtain the boxplot\n",
    "        fig, ax = plt.subplots()\n",
    "                    \n",
    "        # rectangular box plot\n",
    "        # The arrays of each group are the elements of the list humongous_list\n",
    "        boxplot_returned_dict = ax.boxplot(humongous_list, labels = array_of_groups, notch = boxplot_notch, vert = boxplot_vertically_oriented, patch_artist = boxplot_patch_artist)\n",
    "        \n",
    "        # boxplot_returned_dict: A dictionary mapping each component of the boxplot to \n",
    "        # a list of the Line2D instances created. That dictionary has the following keys \n",
    "        # (assuming vertical boxplots):\n",
    "        # boxes: the main body of the boxplot showing the quartiles and the median's \n",
    "        # confidence intervals if enabled.\n",
    "        # medians: horizontal lines at the median of each box.\n",
    "        # whiskers: the vertical lines extending to the most extreme, non-outlier data \n",
    "        # points.\n",
    "        # caps: the horizontal lines at the ends of the whiskers.\n",
    "        # fliers: points representing data that extend beyond the whiskers (fliers).\n",
    "        # means: points or lines representing the means.\n",
    "          \n",
    "        ax.set_title(f\"Boxplot of {column_to_analyze} by {col_with_group_indication}\")\n",
    "        \n",
    "        if (boxplot_vertically_oriented == True):\n",
    "            # generate vertically-oriented boxplot\n",
    "        \n",
    "            ax.set(ylabel = column_to_analyze)\n",
    "            # If the boxplot was horizontally oriented, this label should be the X instead.\n",
    "            # The X labels were already defined when creating the boxplot\n",
    "                    \n",
    "            if not (reference_value is None):\n",
    "                # Add an horizontal reference_line to compare against the boxes:\n",
    "                # If the boxplot was horizontally-oriented, this line should be vertical instead.\n",
    "                ax.axhline(reference_value, color = 'red', linestyle = 'dashed', label = 'Reference value')\n",
    "                # axhline generates an horizontal (h) line on ax\n",
    "                \n",
    "        else:\n",
    "            # In case it is False, generate horizontally-oriented plot:\n",
    "            ax.set(xlabel = column_to_analyze)\n",
    "            # The Y labels were already defined when creating the boxplot\n",
    "                    \n",
    "            if not (reference_value is None):\n",
    "                # Add an horizontal reference_line to compare against the boxes:\n",
    "                # If the boxplot was horizontally-oriented, this line should be vertical instead.\n",
    "                ax.axvline(reference_value, color = 'red', linestyle = 'dashed', label = 'Reference value')\n",
    "                # axvline generates a vertical (v) line on ax\n",
    "        \n",
    "            # Now, we can add general components of the graphic:\n",
    "            #Create an almost transparent horizontal line to guide the eyes without distracting\n",
    "            ax.yaxis.grid(grid, linestyle='-', which = 'major', color = 'lightgrey', alpha = 0.5)\n",
    "            ax.xaxis.grid(grid, linestyle='-', which = 'major', color = 'lightgrey', alpha = 0.5)       \n",
    "            \n",
    "            #ROTATE X AXIS IN XX DEGREES\n",
    "            plt.xticks(rotation = x_axis_rotation)\n",
    "            # XX = 70 DEGREES x_axis (Default)\n",
    "            #ROTATE Y AXIS IN XX DEGREES:\n",
    "            plt.yticks(rotation = y_axis_rotation)\n",
    "            # XX = 0 DEGREES y_axis (Default)\n",
    "            ax.legend()\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"/\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = \"boxplot\"\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 110 dpi\n",
    "                    png_resolution_dpi = 110\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            #fig.tight_layout()\n",
    "\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\n\") #line break\n",
    "            print(\"Successfully returned 2 dictionaries: anova_summary_dict (dictionary storing ANOVA F-test and p-value); and boxplot_returned_dict (dictionary mapping each component of the boxplot).\\n\")\n",
    "            \n",
    "            print(\"Boxplot interpretation:\")\n",
    "            print(\"Boxplot presents the following key visual components:\")\n",
    "            print(\"The main box represents the Interquartile Range (IQR). It represents the data that is from quartile Q1 to quartile Q3.\")\n",
    "            print(\"Q1 = 1st quartile of the dataset. 25% of values lie below this level (i.e., it is the 0.25-quantile or percentile).\")\n",
    "            print(\"Q2 = 2nd quartile of the dataset. 50% of values lie above and below this level (i.e., it is the 0.50-quantile or percentile).\")\n",
    "            print(\"Q3 = 3rd quartile of the dataset. 75% of values lie below and 25% lie above this level (i.e., it is the 0.75-quantile or percentile).\")\n",
    "            print(\"Boxplot main box (the IQR) is divided by an horizontal line if it is vertically-oriented; or by a vertical line if it is horizontally-oriented.\")\n",
    "            print(\"This line represents the median: it is the midpoint of the dataset.\")\n",
    "            print(\"There are lines extending beyond the main boxes limits. This lines end in horizontal limits, if the boxplot is vertically oriented; or in vertical limits, for an horizontal plot.\")\n",
    "            print(\"The minimum limit of the boxplot is defined as: Q1 - (1.5) x (IQR width) = Q1 - 1.5*(Q3-Q1)\")\n",
    "            print(\"The maximum limit of the boxplot is defined as: Q3 + (1.5) x (IQR width) = Q3 + 1.5*(Q3-Q1)\")\n",
    "            print(\"Finally, there are isolated points (circles) on the plot.\")\n",
    "            print(\"These points lie below the minimum bar, or above the maximum bar line. They are defined as outliers.\")\n",
    "            # https://nickmccullum.com/python-visualization/boxplot/\n",
    "            \n",
    "            return anova_summary_dict, boxplot_returned_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "23dff56e-c692-4df1-908b-e7699d3c94b5"
   },
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "azdata_cell_guid": "9f238dd8-d32f-4a86-baa9-8335446a418d"
   },
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\")\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5f042317-525c-40fc-8d22-96e71cc35196"
   },
   "source": [
    "# **Function for time series visualization**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "azdata_cell_guid": "ef571494-3eb2-4dcc-9ebb-00c1fd6e9aad",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def time_series_vis (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (add_scatter_dots == True):\n",
    "        marker_value = 'o'\n",
    "    else:\n",
    "        marker_value = ''\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (lab1 is None):\n",
    "        \n",
    "        label_1 = lab1\n",
    "    \n",
    "    else:\n",
    "        label_1 = \"Y1\"\n",
    "\n",
    "    if not (x1 is None):\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = marker_value, color='blue', label=label_1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #runs only when both are present\n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = marker_value, color='red', label=label_2)\n",
    "    \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = marker_value, color='green', label=label_3)\n",
    "    \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = marker_value, color='black', label=label_4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = marker_value, color='magenta', label=label_5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = marker_value, color='yellow', label=label_6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #graphic's title\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #X-axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Y-axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"time_series_vis\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f04c07fe-290c-4eaf-898b-921ff34a4c65"
   },
   "source": [
    "# **Functions for histogram visualization**\n",
    "- Function `histogram`: ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons.\n",
    "- Function `histogram_alternative`: histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "azdata_cell_guid": "9d2dc092-e26a-4491-a056-644c9412de6a",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def histogram (y, bar_width, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, normal_curve_overlay = True, data_units_label = None, y_title = None, histogram_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ideal bin interval calculated through Montgomery's method. \n",
    "    # Histogram is obtained from this calculated bin size.\n",
    "    # Douglas C. Montgomery (2009). Introduction to Statistical Process Control, \n",
    "    # Sixth Edition, John Wiley & Sons.\n",
    "    \n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "\n",
    "    y = y.reset_index(drop=True)\n",
    "    #faz com que os indices desta serie sejam consecutivos e a partir de zero\n",
    "\n",
    "    #Estatisticas gerais: media (mu) e desvio-padrao (sigma)\n",
    "    mu = y.mean() \n",
    "    sigma = y.std() \n",
    "\n",
    "    #Calculo do bin-size\n",
    "    highest = y.max()\n",
    "    lowest = y.min()\n",
    "    rangehist = highest - lowest\n",
    "    rangehist = abs(rangehist)\n",
    "    #garante que sera um numero positivo\n",
    "    samplesize = y.count() #contagem do total de entradas\n",
    "    ncells = (samplesize)**0.5 #potenciacao: ** - raiz quadrada de samplesize\n",
    "    #resultado da raiz quadrada e sempre positivo\n",
    "    ncells = round(ncells) #numero \"redondo\" mais proximo\n",
    "    ncells = int(ncells) #parte inteira do numero arredondado\n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    binsize = rangehist/ncells\n",
    "    binsize = round(binsize)\n",
    "    binsize = int(binsize) #precisa ser inteiro\n",
    "    \n",
    "    #Construcao da tabela de frequencias\n",
    "\n",
    "    j = 0 #indice da tabela de frequencias\n",
    "    #Este indice e diferente do ordenamento dos valores em ordem crescente\n",
    "    xhist = []\n",
    "    #Lista vazia que contera os x do histograma\n",
    "    yhist = []\n",
    "    #Listas vazia que conteras o y do histograma\n",
    "    hist_labels = []\n",
    "    #Esta lista gravara os limites da barra na forma de strings\n",
    "\n",
    "    pontomediodabarra = lowest + binsize/2 \n",
    "    limitedabarra = lowest + binsize\n",
    "    #ponto medio da barra \n",
    "    #limite da primeira barra do histograma\n",
    "    seriedohist1 = y\n",
    "    seriedohist1 = seriedohist1.sort_values(ascending=True)\n",
    "    #serie com os valores em ordem crescente\n",
    "    seriedohist1 = seriedohist1.reset_index(drop=True)\n",
    "    #garante que a nova serie tenha indices consecutivos, iniciando em zero\n",
    "    i = 0 #linha inicial da serie do histograma em ordem crescente\n",
    "    valcomparado = seriedohist1[i]\n",
    "    #primeiro valor da serie, o mais baixo\n",
    "\n",
    "    while (j <= (ncells-1)):\n",
    "        \n",
    "        #para quando termina o numero de linhas da tabela\n",
    "        xhist.append(pontomediodabarra)\n",
    "        #tempo da tabela de frequencias\n",
    "        cont = 0\n",
    "        #variavel de contagem do histograma\n",
    "        #contagem deve ser reiniciada\n",
    "       \n",
    "        if (i < samplesize):\n",
    "            #2 condicionais para impedir que um termo de indice inexistente\n",
    "            #seja acessado\n",
    "            while (valcomparado <= limitedabarra) and (valcomparado < highest):\n",
    "                #o segundo criterio garante a parada em casos em que os dados sao\n",
    "                #muito proximos\n",
    "                    cont = cont + 1 #adiciona contagem a tabela de frequencias\n",
    "                    i = i + 1\n",
    "                    \n",
    "                    if (i < samplesize): \n",
    "                        valcomparado = seriedohist1[i]\n",
    "        \n",
    "        yhist.append(cont) #valor de ocorrencias contadas\n",
    "        \n",
    "        limite_infdabarra = pontomediodabarra - binsize/2\n",
    "        rotulo = \"%.2f - %.2f\" %(limite_infdabarra, limitedabarra)\n",
    "        #intervalo da tabela de frequencias\n",
    "        #%.2f: 2 casas decimais de aproximação\n",
    "        hist_labels.append(rotulo)\n",
    "        \n",
    "        pontomediodabarra = pontomediodabarra + binsize\n",
    "        #tanto os pontos medios quanto os limites se deslocam do mesmo intervalo\n",
    "        \n",
    "        limitedabarra = limitedabarra + binsize\n",
    "        #proxima barra\n",
    "        \n",
    "        j = j + 1\n",
    "    \n",
    "    #Temos que verificar se o valor maximo foi incluido\n",
    "    #isso porque o processo de aproximacao por numero inteiro pode ter\n",
    "    #arredondado para baixo e excluido o limite superior\n",
    "    #Porem, note que na ultima iteracao o limite superior da barra foi \n",
    "    #somado de binsize, mas como j ja e maior que ncells-1, o loop parou\n",
    "    \n",
    "    #assim, o limitedabarra nesse momento e o limite da barra que seria\n",
    "    #construida em seguida, nao da ultima barra da tabela de frequencias\n",
    "    #isso pode fazer com que esta barra ja seja maior que o highest\n",
    "    \n",
    "    #note porem que nao aumentamos o valor do limite inferior da barra\n",
    "    #por isso, basta vermos se ele mais o binsize sao menores que o valor mais alto\n",
    "        \n",
    "    while ((limite_infdabarra+binsize) < highest):\n",
    "        \n",
    "        #vamos criar novas linhas ate que o ponto mais alto do histograma\n",
    "        #tenha sido contado\n",
    "        ncells = ncells + 1 #adiciona uma linha a tabela de frequencias\n",
    "        xhist.append(pontomediodabarra)\n",
    "        \n",
    "        cont = 0 #variavel de contagem do histograma\n",
    "        \n",
    "        while (valcomparado <= limitedabarra):\n",
    "                cont = cont + 1 #adiciona contagem a tabela de frequencias\n",
    "                i = i + 1\n",
    "                if (i < samplesize):\n",
    "                    valcomparado = seriedohist1[i]\n",
    "                    #apenas se i ainda nao e maior que o total de dados\n",
    "                \n",
    "                else: \n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        #parar o loop se i atingiu um tamanho maior que a quantidade \n",
    "        #de dados.Temos que ter este cuidado porque estamos acrescentando\n",
    "        #mais linhas a tabela de frequencias para corrigir a aproximacao\n",
    "        #de ncells por um numero inteiro\n",
    "        \n",
    "        yhist.append(cont) #valor de ocorrencias contadas\n",
    "        \n",
    "        limite_infdabarra = pontomediodabarra - binsize/2\n",
    "        rotulo = \"%.2f - %.2f\" %(limite_infdabarra, limitedabarra)\n",
    "        #intervalo da tabela de frequencias - 2 casas decimais\n",
    "        hist_labels.append(rotulo)\n",
    "        \n",
    "        pontomediodabarra = pontomediodabarra + binsize\n",
    "        #tanto os pontos medios quanto os limites se deslocam do mesmo intervalo\n",
    "        \n",
    "        limitedabarra = limitedabarra + binsize\n",
    "        #proxima barra\n",
    "        \n",
    "    estatisticas_col1 = []\n",
    "    #contera as descricoes das colunas da tabela de estatisticas gerais\n",
    "    estatisticas_col2 = []\n",
    "    #contera os valores da tabela de estatisticas gerais\n",
    "    \n",
    "    estatisticas_col1.append(\"Count of data evaluated\")\n",
    "    estatisticas_col2.append(samplesize)\n",
    "    estatisticas_col1.append(\"Average (mu)\")\n",
    "    estatisticas_col2.append(mu)\n",
    "    estatisticas_col1.append(\"Standard deviation (sigma)\")\n",
    "    estatisticas_col2.append(sigma)\n",
    "    estatisticas_col1.append(\"Highest value\")\n",
    "    estatisticas_col2.append(highest)\n",
    "    estatisticas_col1.append(\"Lowest value\")\n",
    "    estatisticas_col2.append(lowest)\n",
    "    estatisticas_col1.append(\"Data range (maximum value - lowest value)\")\n",
    "    estatisticas_col2.append(rangehist)\n",
    "    estatisticas_col1.append(\"Bin size (bar width)\")\n",
    "    estatisticas_col2.append(binsize)\n",
    "    estatisticas_col1.append(\"Total rows in frequency table\")\n",
    "    estatisticas_col2.append(ncells)\n",
    "    #como o comando append grava linha a linha em sequencia, garantimos\n",
    "    #a correspondencia das colunas\n",
    "    #Assim como em qualquer string, incluindo de rotulos de graficos\n",
    "    #os \\n sao lidos como quebra de linha\n",
    "    \n",
    "    d1 = {\"General Statistics\": estatisticas_col1, \"Calculated Value\": estatisticas_col2}\n",
    "    #dicionario das duas series, para criar o dataframe com as descricoes\n",
    "    estatisticas_gerais = pd.DataFrame(data = d1)\n",
    "    \n",
    "    #Casos os títulos estejam presentes (valor nao e None):\n",
    "    #vamos utiliza-los\n",
    "    #Caso contrario, vamos criar nomenclaturas genericas para o histograma\n",
    "    \n",
    "    eixo_y = \"Counting/Frequency\"\n",
    "    \n",
    "    if not (data_units_label is None):\n",
    "        xlabel = data_units_label\n",
    "    \n",
    "    else:\n",
    "        xlabel = \"Frequency\\n table data\"\n",
    "    \n",
    "    if not (y_title is None):\n",
    "        eixo_x = y_title\n",
    "        #lembre-se que no histograma, os dados originais vao pro eixo X\n",
    "        #O eixo Y vira o eixo da contagem/frequencia daqueles dados\n",
    "    \n",
    "    else:\n",
    "        eixo_x = \"X: Mean value of the interval\"\n",
    "    \n",
    "    if not (histogram_title is None):\n",
    "        string1 = \"- $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        main_label = histogram_title + string1\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        main_label = \"Data Histogram - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        #os simbolos $\\ $ substituem o simbolo pela letra grega\n",
    "    \n",
    "    d2 = {\"Considered interval\": hist_labels, eixo_x: xhist, eixo_y: yhist}\n",
    "    #dicionario que compoe a tabela de frequencias\n",
    "    tab_frequencias = pd.DataFrame(data = d2)\n",
    "    #cria a tabela de frequencias como um dataframe de saida\n",
    "   \n",
    "    #parametros da normal ja calculados:\n",
    "    #mu e sigma\n",
    "    #numero de bins: ncells\n",
    "    #limites de especificacao: lsl,usl - target\n",
    "    \n",
    "    #valor maximo do histograma\n",
    "    max_hist = max(yhist)\n",
    "    #seleciona o valor maximo da serie, para ajustar a curva normal\n",
    "    #isso porque a normal é criada com valores entre 0 e 1\n",
    "    #multiplicando ela por max_hist, fazemos ela se adequar a altura do histograma\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        #construir a normal ajustada/esperada\n",
    "        #vamos criar pontos ao redor da media mu - 4sigma ate mu + 4sigma, \n",
    "        #de modo a garantir a quase totalidade da curva normal. \n",
    "        #O incremento será de 0.10 sigma a cada iteracao\n",
    "        x_inf = mu -(4)*sigma\n",
    "        x_sup = mu + 4*sigma\n",
    "        x_inc = (0.10)*sigma\n",
    "        \n",
    "        x_normal_adj = []\n",
    "        y_normal_adj = []\n",
    "        \n",
    "        x_adj = x_inf\n",
    "        y_adj = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (x_adj - mu))**2))\n",
    "        x_normal_adj.append(x_adj)\n",
    "        y_normal_adj.append(y_adj)\n",
    "        \n",
    "        while(x_adj < x_sup): \n",
    "            \n",
    "            x_adj = x_adj + x_inc\n",
    "            y_adj = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (x_adj - mu))**2))\n",
    "            x_normal_adj.append(x_adj)\n",
    "            y_normal_adj.append(y_adj)\n",
    "        \n",
    "        #vamos ajustar a altura da curva ao histograma. Para isso, precisamos\n",
    "        #calcular quantas vezes o ponto mais alto do histograma é maior que o ponto\n",
    "        #mais alto da normal (chamaremos essa relação de fator). A seguir,\n",
    "        #multiplicamos cada elemento da normal por este mesmo fator\n",
    "        max_normal = max(y_normal_adj) \n",
    "        #maximo da normal ajustada, numero entre 0 e 1\n",
    "        \n",
    "        fator = (max_hist)/(max_normal)\n",
    "        size_normal = len(y_normal_adj) #quantidade de dados criados\n",
    "        \n",
    "        i = 0\n",
    "        while (i < size_normal):\n",
    "            y_normal_adj[i] = (y_normal_adj[i])*(fator)\n",
    "            i = i + 1\n",
    "    \n",
    "    #Fazer o grafico\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "    \n",
    "        #adicionar a normal\n",
    "        ax.plot(x_normal_adj, y_normal_adj, color = 'black', label = 'Adjusted/expected\\n normal curve')\n",
    "    \n",
    "    ax.set_xlabel(eixo_x)\n",
    "    ax.set_ylabel(eixo_y)\n",
    "    ax.set_title(main_label)\n",
    "    ax.set_xticks(xhist)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(grid)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"General statistics:\\n\")\n",
    "    print(estatisticas_gerais)\n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Frequency table:\\n\")\n",
    "    print(tab_frequencias)\n",
    "\n",
    "    return estatisticas_gerais, tab_frequencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "azdata_cell_guid": "3fe31404-56d2-4590-87a3-58d4ead49706",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def histogram_alternative (y, total_of_bins, bar_width, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, data_units_label = None, y_title = None, histogram_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "    \n",
    "    #this variable here is to simply guarantee the compatibility of the function,\n",
    "    # with no extensive code modifications. It has no real effect.\n",
    "    normal_curve_overlay = True\n",
    "    \n",
    "\n",
    "    y = y.reset_index(drop=True)\n",
    "    #faz com que os indices desta serie sejam consecutivos e a partir de zero\n",
    "\n",
    "    #Estatisticas gerais: media (mu) e desvio-padrao (sigma)\n",
    "    mu = y.mean() \n",
    "    sigma = y.std() \n",
    "\n",
    "    #Calculo do bin-size\n",
    "    highest = y.max()\n",
    "    lowest = y.min()\n",
    "    rangehist = highest - lowest\n",
    "    rangehist = abs(rangehist)\n",
    "    #garante que sera um numero positivo\n",
    "    samplesize = y.count() #contagem do total de entradas\n",
    "    ncells = (samplesize)**0.5 #potenciacao: ** - raiz quadrada de samplesize\n",
    "    #resultado da raiz quadrada e sempre positivo\n",
    "    ncells = round(ncells) #numero \"redondo\" mais proximo\n",
    "    ncells = int(ncells) #parte inteira do numero arredondado\n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    binsize = rangehist/ncells\n",
    "    binsize = round(binsize)\n",
    "    binsize = int(binsize) #precisa ser inteiro\n",
    "    \n",
    "    #Construcao da tabela de frequencias\n",
    "\n",
    "    j = 0 #indice da tabela de frequencias\n",
    "    #Este indice e diferente do ordenamento dos valores em ordem crescente\n",
    "    xhist = []\n",
    "    #Lista vazia que contera os x do histograma\n",
    "    yhist = []\n",
    "    #Listas vazia que conteras o y do histograma\n",
    "    hist_labels = []\n",
    "    #Esta lista gravara os limites da barra na forma de strings\n",
    "\n",
    "    pontomediodabarra = lowest + binsize/2 \n",
    "    limitedabarra = lowest + binsize\n",
    "    #ponto medio da barra \n",
    "    #limite da primeira barra do histograma\n",
    "    seriedohist1 = y\n",
    "    seriedohist1 = seriedohist1.sort_values(ascending=True)\n",
    "    #serie com os valores em ordem crescente\n",
    "    seriedohist1 = seriedohist1.reset_index(drop=True)\n",
    "    #garante que a nova serie tenha indices consecutivos, iniciando em zero\n",
    "    i = 0 #linha inicial da serie do histograma em ordem crescente\n",
    "    valcomparado = seriedohist1[i]\n",
    "    #primeiro valor da serie, o mais baixo\n",
    "        \n",
    "    estatisticas_col1 = []\n",
    "    #contera as descricoes das colunas da tabela de estatisticas gerais\n",
    "    estatisticas_col2 = []\n",
    "    #contera os valores da tabela de estatisticas gerais\n",
    "    \n",
    "    estatisticas_col1.append(\"Count of data evaluated\")\n",
    "    estatisticas_col2.append(samplesize)\n",
    "    estatisticas_col1.append(\"Average (mu)\")\n",
    "    estatisticas_col2.append(mu)\n",
    "    estatisticas_col1.append(\"Standard deviation (sigma)\")\n",
    "    estatisticas_col2.append(sigma)\n",
    "    estatisticas_col1.append(\"Highest value\")\n",
    "    estatisticas_col2.append(highest)\n",
    "    estatisticas_col1.append(\"Lowest value\")\n",
    "    estatisticas_col2.append(lowest)\n",
    "    estatisticas_col1.append(\"Data range (maximum value - lowest value)\")\n",
    "    estatisticas_col2.append(rangehist)\n",
    "    estatisticas_col1.append(\"Bin size (bar width)\")\n",
    "    estatisticas_col2.append(binsize)\n",
    "    estatisticas_col1.append(\"Total rows in frequency table\")\n",
    "    estatisticas_col2.append(ncells)\n",
    "    #como o comando append grava linha a linha em sequencia, garantimos\n",
    "    #a correspondencia das colunas\n",
    "    #Assim como em qualquer string, incluindo de rotulos de graficos\n",
    "    #os \\n sao lidos como quebra de linha\n",
    "    \n",
    "    d1 = {\"General Statistics\": estatisticas_col1, \"Calculated Value\": estatisticas_col2}\n",
    "    #dicionario das duas series, para criar o dataframe com as descricoes\n",
    "    estatisticas_gerais = pd.DataFrame(data = d1)\n",
    "    \n",
    "    #Casos os títulos estejam presentes (valor nao e None):\n",
    "    #vamos utiliza-los\n",
    "    #Caso contrario, vamos criar nomenclaturas genericas para o histograma\n",
    "    \n",
    "    eixo_y = \"Counting/Frequency\"\n",
    "    \n",
    "    if not (data_units_label is None):\n",
    "        xlabel = data_units_label\n",
    "    \n",
    "    else:\n",
    "        xlabel = \"Frequency\\n table data\"\n",
    "    \n",
    "    if not (y_title is None):\n",
    "        eixo_x = y_title\n",
    "        #lembre-se que no histograma, os dados originais vao pro eixo X\n",
    "        #O eixo Y vira o eixo da contagem/frequencia daqueles dados\n",
    "    \n",
    "    else:\n",
    "        eixo_x = \"X: Mean value of the interval\"\n",
    "    \n",
    "    if not (histogram_title is None):\n",
    "        string1 = \"- $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        main_label = histogram_title + string1\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        main_label = \"Data Histogram - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        #os simbolos $\\ $ substituem o simbolo pela letra grega\n",
    "    \n",
    "    d2 = {\"Considered interval\": hist_labels, eixo_x: xhist, eixo_y: yhist}\n",
    "    #dicionario que compoe a tabela de frequencias\n",
    "    tab_frequencias = pd.DataFrame(data = d2)\n",
    "    #cria a tabela de frequencias como um dataframe de saida\n",
    "    \n",
    "    #parametros da normal ja calculados:\n",
    "    #mu e sigma\n",
    "    #numero de bins: ncells\n",
    "    #limites de especificacao: lsl,usl - target\n",
    "    \n",
    "    #valor maximo do histograma\n",
    "    #max_hist = max(yhist)\n",
    "    #seleciona o valor maximo da serie, para ajustar a curva normal\n",
    "    #isso porque a normal é criada com valores entre 0 e 1\n",
    "    #multiplicando ela por max_hist, fazemos ela se adequar a altura do histograma\n",
    "    \n",
    "    \n",
    "    #Fazer o grafico\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    ax.hist(y, bins = total_of_bins, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    #ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    ax.set_xlabel(eixo_x)\n",
    "    ax.set_ylabel(eixo_y)\n",
    "    ax.set_title(main_label)\n",
    "    #ax.set_xticks(xhist)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(grid)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram_alternative\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"General statistics:\\n\")\n",
    "    print(estatisticas_gerais)\n",
    "    # This function is supposed to be used in cases where the differences between data\n",
    "    # is very small. In such cases, there will be no trust values calculated for the \n",
    "    # frequency table. Therefore, we omit it here, but it can be accessed from the\n",
    "    # returned dataframe.\n",
    "\n",
    "    return estatisticas_gerais, tab_frequencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c949e855-3fcf-4b25-b49e-60443d7121e2"
   },
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "azdata_cell_guid": "b42a2c69-94aa-44d1-b53b-449b184531c8"
   },
   "outputs": [],
   "source": [
    "def export_pd_dataframe_as_csv (dataframe_obj_to_be_exported, new_file_name_without_extension, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_obj_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # new_file_name_without_extension - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "    # will export a file 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name_without_extension)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_obj_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_without_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f2293b1f-b51a-4207-9d4e-8b48be925007"
   },
   "source": [
    "# **Function for importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3e55abc-ffc5-49dd-8a38-9467c7917894"
   },
   "outputs": [],
   "source": [
    "def import_export_model_list_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_or_list_file_name = None, directory_path = '', model_type = 'keras', dict_or_list_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickle as pkl\n",
    "    import dill\n",
    "    import tensorflow as tf\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_or_list_only' if only a dictionary or list will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    # model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_or_list_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_or_list_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep dictionary_or_list_file_name = None if no \n",
    "    # dictionary or list will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type = 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "    # lambda layers. Such models are compressed as tar.gz.\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_or_list_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_or_list_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_or_list_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root (empty string):\n",
    "        directory_path = \"\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_or_list_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_or_list_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary or list.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_or_list_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'keras_lambda'):\n",
    "                model_extension = 'tar.gz'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary or list {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary or list successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_lambda'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    # Try accessing the tar.gz file directly from the environment:\n",
    "                    model_path = key\n",
    "                    # to access from the dictionary:\n",
    "                    # model_path = colab_files_dict[key]\n",
    "                    \n",
    "                    # Extract to a temporary 'tmp' directory:\n",
    "                    #try:\n",
    "                    # Compress the directory using tar\n",
    "                    # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    #except:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                    # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                    # https://docs.python.org/3/library/tarfile.html\n",
    "                    # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'r:gz')\n",
    "                    tar_file.extractall(\"tmp/\")\n",
    "                    tar_file.close()\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # Extract to a temporary 'tmp' directory:\n",
    "                    #try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    #except:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                    # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                    # https://docs.python.org/3/library/tarfile.html\n",
    "                    # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'r:gz')\n",
    "                    tar_file.extractall(\"tmp/\")\n",
    "                    tar_file.close()\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    print(f\"TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "\n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "\n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_or_list_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary or list {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary or list successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_lambda'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    \n",
    "                    # Save your model in the SavedModel format\n",
    "                    model_to_export.save('saved_model/my_model')\n",
    "                    \n",
    "                    #try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar -czvf model_path saved_model/\n",
    "                    \n",
    "                    #except NotFoundError:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                    # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                    # https://docs.python.org/3/library/tarfile.html\n",
    "                    # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'w:gz')\n",
    "                    tar_file.add('saved_model/')\n",
    "                    tar_file.close()\n",
    "                    \n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    files.download(key)\n",
    "                    print(f\"TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # Save your model in the SavedModel format\n",
    "                    model_to_export.save('saved_model/my_model')\n",
    "                    \n",
    "                    #try:\n",
    "                        # Compress the directory using tar\n",
    "                    #    ! tar -czvf model_path saved_model/\n",
    "                    \n",
    "                    #except NotFoundError:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                        # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                        # https://docs.python.org/3/library/tarfile.html\n",
    "                        # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'w:gz')\n",
    "                    tar_file.add('saved_model/')\n",
    "                    tar_file.close()\n",
    "                        \n",
    "                    print(f\"TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e0613e43-7712-4754-81c0-0a55c71ea285"
   },
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "azdata_cell_guid": "9cc2d0c2-93b2-4f69-b86b-718f7c0def2f"
   },
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "92cd6328-f691-4196-bcb4-9cfee4c1563c"
   },
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "4b873657-566a-45aa-936b-5e62a71ab279"
   },
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0ee15163-e50f-449d-9a72-7081f2288099",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c228db09-4eb0-4a0d-b1e9-f9f3354c13f3",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "def34299-89fa-4706-9f6a-4aeabd848e5a"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cc5da34e-2cb0-4749-95bf-c96318c60c0b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "687eacdc-0623-46e3-9d3f-58e6588171d9",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9f284996-5cde-4009-9d50-bcf781d87a79"
   },
   "source": [
    "### **Filtering (selecting) or renaming columns of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "86192009-fca6-49df-bd7f-7d72d0986e86"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'filter'\n",
    "# MODE = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "# MODE = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "\n",
    "COLS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "#New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = col_filter_rename (df = DATASET, cols_list = COLS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8fb0e824-15da-4239-bb89-e6902de140af"
   },
   "source": [
    "### **Calculating general statistics from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2edf48d0-67f2-4665-8b20-1dbfb6b0f293"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "# Statistics dataframe saved as general_stats. \n",
    "# Simply modify this object on the left of equality:\n",
    "general_stats = GENERAL_STATISTICS (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f940ba8c-a150-4412-9157-0ac74e60615f"
   },
   "source": [
    "### **Getting data quantiles from a given column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c9aff5b6-cb41-44ce-bf17-6b0a53b7a0f4"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "# Quantiles dataframe saved as quantiles_summ_df\n",
    "# Simply modify this object on the left of equality:\n",
    "quantiles_summ_df = GET_QUANTILES (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d9da5f4f-c052-4b13-bbde-34fba24eb6fe"
   },
   "source": [
    "### **Getting a particular P-percent quantile limit (P is a percent, from 0 to 100)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6d13497e-84e9-4c3c-8371-c7627681223f"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "P_PERCENT = 100\n",
    "# P_PERCENT: float value from 0 to 100 representing the percent of the quantile\n",
    "# if P_PERCENT = 31.2, then 31.2% of the data will fall below the returned value\n",
    "# if P_PERCENT = 75, then 75% of the data will fall below the returned value\n",
    "# if P_PERCENT = 0, the minimum value is returned.\n",
    "# if P_PERCENT = 100, the maximum value is returned.\n",
    "\n",
    "# P-Percent quantile limit returned as the float value quantile_lim\n",
    "# Simply modify this variable on the left of equality:\n",
    "quantile_lim = GET_P_PERCENT_QUANTILE_LIM (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, p_percent = P_PERCENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d552d05c-5310-42e2-9520-fa8344b2fa02"
   },
   "source": [
    "### **Applying a list of row filters to a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "62f4a6cd-804b-4d83-bae4-7d3006c062bb"
   },
   "outputs": [],
   "source": [
    "# Warning: this function filter the rows and results into a smaller dataset, \n",
    "# since it removes the non-selected entries.\n",
    "# If you want to pass a filter to simply label the selected rows, use the function \n",
    "# LABEL_DATAFRAME_SUBSETS, which do not eliminate entries from the dataframe.\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 AND (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 OR (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "# Filtered dataframe saved as filtered_df\n",
    "# Simply modify this object on the left of equality:\n",
    "filtered_df = APPLY_ROW_FILTERS_LIST (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e5447d71-7f76-4f8b-ac2f-6d4f9c295115"
   },
   "source": [
    "### **Selecting subsets from a dataframe (using row filters) and labelling these subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "47ff8178-3494-4a67-a66a-0e77e0efc178"
   },
   "outputs": [],
   "source": [
    "# Attention: this function selects subsets from the dataframe and label them, \n",
    "# allowing the seggregation of the data.\n",
    "# If you want to filter the dataframe to eliminate non-selected rows, use the \n",
    "# function APPLY_ROW_FILTERS_LIST\n",
    "    \n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "## define the filters and only them define the filters list\n",
    "# EXAMPLES OF BOOLEAN FILTERS TO COMPOSE THE LIST\n",
    "boolean_filter1 = ((None) & (None)) # (condition1 AND (&) condition2)\n",
    "boolean_filter2 = ((None) | (None)) # condition1 OR (|) condition2\n",
    "# boolean filters result into boolean values True or False.\n",
    "\n",
    "## Examples of filters:\n",
    "## filter1 = (condition 1) & (condition 2)\n",
    "## filter1 = (df['column1'] > = 0) & (df['column2']) < 0)\n",
    "## filter2 = (condition)\n",
    "## filter2 = (df['column3'] <= 2.5)\n",
    "## filter3 = (df['column4'] > 10.7)\n",
    "## filter3 = (condition 1) | (condition 2)\n",
    "## filter3 = (df['column5'] != 'string1') | (df['column5'] == 'string2')\n",
    "    \n",
    "## comparative operators: > (higher); >= (higher or equal); < (lower); \n",
    "## <= (lower or equal); == (equal); != (different)\n",
    "    \n",
    "## concatenation operators: & (and): the filter is True only if the \n",
    "## two conditions concatenated through & are True\n",
    "## | (or): the filter is True if at least one of the two conditions concatenated\n",
    "## through | are True.\n",
    "    \n",
    "## separate conditions with parentheses. Use parentheses to define a order\n",
    "## of definition of the conditions:\n",
    "## filter = ((condition1) & (condition2)) | (condition3)\n",
    "## Here, firstly ((condition1) & (condition2)) = subfilter is evaluated. \n",
    "## Then, the resultant (subfilter) | (condition3) is evaluated.\n",
    "\n",
    "## Pandas .isin method: you can also use this method to filter rows belonging to\n",
    "## a given subset (the row that is in the subset is selected). The syntax is:\n",
    "## is_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\n",
    "## or: filter = (dataframe_column_series).isin([value1, value2, ...])\n",
    "\n",
    "LIST_OF_ROW_FILTERS = [boolean_filter1, boolean_filter2]\n",
    "# LIST_OF_ROW_FILTERS: list of boolean filters to be applied to the dataframe\n",
    "# e.g. list_of_row_filters = [filter1]\n",
    "# applies a single filter saved as filter 1. Notice: even if there is a single\n",
    "# boolean filter, it must be declared inside brackets, as a single-element list.\n",
    "# That is because the function will loop through the list of filters.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4]\n",
    "# will apply, in sequence, 4 filters: filter1, filter2, filter3, and filter4.\n",
    "# Notice that the filters must be declared in the order you want to apply them.\n",
    "\n",
    "NEW_LABELS_LIST = None\n",
    "# Here, each filter will be associated with one label. Therefore, if you want to apply \n",
    "# a filter with several conditions, combine them using the & and | operators.\n",
    "    \n",
    "# The labels are input as a list of strings or of discrete integer values. There must be\n",
    "# exactly one filter to each label: the filter will be used to obtain a subset from\n",
    "# the dataframe, and the label will be applied to all the rows of this subset.\n",
    "# e.g. LIST_OF_ROW_FILTERS = [filter1, filter2], NEW_LABELS_LIST = ['label1', 'label2']\n",
    "# In this case, the rows of the dataset obtained when applying filter1 will receive the\n",
    "# string 'label1' as label. In turns, rows selected from filter2 will be labelled as \n",
    "# 'label2'.\n",
    "# LIST_OF_ROW_FILTERS = [filter1, filter2, filter3], NEW_LABELS_LIST = [1, 2, 3]\n",
    "# Here, the labels are integers: rows filtered through filter1 are marked as 1;\n",
    "# rows selected by filter2 are labelled as 2; and rows selected from filter3 are marked\n",
    "# as 3.\n",
    "    \n",
    "# Warning: the sequence of filters must be correspondent to the sequence of labels. \n",
    "# Rows selected from the first filter are labelled with the first item from the labels\n",
    "# list; rows selected by the 2nd filter are labelled with the 2nd element, and so on.\n",
    "    \n",
    "# If no list of labels is provided, a list of integers starting from zero and with the\n",
    "# same total of elements from LIST_OF_ROW_FILTERS will be created. For instance,\n",
    "# if lLIST_OF_ROW_FILTERS = [filter1, filter2, filter3, filter4], the list of labels\n",
    "# [0, 1, 2, 3] would be created. Again, the rows selected from filter1 would be labelled\n",
    "# as 0; rows selected from filter 2 would be labelled as 1; and so on.\n",
    "\n",
    "NEW_LABELLED_COLUMN_NAME = None\n",
    "# NEW_LABELLED_COLUMN_NAME = None. Alternatively, declare a string (inside quotes), with\n",
    "# the name of the column containing the applied labels. For example, if\n",
    "# NEW_LABELLED_COLUMN_NAME = 'labels', then the new labels are saved in the column 'labels'\n",
    "# If no NEW_LABELLED_COLUMN_NAME is provided, the default name = \"df_label\" is provided.\n",
    "\n",
    "# Labelled dataframe saved as labelled_df\n",
    "# Simply modify this object on the left of equality:\n",
    "labelled_df = LABEL_DATAFRAME_SUBSETS (df = DATASET, list_of_row_filters = LIST_OF_ROW_FILTERS, new_labels_list = NEW_LABELS_LIST, new_labelled_column_name = NEW_LABELLED_COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8a4e2c16-5640-425d-a260-99f5ef0be09a"
   },
   "source": [
    "### **Performing Analysis of Variance (ANOVA) and obtaining boxplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e2a1d389-54e4-4364-9cd9-2a68a7739571"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "COLUMN_WITH_GROUP_LABELS = None\n",
    "# COLUMN_WITH_GROUP_LABELS: name of the column registering groups, labels, or\n",
    "# factors correspondent to each row. e.g. COLUMN_WITH_GROUP_LABELS = \"group\"\n",
    "# if the correspondent group is registered in a column named 'group'.\n",
    "# If no name is provided, then the default name is used:\n",
    "# COLUMN_WITH_GROUP_LABELS = COLUMN_TO_ANALYZE + \"_label\"\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "# CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "# Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "# to get less restrictive results.\n",
    "\n",
    "REFERENCE_VALUE = None\n",
    "# reference_value: keep it as None or add a float value.\n",
    "# This reference value will be shown as a red constant line on the plot to be compared\n",
    "# with the boxes. e.g. reference_value = 1.0 will plot a red line passing through\n",
    "# column_to_analyze = 1.0\n",
    "BOXPLOT_NOTCH = False\n",
    "# Manipulate parameter notch (boolean, default: False) from the boxplot object\n",
    "# Whether to draw a notched boxplot (True), or a rectangular boxplot (False). \n",
    "# The notches represent the confidence interval (CI) around the median. \n",
    "BOXPLOT_VERTICALLY_ORIENTED = True\n",
    "# Manipulate the parameter vert (boolean, default: True)\n",
    "# If True, draws vertical boxes. If False, draw horizontal boxes.\n",
    "BOXPLOT_PATCH_ARTIST = False \n",
    "# Manipulate parameter patch_artist (boolean, default: False)\n",
    "# If False produces boxes with the Line2D artist. Otherwise, boxes are drawn \n",
    "# with Patch artists.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# Dictionary storing ANOVA F-test and p-value returned as anova_summary_dict\n",
    "# Dictionary mapping each component of the boxplot returned as boxplot_returned_dict\n",
    "# Simply modify these objects on the left of equality:\n",
    "anova_summary_dict, boxplot_returned_dict = anova_boxplot (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_group_labels = COLUMN_WITH_GROUP_LABELS, confidence_level = CONFIDENCE_LEVEL, boxplot_notch = BOXPLOT_NOTCH, boxplot_vertically_oriented = BOXPLOT_VERTICALLY_ORIENTED, boxplot_patch_artist = BOXPLOT_PATCH_ARTIST,  reference_value = REFERENCE_VALUE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fdcec511-da77-4618-8af2-8b83eb07fb64"
   },
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c0f17810-55d4-40e3-a5d9-8028149954d9"
   },
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e9a2d786-23e2-4c5f-9e09-82c8ed6c5d77",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e11420d6-4b12-440f-a95d-af2d2a50321e"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a7a62837-3fc6-4a6e-8db8-8df1810f11f6"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "#Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5736ee3d-c60a-4a26-8221-e8c399b5406c"
   },
   "source": [
    "#### Case 2: import only a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8d7f16d4-976a-48e6-aa0a-07bdd731ee01"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary saved as imported_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1dff68c4-d64c-4adb-8c09-ea6df9c3dfad"
   },
   "source": [
    "#### Case 3: import a model and a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e667c545-4b41-4307-a453-3183480a5b18"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary saved as imported_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5f37179a-431d-4892-9b8c-a482332f8c7b"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "4c585868-9163-447f-939d-09b99f0f4208"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d3c987ec-991c-4611-8227-a20ff6fbfaa8"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d0eeebad-562e-4396-9718-8b3c423490fd"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values = df_gen_charac (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4c43f9b7-a38c-4072-810b-aed28b5dcf13"
   },
   "source": [
    "### **Visualizing time series**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#X1 = dataset.index to use the index as the axis itself\n",
    "X1 = (DATASET['DATE']).astype('datetime64[D]') \n",
    "#Alternatively: None; or other column in quotes, substituting 'DATE'\n",
    "# WARNING: Modify only the object in the first parenthesis: DATASET['DATE']\n",
    "# Do not modify the method .astype('datetime64[D]')\n",
    "#Remove .astype('datetime64[D]') if it is not a datetime.\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "#Notice that there is a data transforming step to guarantee that the 'DATE' was interpreted as a timestamp, not as object or string.\n",
    "#The astype method defines the type of variable as 'datetime64[D]'. If we wanted the timestamps to be resolved in seconds, we should use\n",
    "# 'datetime64[ns]'.\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "ADD_SCATTER_DOTS = False #Alternatively: True or False\n",
    "# If ADD_SCATTER_DOTS = False, the dots (scatter plot) are omitted, so only the lines\n",
    "# correspondent to the series are shown.\n",
    "\n",
    "# Notice that adding the dots and omitting the spline lines is equivalent to obtain a\n",
    "# scatter plot. If you want to do so, consider using the scatter_plot_lin_reg function, \n",
    "# capable of calculating the linear regressions.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "time_series_vis (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d7314e49-029e-495d-afbd-daae9f591663"
   },
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ad6a0d45-5b35-41c3-8e8a-265f2981b97c"
   },
   "source": [
    "#### Case 1: automatically calculate the ideal histogram bin size\n",
    "- The ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "744224c5-7105-4256-a568-6c5443e28cae",
    "tags": [
     "CELL_9"
    ]
   },
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ANALYZED_VARIABLE = DATASET['analyzed_variable']\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# ANALYZED_VARIABLE = DATASET['column1']\n",
    "\n",
    "SET_GRAPHIC_BAR_WIDTH = 2.0\n",
    "# This parameter must be visually adjusted for each particular analyzed variable.\n",
    "# Manually set this parameter until you see only a minimal separation between successive\n",
    "# bars (i.e., you know that the bars are not overlapping, but they are not so distant that\n",
    "# the statistic distribution profile is not clear).\n",
    "# You can input any numeric value, and the order of magnitude will vary depending on the\n",
    "# dispersion and on the width of the sample space.\n",
    "# e.g. SET_GRAPHIC_BAR_WIDTH = 3; SET_GRAPHIC_BAR_WIDTH = 0.003\n",
    "\n",
    "X_AXIS_ROTATION = 70 \n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "DATA_UNITS_LABEL = None\n",
    "# Input a string inside quotes for setting a label for the X-axis, that will represent\n",
    "# The type of data that the histogram is evaluating, i.e., what the statistic distribution\n",
    "# shown by the histogram means.\n",
    "# e.g. if DATA_UNITS_LABEL = \"particle_diameter_in_nm\", the axis X will be labelled with\n",
    "# this string. Then, we can know that the diagram represents the distribution (counts of\n",
    "# data for each defined bin) of particles diameters.\n",
    "\n",
    "Y_TITLE = None\n",
    "#Alternatively: string inside quotes for vertical title. e.g. Y_TITLE = \"Analyzed_values\".\n",
    "\n",
    "HISTOGRAM_TITLE = None\n",
    "#Alternatively: string inside quotes for graphic title. e.g. \n",
    "# HISTOGRAM_TITLE = \"Analyzed_values_histogram\".\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframes saved as general_statistics and frequency_tab.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics, frequency_tab = histogram (y = ANALYZED_VARIABLE, bar_width = SET_GRAPHIC_BAR_WIDTH, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, normal_curve_overlay = NORMAL_CURVE_OVERLAY, data_units_label = DATA_UNITS_LABEL, y_title = Y_TITLE, histogram_title = HISTOGRAM_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3e5c15da-0aad-45ec-9d97-37234e1c1c5a"
   },
   "source": [
    "#### Case 2: set number of bins\n",
    "- Use this one if the distance between data is too small, or if the histogram function did not return a valid histogram.\n",
    "- Here, the histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f514f988-01e2-43f4-b872-e913bd830e3e"
   },
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ANALYZED_VARIABLE = DATASET['analyzed_variable']\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# ANALYZED_VARIABLE = DATASET['column1']\n",
    "\n",
    "TOTAL_OF_BINS = 50\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided. Check comments after the histogram_alternative\n",
    "# function call).\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "\n",
    "SET_GRAPHIC_BAR_WIDTH = 2.0\n",
    "# This parameter must be visually adjusted for each particular analyzed variable.\n",
    "# Manually set this parameter until you see only a minimal separation between successive\n",
    "# bars (i.e., you know that the bars are not overlapping, but they are not so distant that\n",
    "# the statistic distribution profile is not clear).\n",
    "# You can input any numeric value, and the order of magnitude will vary depending on the\n",
    "# dispersion and on the width of the sample space.\n",
    "# e.g. SET_GRAPHIC_BAR_WIDTH = 3; SET_GRAPHIC_BAR_WIDTH = 0.003\n",
    "\n",
    "X_AXIS_ROTATION = 70 \n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "DATA_UNITS_LABEL = None\n",
    "# Input a string inside quotes for setting a label for the X-axis, that will represent\n",
    "# The type of data that the histogram is evaluating, i.e., what the statistic distribution\n",
    "# shown by the histogram means.\n",
    "# e.g. if DATA_UNITS_LABEL = \"particle_diameter_in_nm\", the axis X will be labelled with\n",
    "# this string. Then, we can know that the diagram represents the distribution (counts of\n",
    "# data for each defined bin) of particles diameters.\n",
    "\n",
    "Y_TITLE = None\n",
    "#Alternatively: string inside quotes for vertical title. e.g. Y_TITLE = \"Analyzed_values\".\n",
    "\n",
    "HISTOGRAM_TITLE = None\n",
    "#Alternatively: string inside quotes for graphic title. e.g. \n",
    "# HISTOGRAM_TITLE = \"Analyzed_values_histogram\".\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram_alternative.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframes saved as general_statistics and frequency_tab.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics, frequency_tab = histogram_alternative (y = ANALYZED_VARIABLE, total_of_bins = TOTAL_OF_BINS, bar_width = SET_GRAPHIC_BAR_WIDTH, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, data_units_label = DATA_UNITS_LABEL, y_title = Y_TITLE, histogram_title = HISTOGRAM_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "36ca04a9-79c8-4e03-b412-94299e1525d0"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c4f21976-cbaf-4469-b8b7-453c203ffb49"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7652a8a5-ce06-4ba9-8b54-b5fff5da8b74"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6f3a717c-94cd-41b9-a346-26fa77477ca5"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1f3576ed-1c72-406c-b99e-a8889f56cba2"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3098ed59-e727-4be1-a96a-91dcea71c12c"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "fa65404c-fe44-4226-92b5-f778aca16eb3"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c52ba421-a45b-4cad-8188-58adfc3fa680"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ca2693ff-02c6-4f3c-9a8d-57581ca7359d"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
