{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "088eb617-d763-42c1-8ea3-f11276004476"
   },
   "source": [
    "# **Time Series Characterization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "074c5967-77ba-45b4-ad26-66724c2060a3"
   },
   "source": [
    "## _ETL Workflow Notebook 4_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4f360436-f3a2-4be1-bdae-bca4d0d887b9"
   },
   "source": [
    "## Content:\n",
    "1. Lag-diagnosis: obtaining autocorrelation (ACF) and partial autocorrelation function (PACF) plots of the time series; \n",
    "2. Obtaining the 'd' parameter of ARIMA (p, q, d) model; \n",
    "3. Obtaining the best ARIMA (p, q, d) model; \n",
    "4. Forecasting with ARIMA model; \n",
    "5. Obtaining rolling window statistics of the dataframe; \n",
    "6. decomposing seasonality and trend of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# To install a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow\n",
    "# to update a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow --upgrade\n",
    "# to update pip, unmark and run:\n",
    "# ! pip install pip --upgrade\n",
    "# to show if a library is installed and visualize its information, unmark and run\n",
    "# (e.g. tensorflow):\n",
    "# ! pip show tensorflow\n",
    "# To run a Python file (e.g idsw_etl.py) saved in the notebook's workspace directory,\n",
    "# unmark and run:\n",
    "# import idsw_etl\n",
    "# or:\n",
    "# import idsw_etl as etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a8281f93-1e79-4d61-9552-259afd5adb08",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMAResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "de0c6dcb-d75c-4067-b049-87de615efc08"
   },
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5d363cec-c051-4c11-94b2-97741c1679b4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b982d22b-06c3-41b7-92ff-4bf5c20cc59b"
   },
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d23267cb-375f-4ca9-978c-8b89d103a95f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", load_all_sheets_at_once = False, sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading Excel files:\n",
    "    \n",
    "    # load_all_sheets_at_once = False - This parameter has effect only when for Excel files.\n",
    "    # If load_all_sheets_at_once = True, the function will return a list of dictionaries, each\n",
    "    # dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "    # value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "    # and its value will be the pandas dataframe object obtained from that sheet.\n",
    "    # This argument has preference over sheet_to_load. If it is True, all sheets will be loaded.\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\\n\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\\n\")\n",
    "        # For Excel type files, Pandas automatically detects the decimal separator and requires only the parameter parse_dates.\n",
    "        # Firstly, the argument infer_datetime_format was present on read_excel function, but was removed.\n",
    "        # From version 1.4 (beta, in 10 May 2022), it will be possible to pass the parameter 'decimal' to\n",
    "        # read_excel function for detecting decimal cases in strings. For numeric variables, it is not needed, though\n",
    "        \n",
    "        if (load_all_sheets_at_once == True):\n",
    "            \n",
    "            # Corresponds to setting sheet_name = None\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "            \n",
    "            # xlsx_doc is a dictionary containing the sheet names as keys, and dataframes as items.\n",
    "            # Let's convert it to the desired format.\n",
    "            # Dictionary dict, dict.keys() is the array of keys; dict.values() is an array of the values;\n",
    "            # and dict.items() is an array of tuples with format ('key', value)\n",
    "            \n",
    "            # Create a list of returned datasets:\n",
    "            list_of_datasets = []\n",
    "            \n",
    "            # Let's iterate through the array of tuples. The first element returned is the key, and the\n",
    "            # second is the value\n",
    "            for sheet_name, dataframe in (xlsx_doc.items()):\n",
    "                # sheet_name = key; dataframe = value\n",
    "                # Define the dictionary with the standard format:\n",
    "                df_dict = {'sheet': sheet_name,\n",
    "                            'df': dataframe}\n",
    "                \n",
    "                # Add the dictionary to the list:\n",
    "                list_of_datasets.append(df_dict)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(f\"A total of {len(list_of_datasets)} dataframes were retrieved from the Excel file.\\n\")\n",
    "            print(f\"The dataframes correspond to the following Excel sheets: {list(xlsx_doc.keys())}\\n\")\n",
    "            print(\"Returning a list of dictionaries. Each dictionary contains the key \\'sheet\\', with the original sheet name; and the key \\'df\\', with the Pandas dataframe object obtained.\\n\")\n",
    "            print(f\"Check the 10 first rows of the dataframe obtained from the first sheet, named {list_of_datasets[0]['sheet']}:\\n\")\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            except: # regular mode\n",
    "                print((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            return list_of_datasets\n",
    "            \n",
    "        elif (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5cd54924-c937-4e16-9a11-c77b96af386c"
   },
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7eec53d9-bdab-4e5f-a108-8cd101d4aedc",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def json_obj_to_pandas_dataframe (json_obj_to_convert, json_obj_type = 'list', json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "    # dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "    # example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "    # structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "    # file containing JSON, you could read the txt and save its content as a string.\n",
    "    # json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "    # 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "    # 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]    \n",
    "\n",
    "    # json_obj_type = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "    # json_obj_type = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "\n",
    "    \n",
    "    if (json_obj_type == 'string'):\n",
    "        # Use the json.loads method to convert the string to json\n",
    "        json_file = json.loads(json_obj_to_convert)\n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "        # like a dataframe.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    \n",
    "    elif (json_obj_type == 'list'):\n",
    "        \n",
    "        # make the json_file the object itself:\n",
    "        json_file = json_obj_to_convert\n",
    "    \n",
    "    else:\n",
    "        print (\"Enter a valid JSON object type: \\'list\\', in case the JSON object is a list of dictionaries in JSON format; or \\'string\\', if the JSON is stored as a text (string variable).\")\n",
    "        return \"error\"\n",
    "    \n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7e26ddd9-cf8b-4fd3-be17-507a0da36454"
   },
   "source": [
    "# **Function for calculating differences between successive timestamps (delay)**\n",
    "- Use this function for creating a column containing differences between two successive timestamps from a same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d7d11050-4af6-49fe-b44e-f482df376790",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def CALCULATE_DELAY (df, timestamp_tag_column, new_timedelta_column_name  = None, returned_timedelta_unit = None, return_avg_delay = True):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #THIS FUNCTION CALCULATES THE DIFFERENCE (timedelta - delay) BETWEEN TWO SUCCESSIVE\n",
    "    # Timestamps from a same column\n",
    "    \n",
    "    #df: dataframe containing the two timestamp columns.\n",
    "    #timestamp_tag_column: string containing the name of the column with the timestamps\n",
    "    \n",
    "    #new_timedelta_column_name: name of the new column. If no value is provided, the default\n",
    "    #name [timestamp_tag_column1]-[timestamp_tag_column2] will be given:\n",
    "    \n",
    "    # return_avg_delay = True will print and return the value of the average delay.\n",
    "    # return_avg_delay = False will omit this information\n",
    "    \n",
    "    if (new_timedelta_column_name is None):\n",
    "        \n",
    "        #apply the default name:\n",
    "        new_timedelta_column_name = \"time_delay\"\n",
    "    \n",
    "    #Pandas Timedelta class: applicable to timedelta objects\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.html\n",
    "    #The delta method from the Timedelta class converts returns the timedelta in\n",
    "    #nanoseconds, guaranteeing the internal compatibility:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timedelta.delta.html#pandas.Timedelta.delta\n",
    "    \n",
    "    #returned_timedelta_unit: unit of the new column. If no value is provided, the unit will be\n",
    "    # considered as nanoseconds. \n",
    "    # POSSIBLE VALUES FOR THE TIMEDELTA UNIT:\n",
    "    #'year', 'month', 'day', 'hour', 'minute', 'second'.\n",
    "    \n",
    "    # START: CONVERT ALL TIMESTAMPS/DATETIMES/STRINGS TO pandas.Timestamp OBJECTS.\n",
    "    # This will prevent any compatibility problems.\n",
    "    \n",
    "    #The pd.Timestamp function can handle a single timestamp per call. Then, we must\n",
    "    # loop trough the series, and apply the function to each element.\n",
    "    \n",
    "    # Create dataframe local copy to manipulate, avoiding that Pandas operates on\n",
    "    # the original object; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # 1. Start a list to store the Pandas timestamps:\n",
    "    timestamp_list = []\n",
    "    \n",
    "    # 2. Loop through each element of the timestamp column, and apply the function\n",
    "    # to guarantee that all elements are Pandas timestamps\n",
    "    \n",
    "    for timestamp in DATASET[timestamp_tag_column]:\n",
    "        #Access each element 'timestamp' of the series df[timestamp_tag_column1]\n",
    "        timestamp_list.append(pd.Timestamp(timestamp, unit = 'ns'))\n",
    "    \n",
    "    # 3. Save the list as the column timestamp_tag_column itself:\n",
    "    DATASET[timestamp_tag_column] = timestamp_list\n",
    "    \n",
    "    # 4. Sort the dataframe in ascending order of timestamps:\n",
    "    DATASET = DATASET.sort_values(by = timestamp_tag_column, ascending = True)\n",
    "    # Reset indices:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # Now, let's create a list of the following timestamps\n",
    "    following_timestamp = []\n",
    "    # Let's skip the index 0, correspondent to the first timestamp:\n",
    "    \n",
    "    for i in range (1, len(timestamp_list)):\n",
    "        \n",
    "        # this loop goes from i = 1 to i = len(timestamp_list) - 1, the last index\n",
    "        # of the list. If we simply declared range (len(timestamp_list)), the loop\n",
    "        # will start from 0, the default\n",
    "        \n",
    "        #append the element from timestamp_list to following_timestamp:\n",
    "        following_timestamp.append(timestamp_list[i])\n",
    "    \n",
    "    # Notice that this list has one element less than the original list, because we started\n",
    "    # copying from index 1, not 0. Therefore, let's repeat the last element of timestamp_list:\n",
    "    following_timestamp.append(timestamp_list[i])\n",
    "    # Notice that, once we did not restarted the variable i, it keeps its last value obtained\n",
    "    # during the loop, correspondent to the index of the last element.\n",
    "    # Now, let's store it into a column (series) of the dataframe:\n",
    "    timestamp_tag_column2 = timestamp_tag_column + \"_delayed\"\n",
    "    DATASET[timestamp_tag_column2] = following_timestamp\n",
    "    \n",
    "    # Pandas Timestamps can be subtracted to result into a Pandas Timedelta.\n",
    "    # We will apply the delta method from Pandas Timedeltas.\n",
    "    \n",
    "    # 4. Create a timedelta object as the difference between the timestamps:\n",
    "    \n",
    "    # NOTICE: Even though a list could not be submitted to direct operations like\n",
    "    # sum, subtraction and multiplication, the series and NumPy arrays can. When we\n",
    "    # copied the list as a new column on the dataframes, we converted the lists to series\n",
    "    # called df[timestamp_tag_column1] and df[timestamp_tag_column2]. These two series now\n",
    "    # can be submitted to direct operations.\n",
    "    \n",
    "    # Delay = next measurement (tag_column2, timestamp higher) - current measurement\n",
    "    # (tag_column2, timestamp lower). Since we repeated the last timestamp twice,\n",
    "    # in the last row it will be subtracted from itself, resulting in zero.\n",
    "    # This is the expected, since we do not have a delay yet\n",
    "    timedelta_obj = DATASET[timestamp_tag_column2] - DATASET[timestamp_tag_column]\n",
    "    \n",
    "    #This timedelta_obj is a series of timedelta64 objects. The Pandas Timedelta function\n",
    "    # can process only one element of the series in each call. Then, we must loop through\n",
    "    # the series to obtain the float values in nanoseconds. Even though this loop may \n",
    "    # look unecessary, it uses the Delta method to guarantee the internal compatibility.\n",
    "    # Then, no errors due to manipulation of timestamps with different resolutions, or\n",
    "    # due to the presence of global variables, etc. will happen. This is the safest way\n",
    "    # to manipulate timedeltas.\n",
    "    \n",
    "    #5. Create an empty list to store the timedeltas in nanoseconds\n",
    "    TimedeltaList = []\n",
    "    \n",
    "    #6. Loop through each timedelta_obj and convert it to nanoseconds using the Delta\n",
    "    # method. Both pd.Timedelta function and the delta method can be applied to a \n",
    "    # a single object.\n",
    "    #len(timedelta_obj) is the total of timedeltas present.\n",
    "    \n",
    "    for i in range(len(timedelta_obj)):\n",
    "        \n",
    "        #This loop goes from i = 0 to i = [len(timedelta_obj) - 1], so that\n",
    "        #all indices are evaluated.\n",
    "        \n",
    "        #append the element resultant from the delta method application on the\n",
    "        # i-th element of the list timedelta_obj, i.e., timedelta_obj[i].\n",
    "        TimedeltaList.append(pd.Timedelta(timedelta_obj[i]).delta)\n",
    "    \n",
    "    #Notice that the loop is needed because Pandas cannot handle a series/list of\n",
    "    #Timedelta objects simultaneously. It can manipulate a single object\n",
    "    # in each call or iteration.\n",
    "    \n",
    "    #Now the list contains the timedeltas in nanoseconds and guarantees internal\n",
    "    #compatibility.\n",
    "    # The delta method converts the Timedelta object to an integer number equals to the\n",
    "    # value of the timedelta in nanoseconds. Then we are now dealing with numbers, not\n",
    "    # with timestamps.\n",
    "    # Even though some steps seem unecessary, they are added to avoid errors and bugs\n",
    "    # hard to identify, resultant from a timestamp assigned to the wrong type of\n",
    "    # object.\n",
    "    \n",
    "    #The list is not as the series (columns) and arrays: it cannot be directly submitted to \n",
    "    # operations like sum, division, and multiplication. For doing so, we can loop through \n",
    "    # each element, what would be the case for using the Pandas Timestamp and Timedelta \n",
    "    # functions, which can only manipulate one object per call.\n",
    "    # For simpler operations like division, we can convert the list to a NumPy array and\n",
    "    # submit the entire array to the operation at the same time, avoiding the use of \n",
    "    # memory consuminh iterative methods.\n",
    "    \n",
    "    #Convert the timedelta list to a NumPy array:\n",
    "    # Notice that we could have created a column with the Timedeltalist, so that it would\n",
    "    # be converted to a series. On the other hand, we still did not defined the name of the\n",
    "    # new column. So, it is easier to simply convert it to a NumPy array, and then copy\n",
    "    # the array as a new column.\n",
    "    TimedeltaList = np.array(TimedeltaList)\n",
    "    \n",
    "    #Convert the array to the desired unit by dividing it by the proper factor:\n",
    "    \n",
    "    if (returned_timedelta_unit == 'year'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #5. Convert it to years. 1 year = 365 days + 6 h = 365 days + 6/24 h/(h/day)\n",
    "        # = (365 + 1/4) days = 365.25 days\n",
    "        \n",
    "        TimedeltaList = TimedeltaList / (365.25) #in years\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in years. Considered 1 year = 365 days + 6 h.\\n\")\n",
    "    \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'month'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #5. Convert it to months. Consider 1 month = 30 days\n",
    "        \n",
    "        TimedeltaList = TimedeltaList / (30.0) #in months\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in months. Considered 1 month = 30 days.\\n\")\n",
    "        \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'day'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #4. Convert it to days (1 day = 24 h):\n",
    "        TimedeltaList = TimedeltaList / 24.0 #in days\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in days.\\n\")\n",
    "        \n",
    "    \n",
    "    elif (returned_timedelta_unit == 'hour'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #3. Convert it to hours (1 h = 60 min):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in hours\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in hours [h].\\n\")\n",
    "    \n",
    "\n",
    "    elif (returned_timedelta_unit == 'minute'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #2. Convert it to minutes (1 min = 60 s):\n",
    "        TimedeltaList = TimedeltaList / 60.0 #in minutes\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in minutes [min].\\n\")\n",
    "        \n",
    "        \n",
    "    elif (returned_timedelta_unit == 'second'):\n",
    "        \n",
    "        #1. Convert the list to seconds (1 s = 10**9 ns, where 10**9 represents\n",
    "        #the potentiation operation in Python, i.e., 10^9. e.g. 10**2 = 100):\n",
    "        TimedeltaList = TimedeltaList / (10**9) #in seconds\n",
    "        \n",
    "        #The .0 after the numbers guarantees a float division.\n",
    "        \n",
    "        print(\"Returned timedelta in seconds [s].\\n\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        returned_timedelta_unit = 'ns'\n",
    "        print(\"No unit or invalid unit provided for timedelta. Then, returned timedelta in nanoseconds (1s = 10^9 ns).\\n\")\n",
    "        \n",
    "        #In case None unit is provided or a non-valid value or string is provided,\n",
    "        #The calculus will be in nanoseconds.\n",
    "    \n",
    "    #Finally, create a column in the dataframe named as new_timedelta_column_name \n",
    "    # with the elements of TimedeltaList converted to the correct unit of time:\n",
    "    \n",
    "    #Append the selected unit as a suffix on the new_timedelta_column_name:\n",
    "    new_timedelta_column_name = new_timedelta_column_name + \"_\" + returned_timedelta_unit\n",
    "    \n",
    "    DATASET[new_timedelta_column_name] = TimedeltaList\n",
    "      \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Time delays successfully calculated. Check dataset\\'s 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    if (return_avg_delay == True):\n",
    "        \n",
    "        # Let's calculate the average delay, print and return it:\n",
    "        # Firstly, we must remove the last element of the TimedeltaList.\n",
    "        # Remember that this element is 0 because there is no delay. It was added to allow\n",
    "        # the element-wise operations between the series.\n",
    "        # Let's eliminate the last element from TimedeltaList. Since this list was already\n",
    "        # copied to the dataframe, there is no risk of losing information.\n",
    "        \n",
    "        # Index of the last element:\n",
    "        last_element_index = len(TimedeltaList) - 1\n",
    "        \n",
    "        # Slice TimedeltaList until the element of index last_element_index - 1.\n",
    "        # It will eliminate the last element before we obtain the average:\n",
    "        TimedeltaList = TimedeltaList[:last_element_index]\n",
    "        # slice[i:j] slices including index i to index j-1; if the first element is not included,\n",
    "        # the slices goes from the 1st element; if the last element is not included, slices goes to\n",
    "        # the last element.\n",
    "        \n",
    "        # Now we calculate the average value:\n",
    "        avg_delay = np.average(TimedeltaList)\n",
    "        \n",
    "        print(f\"Average delay = {avg_delay} {returned_timedelta_unit}\\n\")\n",
    "        \n",
    "        # Return the dataframe and the average value:\n",
    "        return DATASET, avg_delay\n",
    "    \n",
    "    #Finally, return the dataframe with the new column:\n",
    "    \n",
    "    else: \n",
    "        # Return only the dataframe\n",
    "        return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8f53f95c-c6e2-42c8-a013-29aeeeedb071"
   },
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9acdae7f-402d-4bbd-a155-326c371149be",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Create dataframe local copies to manipulate, avoiding that Pandas operates on\n",
    "    # the original objects; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    \n",
    "    # Start a list of copied dataframes:\n",
    "    LIST_OF_DATAFRAMES = []\n",
    "    \n",
    "    # Loop through each element from list_of_dataframes:\n",
    "    for dataframe in list_of_dataframes:\n",
    "        \n",
    "        # create a copy of the object:\n",
    "        copied_df = dataframe.copy(deep = True)\n",
    "        # Append this element to the LIST_OF_DATAFRAMES:\n",
    "        LIST_OF_DATAFRAMES.append(copied_df)\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "        \n",
    "        # In this case, we must save a list of columns of each one of the dataframes, containing\n",
    "        # the different column names observed. That is because the concat method eliminates the\n",
    "        # original column names when AXIS = 1\n",
    "        # We can start the LIST_OF_COLUMNS as the columns from the first object on the\n",
    "        # LIST_OF_DATAFRAMES, eliminating one iteration cycle. Since the columns method generates\n",
    "        # an array, we use the list attribute to convert the array to a regular list:\n",
    "        \n",
    "        i = 0\n",
    "        analyzed_df = LIST_OF_DATAFRAMES[i]\n",
    "        LIST_OF_COLUMNS = list(analyzed_df.columns)\n",
    "        \n",
    "        # Now, loop through each other element on LIST_OF_DATAFRAMES. Since index 0 was already\n",
    "        # considered, start from index 1:\n",
    "        for i in range (1, len(LIST_OF_DATAFRAMES)):\n",
    "            \n",
    "            analyzed_df = LIST_OF_DATAFRAMES[i]\n",
    "            \n",
    "            # Now, loop through each column, named 'col', from the list of columns of analyzed_df:\n",
    "            for col in list(analyzed_df.columns):\n",
    "                \n",
    "                # If 'col' is not in LIST_OF_COLUMNS, append it to the list with its current name.\n",
    "                # The order of the columns on the concatenated dataframe will be the same (the order\n",
    "                # they appear):\n",
    "                if not (col in LIST_OF_COLUMNS):\n",
    "                    LIST_OF_COLUMNS.append(col)\n",
    "                \n",
    "                else:\n",
    "                    # There is already a column with this name. So, append col with a suffix:\n",
    "                    LIST_OF_COLUMNS.append(col + \"_df_\" + str(i))\n",
    "                    \n",
    "        # Now, we have a list of all column names, that we will use for retrieving the headers after\n",
    "        # concatenation.\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\\n\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\\n\")\n",
    "        concat_df = pd.concat(LIST_OF_DATAFRAMES, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(LIST_OF_DATAFRAMES, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    if (AXIS == 1):\n",
    "        # If we concatentated columns, we lost the columns' names (headers). So, use the list\n",
    "        # LIST_OF_COLUMNS as the new headers for this case:\n",
    "        concat_df.columns = LIST_OF_COLUMNS\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(concat_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "05f0965f-88ec-49b2-a1f4-05326d4a1efe"
   },
   "source": [
    "# **Function for column filtering (selecting); ordering; or renaming all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cd38670f-b966-4b8a-901a-617f721d75d5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def select_order_or_rename_columns (df, columns_list, mode = 'select_or_order_columns'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # MODE = 'select_or_order_columns' for filtering only the list of columns passed as columns_list,\n",
    "    # and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "    # the order of elements on the list will be the new order of columns.\n",
    "\n",
    "    # MODE = 'rename_columns' for renaming the columns with the names passed as columns_list. In this\n",
    "    # mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "    # the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "    # will result into columns with incorrect names.\n",
    "    \n",
    "    # columns_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: columns_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{DATASET.columns}\\n\")\n",
    "    \n",
    "    if ((columns_list is None) | (columns_list == np.nan)):\n",
    "        # empty list\n",
    "        columns_list = []\n",
    "    \n",
    "    if (len(columns_list) == 0):\n",
    "        print(\"Please, input a valid list of columns.\\n\")\n",
    "        return DATASET\n",
    "    \n",
    "    if (mode == 'select_or_order_columns'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        DATASET = DATASET[columns_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\\n\")\n",
    "        print(\"Check the new dataframe:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET)\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET)\n",
    "        \n",
    "    elif (mode == 'rename_columns'):\n",
    "        \n",
    "        # Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(columns_list) == len(DATASET.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\\n\")\n",
    "            return DATASET\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            DATASET.columns = columns_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\\n\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\\n\")\n",
    "            print(\"Check the new dataframe:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(DATASET)\n",
    "\n",
    "            except: # regular mode\n",
    "                print(DATASET)\n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'select_or_order_columns\\' or \\'rename_columns\\'.\")\n",
    "        return DATASET\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "41d287b7-e888-4f13-a8d4-58f485ac0793"
   },
   "source": [
    "# **Function for Lag Diagnosis - Obtantion of Autocorrelation (ACF) and Partial Autocorrelation Function (PACF) Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1ede50aa-acc3-4f0b-91a6-e12ca2a361ae",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def lag_diagnosis (df, column_to_analyze, number_of_lags = 40, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    #column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    #number_of_lags: integer value. e.g. number_of_lags = 50\n",
    "    # represents how much lags will be tested, and the length of the horizontal axis.\n",
    "    \n",
    "    # Set a copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    #Define the series to be analyzed:\n",
    "    y = DATASET[column_to_analyze]\n",
    "    \n",
    "    #Create the figure:\n",
    "    fig = plt.figure(figsize = (12, 8)) \n",
    "    ax1 = fig.add_subplot(211)\n",
    "    #ax1.set_xlabel(\"Lags\")\n",
    "    ax1.set_ylabel(\"Autocorrelation_Function_ACF\")\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax1.grid(grid)\n",
    "    \n",
    "    fig = sm.graphics.tsa.plot_acf(y.values.squeeze(), lags = number_of_lags, ax = ax1, color = 'darkblue')\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(y, lags = number_of_lags, ax = ax2, color = 'darkblue', method = 'ywm')\n",
    "    ax2.set_xlabel(\"Lags\")\n",
    "    ax2.set_ylabel(\"Partial_Autocorrelation_Function_PACF\")\n",
    "        \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax2.grid(grid)\n",
    "        \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"lag_diagnosis\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    #Print background and interpretation of the graphic:\n",
    "    print(\"\\n\") #line break\n",
    "    print(\"Use this plot to define the parameters (p, q) for testing ARIMA and ARMA models.\\n\")\n",
    "    print(\"p defines the order of the autoregressive part (AR) of the time series.\")\n",
    "    print(\"p = lags correspondent to the spikes of PACF plot (2nd plot) that are outside the error (blue region).\\n\")\n",
    "    print(\"For instance, if there are spikes in both lag = 1 and lag = 2, then p = 2, or p = 1\\n\")\n",
    "    print(\"q defines the order of the moving average part (MA) of the time series.\")\n",
    "    print(\"q = lags correspondent to the spikes of ACF plot that are outside blue region.\\n\")\n",
    "    print(\"For instance, if all spikes until lag = 6 are outside the blue region, then q = 1, 2, 3, 4, 5, 6.\\n\")\n",
    "    print(\"WARNING: do not test the ARIMA/ARMA model for p = 0, or q = 0.\")\n",
    "    print(\"For lag = 0, the correlation and partial correlation coefficients are always equal to 1, because the data is always perfectly correlated to itself.\") \n",
    "    print(\"Therefore, ignore the first spikes (lag = 0) from ACF and PACF plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8cdb133d-e81b-4ea7-8446-aada3dd8dce5",
    "id": "pX17O_T3-e7f"
   },
   "source": [
    "# **Function for obtaining the parameter 'd' of ARIMA (p, d, q) model**\n",
    "\n",
    "- The value of \"d\" corresponds to the total of differentiations for making the process stationary.\n",
    "- If signal St is non-stationary, we can convert them into stationary signal Tt by differencing: `Tt = St - St-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cb708246-3e25-464f-a2b7-3d4fdd336368",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def test_d_parameters (df, column_to_analyze, number_of_lags = 40, max_tested_d = 2, confidence_level = 0.95, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    #max_tested_d: differential order (integer value)\n",
    "    #change the integer if you want to test other cases. By default, max_tested_d = 2, meaning\n",
    "    # that the values d = 0, 1, and 2 will be tested.\n",
    "    # If max_tested_d = 1, d = 0 and 1 will be tested.\n",
    "    # If max_tested_d = 3, d = 0, 1, 2, and 3 will be tested, and so on.\n",
    "    \n",
    "    #column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "    # Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "    # to get less restrictive results.\n",
    "    \n",
    "    #number_of_lags: integer value. e.g. number_of_lags = 50\n",
    "    # represents how much lags will be tested, and the length of the horizontal axis.\n",
    "    \n",
    "    \n",
    "    # Set a copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    #Define the series to be analyzed:\n",
    "    time_series = DATASET[column_to_analyze]\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    #Create the figure:\n",
    "    # Original Series\n",
    "    fig, axes = plt.subplots((max_tested_d + 1), 2, sharex = True, figsize = (12, 8)) \n",
    "    # sharex = share axis X\n",
    "    # number of subplots equals to the total of orders tested (in this case, 2)\n",
    "    # If max_tested_d = 2, we must have a subplot for d = 0, d = 1 and d = 2, i.e.,\n",
    "    # wer need 3 subplots = max_tested_d + 1\n",
    "    axes[0, 0].plot(time_series, color = 'darkblue', alpha = OPACITY); axes[0, 0].set_title('Original Series')\n",
    "    sm.graphics.tsa.plot_acf(time_series, lags = number_of_lags, ax = axes[0, 1], color = 'darkblue', alpha = 0.30)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    axes[0, 0].grid(grid)\n",
    "    \n",
    "    # Create a subplot for each possible 'd'.\n",
    "    # Notice that d = 0 was already tested.\n",
    "    for i in range(1, (max_tested_d + 1)):\n",
    "        # This loop goes from i = 1 to i = (max_tested_d + 1) - 1 = max_tested_d.\n",
    "        # If max_tested_d = 2, this loop goes from i = 1 to i = 2.\n",
    "        # If only one value was declared in range(X), then the loop would start from 0.\n",
    "        \n",
    "        # Difference the time series:\n",
    "        time_series = time_series.diff()\n",
    "        \n",
    "        #the indexing of the list d goes from zero to len(d) - 1\n",
    "        # 1st Differencing\n",
    "        axes[i, 0].plot(time_series, color = 'darkblue', alpha = OPACITY); axes[i, 0].set_title('%d Order Differencing' %(i))\n",
    "        sm.graphics.tsa.plot_acf(time_series.diff().dropna(), lags = number_of_lags, ax = axes[i, 1], color = 'darkblue', alpha = 0.30)\n",
    "                \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        axes[i, 0].grid(grid)\n",
    "    \n",
    "        print('ADF Statistic for %d Order Differencing' %(i))\n",
    "        result = adfuller(time_series.dropna())\n",
    "        print('ADF Statistic: %f' % result[0])\n",
    "        print('p-value: %f' % result[1])\n",
    "        print(\"Null-hypothesis: the process is non-stationary. p-value represents this probability.\")\n",
    "\n",
    "        if (result[1] < (1-confidence_level)):\n",
    "            print(\"For a %.2f confidence level, the %d Order Difference is stationary.\" %(confidence_level, i))\n",
    "            print(\"You may select d = %d\\n\" %(i))\n",
    "\n",
    "        else:\n",
    "            print(\"For a %.2f confidence level, the %d Order Difference is non-stationary.\\n\" %(confidence_level, i))\n",
    "        \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"test_d_parameters\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"d = differentiation order for making the process stationary.\\n\")\n",
    "    print(\"If d = N, then we have to make N successive differentiations.\")\n",
    "    print(\"A differentiation consists on substracting a signal Si from its previous signal Si-1.\\n\")\n",
    "    print(\"Example: 1st-order differentiating consists on taking the differences on the original time series.\")\n",
    "    print(\"The 2nd-order, in turns, consists in differentiating the 1st-order differentiation series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8cdb133d-e81b-4ea7-8446-aada3dd8dce5",
    "id": "pX17O_T3-e7f"
   },
   "source": [
    "# **Function for obtaining the best ARIMA (p,d,q) model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "971515b3-f4e6-4968-8351-5848bf20387c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def best_arima_model (df, column_to_analyze, p_vals, d, q_vals, timestamp_tag_column = None, confidence_level = 0.95, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    # https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMAResults.html#statsmodels.tsa.arima.model.ARIMAResults\n",
    "    # https://www.statsmodels.org/stable/examples/notebooks/generated/tsa_arma_1.html?highlight=statsmodels%20graphics%20tsaplots%20plot_predict\n",
    "    \n",
    "    ## d = 0 corresponds to the ARMA model\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.graphics.tsaplots import plot_predict\n",
    "    #this model is present only in the most recent versions of statsmodels\n",
    "\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # timestamp_tag_column = None - keep it as None if you do not want to inform the timestamps\n",
    "    # Alternatively, declare a string (inside quotes), \n",
    "    # containing the name of the column containing the time information. \n",
    "    # e.g. timestamp_tag_column = \"DATE\" will take the timestamps from column 'DATE'.\n",
    "    # If no column is provided, the index in the dataframe will be used.\n",
    "    \n",
    "    #p_vals: list of integers correspondent to the lags (spikes) in the PACF plot.\n",
    "    # From function lag_diagnosis\n",
    "    #q_vals: list of integers correspondent to the lags (spikes) in ACF plot\n",
    "    # From function lag_diagnosis\n",
    "    #d = difference for making the process stationary\n",
    "    # From function test_d_parameters\n",
    "    \n",
    "    ## WARNING: do not test the ARIMA/ARMA model for p = 0, or q = 0.\n",
    "    ## For lag = 0, the correlation and partial correlation coefficients \n",
    "    ## are always equal to 1, because the data is perfectly correlated to itself. \n",
    "    ## Therefore, ignore the first spikes (lag = 0) of ACF and PACF plots.\n",
    "    \n",
    "    ALPHA = 1 - confidence_level\n",
    "    # CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "    # Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "    # to get less restrictive results.\n",
    "    \n",
    "    # Set a copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    #x: timestamp or index series, if no timestamp is provided:\n",
    "    if (timestamp_tag_column is None):\n",
    "        #Use the indices of the dataframe\n",
    "        x = DATASET.index\n",
    "    \n",
    "    else:\n",
    "        #Use the timestamp properly converted to datetime (as in the graphics functions):\n",
    "        x = (DATASET[timestamp_tag_column]).astype('datetime64[ns]')\n",
    "        DATASET[timestamp_tag_column] = x\n",
    "        # To pass the date to ARIMA model, we must specify it as the index, and also pass the\n",
    "        # start and end dates\n",
    "        DATASET = DATASET.set_index(timestamp_tag_column)\n",
    "    \n",
    "    #y: tested variable series\n",
    "    y = DATASET[column_to_analyze]\n",
    "    \n",
    "    # date to start the plot (the second element from series x - we will ignore the first one):\n",
    "    start_date = x[1]\n",
    "    # last date to plot (last element from the series)\n",
    "    end_date = x[(len(x) - 1)]\n",
    "    print(f\"ARIMA model from date (or measurement) = {start_date}; to date (or measurement) = {end_date}.\\n\")\n",
    "    \n",
    "    #calculate the first aic and bic\n",
    "    #first argument = time series y\n",
    "    \n",
    "    returned_p = p_vals[0]\n",
    "    returned_q = q_vals[0]\n",
    "    returned_d = d\n",
    "    #use the d-value selected in the non-stationary analysis.\n",
    "    #set the integration in \n",
    "    #at this moment, the function simply returns the first elements.\n",
    "    \n",
    "    ARIMA_model = ARIMA(y, order = (returned_p, returned_d, returned_q))\n",
    "\n",
    "    #order = (p, d, q) - these are the parameters of the autoregression (p = 2), \n",
    "    #integration (d = parameter selected in previous analysis), and\n",
    "    #moving average (q = 1, 2, 3, 4, 5, etc)\n",
    "\n",
    "    ARIMA_Results = ARIMA_model.fit()\n",
    "    aic_val = ARIMA_Results.aic\n",
    "    #AIC value for the first combination.\n",
    "    bic_val = ARIMA_Results.bic\n",
    "    #BIC value for the first combination, to start the loops.\n",
    "    \n",
    "    # Mean absolute error:\n",
    "    mae = ARIMA_Results.mae\n",
    "    # log likelihood calculated:\n",
    "    loglikelihood = ARIMA_Results.llf\n",
    "    \n",
    "    returned_ARIMA_Results = ARIMA_Results\n",
    "    #returned object\n",
    "    \n",
    "    for p in p_vals:\n",
    "        #test each possible value for p (each d, p combination)\n",
    "        #each p in p_vals list is used.\n",
    "        for q in q_vals:\n",
    "            #test each possible value for q (each p, d, q combination)\n",
    "            #each q in q_vals list is used.\n",
    "                \n",
    "            ARIMA_model = ARIMA(y, order = (p, returned_d, q))\n",
    "            ARIMA_Results = ARIMA_model.fit()\n",
    "            aic_tested = ARIMA_Results.aic\n",
    "            bic_tested = ARIMA_Results.bic\n",
    "            mae_tested = ARIMA_Results.mae\n",
    "            loglikelihood_tested = ARIMA_Results.llf\n",
    "            \n",
    "            if ((mae_tested < mae) & (abs(loglikelihood_tested) > abs(loglikelihood))):\n",
    "                \n",
    "                # Check if the absolute error was reduced and the likelihood was increased\n",
    "                \n",
    "                #if better parameters were found, they should be used\n",
    "                #update AIC, BIC; the p, d, q returned values;\n",
    "                #and the ARIMA_Results from the ARIMA:\n",
    "                aic_val = aic_tested\n",
    "                bic_val = bic_tested\n",
    "                mae = mae_tested\n",
    "                loglikelihood = loglikelihood_tested\n",
    "                returned_p = p\n",
    "\n",
    "                returned_q = q\n",
    "                #return the Statsmodels object:\n",
    "                returned_ARIMA_Results = ARIMA_Results\n",
    "    \n",
    "    #Create a dictionary containing the best parameters and the metrics AIC and BIC\n",
    "    arima_summ_dict = {\"p\": returned_p, \"d\": returned_d, \"q\": returned_q,\n",
    "                   \"AIC\": returned_ARIMA_Results.aic, \"BIC\": returned_ARIMA_Results.bic,\n",
    "                    \"MAE\": returned_ARIMA_Results.mae, \"log_likelihood\": returned_ARIMA_Results.llf}\n",
    "    \n",
    "    #Show ARIMA results:\n",
    "    print(returned_ARIMA_Results.summary())\n",
    "    print(\"\\n\")\n",
    "    #Break the line and show the combination\n",
    "    print(\"Best combination found: (p, d, q) = (%d, %d, %d)\\n\" %(returned_p, returned_d, returned_q))\n",
    "    #Break the line and print the next indication:\n",
    "    print(f\"Time series, values predicted by the model, and the correspondent {confidence_level * 100}% Confidence interval for the predictions:\\n\")\n",
    "    #Break the line and print the ARIMA graphic:\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    #Start the figure:\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    # Add line of the actual values:\n",
    "    ax.plot(x, y, linestyle = '-', marker = '', color = 'darkblue', label = column_to_analyze)\n",
    "    # Use the name of the analyzed column as the label\n",
    "    fig = plot_predict(returned_ARIMA_Results, start = start_date, end = end_date,  ax = ax, alpha = ALPHA)\n",
    "    ## https://www.statsmodels.org/v0.12.2/generated/statsmodels.tsa.arima_model.ARIMAResults.plot_predict.html\n",
    "    # ax = ax to plot the arima on the original plot of the time series\n",
    "    # start = x[1]: starts the ARIMA graphic from the second point of the series x\n",
    "    # if x is the index, then it will be from the second x; if x is a time series, then\n",
    "    # x[1] will be the second timestamp\n",
    "    #We defined the start in x[1], instead of index = 0, because the Confidence Interval for the \n",
    "    #first point is very larger than the others (there is perfect autocorrelation for lag = 0). \n",
    "    #Therefore, it would have resulted in the need for using a very broader y-scale, what\n",
    "    #would compromise the visualization.\n",
    "    # We could set another index or even a timestamp to start:\n",
    "    # start=pd.to_datetime('1998-01-01')\n",
    "    \n",
    "    ax.set_alpha(OPACITY)\n",
    "    \n",
    "    if not (plot_title is None):\n",
    "        #graphic's title\n",
    "        ax.set_title(plot_title)\n",
    "    \n",
    "    else:\n",
    "        #set a default title:\n",
    "        ax.set_title(\"ARIMA_model\")\n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #X-axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    else:\n",
    "        #set a default title:\n",
    "        ax.set_xlabel(\"Time\")\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Y-axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    else:\n",
    "        #set a default title:\n",
    "        ax.set_ylabel(column_to_analyze)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend(loc = \"upper left\")\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"arima_model\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    # Get dataframe with the predictions and confidence intervals\n",
    "    arima_predictions = returned_ARIMA_Results.get_prediction(start = x[0], end = None, dynamic = False, full_results = True, alpha = ALPHA)\n",
    "    # Here, we started in start = x[0] to obtain a correspondent full dataset, and did not set an end.\n",
    "    # The start can be an integer representing the index of the data in the dataframe,\n",
    "    # or a timestamp. Check:\n",
    "    # https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMAResults.get_prediction.html#statsmodels.tsa.arima.model.ARIMAResults.get_prediction\n",
    "    # Again, no matter if x is an index or a time series, x[0] starts from the first element\n",
    "    # You could set another index or a oparticular timestamp to start, like:\n",
    "    # start=pd.to_datetime('1998-01-01')\n",
    "    # The dynamic = False argument ensures that we produce one-step ahead forecasts, \n",
    "    # meaning that forecasts at each point are generated using the full history up \n",
    "    # to that point.\n",
    "    predicted_mean_vals = arima_predictions.predicted_mean\n",
    "    predicted_conf_intervals = arima_predictions.conf_int(alpha = ALPHA)\n",
    "    # predicted_conf_intervals has two columns: first one is the inferior confidence limit\n",
    "    # second one is the superior confidence limit\n",
    "    # each column from this dataframe gets a name derived from the name of the original series.\n",
    "    # So, let's rename them:\n",
    "    predicted_conf_intervals.columns = ['lower_cl', 'upper_cl']\n",
    "    \n",
    "    # let's create a copy of the dataframe to be returned with the new information:\n",
    "    # This will avoid manipulating the df:\n",
    "    arima_df = DATASET.copy(deep = True)\n",
    "    # This DATASET already contains the timestamp column as the index, so it is adequate for\n",
    "    # obtaining predictions associated to the correct time values.\n",
    "    \n",
    "    #create a column for the predictions:\n",
    "    arima_df['arima_predictions'] = predicted_mean_vals\n",
    "    \n",
    "    #create a column for the inferior (lower) confidence interval.\n",
    "    # Copy all the rows from the column 0 of predicted_conf_intervals\n",
    "    arima_df['lower_cl'] = predicted_conf_intervals['lower_cl']\n",
    "    \n",
    "    #create a column for the superior (upper) confidence interval.\n",
    "    # Copy all the rows from the column 1 of predicted_conf_intervals\n",
    "    arima_df['upper_cl'] = predicted_conf_intervals['upper_cl']\n",
    "    \n",
    "    # Let's turn again the timestamps into a column, restart the indices and re-order the columns,\n",
    "    # so that the last column will be the response, followed by ARIMA predictions\n",
    "    \n",
    "    ordered_columns_list = []\n",
    "    \n",
    "    if (timestamp_tag_column is not None):\n",
    "        #Use the indices of the dataframe\n",
    "        ordered_columns_list.append(timestamp_tag_column)\n",
    "        # Create a timestamp_tag_column in the dataframe containing the values in the index:\n",
    "        arima_df[timestamp_tag_column] = np.array(arima_df.index)\n",
    "        # Reset the indices from the dataframe:\n",
    "        arima_df = arima_df.reset_index(drop = True)\n",
    "    \n",
    "    # create a list of columns with fixed position:\n",
    "    fixed_columns_list = [column_to_analyze, 'arima_predictions', 'lower_cl', 'upper_cl']\n",
    "    \n",
    "    # Now, loop through all the columns:\n",
    "    for column in list(arima_df.columns):\n",
    "        \n",
    "        # If the column is not one from fixed_columns_list, and it is not on ordered_columns_list\n",
    "        # yet, add it to ordered_columns_list (timestamp_tag_column may be on the list):\n",
    "        if ((column not in fixed_columns_list) & (column not in ordered_columns_list)):\n",
    "            ordered_columns_list.append(column)\n",
    "    \n",
    "    # Now, concatenate ordered_columns_list to fixed_columns_list. \n",
    "    # If a = ['a', 'b'] and b = ['c', 'd'], a + b = ['a', 'b', 'c', 'd'] and b + a = [ 'c', 'd', 'a', 'b']\n",
    "    ordered_columns_list = ordered_columns_list + fixed_columns_list\n",
    "    \n",
    "    # Finally, select the columns from arima_df passing this list as argument:\n",
    "    arima_df = arima_df[ordered_columns_list]\n",
    "    print(\"\\n\")\n",
    "    print(\"Check the dataframe containing the ARIMA predictions:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(arima_df)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(arima_df)\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"Notice that the presence of data outside the confidence interval limits of the ARIMA forecast is a strong indicative of outliers or of untrust time series.\\n\")\n",
    "    print(\"For instance: if you observe a very sharp and sudden deviation from the predictive time series, it can be an indicative of incomplete information or outliers presence.\\n\")\n",
    "    print(\"A famous case is the pandemic data: due to lags or latencies on the time needed for consolidating the information in some places, the data could be incomplete in a given day, leading to a sharp decrease that did not actually occurred.\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"REMEMBER: q represents the moving average (MA) part of the time series.\")\n",
    "    print(f\"Then, it is interesting to group the time series {column_to_analyze} by each q = {returned_q} periods when modelling or analyzing it.\")\n",
    "    print(\"For that, you can use moving window or rolling functions.\\n\")\n",
    "    \n",
    "    return returned_ARIMA_Results, arima_summ_dict, arima_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7bdc5021-02ad-4093-9fa6-2d13f42d147f"
   },
   "source": [
    "# **Function for ARIMA forecasting**\n",
    "- This function calls `CALCULATE_DELAY` and `UNION_DATAFRAMES`, so you must guarantee that these functions are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "e6e13269-db8f-4bc9-b2c4-1ebac98a0045",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def arima_forecasting (arima_model_object, df = None, column_to_forecast = None, timestamp_tag_column = None, time_unit = None, number_of_periods_to_forecast = 7, confidence_level = 0.95, plot_predicted_time_series = True, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.graphics.tsaplots import plot_predict\n",
    "    #this model is present only in the most recent versions of statsmodels\n",
    "\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    \n",
    "    # arima_model_object : object containing the ARIMA model previously obtained.\n",
    "    # e.g. arima_model_object = returned_ARIMA_Results if the model was obtained as returned_ARIMA_Results\n",
    "    # do not declare in quotes, since it is an object, not a string.\n",
    "    \n",
    "    # time_unit: Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "    # Alternatively: keep it None, for the results in nanoseconds, or input time_unit = \n",
    "    # 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "    # It will be the input for the functions CALCULATE_DELAY and ADD_TIMEDELTA.\n",
    "    \n",
    "    # number_of_periods_to_forecast = 7\n",
    "    # integer value representing the total of periods to forecast. The periods will be in the\n",
    "    # unit (dimension) of the original dataset. If 1 period = 1 day, 7 periods will represent\n",
    "    # seven days.\n",
    "    \n",
    "    # Keep plot_predicted_time_series = True to see the graphic of the predicted values.\n",
    "    # Alternatively, set plot_predicted_time_series = True not to show the plot.\n",
    "    \n",
    "    # df = None, column_to_analyze = None - keep it as None if you do not want to show\n",
    "    # the ARIMA predictions combined to the original data; or if you do not want to append\n",
    "    # the ARIMA predictions to the original dataframe.\n",
    "    # Alternatively, set:\n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # column_to_forecast: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed.\n",
    "    # Keep it as None if the graphic of the predictions will not be shown with the\n",
    "    # responses or if the combined dataset will not be returned.\n",
    "    # e.g. column_to_forecast = \"column1\" will analyze and forecast values for \n",
    "    # the column named as 'column1'.\n",
    "    \n",
    "    # timestamp_tag_column: string (inside quotes), \n",
    "    # containing the name of the column containing timestamps.\n",
    "    # Keep it as None if the graphic of the predictions will not be shown with the\n",
    "    # responses or if the combined dataset will not be returned.\n",
    "    # e.g. timestamp_tag_column = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    ## ARIMA predictions:\n",
    "    ## The .forecast and .predict methods only produce point predictions:\n",
    "    ## y_forecast = ARIMA_Results.forecast(7) results in a group of values (7 predictions)\n",
    "    ## without the confidence intervals\n",
    "    ## On the other hand, the .get_forecast and .get_prediction methods \n",
    "    ## produce full results including prediction intervals.\n",
    "\n",
    "    ## In our example, we can do:\n",
    "    ## forecast = ARIMA_Results.get_forecast(123)\n",
    "    ## yhat = forecast.predicted_mean\n",
    "    ## yhat_conf_int = forecast.conf_int(alpha=0.05)\n",
    "    ## If your data is a Pandas Series, then yhat_conf_int will be a DataFrame with \n",
    "    ## two columns, lower <name> and upper <name>, where <name> is the name of the Pandas \n",
    "    ## Series.\n",
    "    ## If your data is a numpy array (or Python list), then yhat_conf_int will be an \n",
    "    ## (n_forecasts, 2) array, where the first column is the lower part of the interval \n",
    "    ## and the second column is the upper part.\n",
    "   \n",
    "    numeric_data_types = [np.float16, np.float32, np.float64, np.int16, np.int32, np.int64]\n",
    "    \n",
    "    ALPHA = 1 - confidence_level\n",
    "    # CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "    # Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "    # to get less restrictive results.\n",
    "    \n",
    "    # Calculate the predictions:\n",
    "    # It does not depend on the presence of a dataframe df\n",
    "    # https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMAResults.get_forecast.html#statsmodels.tsa.arima.model.ARIMAResults.get_forecast\n",
    "    \n",
    "    arima_forecasts = arima_model_object.get_forecast(number_of_periods_to_forecast, dynamic = False, full_results = True, alpha = ALPHA)\n",
    "    \n",
    "    forecast_mean_vals = arima_forecasts.predicted_mean\n",
    "    forecast_conf_intervals = arima_forecasts.conf_int(alpha = ALPHA)\n",
    "    # forecast_conf_intervals has two columns: first one is the inferior confidence limit\n",
    "    # second one is the superior confidence limit\n",
    "    # each column from this dataframe gets a name derived from the name of the original series.\n",
    "    # So, let's rename them:\n",
    "    forecast_conf_intervals.columns = ['lower_cl', 'upper_cl']\n",
    "    \n",
    "    #create a series for the inferior confidence interval.\n",
    "    lower_cl = forecast_conf_intervals['lower_cl'].copy(deep = True)\n",
    "    #create a series for the superior confidence interval.\n",
    "    upper_cl = forecast_conf_intervals['upper_cl'].copy(deep = True)\n",
    "    \n",
    "    # If there is no df, we can already obtain a X series, that will be a series of indices\n",
    "    # starting from period 1 (the first forecast. Period zero corresponds to the last actual value):\n",
    "    if (df is None):\n",
    "        \n",
    "        x_forecast = []\n",
    "        \n",
    "        for j in range (1, (number_of_periods_to_forecast + 1)):\n",
    "            #Goes from j = 1, the first forecast period; to j = (number_of_periods_to_forecast+1)-1\n",
    "            # = number_of_periods_to_forecast, which must be the last forecast.\n",
    "            x_forecast.append(j)\n",
    "        \n",
    "        # Now, create the dictionary of forecasts:\n",
    "        forecast_dict = {\n",
    "            \n",
    "            \"x\": x_forecast,\n",
    "            \"forecast_mean_vals\": forecast_mean_vals,\n",
    "            \"lower_cl\": lower_cl,\n",
    "            \"upper_cl\": upper_cl,\n",
    "            'source': 'forecast'\n",
    "        }\n",
    "        \n",
    "        # Convert it to a dataframe:\n",
    "        forecast_df = pd.DataFrame(data = forecast_dict)\n",
    "        x_forecast_series = forecast_df['x']\n",
    "        y_forecast_series = forecast_df['forecast_mean_vals']\n",
    "        lcl_series = forecast_df['lower_cl']\n",
    "        ucl_series = forecast_df['upper_cl']\n",
    "    \n",
    "    \n",
    "    # If there is a dataframe df, we must combine the original data from df\n",
    "    # with the new predictions:\n",
    "    else:\n",
    "        \n",
    "        # Start a dataset copy to manipulate:\n",
    "        DATASET = df.copy(deep = True)\n",
    "        \n",
    "        # Create a column with a label indicating that the data in DATASET (before concatenation \n",
    "        # with predictions) is from the original dataframe:\n",
    "        DATASET['source'] = 'input_dataframe'\n",
    "        # In turns, The source column in the dataframe from the forecasts will \n",
    "        # be labelled with the string 'forecast'\n",
    "        \n",
    "        # Check if a column_to_forecast was indicated in the input dataframe:\n",
    "        if not (column_to_forecast is None):\n",
    "            \n",
    "            # If there is a response column indicated, then the forecast column of the \n",
    "            # generated predictions must be stored in a column with the exact same name, so that\n",
    "            # the columns can be correctly appended\n",
    "            y_forecast_label = column_to_forecast\n",
    "            # Also, create a separate series for the original data, that will be used for\n",
    "            # differentiating between data and forecasts on the plot:\n",
    "            y_original_series = DATASET[column_to_forecast].copy(deep = True)\n",
    "        \n",
    "        # If no column was indicated, set a default column name for the forecasts:\n",
    "        else:\n",
    "            # Set the default name:\n",
    "            y_forecast_label = \"y_forecast\"\n",
    "        \n",
    "        \n",
    "        # Check if a timestamp_tag_column was input. If not, use the indices themselves as times.\n",
    "        # Create a new standard name for the column in forecasts:\n",
    "        if (timestamp_tag_column is None):\n",
    "            \n",
    "            # Let's set an index series as the index of the dataframe:\n",
    "            DATASET['index_series'] = DATASET.index\n",
    "            # Check if this series contains an object. If it has, then, user set the timestamps\n",
    "            # as the indices\n",
    "            index_series_type = DATASET['index_series'].dtype\n",
    "            \n",
    "            # If it is an object, the user may be trying to pass the date as index. \n",
    "            # So, let's try to convert it to datetime:\n",
    "            if ((index_series_type not in numeric_data_types) | (index_series_type == 'O') | (index_series_type == 'object')):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET['index_series'] = (DATASET['index_series']).astype('datetime64[ns]')\n",
    "                    \n",
    "                    # Rename column 'index_series':\n",
    "                    # https://www.statology.org/pandas-rename-columns/\n",
    "                    DATASET.rename(columns = {'index_series': 'timestamp'}, inplace = True)\n",
    "                    # Set 'timestamp' as the timestamp_tag_column:\n",
    "                    timestamp_tag_column = 'timestamp'\n",
    "                    \n",
    "                except:\n",
    "                    # A variable that is not neither numeric nor date was passed. Reset the index:\n",
    "                    DATASET = DATASET.reset_index(drop = True)\n",
    "                    # Update the index series\n",
    "                    DATASET['index_series'] = DATASET.index\n",
    "                    \n",
    "            # Now, try to manipulate the 'index_series'. An exception will be raised in case\n",
    "            # the name was changed because the index contains a timestamp\n",
    "            try:\n",
    "                \n",
    "                # convert the 'index_series' to the default name for column X:\n",
    "                x_forecast_label  = \"x_forecast\"\n",
    "                DATASET.rename(columns = {'index_series': x_forecast_label}, inplace = True)\n",
    "                x_original_series = DATASET[x_forecast_label].copy(deep = True)\n",
    "            \n",
    "                # Let's create the series for forecasted X. Simply add more values to x\n",
    "                # until reaching the number_of_periods_to_forecast:\n",
    "            \n",
    "                # Get the value of the last X of the dataframe\n",
    "                # index start from zero, so the last one is length - 1\n",
    "                last_x = x_original_series[(len(x_original_series) - 1)]\n",
    "\n",
    "                #Start the list\n",
    "                x_forecast = []\n",
    "\n",
    "                #Append its first value: last X plus 1 period\n",
    "                x_forecast.append(last_x + 1)\n",
    "            \n",
    "                j = 1\n",
    "                while (j < number_of_periods_to_forecast):\n",
    "                    #Last cycle occurs when j == number_of_periods_to_forecast - 1\n",
    "                    # Since indexing starts from zero, there will be number_of_periods_to_forecast\n",
    "                    # elements in the list.\n",
    "                    # Also, we already added the first period, so we are starting from the 2nd\n",
    "                    # forecast period, j = 1.\n",
    "                    x_forecast.append((x_forecast[(j - 1)] + 1))\n",
    "\n",
    "                    #Go to next iteration:\n",
    "                    j = j + 1\n",
    "                \n",
    "                # Now, x_forecast stores the next indices for the situation where no timestamp\n",
    "                # was provided initially.\n",
    "            \n",
    "            except:\n",
    "                #simply pass\n",
    "                pass\n",
    "        \n",
    "        \n",
    "        # Again, check if timestamp_tag_column is None. It may have changed, since we created a value\n",
    "        # for the case where a timestamp is in the index. So, we will not use else: the else would\n",
    "        # ignore the modification in the first if:\n",
    "        \n",
    "        if not (timestamp_tag_column is None):\n",
    "            # Use the timestamp properly converted to datetime (as in the graphics functions).\n",
    "            # The labels must be the same for when the dataframes are merged.\n",
    "            x_forecast_label  = timestamp_tag_column\n",
    "            \n",
    "            # Check if it is an object or is not a numeric variable (e.g. if it is a timestamp):\n",
    "            if ((DATASET[timestamp_tag_column].dtype not in numeric_data_types) | (DATASET[timestamp_tag_column].dtype == 'O') | (DATASET[timestamp_tag_column].dtype == 'object')):\n",
    "                \n",
    "                # Try to convert it to np.datetime64\n",
    "                try:\n",
    "\n",
    "                    DATASET[timestamp_tag_column] = (DATASET[timestamp_tag_column]).astype('datetime64[ns]')\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            # Now, timestamp is either a numeric column or a datetime column.\n",
    "            # Again, create a separate series for the original data, that will be used for\n",
    "            # differentiating between data and forecasts on the plot:\n",
    "            x_original_series = DATASET[timestamp_tag_column].copy(deep = True)\n",
    "            \n",
    "            # Get the last X of the dataframe\n",
    "            # index start from zero, so the last one is length - 1\n",
    "            last_x = x_original_series[(len(x_original_series) - 1)]\n",
    "            \n",
    "            # Check the case where it is a timestamp:\n",
    "            if (type(x_original_series[0]) == np.datetime64):\n",
    "                \n",
    "                # Let's obtain the mean value of the timedeltas between each measurement:\n",
    "                TIMESTAMP_TAG_COLUMN = timestamp_tag_column\n",
    "                NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "                RETURNED_TIMEDELTA_UNIT = time_unit\n",
    "                # If it is none, the value will be returned in nanoseconds.\n",
    "                # keep it None, for the results in nanoseconds\n",
    "                RETURN_AVG_DELAY = True\n",
    "                _, avg_delay = CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)\n",
    "                # The underscore indicates that we will not keep the returned dataframe\n",
    "                # only the average time delay in nanoseconds.\n",
    "                print(\"\\n\")\n",
    "                print(f\"Average delay on the original time series, used for obtaining times of predicted values = {avg_delay}.\\n\")\n",
    "\n",
    "                # Now, avg_delay stores the mean time difference between successive measurements from the\n",
    "                # original dataset.\n",
    "            \n",
    "                # Now, let's create the prediction timestamps, by adding the avg_delay to\n",
    "                # the last X.\n",
    "                # Firstly, convert last_x to a Pandas timestamp, so that we can add a pandas\n",
    "                # timedelta:\n",
    "                last_x = pd.Timestamp(last_x, unit = 'ns')\n",
    "            \n",
    "                # Now, let's create a pandas timedelta object correspondent to avg_delay\n",
    "                # 1. Check units:\n",
    "                if (time_unit is None):\n",
    "                    time_unit = 'ns'\n",
    "            \n",
    "                # Notice that CALCULATE_DELAY and ADD_TIMEDELTA update the unit for us, but\n",
    "                # such functions deal with a dataframe, not with a single value. So, we are\n",
    "                # using a small piece from ADD_TIMEDELTA function to operate with a single\n",
    "                # timestamp.\n",
    "\n",
    "                #2. Create the pandas timedelta object:\n",
    "                timedelta = pd.Timedelta(avg_delay, time_unit)\n",
    "            \n",
    "                #3. Let's create the values of timestamps correspondent to the forecast\n",
    "                x_forecast = []\n",
    "                # The number of elements of this list is number_of_periods_to_forecast\n",
    "                # if number_of_periods_to_forecast, we will forecast a single period further,\n",
    "                # then we need to sum timedelta once. If number_of_periods_to_forecast = 3,\n",
    "                # we will sum timedelta 3 times, and so on.\n",
    "            \n",
    "                # Append the first element (last element + timedelta) - first value forecast\n",
    "                x_forecast.append((last_x + timedelta))\n",
    "            \n",
    "                j = 1\n",
    "                while (j < number_of_periods_to_forecast):\n",
    "                    #Last cycle occurs when j == number_of_periods_to_forecast - 1\n",
    "                    # Since indexing starts from zero, there will be number_of_periods_to_forecast\n",
    "                    # elements in the list.\n",
    "                    # Also, we already added the first period, so we are starting from the 2nd\n",
    "                    # forecast period, j = 1.\n",
    "\n",
    "                    # append the previous element + timedelta.\n",
    "                    # If j = 1, (j - 1) = 0, the first element\n",
    "                    x_forecast.append((x_forecast[(j - 1)] + timedelta))\n",
    "                \n",
    "                    #Go to next iteration:\n",
    "                    j = j + 1\n",
    "        \n",
    "                # Now, x_forecast stores the values of timestamps correspondent to\n",
    "                # the forecasts.\n",
    "                # Convert x_forecast to Pandas Series, so that it will be possible to perform vectorial\n",
    "                # operations:\n",
    "                x_forecast = pd.Series(x_forecast)\n",
    "                # Convert it to datetime64:\n",
    "                x_forecast = (x_forecast).astype('datetime64[ns]')\n",
    "            \n",
    "            else:\n",
    "                # We have a numerical variable used as time. We have to calculate the average 'delay' between\n",
    "                # Successive values. For that, we can again use Pandas.diff method, which may be applied to\n",
    "                # Series or DataFrames\n",
    "                # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\n",
    "                # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.diff.html#pandas.Series.diff\n",
    "                \n",
    "                x_diff_series = x_original_series.copy(deep = True)\n",
    "                x_diff_series = x_diff_series.diff(periods = 1)\n",
    "                # periods - int, default 1 - Periods to shift for calculating difference, accepts negative values.\n",
    "                \n",
    "                # Now, get the average delay as the mean value from series x_diff_series:\n",
    "                avg_delay = x_diff_series.mean()\n",
    "                print(\"\\n\")\n",
    "                print(f\"Average delay on the original time series, used for obtaining times of predicted values = {avg_delay}.\\n\")\n",
    "                \n",
    "                # Let's create the values of times correspondent to the forecast\n",
    "                x_forecast = []\n",
    "                # The number of elements of this list is number_of_periods_to_forecast\n",
    "                # if number_of_periods_to_forecast, we will forecast a single period further,\n",
    "                # then we need to sum timedelta once. If number_of_periods_to_forecast = 3,\n",
    "                # we will sum timedelta 3 times, and so on.\n",
    "            \n",
    "                # Append the first element (last element + avg_delay) - first value forecast\n",
    "                x_forecast.append((last_x + avg_delay))\n",
    "            \n",
    "                j = 1\n",
    "                while (j < number_of_periods_to_forecast):\n",
    "                    #Last cycle occurs when j == number_of_periods_to_forecast - 1\n",
    "                    # Since indexing starts from zero, there will be number_of_periods_to_forecast\n",
    "                    # elements in the list.\n",
    "                    # Also, we already added the first period, so we are starting from the 2nd\n",
    "                    # forecast period, j = 1.\n",
    "\n",
    "                    # append the previous element + timedelta.\n",
    "                    # If j = 1, (j - 1) = 0, the first element\n",
    "                    x_forecast.append((x_forecast[(j - 1)] + avg_delay))\n",
    "                \n",
    "                    #Go to next iteration:\n",
    "                    j = j + 1\n",
    "                \n",
    "        # Notice that all these steps are possible only when the timestamps were\n",
    "        # given, and so they should not be executed if the values are not provided.\n",
    "        \n",
    "        \n",
    "        # Now, steps are the same for all cases where there is a dataframe (Main Else node that we are evaluating): \n",
    "        # create the dictionary of forecasts:\n",
    "        forecast_dict = {\n",
    "\n",
    "            x_forecast_label: x_forecast,\n",
    "            y_forecast_label: forecast_mean_vals,\n",
    "            \"lower_cl\": lower_cl,\n",
    "            \"upper_cl\": upper_cl,\n",
    "            'source': 'forecast'\n",
    "        }\n",
    "\n",
    "        # Convert it to a dataframe:\n",
    "        forecast_df = pd.DataFrame(data = forecast_dict)\n",
    "        x_forecast_series = forecast_df[x_forecast_label]\n",
    "        y_forecast_series = forecast_df[y_forecast_label]\n",
    "        lcl_series = forecast_df['lower_cl']\n",
    "        ucl_series = forecast_df['upper_cl']\n",
    "\n",
    "        # Now, let's concatenate the new dataframe to the old one.\n",
    "        # The use of the variables 'source', x_ and y_forecast_label guarantees that the new\n",
    "        # columns have the same name as the ones of the original dataframe, so that\n",
    "        # the concatenation is performed correctly.\n",
    "\n",
    "        # Now, merge the dataframes:\n",
    "        LIST_OF_DATAFRAMES = [DATASET, forecast_df]\n",
    "        IGNORE_INDEX_ON_UNION = True\n",
    "        SORT_VALUES_ON_UNION = True\n",
    "        UNION_JOIN_TYPE = None\n",
    "        forecast_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)\n",
    "        \n",
    "        # Full series, with input data and forecasts:\n",
    "        x = forecast_df[x_forecast_label]\n",
    "        y = forecast_df[y_forecast_label]\n",
    "        \n",
    "        # Now, let's re-order the dataframe, putting the x_forecast_label as the first column\n",
    "        # and y_forecast_label: forecast_mean_vals, \"lower_cl\", \"upper_cl\", and 'source'\n",
    "        # as the last ones.\n",
    "        \n",
    "        # Set a list of the last columns (fixed positions):\n",
    "        fixed_columns_list = [y_forecast_label, 'lower_cl', 'upper_cl', 'source']\n",
    "        \n",
    "        # Start the list with only x_forecast_label:\n",
    "        ordered_columns_list = [x_forecast_label]\n",
    "        \n",
    "        # Now, loop through all the columns:\n",
    "        for column in list(forecast_df.columns):\n",
    "\n",
    "            # If the column is not one from fixed_columns_list, and it is not on ordered_columns_list\n",
    "            # yet, add it to ordered_columns_list (timestamp_tag_column may be on the list):\n",
    "            if ((column not in fixed_columns_list) & (column not in ordered_columns_list)):\n",
    "                ordered_columns_list.append(column)\n",
    "        \n",
    "        # Now, concatenate ordered_columns_list to fixed_columns_list. \n",
    "        # If a = ['a', 'b'] and b = ['c', 'd'], a + b = ['a', 'b', 'c', 'd'] and b + a = [ 'c', 'd', 'a', 'b']\n",
    "        ordered_columns_list = ordered_columns_list + fixed_columns_list\n",
    "        \n",
    "        # Finally, pass ordered_columns_list as argument for column filtering and re-order:\n",
    "        forecast_df = forecast_df[ordered_columns_list]\n",
    "\n",
    "        \n",
    "    # We are finally in the general case, after obtaining the dataframe through all possible ways:\n",
    "    print(f\"Finished the obtention of the forecast dataset. Check the 10 last rows of the forecast dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(forecast_df.tail(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(forecast_df.tail(10))\n",
    "        \n",
    "    \n",
    "    # Now, let's create the graphics\n",
    "    if (plot_predicted_time_series == True):\n",
    "        \n",
    "        LINE_STYLE = '-'\n",
    "        MARKER = ''\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"ARIMA_forecasts\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            if (timestamp_tag_column is None):\n",
    "                horizontal_axis_title = \"timestamp\"\n",
    "            else:\n",
    "                horizontal_axis_title = timestamp_tag_column\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            if (column_to_forecast is None):\n",
    "                vertical_axis_title = \"time_series\"\n",
    "            else:\n",
    "                vertical_axis_title = column_to_forecast\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "        \n",
    "        if ((df is not None) & ((column_to_forecast is not None))):\n",
    "            \n",
    "            # Plot the original data series:\n",
    "            ax.plot(x, y, linestyle = LINE_STYLE, marker = MARKER, color = 'darkblue', alpha = OPACITY, label = 'input_dataframe')\n",
    "        \n",
    "        # Plot the predictions (completely opaque, so that the input data series will not be\n",
    "        # visible)\n",
    "        ax.plot(x_forecast_series, y_forecast_series, linestyle = LINE_STYLE, marker = MARKER, color = 'red', alpha = 1.0, label = 'forecast')\n",
    "        \n",
    "        # Plot the confidence limits:\n",
    "        ax.plot(x_forecast_series, lcl_series, linestyle = 'dashed', marker = MARKER, color = 'magenta', alpha = 0.70, label = 'lower_confidence_limit')\n",
    "        ax.plot(x_forecast_series, ucl_series, linestyle = 'dashed', marker = MARKER, color = 'magenta', alpha = 0.70, label = 'upper_confidence_limit')    \n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend()\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"arima_forecast\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        #plt.figure(figsize = (12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nARIMA Forecasting completed.\\n\")\n",
    "    \n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9d3022ac-8c44-45ab-9314-8c811db48a1d"
   },
   "source": [
    "# **Function for obtaining rolling window statistics of the dataframe**\n",
    "- Calculating moving average ('mean'), standard deviation ('std'), sum ('sum'), or discrete difference (\"difference\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "414534cb-ecd7-45ec-aeb0-f495eb75c569",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def df_rolling_window_stats (df, window_size = 2, window_statistics = 'mean', min_periods_required = None, window_center = False, window_type = None, window_on = None, row_accross = 'rows', how_to_close_window = None, drop_missing_values = True):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Check Pandas rolling statistics documentation:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/user_guide/window.html#window-generic\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\n",
    "    \n",
    "    ## df: the whole dataframe to be processed.\n",
    "    \n",
    "    # window_statistics = 'mean', 'std', 'sum', or 'difference'\n",
    "    # 'difference' will perform the first discrete difference of element: Yn - Yn-1\n",
    "    \n",
    "    # window_size: integer value or offset time.\n",
    "    # Manipulate parameter window of the .rolling method; or the parameter\n",
    "    # periods of the .diff method.\n",
    "    # window = window_size; or periods = window_size.\n",
    "    # Size of the moving window. If an integer, the fixed number of observations \n",
    "    # used for each window. If an offset, the time period of each window. \n",
    "    # Each window will be a variable sized based on the observations included in the \n",
    "    # time-period. This is only valid for datetime-like indexes. \n",
    "    \n",
    "    # min_periods_required = None. Alternatively, set as an integer value.\n",
    "    # Manipulate parameter min_periods of .rolling method\n",
    "    # min_periods = min_periods_required\n",
    "    # Minimum number of observations in window required to have a value; otherwise, \n",
    "    # result is np.nan. For a window that is specified by an offset, min_periods will \n",
    "    # default to 1. For a window that is specified by an integer, min_periods will default \n",
    "    # to the size of the window.\n",
    "    \n",
    "    # window_center = False.\n",
    "    # Manipulate parameter center of .rolling method\n",
    "    # center = window_center\n",
    "    # If False, set the window labels as the right edge of the window index.\n",
    "    # If True, set the window labels as the center of the window index.\n",
    "    \n",
    "    # window_type = None\n",
    "    # Manipulate parameter win_type of .rolling method\n",
    "    # win_type = window_type\n",
    "    # If None, all points are evenly weighted. If a string, it must be a valid \n",
    "    # scipy.signal window function:\n",
    "    # https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows\n",
    "    # Certain Scipy window types require additional parameters to be passed in the \n",
    "    # aggregation function. The additional parameters must match the keywords specified \n",
    "    # in the Scipy window type method signature.\n",
    "    \n",
    "    # window_on = None\n",
    "    # Manipulate parameter on of .rolling method\n",
    "    # on = window_on\n",
    "    # string; For a DataFrame, a column label or Index level on which to calculate \n",
    "    # the rolling window, rather than the DataFrame’s index. Provided integer column is \n",
    "    # ignored and excluded from result since an integer index is not used to calculate \n",
    "    # the rolling window.\n",
    "    \n",
    "    # row_accross = 'rows'. Alternatively, row_accross = 'columns'\n",
    "    # manipulate the parameter axis of .rolling method:\n",
    "    # if row_accross = 'rows', axis = 0; if row_accross = 'columns', axis = 1.\n",
    "    # If axis = 0 or 'index', roll across the rows.\n",
    "    # If 1 or 'columns', roll across the columns.\n",
    "    \n",
    "    # how_to_close_window = None\n",
    "    # Manipulate parameter closed of .rolling method\n",
    "    # closed = how_to_close_window\n",
    "    # String: If 'right', the first point in the window is excluded from calculations.\n",
    "    # If 'left', the last point in the window is excluded from calculations.\n",
    "    # If 'both', the no points in the window are excluded from calculations.\n",
    "    # If 'neither', the first and last points in the window are excluded from calculations.\n",
    "    # Default None ('right').\n",
    "    \n",
    "    # drop_missing_values = True will remove all missing values created by the methods (all\n",
    "    # rows containing missing values). \n",
    "    # If drop_missing_values = False, the positions containing NAs will be kept.\n",
    "    \n",
    "    DATASET = df.copy(deep = True)\n",
    "    WINDOW = window_size\n",
    "    MIN_PERIODS = min_periods_required\n",
    "    CENTER = window_center\n",
    "    WIN_TYPE = window_type\n",
    "    ON = window_on\n",
    "    \n",
    "    numeric_data_types = [np.float16, np.float32, np.float64, np.int16, np.int32, np.int64]\n",
    "    \n",
    "    # Variable to map if the timestamp was correctly parsed. It will be set of True\n",
    "    # only when it happens:\n",
    "    date_parser_marker = False\n",
    "    \n",
    "    try:\n",
    "        if (type(DATASET[ON][0]) not in numeric_data_types):\n",
    "            # Try to Parse the date:\n",
    "            try:\n",
    "                \n",
    "                DATASET[ON] = (DATASET[ON]).astype('datetime64[ns]')\n",
    "                # Change the value of the marker to map that the date was correctly parsed:\n",
    "                date_parser_marker = True\n",
    "                print(f\"Column {ON} successfully converted to numpy.datetime64[ns].\\n\")     \n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if (row_accross == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "    \n",
    "    else:\n",
    "        # 'rows' or an invalid value was set, so set to 'rows' (Axis = 0)\n",
    "        AXIS = 0\n",
    "    \n",
    "    CLOSED = how_to_close_window\n",
    "    \n",
    "    # Now all the parameters for the rolling method are set. Calculate the dataframe\n",
    "    # for the selected statistic:\n",
    "    \n",
    "    if (window_statistics == 'mean'):\n",
    "        \n",
    "        rolling_window_df = DATASET.rolling(window = WINDOW, min_periods = MIN_PERIODS, center = CENTER, win_type = WIN_TYPE, on = ON, axis = AXIS, closed = CLOSED).mean()\n",
    "        print(f\"Calculated rolling mean for a window size of {WINDOW}. Returning the rolling \\'mean\\' dataframe.\\n\")\n",
    "    \n",
    "    elif (window_statistics == 'std'):\n",
    "        \n",
    "        rolling_window_df = DATASET.rolling(window = WINDOW, min_periods = MIN_PERIODS, center = CENTER, win_type = WIN_TYPE, on = ON, axis = AXIS, closed = CLOSED).std()\n",
    "        print(f\"Calculated rolling standard deviation for a window size of {WINDOW}. Returning the rolling \\'std\\' dataframe.\\n\")\n",
    "    \n",
    "    elif (window_statistics == 'sum'):\n",
    "        \n",
    "        rolling_window_df = DATASET.rolling(window = WINDOW, min_periods = MIN_PERIODS, center = CENTER, win_type = WIN_TYPE, on = ON, axis = AXIS, closed = CLOSED).sum()\n",
    "        print(f\"Calculated rolling sum for a window size of {WINDOW}. Returning the rolling \\'sum\\' dataframe.\\n\")\n",
    "    \n",
    "    elif (window_statistics == 'difference'):\n",
    "        \n",
    "        # Create a list of the columns that can be differentiated and of those that cannot be.\n",
    "        diff_columns = []\n",
    "        excluded_columns = []\n",
    "        for column in list(DATASET.columns):\n",
    "            \n",
    "            if (type(DATASET[column][0]) in numeric_data_types):\n",
    "                diff_columns.append(column)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if ((column == ON) & (date_parser_marker == True)):\n",
    "                    # This is the column we converted to date. Set it as the index.\n",
    "                    # It will allow us to calculate the differences without losing this\n",
    "                    # information\n",
    "                    DATASET = DATASET.set_index(column)\n",
    "                \n",
    "                else:\n",
    "                    excluded_columns.append(column)\n",
    "        \n",
    "        # Select only the columns in diff_columns:\n",
    "        DATASET= DATASET[diff_columns]\n",
    "        \n",
    "        if (len(excluded_columns) > 0):\n",
    "            print(f\"It is not possible to calculate the differences for columns in {excluded_columns}, so they were removed.\\n\")\n",
    "        \n",
    "        rolling_window_df = DATASET.diff(periods = WINDOW, axis = AXIS)\n",
    "        print(f\"Calculated discrete differences ({WINDOW} periods). Returning the differentiated dataframe.\\n\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Please, select a valid rolling window function: \\'mean\\', \\'std\\', \\'sum\\', or \\'difference\\'.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    # drop missing values generated:\n",
    "    if (drop_missing_values):\n",
    "        # Run of it is True. Do not reset index for mode 'difference'.\n",
    "        # In this mode, the index was set as the date.\n",
    "        rolling_window_df.dropna(axis = 0, how = 'any', inplace = True)\n",
    "        \n",
    "        if (window_statistics != 'difference'):\n",
    "            rolling_window_df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    print(\"Check the rolling dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(rolling_window_df)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(rolling_window_df)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"ATTENTION: depending on the window size, the windowed dataset may be considerable smaller than the original dataframe, with several missing values indicated by NA.\\n\")\n",
    "    print(\"For understanding it, consider a dataframe containing daily new cases of an illness, where we want to obtain the 7-day rolling average.\")\n",
    "    print(\"Here, we will obtain 6 rows containing only missing values. The reason is that it is not possible to calculate the 7-periods average for the first 6 rows.\")\n",
    "    print(\"In the first row, we have only 1 data; in the second row, we have only two, the day and the day before; ..., and so on.\")\n",
    "    print(\"We can only calculate a 7-period average from the 7th day, when we have that day and the 6 days before it.\")\n",
    "    print(\"Once it is not possible to obtain the rolling statistic for some rows, missing values are generated.\")\n",
    "    print(\"So, even if the rolling statistic was calculated for only 2 consecutive periods, there would be a row with missing values, since it is not possible to calculate the window statistic for a single entry.\\n\")\n",
    "    print(f\"Naturally, this examples suppose that the user set how_to_close_window = {'right'}, when the first point in the window is excluded from calculations.\")\n",
    "    print(f\"If how_to_close_window = {'left'}, then the last point in the window would be excluded from calculations, so the missing values would appear at the end of the dataset.\")\n",
    "    print(\"Even though it is not so intuitive, in this case we would take an entry and the next ones for calculating the statistic. For instance, the 7-day rolling average would be calculated as the average between a day and the next 6 days.\")\n",
    "    print(f\"Finally, if how_to_close_window = {'both'}, we would have a centralized window, where the some of the values come from the times before; and some come from the times after.\")\n",
    "    print(\"In this last case, the 7-day rolling average would be calculated as the average between a day; the 3 days before; and the 3 next days.\")\n",
    "    print(\"So, missing values would appear in both the beginning and the end of the dataframe.\\n\")\n",
    "    \n",
    "    print(\"For this function, the default is how_to_close_window = {'right'}, i.e., statistics are calculated from the row and the values before it.\\n\")\n",
    "    \n",
    "    return rolling_window_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "19144191-6123-45b5-959b-33ddbca9bb10"
   },
   "source": [
    "# **Function for decomposing time series seasonality and trend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "azdata_cell_guid": "58f26639-33aa-46ba-8a5e-e3fa326dea30",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def seasonal_decomposition (df, response_column_to_analyze, column_with_timestamps = None, decomposition_mode = \"additive\", maximum_number_of_cycles_or_periods_to_test = 100, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.tsa.seasonal import DecomposeResult\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    \n",
    "    #Check seasonal_decompose and DecomposeResult documentations:\n",
    "    # https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\n",
    "    # https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.DecomposeResult.html#statsmodels.tsa.seasonal.DecomposeResult\n",
    "    # seasonal_decompose results in an object from class DecomposeResult.\n",
    "    # Check the documentation of the .plot method for DecomposeResult objects:\n",
    "    # https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.DecomposeResult.plot.html#statsmodels.tsa.seasonal.DecomposeResult.plot\n",
    "    \n",
    "    #number_of_periods_to_forecast = 7\n",
    "    # integer value representing the total of periods to forecast. The periods will be in the\n",
    "    # unit (dimension) of the original dataset. If 1 period = 1 day, 7 periods will represent\n",
    "    # seven days.\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # response_column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed.\n",
    "    # e.g. response_column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    # WARNING: This must be the response variable\n",
    "    \n",
    "    # column_with_timestamps: string (inside quotes), \n",
    "    # containing the name of the column containing timestamps.\n",
    "    # Keep it as None if you want to set the index as the time.\n",
    "    # e.g. response_column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = integer (minimum value is 2) representing\n",
    "    # the total of cycles or periods that may be present on time series. The function will loop through\n",
    "    # 2 to MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST to find the number that minimizes the sum of\n",
    "    # modules (absolute values) of the residues.\n",
    "    # e.g. MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = 4 will test 2, 3 and 4 cycles on the time series.\n",
    "\n",
    "    # decomposition_mode = \"additive\" - manipulate the parameter 'model' from seasonal_decompose.\n",
    "    # model = decomposition_mode\n",
    "    # Alternatively, set decomposition_mode = \"multiplicative\" for decomposing as a multiplicative time series.\n",
    "    \n",
    "    ## 'additive' model: An additive model suggests that the components are added together as: \n",
    "    ## y(t) = Level + Trend + Seasonality + Noise\n",
    "    ## An additive model is linear where changes over time are consistently made by the same amount. A linear trend is \n",
    "    ## a straight line. A linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).\n",
    "    \n",
    "    ## 'multiplicative' model: A multiplicative model suggests that the components are multiplied together as:\n",
    "    ## y(t) = Level * Trend * Seasonality * Noise\n",
    "    ## A multiplicative model is nonlinear, such as quadratic or exponential. Changes increase or decrease over time.\n",
    "    ## A nonlinear trend is a curved line. A non-linear seasonality has an increasing or decreasing frequency \n",
    "    ## and/or amplitude over time.\n",
    "    # https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/#:~:text=The%20statsmodels%20library%20provides%20an%20implementation%20of%20the,careful%20to%20be%20critical%20when%20interpreting%20the%20result.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate?\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    #Check if there is a column with the timestamps:\n",
    "    if not (column_with_timestamps is None):\n",
    "        \n",
    "        DATASET = DATASET.sort_values(by = column_with_timestamps)\n",
    "        \n",
    "        x = DATASET[column_with_timestamps].copy()\n",
    "        \n",
    "        # try to convert it to datetime:\n",
    "        try:\n",
    "            x = x.astype(np.datetime64)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Set it as index to include it in the seasonal decomposition model:\n",
    "        DATASET = DATASET.set_index(column_with_timestamps)\n",
    "    \n",
    "    else:\n",
    "        #the index will be used to plot the charts:\n",
    "        x = DATASET.index\n",
    "        \n",
    "    # Extract the time series from the dataframe:\n",
    "    Y = DATASET[response_column_to_analyze]\n",
    "    \n",
    "    # Set the parameters for modelling:\n",
    "    MODEL = decomposition_mode\n",
    "    MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = maximum_number_of_cycles_or_periods_to_test\n",
    "    \n",
    "    # Check if the arguments are valid:\n",
    "    if MODEL not in [\"additive\", \"multiplicative\"]:\n",
    "        # set model as 'additive'\n",
    "        MODEL = \"additive\"\n",
    "    \n",
    "    print(f\"Testing {MODEL} model for seasonal decomposition.\\n\")\n",
    "    \n",
    "    try:\n",
    "        MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = int(MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST)\n",
    "        # If it is lower than 2, make it equals to 2:\n",
    "        if (MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST < 2):\n",
    "            MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = 2\n",
    "        \n",
    "        print(f\"Testing the presence of until {MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST} periods or cycles in the time series.\\n\")\n",
    "    \n",
    "    except:\n",
    "        print(\"Input a valid MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST. It must be an integer higher or equal to 2, representing the maximum possible number of cycles or periods present in time series.\\n\")\n",
    "        return \"error\"\n",
    "    \n",
    "    # Now, let's loop through the possible number of cycles and periods:\n",
    "    # Start a dictionary to store the number of cycles and the correspondent sum of \n",
    "    # absolute values of the residues:\n",
    "    residues_dict = {}\n",
    "    \n",
    "    for TOTAL_OF_CYCLES in range (2, (MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST + 1)):\n",
    "        \n",
    "        # TOTAL_OF_CYCLES is an integer looping from TOTAL_OF_CYCLES = 2 to\n",
    "        # TOTAL_OF_CYCLES = (MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST + 1) - 1 = (MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            # Start an instance (object) from class DecomposeResult\n",
    "            # Set this object as the resultant from seasonal_decompose\n",
    "            decompose_res_obj = seasonal_decompose(Y, model = MODEL, period = TOTAL_OF_CYCLES, two_sided = True)\n",
    "            # decompose_res_obj is an instance (object) from class DecomposeResult\n",
    "\n",
    "            # Get the array of the residues. Convert it to NumPy array to guarantee the vectorial operations:\n",
    "            residues_array = np.array(decompose_res_obj.resid)\n",
    "            # Convert the values in the array to the absolute values:\n",
    "            residues_array = abs(residues_array)\n",
    "            # Get the sum of the absolute residues:\n",
    "            sum_of_abs_residues = np.sum(residues_array)\n",
    "\n",
    "            # Store it in the dictionary (value correspondent to the key TOTAL_OF_CYCLES):\n",
    "            residues_dict[TOTAL_OF_CYCLES] = sum_of_abs_residues\n",
    "        \n",
    "        except:\n",
    "            # There are no sufficient measurements to test this total of cycles\n",
    "            pass\n",
    "    \n",
    "    # get the list of dictionary's values:\n",
    "    dict_vals = list(residues_dict.values())\n",
    "    # Get the minimum value on the list:\n",
    "    minimum_residue = min(dict_vals)\n",
    "    # Get the index of minimum_residue on the list:\n",
    "    minimum_residue_index = dict_vals.index(minimum_residue)\n",
    "    \n",
    "    # Now, retrieve OPTIMAL_TOTAL_CYCLES. It will be the value with index minimum_residue_index\n",
    "    # in the list of keys:\n",
    "    list_of_keys = list(residues_dict.keys())\n",
    "    OPTIMAL_TOTAL_CYCLES = list_of_keys[minimum_residue_index]\n",
    "    \n",
    "    print(f\"Number of total cycles or periods in time series: {OPTIMAL_TOTAL_CYCLES}.\\n\")\n",
    "    \n",
    "    # Start an instance (object) from class DecomposeResult\n",
    "    # Set this object as the resultant from seasonal_decompose\n",
    "    decompose_res_obj = seasonal_decompose(Y, model = MODEL, period = OPTIMAL_TOTAL_CYCLES, two_sided = True)\n",
    "    # decompose_res_obj is an instance (object) from class DecomposeResult\n",
    "    \n",
    "    # Create a dictionary with the resultants from the seasonal decompose:\n",
    "    # These resultants are obtained as attributes of the decompose_res_obj\n",
    "    \n",
    "    number_of_observations_used = decompose_res_obj.nobs\n",
    "    print(f\"Seasonal decomposition concluded using {number_of_observations_used} observations.\\n\")\n",
    "    \n",
    "    decompose_dict = {\n",
    "        \n",
    "        'timestamp': x,\n",
    "        \"observed_data\": decompose_res_obj.observed,\n",
    "        \"seasonal_component\": decompose_res_obj.seasonal,\n",
    "        \"trend_component\": decompose_res_obj.trend,\n",
    "        \"residuals\": decompose_res_obj.resid\n",
    "    }\n",
    "    \n",
    "    # Convert it into a returned dataframe:\n",
    "    seasonal_decompose_df = pd.DataFrame(data = decompose_dict)\n",
    "    \n",
    "    print(\"Check the first 10 rows of the seasonal decompose dataframe obtained:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(seasonal_decompose_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(seasonal_decompose_df.head(10))\n",
    "    \n",
    "    print(\"\\n\") # line break\n",
    "    print(f\"Check the time series decomposition graphics for the {MODEL} model:\\n\")\n",
    "    \n",
    "    # Plot parameters:\n",
    "    x = decompose_dict['timestamp']\n",
    "    y1 = decompose_dict['observed_data']\n",
    "    lab1 = \"observed_data\"\n",
    "    y2 = decompose_dict['seasonal_component']\n",
    "    lab2 = 'seasonal_component'\n",
    "    y3 = decompose_dict['trend_component']\n",
    "    lab3 = 'trend_component'\n",
    "    y4 = decompose_dict['residuals']\n",
    "    lab4 = 'residuals'\n",
    "    \n",
    "    plot_title = \"seasonal_decomposition_for_\" + response_column_to_analyze\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    # Now, let's obtain the graphic:\n",
    "    # Create the figure:\n",
    "    fig, ax = plt.subplots(4, 1, sharex = True, figsize = (12, 8)) \n",
    "    # sharex = share axis X\n",
    "    # number of subplots equals to the total of series to plot (in this case, 4)\n",
    "    \n",
    "    ax[0].plot(x, y1, linestyle = '-', marker = '', color = 'darkblue', alpha = OPACITY, label = lab1)\n",
    "    # Set title only for this subplot:\n",
    "    ax[0].set_title(plot_title)\n",
    "    ax[0].grid(grid)\n",
    "    ax[0].legend(loc = 'upper right')\n",
    "    # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "    # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "    # https://www.statology.org/matplotlib-legend-position/\n",
    "    \n",
    "    ax[1].plot(x, y2, linestyle = '-', marker = '', color = 'crimson', alpha = OPACITY, label = lab2)\n",
    "    # Add the y-title only for this subplot:\n",
    "    ax[1].set_ylabel(response_column_to_analyze)\n",
    "    ax[1].grid(grid)\n",
    "    ax[1].legend(loc = 'upper right')\n",
    "    \n",
    "    ax[2].plot(x, y3, linestyle = '-', marker = '', color = 'darkgreen', alpha = OPACITY, label = lab3)\n",
    "    ax[2].grid(grid)\n",
    "    ax[2].legend(loc = 'upper right')\n",
    "    \n",
    "    ax[3].plot(x, y4, linestyle = '', marker = 'o', color = 'red', alpha = OPACITY, label = lab4)\n",
    "    # Add an horizontal line in y = zero:\n",
    "    ax[3].axhline(0, color = 'black', linestyle = 'dashed', alpha = OPACITY)\n",
    "    # Set the x label only for this subplot\n",
    "    ax[3].set_xlabel('timestamp')\n",
    "    ax[3].grid(grid)\n",
    "    ax[3].legend(loc = 'upper right')\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"seasonal_decomposition\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    #Finally, return the full dataframe:\n",
    "    print(\"The full dataframe obtained from the decomposition was returned as seasonal_decompose_df.\")\n",
    "    \n",
    "    return seasonal_decompose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "716878dc-b8fe-4c2a-8928-841601359ba0"
   },
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d9895886-a7e6-45c1-8bdb-00b5f4b09843",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def export_pd_dataframe_as_csv (dataframe_obj_to_be_exported, new_file_name_without_extension, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_obj_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # new_file_name_without_extension - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "    # will export a file 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name_without_extension)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_obj_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_without_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "956622d4-a9a8-469a-a6e5-3a52837e3045"
   },
   "source": [
    "# **Function for importing or exporting models, lists, or dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_list_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_or_list_file_name = None, directory_path = '', model_type = 'keras', dict_or_list_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickle as pkl\n",
    "    import dill\n",
    "    import tensorflow as tf\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_or_list_only' if only a dictionary or list will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    # model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_or_list_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_or_list_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep dictionary_or_list_file_name = None if no \n",
    "    # dictionary or list will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type = 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "    # lambda layers. Such models are compressed as tar.gz.\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_or_list_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_or_list_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_or_list_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root (empty string):\n",
    "        directory_path = \"\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_or_list_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_or_list_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary or list.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_or_list_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'keras_lambda'):\n",
    "                model_extension = 'tar.gz'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary or list {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary or list successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_lambda'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    # Try accessing the tar.gz file directly from the environment:\n",
    "                    model_path = key\n",
    "                    # to access from the dictionary:\n",
    "                    # model_path = colab_files_dict[key]\n",
    "                    \n",
    "                    # Extract to a temporary 'tmp' directory:\n",
    "                    #try:\n",
    "                    # Compress the directory using tar\n",
    "                    # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    #except:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                    # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                    # https://docs.python.org/3/library/tarfile.html\n",
    "                    # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'r:gz')\n",
    "                    tar_file.extractall(\"tmp/\")\n",
    "                    tar_file.close()\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # Extract to a temporary 'tmp' directory:\n",
    "                    #try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    #except:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                    # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                    # https://docs.python.org/3/library/tarfile.html\n",
    "                    # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'r:gz')\n",
    "                    tar_file.extractall(\"tmp/\")\n",
    "                    tar_file.close()\n",
    "                    \n",
    "                    model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    print(f\"TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "\n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "\n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_or_list_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary or list {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary or list successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_lambda'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    \n",
    "                    # Save your model in the SavedModel format\n",
    "                    model_to_export.save('saved_model/my_model')\n",
    "                    \n",
    "                    #try:\n",
    "                        # Compress the directory using tar\n",
    "                        # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar -czvf model_path saved_model/\n",
    "                    \n",
    "                    #except NotFoundError:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                    # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                    # https://docs.python.org/3/library/tarfile.html\n",
    "                    # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'w:gz')\n",
    "                    tar_file.add('saved_model/')\n",
    "                    tar_file.close()\n",
    "                    \n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    files.download(key)\n",
    "                    print(f\"TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # Save your model in the SavedModel format\n",
    "                    model_to_export.save('saved_model/my_model')\n",
    "                    \n",
    "                    #try:\n",
    "                        # Compress the directory using tar\n",
    "                    #    ! tar -czvf model_path saved_model/\n",
    "                    \n",
    "                    #except NotFoundError:\n",
    "                        \n",
    "                    from tarfile import TarFile\n",
    "                        # pickle, csv, tarfile, and zipfile are on Python standard library\n",
    "                        # https://docs.python.org/3/library/tarfile.html\n",
    "                        # https://docs.python.org/3/library/zipfile.html#module-zipfile\n",
    "                    tar_file = TarFile.open(model_path, mode = 'w:gz')\n",
    "                    tar_file.add('saved_model/')\n",
    "                    tar_file.close()\n",
    "                        \n",
    "                    print(f\"TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ceb7dee9-0ba7-4522-ab92-676ad6697323"
   },
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0bc5fb5a-3f28-4491-8f70-a16ec56f58c0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a05dd775-202f-4e5b-8e81-b40faa27ff7a"
   },
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "166586df-2ba0-44a5-8df3-cb47fc158710",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7126fff3-b4a5-43e0-9af5-7b5c30927398",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "194de7c1-b459-4b21-a222-50da71347090",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b52a2df0-9437-40db-91e9-e2db8555bcdb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7f3e425a-e22f-4b1c-9e94-4bebde3564d8",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1718038b-685c-4cc2-a7a4-1bee78d6c552",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "eedd8a2f-0903-4acc-b709-5517e0dffa5d",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "22de8878-df14-4e00-8534-dd67d1a6a4a0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4f24d370-0158-413a-8b1d-591434f879ff"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ccebfc94-859b-4727-9c42-8ae85a2f4ca0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f4297857-4169-45c1-8a7a-44fa34a4da46"
   },
   "source": [
    "### **Calculating differences between successive timestamps (delays)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7c498087-9208-4e89-9f23-3d0b26ad3362"
   },
   "source": [
    "#### Case 1: return average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a552e5b2-d069-414f-ade1-d21c27aef4b0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_grouped\"\n",
    "# \"timestamp_grouped\" is the column created by the function which aggregates the timestamps.\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "# Keep inside quotes.\n",
    "\n",
    "NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. NEW_TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = 'day'\n",
    "# Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "RETURN_AVG_DELAY = True\n",
    "# RETURN_AVG_DELAY = True will print and return the value of the average delay.\n",
    "# RETURN_AVG_DELAY = False will omit this information\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality.\n",
    "# Average delay float value istored into variable avg_delay. \n",
    "# Simply modify this object on the left of equality.\n",
    "new_df, avg_delay = CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "22711346-b391-47b5-9d54-1faabcb422c8"
   },
   "source": [
    "#### Case 2: do not return average delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e7c85c00-dc6a-475b-9d6e-30724039c54f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp_grouped\"\n",
    "# \"timestamp_grouped\" is the column created by the function which aggregates the timestamps.\n",
    "# Alternatively: string (inside quotes) containing the name (header) of the timestamp column\n",
    "# on the left (from which the right timestamp will be subtracted).\n",
    "# Keep inside quotes.\n",
    "\n",
    "NEW_TIMEDELTA_COLUMN_NAME = None\n",
    "# Name of the new column. If no value is provided, the default name \n",
    "# [timestamp_tag_column1]-[timestamp_tag_column2] will be given.\n",
    "# Alternatively: keep it as None or input a name (string) for the new column inside quotes:\n",
    "# e.g. TIMEDELTA_COLUMN_NAME = \"Timestamp_difference\"\n",
    "    \n",
    "RETURNED_TIMEDELTA_UNIT = None\n",
    "# Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input RETURNED_TIMEDELTA_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "\n",
    "RETURN_AVG_DELAY = False\n",
    "# RETURN_AVG_DELAY = True will print and return the value of the average delay.\n",
    "# RETURN_AVG_DELAY = False will omit this information\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = CALCULATE_DELAY (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, new_timedelta_column_name  = NEW_TIMEDELTA_COLUMN_NAME, returned_timedelta_unit = RETURNED_TIMEDELTA_UNIT, return_avg_delay = RETURN_AVG_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "229cccfa-9e0b-4942-a07a-1ff8831798ab"
   },
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "34620841-b79a-412b-b233-03fce5a45be2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "353b258e-8c39-4710-b0e8-4d9ed932d6bb",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Lag-diagnosis: obtaining autocorrelation (ACF) and partial autocorrelation function (PACF) plots of the time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5cf4ec47-3f52-4d67-9b23-014a0dcd4fdd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NUMBER_OF_LAGS = 40\n",
    "# NUMBER_OF_LAGS = integer value. e.g. number_of_lags = 50\n",
    "# represents how much lags will be tested, and the length of the horizontal axis.\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'lag_diagnosis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "lag_diagnosis (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, number_of_lags = NUMBER_OF_LAGS, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ff7e28c2-03ff-4b5a-a004-ff21574570bc",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining the 'd' parameter of ARIMA (p, q, d) model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "33b16794-537c-46e5-80bd-8149b400dec9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NUMBER_OF_LAGS = 40\n",
    "# NUMBER_OF_LAGS = integer value. e.g. number_of_lags = 50\n",
    "# represents how much lags will be tested, and the length of the horizontal axis.\n",
    "\n",
    "MAX_TESTED_d = 2\n",
    "#MAX_TESTED_d: differential order (integer value)\n",
    "#change the integer if you want to test other cases. By default, MAX_TESTED_d = 2, meaning\n",
    "# that the values d = 0, 1, and 2 will be tested.\n",
    "# If MAX_TESTED_d = 1, d = 0 and 1 will be tested.\n",
    "# If MAX_TESTED_d = 3, d = 0, 1, 2, and 3 will be tested, and so on.\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "# CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "# Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "# to get less restrictive results.\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'test_d_parameters.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "test_d_parameters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, number_of_lags = NUMBER_OF_LAGS, max_tested_d = MAX_TESTED_d, confidence_level = CONFIDENCE_LEVEL, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d8c42b9d-801a-4577-8fd5-72949230a433",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining the best ARIMA (p, q, d) model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f30f4f48-ba98-42da-9a0f-edabc6b247ff",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE = string (inside quotes) containing the name of the column that will be analyzed. \n",
    "# e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = None\n",
    "# TIMESTAMP_TAG_COLUMN = None - keep it as None if you do not want to inform the timestamps\n",
    "# Alternatively, declare a string (inside quotes), \n",
    "# containing the name of the column containing the time information. \n",
    "# e.g. TIMESTAMP_TAG_COLUMN = \"DATE\" will take the timestamps from column 'DATE'.\n",
    "# If no column is provided, the index in the dataframe will be used.\n",
    "\n",
    "p_VALS_LIST = [1]\n",
    "#p_vals: list of integers correspondent to the lags (spikes) in the PACF plot.\n",
    "# From function lag_diagnosis\n",
    "# e.g. for testing the values 1, 2, and 3, set p_VALS_LIST = [1, 2, 3]\n",
    "# for testing 1 and 5, set p_VALS_LIST = [1, 5]\n",
    "\n",
    "BEST_d_PARAM = 1\n",
    "#d = difference for making the process stationary.\n",
    "# From function test_d_parameters\n",
    "# Integer value. Alternatively, set BEST_d_PARAM as the 'd' value returned from test_d_parameters\n",
    "# e.g. BEST_d_PARAM = 0, for the ARMA model;\n",
    "# e.g. BEST_d_PARAM = 2; BEST_d_PARAM = 3; etc.\n",
    "\n",
    "q_VALS_LIST = [1]\n",
    "#q_vals: list of integers correspondent to the lags (spikes) in ACF plot.\n",
    "# From function lag_diagnosis\n",
    "# e.g. for testing the values 1, 2, and 3, set p_VALS_LIST = [1, 2, 3]\n",
    "# for testing 1 and 5, set p_VALS_LIST = [1, 5]\n",
    "\n",
    "## WARNING: do not test the ARIMA/ARMA model for p = 0, or q = 0.\n",
    "## For lag = 0, the correlation and partial correlation coefficients \n",
    "## are always equal to 1, because the data is perfectly correlated to itself. \n",
    "## Therefore, ignore the first spikes (lag = 0) of ACF and PACF plots.\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "# CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "# Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "# to get less restrictive results.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'arima_model.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#ARIMA model object saved as ARIMA_model; dictionary saved as arima_summ_dict.\n",
    "# Dataframe object containing model predictions and confidence intervals saved as arima_df\n",
    "# Simply modify these object on the left of equality:\n",
    "ARIMA_model, arima_summ_dict, arima_df = best_arima_model (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, p_vals = p_VALS_LIST, d = BEST_d_PARAM, q_vals = q_VALS_LIST, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, confidence_level = CONFIDENCE_LEVEL, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c406ce21-38af-4751-af75-c7cbd157b9b8",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Forecasting with ARIMA model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "05e76d0c-ce07-4ea3-8862-c7527835e912",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_FORECAST = 'column_to_forecast'\n",
    "# COLUMN_TO_FORECAST: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed.\n",
    "# Keep it as None if the graphic of the predictions will not be shown with the\n",
    "# responses or if the combined dataset will not be returned.\n",
    "# e.g. COLUMN_TO_FORECAST = \"column1\" will analyze and forecast values for \n",
    "# the column named as 'column1'.\n",
    "    \n",
    "TIMESTAMP_TAG_COLUMN = None\n",
    "# TIMESTAMP_TAG_COLUMN = None - keep it as None if you do not want to inform the timestamps\n",
    "# Alternatively, declare a string (inside quotes), \n",
    "# containing the name of the column containing the time information. \n",
    "# e.g. TIMESTAMP_TAG_COLUMN = \"DATE\" will take the timestamps from column 'DATE'.\n",
    "# If no column is provided, the index in the dataframe will be used.\n",
    "\n",
    "# DATASET = None, TIMESTAMP_TAG_COLUMN = None - keep it as None if you do not want to show\n",
    "# the ARIMA predictions combined to the original data; or if you do not want to append\n",
    "# the ARIMA predictions to the original dataframe.\n",
    "# Alternatively, set:\n",
    "# DATASET: the whole dataframe to be processed.\n",
    "\n",
    "ARIMA_MODEL_OBJECT = ARIMA_model\n",
    "# ARIMA_MODEL_OBJECT : object containing the ARIMA model previously obtained.\n",
    "# e.g. arima_model_object = returned_ARIMA_Results if the model was obtained as returned_ARIMA_Results\n",
    "# do not declare in quotes, since it is an object, not a string.\n",
    "\n",
    "TIME_UNIT = None\n",
    "# TIME_UNIT: Unit of the new column. If no value is provided, the unit will be considered as nanoseconds. \n",
    "# Alternatively: keep it None, for the results in nanoseconds, or input TIME_UNIT = \n",
    "# 'year', 'month', 'day', 'hour', 'minute', or 'second' (keep these inside quotes).\n",
    "# It will be the input for the function CALCULATE_DELAY.\n",
    "\n",
    "NUMBER_OF_PERIODS_TO_FORECAST = 7\n",
    "# integer value representing the total of periods to forecast. The periods will be in the\n",
    "# unit (dimension) of the original dataset. If 1 period = 1 day, 7 periods will represent\n",
    "# seven days.\n",
    "\n",
    "PLOT_PREDICTED_TIME_SERIES = True\n",
    "# Keep PLOT_PREDICTED_TIME_SERIES = True to see the graphic of the predicted values.\n",
    "# Alternatively, set plot_predicted_time_series = True not to show the plot.\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "# CONFIDENCE_LEVEL = 0.95 = 95% confidence\n",
    "# Set CONFIDENCE_LEVEL = 0.90 to get 0.90 = 90% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can reduce CONFIDENCE_LEVEL \n",
    "# to get less restrictive results.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'arima_forecast.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# Dataframe object containing model predictions and confidence intervals saved as forecast_df\n",
    "# Simply modify this object on the left of equality:\n",
    "forecast_df = arima_forecasting (arima_model_object = ARIMA_MODEL_OBJECT, df = DATASET, column_to_forecast = COLUMN_TO_FORECAST, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, time_unit = TIME_UNIT, number_of_periods_to_forecast = NUMBER_OF_PERIODS_TO_FORECAST, confidence_level = CONFIDENCE_LEVEL, plot_predicted_time_series = PLOT_PREDICTED_TIME_SERIES, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e63623a3-e0bc-41fe-a5eb-879780f9a452",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining rolling window statistics of the dataframe**\n",
    "- Calculating moving average ('mean'), standard deviation ('std'), sum ('sum'), or discrete difference (\"difference\").\n",
    "- REMEMBER: q represents the moving average (MA) part of the time series.\n",
    "    - Then, it is interesting to group the time series by each q periods when modelling or analyzing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b5ea161b-5d2f-419e-aff0-f572720a307b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "WINDOW_STATISTICS = 'mean'\n",
    "# WINDOW_STATISTICS = 'mean', 'std', 'sum', or 'difference'\n",
    "# 'difference' will perform the first discrete difference of element: Yn - Yn-1\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "# WINDOW_SIZE: integer value or offset time.\n",
    "# Manipulate parameter window of the .rolling method; or the parameter\n",
    "# periods of the .diff method.\n",
    "# window = WINDOW_SIZE; or periods = WINDOW_SIZE.\n",
    "# Size of the moving window. If an integer, the fixed number of observations \n",
    "# used for each window. If an offset, the time period of each window. \n",
    "# Each window will be a variable sized based on the observations included in the \n",
    "# time-period. This is only valid for datetime-like indexes. \n",
    "\n",
    "MIN_PERIODS_REQUIRED = None\n",
    "# MIN_PERIODS_REQUIRED = None. Alternatively, set as an integer value.\n",
    "# Manipulate parameter min_periods of .rolling method\n",
    "# min_periods = MIN_PERIODS_REQUIRED\n",
    "# Minimum number of observations in window required to have a value; otherwise, \n",
    "# result is np.nan. For a window that is specified by an offset, min_periods will \n",
    "# default to 1. For a window that is specified by an integer, min_periods will default \n",
    "# to the size of the window.\n",
    "\n",
    "WINDOW_CENTER = False\n",
    "# Manipulate parameter center of .rolling method\n",
    "# center = WINDOW_CENTER\n",
    "# If False, set the window labels as the right edge of the window index.\n",
    "# If True, set the window labels as the center of the window index.\n",
    "\n",
    "WINDOW_TYPE = None\n",
    "# Manipulate parameter win_type of .rolling method\n",
    "# win_type = WINDOW_TYPE\n",
    "# If None, all points are evenly weighted. If a string, it must be a valid \n",
    "# scipy.signal window function:\n",
    "# https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows\n",
    "# Certain Scipy window types require additional parameters to be passed in the \n",
    "# aggregation function. The additional parameters must match the keywords specified \n",
    "# in the Scipy window type method signature.\n",
    "\n",
    "WINDOW_ON = None\n",
    "# Manipulate parameter on of .rolling method\n",
    "# on = WINDOW_ON\n",
    "# string; For a DataFrame, a column label or Index level on which to calculate \n",
    "# the rolling window, rather than the DataFrame’s index. Provided integer column is \n",
    "# ignored and excluded from result since an integer index is not used to calculate \n",
    "# the rolling window.\n",
    "\n",
    "ROW_ACCROSS = 'rows'\n",
    "# ROW_ACCROSS = 'rows'. Alternatively, ROW_ACCROSS = 'columns'\n",
    "# manipulate the parameter axis of .rolling method:\n",
    "# if ROW_ACCROSS = 'rows', axis = 0; if ROW_ACCROSS = 'columns', axis = 1.\n",
    "# If axis = 0 or 'index', roll across the rows.\n",
    "# If 1 or 'columns', roll across the columns.\n",
    "\n",
    "HOW_TO_CLOSE_WINDOW = None\n",
    "# Manipulate parameter closed of .rolling method\n",
    "# closed = HOW_TO_CLOSE_WINDOW\n",
    "# String: If 'right', the first point in the window is excluded from calculations.\n",
    "# If 'left', the last point in the window is excluded from calculations.\n",
    "# If 'both', the no points in the window are excluded from calculations.\n",
    "# If 'neither', the first and last points in the window are excluded from calculations.\n",
    "# Default None ('right').\n",
    "\n",
    "DROP_MISSING_VALUES = True\n",
    "# DROP_MISSING_VALUES = True will remove all missing values created by the methods (all\n",
    "# rows containing missing values). \n",
    "# If DROP_MISSING_VALUES = False, the positions containing NAs will be kept.\n",
    "\n",
    "\n",
    "# Dataframe object containing the rolling window statistics saved as rolling_window_df\n",
    "# Simply modify this object on the left of equality:\n",
    "rolling_window_df = df_rolling_window_stats (df = DATASET, window_size = WINDOW_SIZE, window_statistics = WINDOW_STATISTICS, min_periods_required = MIN_PERIODS_REQUIRED, window_center = WINDOW_CENTER, window_type = WINDOW_TYPE, window_on = WINDOW_ON, row_accross = ROW_ACCROSS, how_to_close_window = HOW_TO_CLOSE_WINDOW, drop_missing_values = DROP_MISSING_VALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "048a8de8-25f1-4dbf-b5c5-1dffd2a6b8b5",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Decomposing seasonality and trend of the time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ffd68137-d8ef-48f7-a9e1-e677aaec0434",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "RESPONSE_COLUMN_TO_ANALYZE = \"response_column_to_analyze\"\n",
    "#response_column_to_analyze: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed.\n",
    "# e.g. response_column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "# WARNING: This must be the response variable\n",
    "COLUMN_WITH_TIMESTAMPS = None\n",
    "#column_with_timestamps: string (inside quotes), \n",
    "# containing the name of the column containing timestamps.\n",
    "# Keep it as None if you want to set the index as the time.\n",
    "# e.g. response_column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "DECOMPOSITION_MODE = \"additive\"\n",
    "# decomposition_mode = \"additive\" - manipulate the parameter 'model' \n",
    "# from seasonal_decompose.\n",
    "# model = decomposition_mode\n",
    "# Alternatively, set decomposition_mode = \"multiplicative\" \n",
    "# for decomposing as a multiplicative time series.\n",
    "MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = 100\n",
    "# MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = integer (minimum value is 2) representing\n",
    "# the total of cycles or periods that may be present on time series. The function will loop through\n",
    "# 2 to MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST to find the number that minimizes the sum of\n",
    "# modules (absolute values) of the residues.\n",
    "# e.g. MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST = 4 will test 2, 3 and 4 cycles on the time series.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'seasonal_decomposition.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "# Dataframe object containing the series resultant from the decomposition (seasonality and trend)\n",
    "# returned as seasonal_decompose_df.\n",
    "# Simply modify these objects on the left of equality:\n",
    "seasonal_decompose_df = seasonal_decomposition (df = DATASET, response_column_to_analyze = RESPONSE_COLUMN_TO_ANALYZE, column_with_timestamps = COLUMN_WITH_TIMESTAMPS, decomposition_mode = DECOMPOSITION_MODE, maximum_number_of_cycles_or_periods_to_test = MAXIMUM_NUMBER_OF_CYCLES_OR_PERIODS_TO_TEST, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d77902c1-5470-4014-ba73-89b2d49de2ef",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "42bd507c-9b7b-4b93-896c-99ab09c54213"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "67a778b5-07ea-402d-9a1b-193ee83d2988",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "47b5eba8-ff78-4d82-bd0c-d267faa16803"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5a6a152d-b805-451b-a284-e52b60618d07",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "63e569bb-9b49-4908-bf9b-542c8d35ebe6"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "13e579c6-8e53-4fca-9d11-8b663c739c62",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c8964a42-0bcd-4ea4-b24c-1a146aac6d67"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7306a69b-2290-4202-a660-6b9ac8601995",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_lambda' for deep learning tensorflow models containing \n",
    "# lambda layers. Such models are compressed as tar.gz.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2ceaabd7-3ca8-4ffd-9a6d-d416c22d37b8"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "4749d8ef-8c0e-40b0-bb36-357f68a31732",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6bfa029f-382c-4cc0-8f24-3a09b816daa1"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2294f7b6-eb11-4af0-8d21-0d44d7b1b385"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e12f93ac-bb7c-4944-a749-69f9d8652ae9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1c5a347e-ce11-40e5-a14c-7fdfbe768f5a"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8408e08e-0847-420d-9059-b9469fb44945",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "19f36aca-0e5b-49f3-b7c2-5fea9a27c6cf"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "77fd0a55-4ddc-44e5-8566-5766657efd6c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a1111491-8122-46e7-8a17-4d08e3c48a8a"
   },
   "source": [
    "# **Lag Diagnosis - Autocorrelation (ACF) and Partial Autocorrelation Function (PACF) Plots - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f390ec52-5aac-4033-bffe-97bb7d438c8d",
    "id": "ugt39n26-e7Y"
   },
   "source": [
    "## **Autocorrelation Function (ACF)**\n",
    "The autocorrelation function (ACF) is a statistical technique that we can use to **identify how correlated the values in a time series are with each other**. \n",
    "- The ACF plots the correlation coefficient against the **lag, which is measured in terms of a number of periods or units**. \n",
    "- A lag corresponds to a certain point in time after which we observe the first value in the time series.\n",
    "- The correlation coefficient can range from -1 (a perfect negative relationship) to +1 (a perfect positive relationship). \n",
    "- **A coefficient of 0 means that there is no relationship between the variables.** Also, most often, it is measured either by Pearson’s correlation coefficient or by Spearman’s rank correlation coefficient.\n",
    "\n",
    "It’s most often used to analyze sequences of numbers from random processes, such as economic or scientific measurements. It can also be used to detect systematic patterns in correlated data sets such as securities prices or climate measurements.\n",
    "\n",
    "Blue regions on an ACF plot are the error bands: **anything within these regions is not statistically significant.** \n",
    "- It means that correlation values outside of this area are very likely a correlation and not a statistical fluke. \n",
    "- The confidence interval is set to 95% by default.\n",
    "\n",
    "Notice that **for a lag zero, ACF is always equal to one, which makes sense because the signal is always perfectly correlated with itself.**\n",
    "\n",
    "To summarize, **autocorrelation is the correlation between a time series (signal) and a delayed version of itself, while the ACF plots the correlation coefficient against the lag, and it’s a visual representation of autocorrelation.**\n",
    "## **Partial Autocorrelation Function (PACF)**\n",
    "Partial autocorrelation is a statistical measure that captures the correlation between two variables after controlling for the effects of other variables. For example, if we are regressing a signal S at lag t (St) with the same signal at lags t-1, t-2 and t-3 (St-1, St-2, St-3), **the partial correlation between St and St-3 is the amount of correlation between St and St-3 that is not explained by their mutual correlations with St-1 and St-2.**\n",
    "- The way of finding PACF between St and St-3 is to use a regression model:\n",
    "\n",
    "`St = ∅1St-1 + ∅2St-2 + ∅3St-3 + ϵ`\n",
    "   \n",
    "where  ∅1, ∅2 and ∅3 are coefficients and ϵ is error. From the regression formula above, the PACF value between St and St-3 is the coefficient ∅3.\n",
    "- This coefficient will give us direct effect of time-series St-3 to the time-series St because the effects of St-2 and St-1 are already captured by ∅1 and ∅2.\n",
    "\n",
    "To summarize, **a partial autocorrelation function captures a “direct” correlation between time series and a lagged version of itself**.\n",
    "## **Stationarity**\n",
    "When it comes to time series forecasting, the stationarity of a time series is one of the most important conditions that the majority of algorithms require. \n",
    "\n",
    "**Briefly, a time-series St is stationary (weak stationarity) if these conditions are met**:\n",
    " \n",
    "1.\tSt has a constant mean.\n",
    "2.\tSt has a constant standard deviation.\n",
    "3.\tThere is no seasonality in St. If St has a repeating pattern within a year, then it has seasonality.\n",
    "\n",
    "We can check the stationarity of the signal visually (approximation) or using some statistical hypothesis for a more precise answer. For that purpose, we can mention two tests:\n",
    "•\tAugmented Dickey-Fuller Test (ADF) with the null hypothesis that the signal is non-stationary.\n",
    "•\tKwiatkowski-Phillips-Schmidt-Shin Test (KPSS) with the null hypothesis that the signal is stationary.\n",
    "\n",
    "If signal St is non-stationary, we can convert them into stationary signal Tt by differencing:\n",
    "\n",
    "`Tt = St - St-1`\n",
    "\n",
    "or calculating percent of change:\n",
    "\n",
    "`Tt = (St - St-1)/(St-1)`\n",
    " \n",
    "Notwithstanding these transformations, signal Tt won’t always be stationary. It is rare but can happen. In that case, if Tt stays non-stationary, we can apply the same transformation to signal Tt. AFC and PACF change as the signal goes through transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fdc34be9-3b80-4bdd-b7ae-638618f3ed53"
   },
   "source": [
    "# **Autoregressive Moving Average (ARMA) and ARIMA Models - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "942442db-f11a-422a-9d35-4aae0ac2c17c",
    "id": "VV5T4ik2-e7a"
   },
   "source": [
    "## **Autoregressive Moving Average (ARMA) Model**\n",
    "The ARMA(p, q) model is a time series forecasting technique used in economics, statistics, and signal processing to characterize relationships between variables. \n",
    "\n",
    "This model can predict future values based on past values and has two parameters, **p and q, which respectively define the order of the autoregressive part (AR) and moving average part (MA).**\n",
    "- **Both AR and MA models require the stationarity of the signal.** \n",
    "- Usually, using non-stationary time series in regression models can lead to a high R-squared value and statistically significant regression coefficients. These results are very likely misleading or spurious.\n",
    "- That is because there is probably no real relationship between them and the only common thing is that they are growing (or decreasing) over time.\n",
    "\n",
    "## **Autoregressive Model (AR)**\n",
    "The autoregressive (AR) model is a statistical model that **expresses the dependence of one variable on an earlier time period. It’s a model where signal St depends only on its own past values.** \n",
    "- For example, AR(3) is a model that **depends on 3 of its past values and can be written as**:\n",
    "\n",
    "`St = β0 + β1St-1 + β2St-2 + β3St-3 + ϵt`\n",
    "\n",
    "where β0, β1, β2¸ β3 are coefficients and ϵt is error. \n",
    "\n",
    "**We can select the order p for AR(p) model based on significant spikes from the PACF plot. One more indication of the AR process is that the ACF plot decays more slowly.**\n",
    "- For instance, we can conclude from the example above (generated PACF graphic for the studied data) that the PACF plot has significant spikes at lags 2 and 3 because of the significant PACF value. \n",
    "- In contrast, for everything within the blue band, we do not have evidence that it is different from zero. Also, we could try other p values of lag that are outside of the blue belt. \n",
    "\n",
    "To conclude, **the number of bars outside the blue boundary of the PACF plot tell us the order of the AR model.**\n",
    "\n",
    "## **Moving Average (MA)**\n",
    "**The MA(q) model calculates its forecast value by taking a weighted average of past errors.** It has the ability to capture trends and patterns in time series data. \n",
    "- For example, MA(3) for a signal St can be formulated as\n",
    "\n",
    "`St = μ + ϵt + γ1ϵt-1 + γ2ϵt-2 + γ3ϵt-3`\n",
    "\n",
    "where μ is the mean of a series, γ1, γ2¸ γ3 are coefficients and ϵt, ϵt-1, ϵt-2, ϵt-3 are **errors that have a normal distribution with mean zero and standard deviation one (sometimes called white noise).**\n",
    "\n",
    "In contrast to the AR model, we can **select the order q for model MA(q) from ACF if this plot has a sharp cut-off after lag q. One more indication of the MA process is that the PACF plot decays more slowly.**\n",
    "- Similarly to selecting p for the AR(p) model, in order to select the appropriate q order for the MA model, **we need to analyze all spikes higher than the blue area, but now in the ACF plot.** \n",
    "- In that sense, from the image above (studied data), we can try using q = 1, 2, 3, 4, or or q = 5.\n",
    "\n",
    "The image below shows an interesting example where we could try q = 3, or q = 6, since these are the bars above the error region:\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "## **ARMA Definition**\n",
    "ARMA(p, q) is a combination of AR(p) and MA(q) models. For example, ARMA(3,3) of signal the St can be formulated as\n",
    "\n",
    "`St = β0 + β1St-1 + β2St-2 + β3St-3 + ϵt + γ1ϵt-1 + γ2ϵt-2 + γ3ϵt-3`\n",
    "\n",
    "where β, γ are coefficients and ϵ error. We already described the way of choosing order p and q in the section for AR and MA models.\n",
    "\n",
    "## Since our search space is not big, **usually values p and q are not higher than 10**.\n",
    "- We will only test values up to 10.\n",
    "\n",
    "Notice that, for spikes in the limiar region of the 95% confidence interval, it is also interesting to analyze their decay rate: **when the inclination of the vertical values (decay rate of the autocorrelation) suddenly decreases, we should stop considering the influence of the lags** (i.e., we stop counting the lags when the derivative suddenly changes).\n",
    "- For instance: there may be an abrupt change of derivative from lag = 6 in the ACF plot: the values of the ACF becomes approximately constant, and a baseline is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "027d3bc2-805b-4a3b-9756-8d985337380e",
    "id": "zwRRnm3r-e7b"
   },
   "source": [
    "## p = spikes on PACF plot that are outside the error (blue region).\n",
    "- For instance, if there are spikes in both lag = 1 and lag = 2, then p = 2, or p = 1.\n",
    "    \n",
    "## In its turn, q represents the lags where spikes of ACF plot that are outside blue region.\n",
    "- For instance, if all spikes until lag = 6 are outside the blue region, then q = 1, 2, 3, 4, 5, 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "98361aac-fced-4189-a5f6-0836b43495fd",
    "id": "IPR1JSTX-e7b"
   },
   "source": [
    "## **Interpretation - Background**\n",
    "\n",
    "To find the best combination (p,q), we can apply a popular technique for hyperparameter optimization called grid search. Grid search is simply an exhaustive search through a manually specified subset of the hyperparameter space of a learning algorithm. \n",
    "- Basically, it means that this method will try each combination of p and q from the specified subset that we provided.\n",
    "\n",
    "Also, in order to find the best combination of p and q, we need to have some **objective function that will measure model performance on a validation set.**  \n",
    "- Usually, we can use **AIC and BIC** for that purpose. \n",
    "\n",
    "#### **The lower the values of these criteria, the better the model is.**\n",
    "\n",
    "### **Akaike Information Criteria (AIC)**\n",
    "**AIC stands for Akaike Information Criteria, and it is a statistical measure that we can use to compare different models for their relative quality.** \n",
    "- It measures the quality of the model in terms of its goodness-of-fit to the data, its simplicity, and how much it relies on the tuning parameters. The formula for AIC is:\n",
    "\n",
    "`AIC = 2k - 2l`\n",
    " \n",
    "where l is a log-likelihood, and k is a number of parameters. For example, the AR(p) model has p+1 parameters.\n",
    "- AIC prefers a **higher log-likelihood** that indicates how strong the model is in fitting the data, and a **simpler model in terms of parameters.**\n",
    "\n",
    "### **Bayesian Information Criteria (BIC)**\n",
    "In addition to AIC, the BIC (Bayesian Information Criteria) uses one more indicator **n that defines the number of samples used for fitting.** The formula for BIC is:\n",
    "\n",
    "`BIC = klog(n) - 2l`\n",
    " \n",
    "### **Cross-Validation for Time-Series**\n",
    "Finally, since we are dealing with time series, we would need to utilize appropriate validation techniques for parameter tuning. This is important because we want to simulate the real-time behavior of the data flow. \n",
    "- For instance, it would not be correct to use a data sample xi to predict data sample xj, if xj comes before xi by time. That is because in real life we cannot use information from the future to predict data in real-time.\n",
    "- Thus, one popular validation technique used for tuning time-series-based machine learning models is cross-validation for time-series. The goal is to see which hyperparameters of the model give the best results in terms of our selected measurement metric on the training data. Then, we use that model for future predictions.\n",
    "For example, if our data consist of five time-points, we can make a train-test split as:\n",
    "•\tTraining [1], Test [2]\n",
    "•\tTraining [1, 2], Test [3]\n",
    "•\tTraining [1, 2, 3], Test [4]\n",
    "•\tTraining [1, 2, 3, 4], Test [5]\n",
    "Of course, one time-point might not be enough as the starting training set, but instead of one, we can start with n starting points and follow the same logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4720aa4f-7bb5-4924-b0d2-718ef3282d76"
   },
   "source": [
    "# **ARIMA Models - Time Series Analysis Handbook - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a8857100-b9a8-4903-af7c-8b9d75283b03",
    "id": "HCYfXdqS-e7c"
   },
   "source": [
    "ARIMA models are built given the following key aspects:\n",
    "\n",
    "**AR**: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.  \n",
    "**I**: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.  \n",
    "**MA**: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "\n",
    "Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used:  \n",
    "**p**: The number of lag observations included in the model, also called the lag order (_deals with **window** of $X\\_t$_)  \n",
    "**d**: The number of times that the raw observations are differenced, also called the degree of differencing (_deals with order of **differencing** of $X\\_t$_)  \n",
    "**q**: The size of the moving average window, also called the order of moving average (_deals with **residuals**_)\n",
    "\n",
    "Given this, the general case of _**ARIMA(p,d,q)**_ can be written as:\n",
    "\n",
    "\\\\begin{equation} X\\_{t} = \\\\alpha _{1}X_{t-1} + \\\\dots + \\\\alpha _{p}X_{t-p} + \\\\varepsilon _{t}+\\\\theta_{1}\\\\varepsilon \\_{t-1}+\\\\cdots +\\\\theta \\_{q}\\\\varepsilon \\_{t-q} \\\\end{equation}\n",
    "\n",
    "Or in words :\n",
    "\n",
    "**Predicted $X\\_t$** = Constant + Linear combination of **Lags of $X$ (up to $p$ lags)** + Linear Combination of **Lagged forecast errors (up to q lags)**. Provided that the time-series is already **differenced (up to d terms)** to ensure stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "95c2a67a-01f4-4a7c-a850-87473c03e891",
    "id": "NnIgbSgo-e7c"
   },
   "source": [
    "#### Finding the order differencing _d_\n",
    "\n",
    "As stated before, ARIMA models are assumed to be stationary. Implementing differencing may induce stationarity for various time series. The quickest way to determine _d_ for our models is to difference and simply run ADF to check for stationarity. We can also look at the PACF and ACF to see if our time series is stationary after _d_ differencing.\n",
    "\n",
    "To illustrate, let's take a look at the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "11267fec-6e47-40e0-ada7-fb16e3ea4179",
    "id": "3sCflFs8-e7c"
   },
   "source": [
    "\n",
    "Initial eyeballing shows that there is a trend for this time series and is non-stationary.  Checking the ADF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d5285bb0-27a6-4b7b-81c4-73bcc26d82f9",
    "id": "MCOYvyu0-e7d"
   },
   "source": [
    "```\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df.value.dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ef7fd801-99dd-4103-9da9-a858b66984c0",
    "id": "rtsySZGS-e7d"
   },
   "source": [
    "```\n",
    "ADF Statistic: -2.464240\n",
    "p-value: 0.124419\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1211d6ed-9038-4ae4-a0bb-9ae89b5df956",
    "id": "q1ZIKSHk-e7d"
   },
   "source": [
    "The null hypothesis of the ADF test is that the time series is non-stationary. So, if the p-value of the test is less than the significance level (0.05) then you reject the null hypothesis and infer that the time series is indeed stationary. For our example, we fail to reject the null hypothesis.\n",
    "\n",
    "Next we difference our time series and check the results of the ADF test. We will also look at the ACF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cbfc4adc-37a3-4620-a0ac-2da92cc5a127",
    "id": "y4CdOohQ-e7d"
   },
   "source": [
    "```\n",
    "plt.rcParams.update({'figure.figsize':(15,8), 'figure.dpi':120})\n",
    "\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(3, 2, sharex=True)\n",
    "axes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(df.value, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(df.value.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(df.value.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(df.value.diff().diff().dropna(), ax=axes[2, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('ADF Statistic for 1st Order Differencing')\n",
    "result = adfuller(df.value.diff().dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "print('\\n ADF Statistic for 2nd Order Differencing')\n",
    "result = adfuller(df.value.diff().diff().dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cace83fc-9e57-42bc-ba6a-a6a162ed1bec",
    "id": "7jzDO0-Y_nZx"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0521cec2-f559-4745-bfce-17a18d036249",
    "id": "bqDmnDbH-e7e"
   },
   "source": [
    "```\n",
    "ADF Statistic for 1st Order Differencing\n",
    "ADF Statistic: -2.722238\n",
    "p-value: 0.070268\n",
    "Critical Values:\n",
    "    1%: -3.500\n",
    "    5%: -2.892\n",
    "    10%: -2.583\n",
    "\n",
    " ADF Statistic for 2nd Order Differencing\n",
    "ADF Statistic: -9.929762\n",
    "p-value: 0.000000\n",
    "Critical Values:\n",
    "    1%: -3.500\n",
    "    5%: -2.892\n",
    "    10%: -2.583\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c747e86e-70c4-4e36-befe-8ab17bf5898a",
    "id": "ZwXyGP_4-e7e"
   },
   "source": [
    "Given the results of our ACF and ADF, **we can see that our time series reaches stationarity after two orders of differencing. However, the ACF of the 2nd order differencing goes into the negative zone fairly quick. This can indicates that the series might have been over differenced. It is now up to us if we want consider the first or second order differencing for our ARIMA models.**\n",
    "\n",
    "**\\- Then, d = 1 or d = 2.**\n",
    "\n",
    "\\- If d = 0, we are using ARMA model.\n",
    "\n",
    "## **The value of \"d\" corresponds to the total of differentiations for making the process stationary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "21a1c80e-2309-4637-b881-dd1af5889b83"
   },
   "source": [
    "# **Time series seasonal decomposition - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9243e4ef-85df-4958-839a-88c4bd232697"
   },
   "source": [
    "- decomposition_mode = \"additive\" - manipulate the parameter 'model' from seasonal_decompose.\n",
    "- Alternatively, set decomposition_mode = \"multiplicative\" for decomposing as a multiplicative time series.\n",
    "\n",
    "### **'additive' model**\n",
    "- An additive model suggests that the components are added together as: \n",
    "\n",
    "`y(t) = Level + Trend + Seasonality + Noise`\n",
    "\n",
    "- An additive model is linear where changes over time are consistently made by the same amount. \n",
    "- A linear trend is a straight line. \n",
    "- A linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).\n",
    "    \n",
    "### **'multiplicative' model** \n",
    "- A multiplicative model suggests that the components are multiplied together as:\n",
    "\n",
    "`y(t) = Level * Trend * Seasonality * Noise`\n",
    "\n",
    "- A multiplicative model is nonlinear, such as quadratic or exponential. Changes increase or decrease over time.\n",
    "- A nonlinear trend is a curved line. \n",
    "- A non-linear seasonality has an increasing or decreasing frequency and/or amplitude over time.\n",
    "\n",
    "Check:\n",
    "https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/#:~:text=The%20statsmodels%20library%20provides%20an%20implementation%20of%20the,careful%20to%20be%20critical%20when%20interpreting%20the%20result"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
