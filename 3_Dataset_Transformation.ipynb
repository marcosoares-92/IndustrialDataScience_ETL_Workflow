{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Transformation**\n",
    "## Transforming the dataset and reverse transforms: log-transform; exponential transform; Box-Cox transform; One-Hot Encoding; feature scaling; importing or exporting models and dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _ETL Workflow Notebook 3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '/', s3_bucket_name = None, s3_obj_key_preffix = None):\n",
    "    \n",
    "    import sagemaker\n",
    "    # sagemaker is AWS SageMaker Python SDK\n",
    "    from sagemaker.session import Session\n",
    "    from google.colab import drive\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = '/copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_key_preffix = None. Keep it None or as an empty string (s3_obj_key_preffix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_preffix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead, to start the S3 client. boto3 is AWS S3 Python SDK:\n",
    "        \n",
    "        # import boto3\n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # ... [here, use the same following code until line new_session = Session()]\n",
    "        # [keep the line for session start. Substitute the line with the .download_data\n",
    "        # method by the following line:]\n",
    "        # s3_client.download_file(s3_bucket_name, s3_file_name_with_extension, path_to_store_imported_s3_bucket)\n",
    "        \n",
    "        # Check if the whole bucket will be downloaded (s3_obj_key_preffix = None):\n",
    "        if (s3_obj_key_preffix is None):\n",
    "            \n",
    "            s3_obj_key_preffix = ''\n",
    "        \n",
    "        # If the path to store is None, also import the bucket to the root path:\n",
    "        if (path_to_store_imported_s3_bucket is None):\n",
    "            \n",
    "            path_to_store_imported_s3_bucket = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start a new sagemaker session:\n",
    "\n",
    "            print(\"Starting a SageMaker session to be associated with the S3 bucket.\")\n",
    "\n",
    "            new_session = Session()\n",
    "            # Check sagemaker session class documentation:\n",
    "            # https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "            session.download_data(path = path_to_store_imported_s3_bucket, bucket = s3_bucket_name, key_prefix = s3_obj_key_preffix)\n",
    "\n",
    "            print(f\"S3 bucket contents successfully imported to path \\'{path_to_store_imported_s3_bucket}\\'.\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe (file_directory_path, file_name_with_extension, has_header = True, txt_csv_col_sep = \"comma\", sheet_to_load = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "    # txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # file_name_with_extension - (string, in quotes): input the name of the file with the extension\n",
    "    # e.g. file_name_with_extension = \"file.xlsx\", or, file_name_with_extension = \"file.csv\"\n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\" for columns separated by comma (\",\")\n",
    "    # txt_csv_col_sep = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        \n",
    "        if (has_header == True):\n",
    "            \n",
    "            if (txt_csv_col_sep == \"comma\"):\n",
    "            \n",
    "                dataset = pd.read_csv(file_path)\n",
    "            \n",
    "            elif (txt_csv_col_sep == \"whitespace\"):\n",
    "                \n",
    "                dataset = pd.read_csv(file_path, delim_whitespace = True)\n",
    "            \n",
    "            else:\n",
    "                print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "        \n",
    "        else:\n",
    "            # has_header == False\n",
    "              \n",
    "            if (txt_csv_col_sep == \"comma\"):\n",
    "            \n",
    "                dataset = pd.read_csv(file_path, header = None)\n",
    "            \n",
    "            elif (txt_csv_col_sep == \"whitespace\"):\n",
    "                \n",
    "                dataset = pd.read_csv(file_path, delim_whitespace = True, header = None)\n",
    "            \n",
    "            else:\n",
    "                print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "        \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\")\n",
    "            \n",
    "        if (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None)\n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None)\n",
    "    \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of the dataset:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for dataframe general characterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_gen_charac (df):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"Dataframe 10 first rows:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_shape  = df.shape\n",
    "    print(f\"Dataframe shape (rows, columns) = {df_shape}.\")\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_columns_list = df.columns\n",
    "    print(f\"Dataframe columns list = {df_columns_list}.\")\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_dtypes = df.dtypes\n",
    "    print(\"Dataframe variables types:\")\n",
    "    print(df_dtypes)\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_general_statistics = df.describe()\n",
    "    print(\"Dataframe general statistics (numerical variables):\")\n",
    "    print(df_general_statistics)\n",
    "    \n",
    "    #Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_missing_values = df.isna().sum()\n",
    "    print(\"Total of missing values for each feature:\")\n",
    "    print(df_missing_values)\n",
    "    \n",
    "    return df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for obtaining the correlation plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Pandas method dataset.corr() calculates the Pearson's correlation coefficients R.\n",
    "- Pearson's correlation coefficients R go from -1 to 1.\n",
    "- These coefficients are R, not R².\n",
    "\n",
    "#### To obtain the coefficients R², we raise the results to the 2nd power, i.e., we calculate (dataset.corr())**2\n",
    "- R² goes from 0 to 1, where 1 represents the perfect correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot (df, show_masked_plot = True, responses_to_return_corr = None, set_returned_limit = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    #show_masked_plot = True - keep as True if you want to see a cleaned version of the plot\n",
    "    # where a mask is applied.\n",
    "    \n",
    "    #responses_to_return_corr - keep as None to return the full correlation tensor.\n",
    "    # If you want to display the correlations for a particular group of features, input them\n",
    "    # as a list, even if this list contains a single element. Examples:\n",
    "    # responses_to_return_corr = ['response1'] for a single response\n",
    "    # responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "    # responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "    # of a column of the dataset that represents a response variable.\n",
    "    # WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "    # of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "    \n",
    "    # set_returned_limit = None - This variable will only present effects in case you have\n",
    "    # provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "    # to return all of the correlation coefficients; or, alternatively, \n",
    "    # provide an integer number to limit the total of coefficients returned. \n",
    "    # e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "    correlation_matrix = df.corr(method='pearson')\n",
    "    \n",
    "    if (show_masked_plot == False):\n",
    "        #Show standard plot\n",
    "        \n",
    "        plt.figure()\n",
    "        sns.heatmap((correlation_matrix)**2, annot=True, fmt=\".2f\")\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(24,8));\n",
    "\n",
    "    #Oncee the pandas method .corr() calculates R, we raised it to the second power \n",
    "    # to obtain R². R² goes from zero to 1, where 1 represents the perfect correlation.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Show masked (cleaner) plot instead of the standard one\n",
    "        \n",
    "        plt.figure()\n",
    "        # Mask for the upper triangle\n",
    "        mask = np.zeros_like((correlation_matrix)**2)\n",
    "\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        # Generate a custom diverging colormap\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        # Heatmap with mask and correct aspect ratio\n",
    "        sns.heatmap(((correlation_matrix)**2), mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(24,8));\n",
    "\n",
    "        #Again, the method dataset.corr() calculates R within the variables of dataset.\n",
    "        #To calculate R², we simply raise it to the second power: (dataset.corr()**2)\n",
    "    \n",
    "    #Sort the values of correlation_matrix in Descending order:\n",
    "    \n",
    "    if (responses_to_return_corr is not None):\n",
    "        \n",
    "        #Select only the desired responses, by passing the list responses_to_return_corr\n",
    "        # as parameter for column filtering:\n",
    "        correlation_matrix = correlation_matrix[responses_to_return_corr]\n",
    "        \n",
    "        #Now sort the values according to the responses, by passing the list\n",
    "        # responses_to_return_corr as the parameter\n",
    "        correlation_matrix = correlation_matrix.sort_values(by = responses_to_return_corr, ascending = False)\n",
    "        \n",
    "        # If a limit of coefficients was determined, apply it:\n",
    "        if (set_returned_limit is not None):\n",
    "                \n",
    "                correlation_matrix = correlation_matrix.head(set_returned_limit)\n",
    "                #Pandas .head(X) method returns the first X rows of the dataframe.\n",
    "                # Here, it returns the defined limit of coefficients, set_returned_limit.\n",
    "                # The default .head() is X = 5.\n",
    "        \n",
    "        print(correlation_matrix)\n",
    "    \n",
    "    print(\"ATTENTION: The correlation plots show the linear correlations R², which go from 0 (none correlation) to 1 (perfect correlation). Obviously, the main diagonal always shows R² = 1, since the data is perfectly correlated to itself.\")\n",
    "    print(\"The returned correlation matrix, on the other hand, presents the linear coefficients of correlation R, not R². R values go from -1 (perfect negative correlation) to 1 (perfect positive correlation).\")\n",
    "    print(\"None of these coefficients take non-linear relations and the presence of a multiple linear correlation in account. For these cases, it is necessary to calculate R² adjusted, which takes in account the presence of multiple preditors and non-linearities.\")\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for obtaining scatter plots and simple linear regressions**\n",
    "- Here, only a single prediction variable will be analyzed by once.\n",
    "- The plots will show Y x X, where X is the predict or independent variable.\n",
    "- The linear regressions will be of the type Y = aX + b, i.e., a single pair (X, Y) analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3c9ebe5e-117c-42bb-ae7d-38c206463e1f",
    "tags": [
     "CELL_8"
    ]
   },
   "source": [
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "azdata_cell_guid": "a1b4ca0e-b55f-4fce-9e94-8ae0a0ac171a",
    "tags": [
     "CELL_10"
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_plot_lin_reg (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 0, y_axis_rotation = 0, show_linear_reg = True, grid = True, add_splines_lines = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110): \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (show_linear_reg == True):\n",
    "        estatisticas = []\n",
    "        estatisticas.append(\"Linear Fitting:\")\n",
    "        estatisticas.append(\"R² = \")\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (x1 is None):\n",
    "        \n",
    "        if not (lab1 is None):\n",
    "            label_1 = lab1\n",
    "        else:\n",
    "            label_1 = \"Y1 x X1\"\n",
    "        \n",
    "        #falsa negativa: passa se os valores nao forem nulos\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = 'o', color='blue', label=label_1)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta1 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg1 = stats.linregress(x1, y1)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg1 = x1.sort_values()\n",
    "            x_reg1 = x_reg1.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg1 = (reg1).intercept + (reg1).slope*(x_reg1)\n",
    "            #gerar string da reta\n",
    "            string1 = \"y = %.2f*x + %.2f\" %((reg1).slope, (reg1).intercept)\n",
    "            reta1.append(string1)\n",
    "            #calcular R2\n",
    "            r_sq1 = (reg1).rvalue**2\n",
    "            reta1.append(r_sq1)\n",
    "            print(\"\\nLinear Fitting 1: \" + string1)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 1) = %.4f\" %(r_sq1))\n",
    "            string_label1 = 'Linear regression: ' + label_1\n",
    "            ax.plot(x_reg1, y_reg1,  linestyle='-', marker='', color='blue', label = string_label1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #roda apenas se ambos estiverem presentes\n",
    "                \n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = 'o', color='red', label=label_2)    \n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta2 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg2 = stats.linregress(x2, y2)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg2 = x2.sort_values()\n",
    "            x_reg2 = x_reg2.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg2 = (reg2).intercept + (reg2).slope*(x_reg2)\n",
    "            #gerar string da reta\n",
    "            string2 = \"y = %.2f*x + %.2f\" %((reg2).slope, (reg2).intercept)\n",
    "            reta2.append(string2)\n",
    "            #calcular R2\n",
    "            r_sq2 = (reg2).rvalue**2\n",
    "            reta2.append(r_sq2)\n",
    "            print(\"\\nLinear Fitting 2: \" + string2)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 2) = %.4f\" %(r_sq2))\n",
    "            string_label2 = 'Linear regression: ' + label_2\n",
    "            ax.plot(x_reg2, y_reg2,  linestyle='-', marker='', color='red', label = string_label2)\n",
    "        \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = 'o', color='green', label=label_3)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta3 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg3 = stats.linregress(x3, y3)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg3 = x3.sort_values()\n",
    "            x_reg3 = x_reg3.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg3 = (reg3).intercept + (reg3).slope*(x_reg3)\n",
    "            #gerar string da reta\n",
    "            string3 = \"y = %.2f*x + %.2f\" %((reg3).slope, (reg3).intercept)\n",
    "            reta3.append(string3)\n",
    "            #calcular R2\n",
    "            r_sq3 = (reg3).rvalue**2\n",
    "            reta3.append(r_sq3)\n",
    "            print(\"\\nLinear Fitting 3: \" + string3)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 3) = %.4f\" %(r_sq3))\n",
    "            string_label3 = 'Linear regression: ' + label_3\n",
    "            ax.plot(x_reg3, y_reg3,  linestyle='-', marker='', color='green', label = string_label3)\n",
    "        \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = 'o', color='black', label=label_4)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta4 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg4 = stats.linregress(x4, y4)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg4 = x4.sort_values()\n",
    "            x_reg4 = x_reg4.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg4 = (reg4).intercept + (reg4).slope*(x_reg4)\n",
    "            #gerar string da reta\n",
    "            string4 = \"y = %.2f*x + %.2f\" %((reg4).slope, (reg4).intercept)\n",
    "            reta4.append(string4)\n",
    "            #calcular R2\n",
    "            r_sq4 = (reg4).rvalue**2\n",
    "            reta4.append(r_sq4)\n",
    "            print(\"\\nLinear Fitting 4: \" + string4)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 4) = %.4f\" %(r_sq4))\n",
    "            string_label4 = 'Linear regression: ' + label_4\n",
    "            ax.plot(x_reg4, y_reg4,  linestyle='-', marker='', color='black', label = string_label4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = 'o', color='magenta', label=label_5)\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            reta5 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg5 = stats.linregress(x5, y5)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg5 = x5.sort_values()\n",
    "            x_reg5 = x_reg5.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg5 = (reg5).intercept + (reg5).slope*(x_reg5)\n",
    "            #gerar string da reta\n",
    "            string5 = \"y = %.2f*x + %.2f\" %((reg5).slope, (reg5).intercept)\n",
    "            reta5.append(string5)\n",
    "            #calcular R2\n",
    "            r_sq5 = (reg5).rvalue**2\n",
    "            reta5.append(r_sq5)\n",
    "            print(\"\\nLinear Fitting 5: \" + string5)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 5) = %.4f\" %(r_sq5))\n",
    "            string_label5 = 'Linear regression: ' + label_5\n",
    "            ax.plot(x_reg5, y_reg5,  linestyle='-', marker='', color='magenta', label = string_label5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = 'o', color='yellow', label=label_6)\n",
    "            \n",
    "        if (show_linear_reg == True):\n",
    "            reta6 = []\n",
    "            #Calculo da regressao linear:\n",
    "            reg6 = stats.linregress(x6, y6)\n",
    "            #organizar os X, para que os splines formem a reta correta\n",
    "            x_reg6 = x6.sort_values()\n",
    "            x_reg6 = x_reg6.reset_index(drop = True)\n",
    "            #curva obtida:\n",
    "            y_reg6 = (reg6).intercept + (reg6).slope*(x_reg6)\n",
    "            #gerar string da reta\n",
    "            string6 = \"y = %.2f*x + %.2f\" %((reg6).slope, (reg6).intercept)\n",
    "            reta6.append(string6)\n",
    "            #calcular R2\n",
    "            r_sq6 = (reg6).rvalue**2\n",
    "            reta6.append(r_sq6)\n",
    "            print(\"\\nLinear Fitting 6: \" + string6)\n",
    "            #concatena as strings\n",
    "            print(\"\\nR² (fitting 6) = %.4f\" %(r_sq6))\n",
    "            string_label6 = 'Linear regression: ' + label_6\n",
    "            ax.plot(x_reg6, y_reg6,  linestyle='-', marker='', color='yellow', label = string_label6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #titulo do grafico\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #Titulo do eixo X\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Titulo do eixo Y\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"scatter_plot_lin_reg\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    if (show_linear_reg == True):\n",
    "        \n",
    "        if not (x2 is None):\n",
    "            \n",
    "            if not (x3 is None):\n",
    "                \n",
    "                if not (x4 is None):\n",
    "                    \n",
    "                    if not (x5 is None):\n",
    "                        \n",
    "                        if not (x6 is None):\n",
    "                            #todos estao presentes\n",
    "                            d = {'Statistics': estatisticas,\n",
    "                                 label_1: reta1,\n",
    "                                 label_2: reta2,\n",
    "                                 label_3: reta3,\n",
    "                                 label_4: reta4,\n",
    "                                 label_5: reta5,\n",
    "                                 label_6: reta6}\n",
    "                        \n",
    "                        else:\n",
    "                            #apenas 5 estão presentes:\n",
    "                            d = {'Statistics': estatisticas,\n",
    "                                 label_1: reta1,\n",
    "                                 label_2: reta2,\n",
    "                                 label_3: reta3,\n",
    "                                 label_4: reta4,\n",
    "                                 label_5: reta5}\n",
    "                    \n",
    "                    else:\n",
    "                        #apenas 4 estão presentes:\n",
    "                        d = {'Statistics': estatisticas,\n",
    "                             label_1: reta1,\n",
    "                             label_2: reta2,\n",
    "                             label_3: reta3,\n",
    "                             label_4: reta4}\n",
    "                \n",
    "                else:\n",
    "                    #apenas 3 estão presentes:\n",
    "                    d = {'Statistics': estatisticas,\n",
    "                         label_1: reta1,\n",
    "                         label_2: reta2,\n",
    "                         label_3: reta3}\n",
    "            \n",
    "            else:\n",
    "                #apenas 2 estão presentes:\n",
    "                d = {'Statistics': estatisticas,\n",
    "                     label_1: reta1,\n",
    "                     label_2: reta2}\n",
    "        \n",
    "        else:\n",
    "            #apenas 1 esta presente:\n",
    "            d = {'Statistics': estatisticas,\n",
    "                 label_1: reta1}\n",
    "        \n",
    "        lin_reg_summary = pd.DataFrame(data = d)\n",
    "        \n",
    "        return lin_reg_summary     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for time series visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "source": [
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "azdata_cell_guid": "ef571494-3eb2-4dcc-9ebb-00c1fd6e9aad",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def time_series_vis (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (add_scatter_dots == True):\n",
    "        marker_value = 'o'\n",
    "    else:\n",
    "        marker_value = ''\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (lab1 is None):\n",
    "        \n",
    "        label_1 = lab1\n",
    "    \n",
    "    else:\n",
    "        label_1 = \"Y1\"\n",
    "\n",
    "    if not (x1 is None):\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = marker_value, color='blue', label=label_1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #runs only when both are present\n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = marker_value, color='red', label=label_2)\n",
    "    \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = marker_value, color='green', label=label_3)\n",
    "    \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = marker_value, color='black', label=label_4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = marker_value, color='magenta', label=label_5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = marker_value, color='yellow', label=label_6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #graphic's title\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #X-axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Y-axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"time_series_vis\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Functions for histogram visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function `histogram`: ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons.\n",
    "- Function `histogram_alternative`: histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "azdata_cell_guid": "9d2dc092-e26a-4491-a056-644c9412de6a",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def histogram (y, bar_width, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, normal_curve_overlay = True, data_units_label = None, y_title = None, histogram_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ideal bin interval calculated through Montgomery's method. \n",
    "    # Histogram is obtained from this calculated bin size.\n",
    "    # Douglas C. Montgomery (2009). Introduction to Statistical Process Control, \n",
    "    # Sixth Edition, John Wiley & Sons.\n",
    "    \n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "\n",
    "    y = y.reset_index(drop=True)\n",
    "    #faz com que os indices desta serie sejam consecutivos e a partir de zero\n",
    "\n",
    "    #Estatisticas gerais: media (mu) e desvio-padrao (sigma)\n",
    "    mu = y.mean() \n",
    "    sigma = y.std() \n",
    "\n",
    "    #Calculo do bin-size\n",
    "    highest = y.max()\n",
    "    lowest = y.min()\n",
    "    rangehist = highest - lowest\n",
    "    rangehist = abs(rangehist)\n",
    "    #garante que sera um numero positivo\n",
    "    samplesize = y.count() #contagem do total de entradas\n",
    "    ncells = (samplesize)**0.5 #potenciacao: ** - raiz quadrada de samplesize\n",
    "    #resultado da raiz quadrada e sempre positivo\n",
    "    ncells = round(ncells) #numero \"redondo\" mais proximo\n",
    "    ncells = int(ncells) #parte inteira do numero arredondado\n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    binsize = rangehist/ncells\n",
    "    binsize = round(binsize)\n",
    "    binsize = int(binsize) #precisa ser inteiro\n",
    "    \n",
    "    #Construcao da tabela de frequencias\n",
    "\n",
    "    j = 0 #indice da tabela de frequencias\n",
    "    #Este indice e diferente do ordenamento dos valores em ordem crescente\n",
    "    xhist = []\n",
    "    #Lista vazia que contera os x do histograma\n",
    "    yhist = []\n",
    "    #Listas vazia que conteras o y do histograma\n",
    "    hist_labels = []\n",
    "    #Esta lista gravara os limites da barra na forma de strings\n",
    "\n",
    "    pontomediodabarra = lowest + binsize/2 \n",
    "    limitedabarra = lowest + binsize\n",
    "    #ponto medio da barra \n",
    "    #limite da primeira barra do histograma\n",
    "    seriedohist1 = y\n",
    "    seriedohist1 = seriedohist1.sort_values(ascending=True)\n",
    "    #serie com os valores em ordem crescente\n",
    "    seriedohist1 = seriedohist1.reset_index(drop=True)\n",
    "    #garante que a nova serie tenha indices consecutivos, iniciando em zero\n",
    "    i = 0 #linha inicial da serie do histograma em ordem crescente\n",
    "    valcomparado = seriedohist1[i]\n",
    "    #primeiro valor da serie, o mais baixo\n",
    "\n",
    "    while (j <= (ncells-1)):\n",
    "        \n",
    "        #para quando termina o numero de linhas da tabela\n",
    "        xhist.append(pontomediodabarra)\n",
    "        #tempo da tabela de frequencias\n",
    "        cont = 0\n",
    "        #variavel de contagem do histograma\n",
    "        #contagem deve ser reiniciada\n",
    "       \n",
    "        if (i < samplesize):\n",
    "            #2 condicionais para impedir que um termo de indice inexistente\n",
    "            #seja acessado\n",
    "            while (valcomparado <= limitedabarra) and (valcomparado < highest):\n",
    "                #o segundo criterio garante a parada em casos em que os dados sao\n",
    "                #muito proximos\n",
    "                    cont = cont + 1 #adiciona contagem a tabela de frequencias\n",
    "                    i = i + 1\n",
    "                    \n",
    "                    if (i < samplesize): \n",
    "                        valcomparado = seriedohist1[i]\n",
    "        \n",
    "        yhist.append(cont) #valor de ocorrencias contadas\n",
    "        \n",
    "        limite_infdabarra = pontomediodabarra - binsize/2\n",
    "        rotulo = \"%.2f - %.2f\" %(limite_infdabarra, limitedabarra)\n",
    "        #intervalo da tabela de frequencias\n",
    "        #%.2f: 2 casas decimais de aproximação\n",
    "        hist_labels.append(rotulo)\n",
    "        \n",
    "        pontomediodabarra = pontomediodabarra + binsize\n",
    "        #tanto os pontos medios quanto os limites se deslocam do mesmo intervalo\n",
    "        \n",
    "        limitedabarra = limitedabarra + binsize\n",
    "        #proxima barra\n",
    "        \n",
    "        j = j + 1\n",
    "    \n",
    "    #Temos que verificar se o valor maximo foi incluido\n",
    "    #isso porque o processo de aproximacao por numero inteiro pode ter\n",
    "    #arredondado para baixo e excluido o limite superior\n",
    "    #Porem, note que na ultima iteracao o limite superior da barra foi \n",
    "    #somado de binsize, mas como j ja e maior que ncells-1, o loop parou\n",
    "    \n",
    "    #assim, o limitedabarra nesse momento e o limite da barra que seria\n",
    "    #construida em seguida, nao da ultima barra da tabela de frequencias\n",
    "    #isso pode fazer com que esta barra ja seja maior que o highest\n",
    "    \n",
    "    #note porem que nao aumentamos o valor do limite inferior da barra\n",
    "    #por isso, basta vermos se ele mais o binsize sao menores que o valor mais alto\n",
    "        \n",
    "    while ((limite_infdabarra+binsize) < highest):\n",
    "        \n",
    "        #vamos criar novas linhas ate que o ponto mais alto do histograma\n",
    "        #tenha sido contado\n",
    "        ncells = ncells + 1 #adiciona uma linha a tabela de frequencias\n",
    "        xhist.append(pontomediodabarra)\n",
    "        \n",
    "        cont = 0 #variavel de contagem do histograma\n",
    "        \n",
    "        while (valcomparado <= limitedabarra):\n",
    "                cont = cont + 1 #adiciona contagem a tabela de frequencias\n",
    "                i = i + 1\n",
    "                if (i < samplesize):\n",
    "                    valcomparado = seriedohist1[i]\n",
    "                    #apenas se i ainda nao e maior que o total de dados\n",
    "                \n",
    "                else: \n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        #parar o loop se i atingiu um tamanho maior que a quantidade \n",
    "        #de dados.Temos que ter este cuidado porque estamos acrescentando\n",
    "        #mais linhas a tabela de frequencias para corrigir a aproximacao\n",
    "        #de ncells por um numero inteiro\n",
    "        \n",
    "        yhist.append(cont) #valor de ocorrencias contadas\n",
    "        \n",
    "        limite_infdabarra = pontomediodabarra - binsize/2\n",
    "        rotulo = \"%.2f - %.2f\" %(limite_infdabarra, limitedabarra)\n",
    "        #intervalo da tabela de frequencias - 2 casas decimais\n",
    "        hist_labels.append(rotulo)\n",
    "        \n",
    "        pontomediodabarra = pontomediodabarra + binsize\n",
    "        #tanto os pontos medios quanto os limites se deslocam do mesmo intervalo\n",
    "        \n",
    "        limitedabarra = limitedabarra + binsize\n",
    "        #proxima barra\n",
    "        \n",
    "    estatisticas_col1 = []\n",
    "    #contera as descricoes das colunas da tabela de estatisticas gerais\n",
    "    estatisticas_col2 = []\n",
    "    #contera os valores da tabela de estatisticas gerais\n",
    "    \n",
    "    estatisticas_col1.append(\"Count of data evaluated\")\n",
    "    estatisticas_col2.append(samplesize)\n",
    "    estatisticas_col1.append(\"Average (mu)\")\n",
    "    estatisticas_col2.append(mu)\n",
    "    estatisticas_col1.append(\"Standard deviation (sigma)\")\n",
    "    estatisticas_col2.append(sigma)\n",
    "    estatisticas_col1.append(\"Highest value\")\n",
    "    estatisticas_col2.append(highest)\n",
    "    estatisticas_col1.append(\"Lowest value\")\n",
    "    estatisticas_col2.append(lowest)\n",
    "    estatisticas_col1.append(\"Data range (maximum value - lowest value)\")\n",
    "    estatisticas_col2.append(rangehist)\n",
    "    estatisticas_col1.append(\"Bin size (bar width)\")\n",
    "    estatisticas_col2.append(binsize)\n",
    "    estatisticas_col1.append(\"Total rows in frequency table\")\n",
    "    estatisticas_col2.append(ncells)\n",
    "    #como o comando append grava linha a linha em sequencia, garantimos\n",
    "    #a correspondencia das colunas\n",
    "    #Assim como em qualquer string, incluindo de rotulos de graficos\n",
    "    #os \\n sao lidos como quebra de linha\n",
    "    \n",
    "    d1 = {\"General Statistics\": estatisticas_col1, \"Calculated Value\": estatisticas_col2}\n",
    "    #dicionario das duas series, para criar o dataframe com as descricoes\n",
    "    estatisticas_gerais = pd.DataFrame(data = d1)\n",
    "    \n",
    "    #Casos os títulos estejam presentes (valor nao e None):\n",
    "    #vamos utiliza-los\n",
    "    #Caso contrario, vamos criar nomenclaturas genericas para o histograma\n",
    "    \n",
    "    eixo_y = \"Counting/Frequency\"\n",
    "    \n",
    "    if not (data_units_label is None):\n",
    "        xlabel = data_units_label\n",
    "    \n",
    "    else:\n",
    "        xlabel = \"Frequency\\n table data\"\n",
    "    \n",
    "    if not (y_title is None):\n",
    "        eixo_x = y_title\n",
    "        #lembre-se que no histograma, os dados originais vao pro eixo X\n",
    "        #O eixo Y vira o eixo da contagem/frequencia daqueles dados\n",
    "    \n",
    "    else:\n",
    "        eixo_x = \"X: Mean value of the interval\"\n",
    "    \n",
    "    if not (histogram_title is None):\n",
    "        string1 = \"- $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        main_label = histogram_title + string1\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        main_label = \"Data Histogram - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        #os simbolos $\\ $ substituem o simbolo pela letra grega\n",
    "    \n",
    "    d2 = {\"Considered interval\": hist_labels, eixo_x: xhist, eixo_y: yhist}\n",
    "    #dicionario que compoe a tabela de frequencias\n",
    "    tab_frequencias = pd.DataFrame(data = d2)\n",
    "    #cria a tabela de frequencias como um dataframe de saida\n",
    "    \n",
    "    #parametros da normal ja calculados:\n",
    "    #mu e sigma\n",
    "    #numero de bins: ncells\n",
    "    #limites de especificacao: lsl,usl - target\n",
    "    \n",
    "    #valor maximo do histograma\n",
    "    max_hist = max(yhist)\n",
    "    #seleciona o valor maximo da serie, para ajustar a curva normal\n",
    "    #isso porque a normal é criada com valores entre 0 e 1\n",
    "    #multiplicando ela por max_hist, fazemos ela se adequar a altura do histograma\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        #construir a normal ajustada/esperada\n",
    "        #vamos criar pontos ao redor da media mu - 4sigma ate mu + 4sigma, \n",
    "        #de modo a garantir a quase totalidade da curva normal. \n",
    "        #O incremento será de 0.10 sigma a cada iteracao\n",
    "        x_inf = mu -(4)*sigma\n",
    "        x_sup = mu + 4*sigma\n",
    "        x_inc = (0.10)*sigma\n",
    "        \n",
    "        x_normal_adj = []\n",
    "        y_normal_adj = []\n",
    "        \n",
    "        x_adj = x_inf\n",
    "        y_adj = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (x_adj - mu))**2))\n",
    "        x_normal_adj.append(x_adj)\n",
    "        y_normal_adj.append(y_adj)\n",
    "        \n",
    "        while(x_adj < x_sup): \n",
    "            \n",
    "            x_adj = x_adj + x_inc\n",
    "            y_adj = ((1 / (np.sqrt(2 * np.pi) * sigma)) *np.exp(-0.5 * (1 / sigma * (x_adj - mu))**2))\n",
    "            x_normal_adj.append(x_adj)\n",
    "            y_normal_adj.append(y_adj)\n",
    "        \n",
    "        #vamos ajustar a altura da curva ao histograma. Para isso, precisamos\n",
    "        #calcular quantas vezes o ponto mais alto do histograma é maior que o ponto\n",
    "        #mais alto da normal (chamaremos essa relação de fator). A seguir,\n",
    "        #multiplicamos cada elemento da normal por este mesmo fator\n",
    "        max_normal = max(y_normal_adj) \n",
    "        #maximo da normal ajustada, numero entre 0 e 1\n",
    "        \n",
    "        fator = (max_hist)/(max_normal)\n",
    "        size_normal = len(y_normal_adj) #quantidade de dados criados\n",
    "        \n",
    "        i = 0\n",
    "        while (i < size_normal):\n",
    "            y_normal_adj[i] = (y_normal_adj[i])*(fator)\n",
    "            i = i + 1\n",
    "    \n",
    "    #Fazer o grafico\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "    \n",
    "        #adicionar a normal\n",
    "        ax.plot(x_normal_adj, y_normal_adj, color = 'black', label = 'Adjusted/expected\\n normal curve')\n",
    "    \n",
    "    ax.set_xlabel(eixo_x)\n",
    "    ax.set_ylabel(eixo_y)\n",
    "    ax.set_title(main_label)\n",
    "    ax.set_xticks(xhist)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(grid)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"General statistics:\\n\")\n",
    "    print(estatisticas_gerais)\n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Frequency table:\\n\")\n",
    "    print(tab_frequencias)\n",
    "\n",
    "    return estatisticas_gerais, tab_frequencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "azdata_cell_guid": "3fe31404-56d2-4590-87a3-58d4ead49706",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def histogram_alternative (y, total_of_bins, bar_width, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, data_units_label = None, y_title = None, histogram_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    #Calculo do bin size - largura do histograma:\n",
    "    #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "    #2: Calcular rangehist = highest - lowest\n",
    "    #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "    #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "    #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "    #5: Calcular binsize = rangehist/(ncells)\n",
    "    #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "    #isso porque a largura do histograma tem que ser um numero positivo\n",
    "    \n",
    "    #this variable here is to simply guarantee the compatibility of the function,\n",
    "    # with no extensive code modifications. It has no real effect.\n",
    "    normal_curve_overlay = True\n",
    "    \n",
    "\n",
    "    y = y.reset_index(drop=True)\n",
    "    #faz com que os indices desta serie sejam consecutivos e a partir de zero\n",
    "\n",
    "    #Estatisticas gerais: media (mu) e desvio-padrao (sigma)\n",
    "    mu = y.mean() \n",
    "    sigma = y.std() \n",
    "\n",
    "    #Calculo do bin-size\n",
    "    highest = y.max()\n",
    "    lowest = y.min()\n",
    "    rangehist = highest - lowest\n",
    "    rangehist = abs(rangehist)\n",
    "    #garante que sera um numero positivo\n",
    "    samplesize = y.count() #contagem do total de entradas\n",
    "    ncells = (samplesize)**0.5 #potenciacao: ** - raiz quadrada de samplesize\n",
    "    #resultado da raiz quadrada e sempre positivo\n",
    "    ncells = round(ncells) #numero \"redondo\" mais proximo\n",
    "    ncells = int(ncells) #parte inteira do numero arredondado\n",
    "    #ncells = numero de linhas da tabela de frequencias\n",
    "    binsize = rangehist/ncells\n",
    "    binsize = round(binsize)\n",
    "    binsize = int(binsize) #precisa ser inteiro\n",
    "    \n",
    "    #Construcao da tabela de frequencias\n",
    "\n",
    "    j = 0 #indice da tabela de frequencias\n",
    "    #Este indice e diferente do ordenamento dos valores em ordem crescente\n",
    "    xhist = []\n",
    "    #Lista vazia que contera os x do histograma\n",
    "    yhist = []\n",
    "    #Listas vazia que conteras o y do histograma\n",
    "    hist_labels = []\n",
    "    #Esta lista gravara os limites da barra na forma de strings\n",
    "\n",
    "    pontomediodabarra = lowest + binsize/2 \n",
    "    limitedabarra = lowest + binsize\n",
    "    #ponto medio da barra \n",
    "    #limite da primeira barra do histograma\n",
    "    seriedohist1 = y\n",
    "    seriedohist1 = seriedohist1.sort_values(ascending=True)\n",
    "    #serie com os valores em ordem crescente\n",
    "    seriedohist1 = seriedohist1.reset_index(drop=True)\n",
    "    #garante que a nova serie tenha indices consecutivos, iniciando em zero\n",
    "    i = 0 #linha inicial da serie do histograma em ordem crescente\n",
    "    valcomparado = seriedohist1[i]\n",
    "    #primeiro valor da serie, o mais baixo\n",
    "        \n",
    "    estatisticas_col1 = []\n",
    "    #contera as descricoes das colunas da tabela de estatisticas gerais\n",
    "    estatisticas_col2 = []\n",
    "    #contera os valores da tabela de estatisticas gerais\n",
    "    \n",
    "    estatisticas_col1.append(\"Count of data evaluated\")\n",
    "    estatisticas_col2.append(samplesize)\n",
    "    estatisticas_col1.append(\"Average (mu)\")\n",
    "    estatisticas_col2.append(mu)\n",
    "    estatisticas_col1.append(\"Standard deviation (sigma)\")\n",
    "    estatisticas_col2.append(sigma)\n",
    "    estatisticas_col1.append(\"Highest value\")\n",
    "    estatisticas_col2.append(highest)\n",
    "    estatisticas_col1.append(\"Lowest value\")\n",
    "    estatisticas_col2.append(lowest)\n",
    "    estatisticas_col1.append(\"Data range (maximum value - lowest value)\")\n",
    "    estatisticas_col2.append(rangehist)\n",
    "    estatisticas_col1.append(\"Bin size (bar width)\")\n",
    "    estatisticas_col2.append(binsize)\n",
    "    estatisticas_col1.append(\"Total rows in frequency table\")\n",
    "    estatisticas_col2.append(ncells)\n",
    "    #como o comando append grava linha a linha em sequencia, garantimos\n",
    "    #a correspondencia das colunas\n",
    "    #Assim como em qualquer string, incluindo de rotulos de graficos\n",
    "    #os \\n sao lidos como quebra de linha\n",
    "    \n",
    "    d1 = {\"General Statistics\": estatisticas_col1, \"Calculated Value\": estatisticas_col2}\n",
    "    #dicionario das duas series, para criar o dataframe com as descricoes\n",
    "    estatisticas_gerais = pd.DataFrame(data = d1)\n",
    "    \n",
    "    #Casos os títulos estejam presentes (valor nao e None):\n",
    "    #vamos utiliza-los\n",
    "    #Caso contrario, vamos criar nomenclaturas genericas para o histograma\n",
    "    \n",
    "    eixo_y = \"Counting/Frequency\"\n",
    "    \n",
    "    if not (data_units_label is None):\n",
    "        xlabel = data_units_label\n",
    "    \n",
    "    else:\n",
    "        xlabel = \"Frequency\\n table data\"\n",
    "    \n",
    "    if not (y_title is None):\n",
    "        eixo_x = y_title\n",
    "        #lembre-se que no histograma, os dados originais vao pro eixo X\n",
    "        #O eixo Y vira o eixo da contagem/frequencia daqueles dados\n",
    "    \n",
    "    else:\n",
    "        eixo_x = \"X: Mean value of the interval\"\n",
    "    \n",
    "    if not (histogram_title is None):\n",
    "        string1 = \"- $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        main_label = histogram_title + string1\n",
    "        #concatena a string do titulo a string com a media e desvio-padrao\n",
    "        #%.2f: o numero entre %. e f indica a quantidade de casas decimais da \n",
    "        #variavel float f. No caso, arredondamos para 2 casas\n",
    "        #NAO SE ESQUECA DO PONTO: ele que indicara que sera arredondado o \n",
    "        #numero de casas\n",
    "    \n",
    "    else:\n",
    "        main_label = \"Data Histogram - $\\mu = %.2f$, $\\sigma = %.2f$\" %(mu, sigma)\n",
    "        #os simbolos $\\ $ substituem o simbolo pela letra grega\n",
    "    \n",
    "    d2 = {\"Considered interval\": hist_labels, eixo_x: xhist, eixo_y: yhist}\n",
    "    #dicionario que compoe a tabela de frequencias\n",
    "    tab_frequencias = pd.DataFrame(data = d2)\n",
    "    #cria a tabela de frequencias como um dataframe de saida\n",
    "   \n",
    "    #parametros da normal ja calculados:\n",
    "    #mu e sigma\n",
    "    #numero de bins: ncells\n",
    "    #limites de especificacao: lsl,usl - target\n",
    "    \n",
    "    #valor maximo do histograma\n",
    "    #max_hist = max(yhist)\n",
    "    #seleciona o valor maximo da serie, para ajustar a curva normal\n",
    "    #isso porque a normal é criada com valores entre 0 e 1\n",
    "    #multiplicando ela por max_hist, fazemos ela se adequar a altura do histograma\n",
    "    \n",
    "    \n",
    "    #Fazer o grafico\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    ax.hist(y, bins = total_of_bins, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    #ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    ax.set_xlabel(eixo_x)\n",
    "    ax.set_ylabel(eixo_y)\n",
    "    ax.set_title(main_label)\n",
    "    #ax.set_xticks(xhist)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(grid)\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram_alternative\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"General statistics:\\n\")\n",
    "    print(estatisticas_gerais)\n",
    "    # This function is supposed to be used in cases where the differences between data\n",
    "    # is very small. In such cases, there will be no trust values calculated for the \n",
    "    # frequency table. Therefore, we omit it here, but it can be accessed from the\n",
    "    # returned dataframe.\n",
    "\n",
    "    return estatisticas_gerais, tab_frequencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for testing data normality and visualizing probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "CELL_10"
    ]
   },
   "outputs": [],
   "source": [
    "def test_data_normality (y, alpha = 0.10, show_probability_plot = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "    # Check https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    # Check https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    # WARNING: The statistical tests require at least 20 samples\n",
    "    \n",
    "    # Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "    # Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "    # results.\n",
    "    \n",
    "    # y = series of data that will be tested.\n",
    "    # y = dataset['Y']\n",
    "    \n",
    "    #Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "    # variable Y (normal distribution tested). \n",
    "    # Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "    \n",
    "    lista1 = []\n",
    "    #esta lista sera a primeira coluna, com as descrições das demais\n",
    "    lista1.append(\"p-value: probability that data is described by the normal distribution.\")\n",
    "    lista1.append(\"Probability of being described by the normal distribution (\\%).\")\n",
    "    lista1.append(\"alpha\")\n",
    "    lista1.append(\"Criterium: is not described by normal if p < alpha = %.3f.\" %(alpha))\n",
    "    #%.3f apresenta f com 3 casas decimais\n",
    "    #%f se refere a uma variavel float\n",
    "    #informa ao usuario o valor definido para a rejeição\n",
    "    lista1.append(\"Are data described by the normal?\")\n",
    "    #Note que o comando append adiciona os elementos em sequencia, linha a linha\n",
    "    #nao se especifica indice, pois ja esta subentendido que esta na proxima\n",
    "    #linha\n",
    "    \n",
    "    #Scipy.stats’ normality test\n",
    "    # It is based on D’Agostino and Pearson’s test that combines \n",
    "    # skew and kurtosis to produce an omnibus test of normality.\n",
    "    _, scipystats_test_pval = stats.normaltest(y)\n",
    "    # The underscore indicates an output to be ignored, which is s^2 + k^2, \n",
    "    # where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.\n",
    "    # https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    #create list with only the p-val\n",
    "    p_scipy = []\n",
    "    p_scipy.append(scipystats_test_pval) #p-value\n",
    "    p_scipy.append(100*scipystats_test_pval) #p in percent\n",
    "    p_scipy.append(alpha)\n",
    "    \n",
    "    if (scipystats_test_pval < alpha):\n",
    "        p_scipy.append(\"p = %.3f < %.3f\" %(scipystats_test_pval, alpha))\n",
    "        p_scipy.append(\"Data not described by normal.\")\n",
    "    else:\n",
    "        p_scipy.append(\"p = %.3f >= %.3f\" %(scipystats_test_pval, alpha))\n",
    "        p_scipy.append(\"Data described by normal.\")    \n",
    "    \n",
    "    #Lilliefors’ test\n",
    "    lilliefors_test = diagnostic.kstest_normal(y, dist='norm', pvalmethod='table')\n",
    "    #Return: linha 1: ksstat: float\n",
    "    #Kolmogorov-Smirnov test statistic with estimated mean and variance.\n",
    "    #Linha 2: p-value:float\n",
    "    #If the pvalue is lower than some threshold, e.g. 0.10, then we can reject the Null hypothesis that the sample comes from a normal distribution.\n",
    "    \n",
    "    #criar lista apenas com o p-valor\n",
    "    p_lillie = []\n",
    "    p_lillie.append(lilliefors_test[1]) #p-valor\n",
    "    p_lillie.append(100*lilliefors_test[1]) #p em porcentagem\n",
    "    p_lillie.append(alpha)\n",
    "    \n",
    "    if (lilliefors_test[1] < alpha):\n",
    "        p_lillie.append(\"p = %.3f < %.3f\" %(lilliefors_test[1], alpha))\n",
    "        p_lillie.append(\"Data not described by normal.\")\n",
    "    else:\n",
    "        p_lillie.append(\"p = %.3f >= %.3f\" %(lilliefors_test[1], alpha))\n",
    "        p_lillie.append(\"Data described by normal.\")\n",
    "        \n",
    "    \n",
    "    #Anderson-Darling\n",
    "    ad_test = diagnostic.normal_ad(y, axis=0)\n",
    "    #Return: Linha 1: ad2: float\n",
    "    #Anderson Darling test statistic.\n",
    "    #Linha 2: p-val: float\n",
    "    #The p-value for hypothesis that the data comes from a normal distribution with unknown mean and variance.\n",
    "    \n",
    "    #criar lista apenas com o p-valor\n",
    "    p_ad = []\n",
    "    p_ad.append(ad_test[1]) #p-valor\n",
    "    p_ad.append(100*ad_test[1]) #p em porcentagem\n",
    "    p_ad.append(alpha)\n",
    "    \n",
    "    if (ad_test[1] < alpha):\n",
    "        p_ad.append(\"p = %.3f < %.3f\" %(ad_test[1], alpha))\n",
    "        p_ad.append(\"Data not described by normal.\")\n",
    "    else:\n",
    "        p_ad.append(\"p = %.3f >= %.3f\" %(ad_test[1], alpha))\n",
    "        p_ad.append(\"Data described by normal.\")\n",
    "    \n",
    "    #NOTA: o comando %f apresenta a variavel float com todas as casas\n",
    "    #decimais possiveis. Se desejamos um numero certo de casas decimais\n",
    "    #acrescentamos esse numero a frente. Exemplos: %.1f: 1 casa decimal\n",
    "    # %.2f: 2 casas; %.3f: 3 casas decimais, %.4f: 4 casas\n",
    "    \n",
    "    data_normality_dict = {'Parameters and Interpretation': lista1, 'D’Agostino and Pearson normality test': , 'Lilliefors Test': p_lillie, 'Anderson-Darling Test': p_ad}\n",
    "    \n",
    "    #dicionario dos valores obtidos\n",
    "    data_normality_res = pd.DataFrame(data = data_normality_dict)\n",
    "    #dataframe de saída\n",
    "    \n",
    "    print(\"Check data normality results:\\n\")\n",
    "    print(data_normality_res)\n",
    "    print(\"\\n\") #line break\n",
    "    \n",
    "    # Calculate data skewness and kurtosis\n",
    "    \n",
    "    # Skewness\n",
    "    data_skew = stats.skew(y)\n",
    "    # skewness = 0 : normally distributed.\n",
    "    # skewness > 0 : more weight in the left tail of the distribution.\n",
    "    # skewness < 0 : more weight in the right tail of the distribution.\n",
    "    # https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "    \n",
    "    # Kurtosis\n",
    "    data_kurtosis = stats.kurtosis(y, fisher = True)\n",
    "    # scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function \n",
    "    # calculates the kurtosis (Fisher or Pearson) of a data set. It is the the fourth \n",
    "    # central moment divided by the square of the variance. \n",
    "    # It is a measure of the “tailedness” i.e. descriptor of shape of probability \n",
    "    # distribution of a real-valued random variable. \n",
    "    # In simple terms, one can say it is a measure of how heavy tail is compared \n",
    "    # to a normal distribution.\n",
    "    # fisher parameter: fisher : Bool; Fisher’s definition is used (normal 0.0) if True; \n",
    "    # else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "    # https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "    print(\"A normal distribution should present no skewness (distribution distortion); and no kurtosis (long-tail).\")\n",
    "    print(f\"For the data analyzed: skewness = {data_skew}; kurtosis = {data_kurtosis}\")\n",
    "    \n",
    "    if (data_skew < 0):\n",
    "        \n",
    "        print(f\"Skewness {data_skew} < 0: more weight in the left tail of the distribution.\")\n",
    "    \n",
    "    elif (data_skew > 0):\n",
    "        \n",
    "        print(f\"Skewness {data_skew} > 0: more weight in the right tail of the distribution.\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Skewness {data_skew} = 0: no distortion of the distribution.\")\n",
    "    \n",
    "    \n",
    "    print(f\"Data kurtosis = {data_kurtosis}\")\n",
    "    \n",
    "    if (data_kurtosis == 0):\n",
    "        \n",
    "        print(\"Data kurtosis = 0. No long-tail effects detected.\")\n",
    "    \n",
    "    #Calculate the mode of the distribution:\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "    data_mode = stats.mode(y, axis = None)[0]\n",
    "    # returns an array of arrays. The first array is called mode=array and contains the mode.\n",
    "    # Axis: Default is 0. If None, compute over the whole array.\n",
    "    # we set axis = None to compute the general mode.\n",
    "    \n",
    "    #Create general statistics dictionary:\n",
    "    general_statistics_dict = {\n",
    "        \n",
    "        \"Count_of_analyzed_values\": len(y)\n",
    "        \"Data_mean\": np.mean(y),\n",
    "        \"Data_mean_ignoring_missing_values\": np.nanmean(y),\n",
    "        \"Data_variance\": np.var(y),\n",
    "        \"Data_variance_ignoring_missing_values\": np.nanvar(y),\n",
    "        \"Data_standard_deviation\": np.std(y),\n",
    "        \"Data_standard_deviation_ignoring_missing_values\": np.nanstd(y),\n",
    "        \"Data_skewness\": data_skew,\n",
    "        \"Data_kurtosis\": data_kurtosis,\n",
    "        \"Data_mode\": data_mode\n",
    "        \n",
    "    }\n",
    "    \n",
    "    print(\"Skewness and kurtosis successfully returned in the dictionary general_statistics_dict.\\n\")\n",
    "    print(general_statistics_dict)\n",
    "    print(\"/n\")\n",
    "    \n",
    "    if (show_probability_plot == True):\n",
    "        #Obtain the probability plot  \n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.set_title(\"Probability Plot for Normal Distribution\")\n",
    "\n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 70 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)   \n",
    "\n",
    "        res = stats.probplot(y, dist = 'norm', fit = True, plot = ax)\n",
    "        #This function resturns a tuple, so we must store it into res\n",
    "        \n",
    "        #Other distributions to check, see scipy Stats documentation. \n",
    "        # you could test dist=stats.loggamma, where stats was imported from scipy\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "\n",
    "        ax.grid(grid)\n",
    "        ax.legend()\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"probability_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "    \n",
    "    return data_normality_res, general_statistics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for column filtering (selecting) or column renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_filter_rename (df, cols_list, mode = 'filter'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #mode = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "    #mode = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "    \n",
    "    #cols_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{df.columns}\")\n",
    "    \n",
    "    if (mode == 'filter'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        df = df[cols_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\")\n",
    "        \n",
    "    elif (mode == 'rename'):\n",
    "        \n",
    "        #Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(cols_list) == len(df.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\")\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            df.columns = cols_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'filter\\' or \\'rename\\'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for log-transforming the variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One curve derived from the normal is the log-normal.\n",
    "- If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "- A log normal curve resembles a normal, but with skewness (distortion); and kurtosis (long-tail).\n",
    "\n",
    "Applying the log is a methodology for **normalizing the variables**: the sample space gets shrinkled after the transformation, making the data more adequate for being processed by Machine Learning algorithms.\n",
    "- Preferentially apply the transformation to the whole dataset, so that all variables will be of same order of magnitude.\n",
    "- Obviously, it is not necessary for variables ranging from -100 to 100 in numerical value, where most outputs from the log transformation are.\n",
    "\n",
    "#### **WARNING**: This function will eliminate rows where the selected variables present values lower or equal to zero (condition for the logarithm to be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform (df, subset = None, create_new_columns = True, new_columns_suffix = \"_log\"):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #### WARNING: This function will eliminate rows where the selected variables present \n",
    "    #### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "    \n",
    "    # subset = None\n",
    "    # Set subset = None to transform the whole dataset. Alternatively, pass a list with \n",
    "    # columns names for the transformation to be applied. For instance:\n",
    "    # subset = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "    # as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "    # Declaring the full list of columns is equivalent to setting subset = None.\n",
    "    \n",
    "    # create_new_columns = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into new\n",
    "    # columns. Or set create_new_columns = False to overwrite the existing columns\n",
    "    \n",
    "    #new_columns_suffix = \"_log\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "    # \"collumn1_log\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # Check if a subset was defined. If so, make columns_list = subset \n",
    "    if not (subset is None):\n",
    "        \n",
    "        columns_list = subset\n",
    "    \n",
    "    else:\n",
    "        #There is no declared subset. Then, make columns_list equals to the list of\n",
    "        # columns of the dataframe.\n",
    "        columns_list = subset.columns\n",
    "    \n",
    "    #Loop through each column:\n",
    "    for column in columns_list:\n",
    "        #access each element in the list column_list. The element is named 'column'.\n",
    "        \n",
    "        #boolean filter to check if the entry is higher than zero, condition for the log\n",
    "        # to be applied\n",
    "        boolean_filter = (df[column] > 0)\n",
    "        #This filter is equals True only for the rows where the column is higher than zero.\n",
    "        \n",
    "        #Apply the boolean filter to the dataframe, removing the entries where the column\n",
    "        # cannot be log transformed.\n",
    "        # The boolean_filter selects only the rows for which the filter values are True.\n",
    "        df = df[boolean_filter]\n",
    "        \n",
    "        #Check if a new column will be created, or if the original column should be\n",
    "        # substituted.\n",
    "        if (create_new_columns == True):\n",
    "            # Create a new column.\n",
    "            \n",
    "            # The new column name will be set as column + new_columns_suffix\n",
    "            new_column_name = column + new_columns_suffix\n",
    "        \n",
    "        else:\n",
    "            # Overwrite the existing column. Simply set new_column_name as the value 'column'\n",
    "            new_column_name = column\n",
    "        \n",
    "        # Calculate the column value as the log transform of the original series (column)\n",
    "        df[new_column_name] = np.log(df[column])\n",
    "    \n",
    "    print(\"The columns were successfully log-transformed. Check the 10 first rows of the new dataset:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# One curve derived from the normal is the log-normal.\n",
    "# If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "# A log normal curve resembles a normal, but with skewness (distortion); \n",
    "# and kurtosis (long-tail).\n",
    "\n",
    "# Applying the log is a methodology for normalizing the variables: \n",
    "# the sample space gets shrinkled after the transformation, making the data more \n",
    "# adequate for being processed by Machine Learning algorithms. Preferentially apply \n",
    "# the transformation to the whole dataset, so that all variables will be of same order \n",
    "# of magnitude.\n",
    "# Obviously, it is not necessary for variables ranging from -100 to 100 in numerical \n",
    "# value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for reversing the log-transform - applying the exponential transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_log_transform(df, subset = None, create_new_columns = True, new_columns_suffix = \"_originalScale\"):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #### WARNING: This function will eliminate rows where the selected variables present \n",
    "    #### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "    \n",
    "    # subset = None\n",
    "    # Set subset = None to transform the whole dataset. Alternatively, pass a list with \n",
    "    # columns names for the transformation to be applied. For instance:\n",
    "    # subset = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "    # as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "    # Declaring the full list of columns is equivalent to setting subset = None.\n",
    "    \n",
    "    # create_new_columns = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into new\n",
    "    # columns. Or set create_new_columns = False to overwrite the existing columns\n",
    "    \n",
    "    #new_columns_suffix = \"_log\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_originalScale\", the new column will be named \n",
    "    # as \"collumn1_originalScale\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # Check if a subset was defined. If so, make columns_list = subset \n",
    "    if not (subset is None):\n",
    "        \n",
    "        columns_list = subset\n",
    "    \n",
    "    else:\n",
    "        #There is no declared subset. Then, make columns_list equals to the list of\n",
    "        # columns of the dataframe.\n",
    "        columns_list = subset.columns\n",
    "    \n",
    "    #Loop through each column:\n",
    "    for column in columns_list:\n",
    "        #access each element in the list column_list. The element is named 'column'.\n",
    "        \n",
    "        # The exponential transformation can be applied to zero and negative values,\n",
    "        # so we remove the boolean filter.\n",
    "        \n",
    "        #Check if a new column will be created, or if the original column should be\n",
    "        # substituted.\n",
    "        if (create_new_columns == True):\n",
    "            # Create a new column.\n",
    "            \n",
    "            # The new column name will be set as column + new_columns_suffix\n",
    "            new_column_name = column + new_columns_suffix\n",
    "        \n",
    "        else:\n",
    "            # Overwrite the existing column. Simply set new_column_name as the value 'column'\n",
    "            new_column_name = column\n",
    "        \n",
    "        # Calculate the column value as the log transform of the original series (column)\n",
    "        df[new_column_name] = np.exp(df[column])\n",
    "    \n",
    "    print(\"The log_transform was successfully reversed through the exponential transformation. Check the 10 first rows of the new dataset:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for obtaining and applying Box-Cox transform**\n",
    "- Transform data into a series that are represented by the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "CELL_13"
    ]
   },
   "outputs": [],
   "source": [
    "def box_cox_transform (df, column_to_transform, mode = 'calculate_and_apply', lambda_boxcox = None, suffix = '_BoxCoxTransf', specification_lims = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    \n",
    "    # This function will process a single column column_to_transform \n",
    "    # of the dataframe df per call.\n",
    "    \n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "    ## Box-Cox transform is given by:\n",
    "    ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "    ## log(x),                  for lmbda = 0\n",
    "    \n",
    "    # column_to_transform must be a string with the name of the column.\n",
    "    # e.g. column_to_transform = 'column1' to transform a column named as 'column1'\n",
    "    \n",
    "    # mode = 'calculate_and_apply'\n",
    "    # Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "    # transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "    # To 'apply_only', lambda_box must be provided.\n",
    "    \n",
    "    # lambda_boxcox must be a float value. e.g. lamda_boxcox = 1.7\n",
    "    # If you calculated lambda from the function box_cox_transform and saved the\n",
    "    # transformation data summary dictionary as data_sum_dict, simply set:\n",
    "    # lambda_boxcox = data_sum_dict['lambda_boxcox']\n",
    "    # This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "    # contains the lambda. \n",
    "    \n",
    "    # Analogously, spec_lim_dict['Inf_spec_lim_transf'] access\n",
    "    # the inferior specification limit transformed; and spec_lim_dict['Sup_spec_lim_transf'] \n",
    "    # access the superior specification limit transformed.\n",
    "    \n",
    "    # If lambda_boxcox is None, \n",
    "    # the mode will be automatically set as 'calculate_and_apply'.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "    # identified as 'Y_BoxCoxTransf'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "    \n",
    "    #specification_lims = None if there are no specification limits for the variable being\n",
    "    # transformed by the function.\n",
    "    #In case there were originally specification limits for the variable (column) being\n",
    "    # transformed, declare them as a list, array, or tuple of two numbers (float).\n",
    "    # e.g. if the column represents a variable with specifications between 10 to 20 kg, declare\n",
    "    # specification_lims = [10, 20]. If it represents a variable which specifications should\n",
    "    # be betweewn 0 to 12.5 L, declare specification_lims = [0, 12.5]\n",
    "    # Then, the function will return the specifications transformed by the same Box-Cox\n",
    "    # transformation applied to the data. Remember: if data were transformed, so should be\n",
    "    # the specification limits.\n",
    "\n",
    "    y = df[column_to_transform]\n",
    "    \n",
    "    boolean_check1 = (lambda_boxcox is None)\n",
    "    # | is the 'or' operator.\n",
    "    # If boolean_check1 is True, automatically set mode = 'calculate_and_apply'\n",
    "    \n",
    "    if (boolean_check1 == True):\n",
    "        print(\"Invalid value set for \\'lambda_boxcox'\\. Setting mode to \\'calculate_and_apply\\'.\")\n",
    "        mode = 'calculate_and_apply'\n",
    "    \n",
    "    boolean_chek2 = (mode != 'calculate_and_apply') & (mode != 'apply_only')\n",
    "    # & is the 'and' operator. != is the 'is different from' operator.\n",
    "    #Check if neither 'calculate_and_apply' nor 'apply_only' were selected\n",
    "    \n",
    "    if (boolean_check2 == True):\n",
    "        print(\"Invalid value set for \\'mode'\\. Setting mode to \\'calculate_and_apply\\'.\")\n",
    "        mode = 'calculate_and_apply'\n",
    "    \n",
    "    if (mode == 'calculate_and_apply'):\n",
    "        # Calculate lambda_boxcox\n",
    "        lambda_boxcox = stats.boxcox_normmax(y, method='pearsonr')\n",
    "        #calcula o lambda da transformacao box-cox utilizando o metodo da maxima verossimilhanca\n",
    "        #por meio da maximizacao do coeficiente de correlacao de pearson da funcao\n",
    "        #y = boxcox(x), onde boxcox representa a transformacao\n",
    "    \n",
    "    # For other cases, we will apply the lambda_boxcox set as the function parameter.\n",
    "\n",
    "    #Calculo da variavel transformada\n",
    "    y_transform = stats.boxcox(y, lmbda=lambda_boxcox, alpha=None)\n",
    "    #Calculo da transformada\n",
    "    \n",
    "    if not (suffix is None):\n",
    "        #only if a suffix was declared\n",
    "        #concatenate the column name to the suffix\n",
    "        new_col = column_to_transform + suffix\n",
    "    \n",
    "    else:\n",
    "        #concatenate the column name to the standard '_BoxCoxTransf' suffix\n",
    "        new_col = column_to_transform + '_BoxCoxTransf'\n",
    "    \n",
    "    data_transformed_df = df\n",
    "    data_transformed_df[new_col] = y_transform\n",
    "    #dataframe contendo os dados transformados\n",
    "    \n",
    "    print(\"Data successfully transformed. Check the 10 first transformed rows:\\n\")\n",
    "    print(data_transformed_df.head(10))\n",
    "    print(\"\\n\") #line break\n",
    "    \n",
    "    #testes de normalidade da variavel transformada\n",
    "    #Lilliefors’ test\n",
    "    lilliefors_test = diagnostic.kstest_normal(y, dist='norm', pvalmethod='table')\n",
    "    #Return: linha 1: ksstat: float\n",
    "    #Kolmogorov-Smirnov test statistic with estimated mean and variance.\n",
    "    #Linha 2: p-value:float\n",
    "    #If the pvalue is lower than some threshold, e.g. 0.10, then we can reject the Null hypothesis that the sample comes from a normal distribution.\n",
    "    \n",
    "    p_lillie = (lilliefors_test[1])\n",
    "    #apenas o p-valor na lista\n",
    "    \n",
    "    #Anderson-Darling\n",
    "    ad_test = diagnostic.normal_ad(y, axis=0)\n",
    "    #Return: Linha 1: ad2: float\n",
    "    #Anderson Darling test statistic.\n",
    "    #Linha 2: p-val: float\n",
    "    #The p-value for hypothesis that the data comes from a normal distribution with unknown mean and variance.\n",
    "    \n",
    "    p_ad = (ad_test[1])\n",
    "    #apenas o p-valor na lista\n",
    "    \n",
    "    data_sum_dict = {'lambda_boxcox': lambda_boxcox, 'Lilliefors_p_value': p_lillie, 'AndersonDarling_p_value': p_ad}\n",
    "    #dicionario dos p-valores e do lambda\n",
    "    \n",
    "    print(\"Box-Cox Transformation Summary:\\n\")\n",
    "    print(data_sum_dict)\n",
    "    print(\"\\n\") #line break\n",
    "    \n",
    "    if not (specification_lims is None):\n",
    "        #apenas executa este passo quando o limite de especificação for fornecido\n",
    "        \n",
    "        #Convert the list of specifications into a NumPy array:\n",
    "        spec_lim_array = np.array(specification_lims)\n",
    "        \n",
    "        #Apply the Box-Cox transform to this array and store the results in the same array:\n",
    "        spec_lim_array = stats.boxcox(spec_lim_array, lmbda=lambda_boxcox, alpha=None)\n",
    "        \n",
    "        spec_lim_dict = {['Inf_spec_lim_transf', 'Sup_spec_lim_transf']: spec_lim_array}\n",
    "        \n",
    "        print(\"New specification limits successfully obtained:\\n\")\n",
    "        print(spec_lim_dict)\n",
    "    \n",
    "    if not (specification_lims is None):\n",
    "        #Caso haja limites de especificacao, retorna os limites transformados\n",
    "        return data_transformed_df, data_sum_dict, spec_lim_dict\n",
    "    \n",
    "    #caso nao haja limite de especificacao:\n",
    "    else:\n",
    "        return data_transformed_df, data_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "CELL_13"
    ]
   },
   "outputs": [],
   "source": [
    "def reverse_box_cox (df, column_to_transform, lambda_boxcox, suffix = '_ReversedBoxCox'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # This function will process a single column column_to_transform \n",
    "    # of the dataframe df per call.\n",
    "    \n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "    ## Box-Cox transform is given by:\n",
    "    ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "    ## log(x),                  for lmbda = 0\n",
    "    \n",
    "    # column_to_transform must be a string with the name of the column.\n",
    "    # e.g. column_to_transform = 'column1' to transform a column named as 'column1'\n",
    "    \n",
    "    # lambda_boxcox must be a float value. e.g. lamda_boxcox = 1.7\n",
    "    # If you calculated lambda from the function box_cox_transform and saved the\n",
    "    # transformation data summary dictionary as data_sum_dict, simply set:\n",
    "    # lambda_boxcox = data_sum_dict['lambda_boxcox']\n",
    "    # This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "    # contains the lambda. \n",
    "    \n",
    "    # Analogously, spec_lim_dict['Inf_spec_lim_transf'] access\n",
    "    # the inferior specification limit transformed; and spec_lim_dict['Sup_spec_lim_transf'] \n",
    "    # access the superior specification limit transformed.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "    # identified as '_ReversedBoxCox'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "    \n",
    "    y = df[column_to_transform]\n",
    "    \n",
    "    if (lambda_boxcox == 0):\n",
    "        #ytransf = np.log(y), according to Box-Cox definition. Then\n",
    "        #y_retransform = np.exp(y)\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = np.exp(y)\n",
    "    \n",
    "    else:\n",
    "        #apply Box-Cox function:\n",
    "        #y_transf = (y**lmbda - 1) / lmbda. Then,\n",
    "        #y_retransf ** (lmbda) = (y_transf * lmbda) + 1\n",
    "        #y_retransf = ((y_transf * lmbda) + 1) ** (1/lmbda), where ** is the potentiation\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = ((y * lambda_boxcox) + 1) ** (1/lambda_boxcox)\n",
    "    \n",
    "    if not (suffix is None):\n",
    "        #only if a suffix was declared\n",
    "        #concatenate the column name to the suffix\n",
    "        new_col = column_to_transform + suffix\n",
    "    \n",
    "    else:\n",
    "        #concatenate the column name to the standard '_ReversedBoxCox' suffix\n",
    "        new_col = column_to_transform + '_ReversedBoxCox'\n",
    "    \n",
    "    data_retransformed_df = df\n",
    "    data_retransformed_df[new_col] = y_transform\n",
    "    #dataframe contendo os dados transformados\n",
    "    \n",
    "    print(\"Data successfully retransformed. Check the 10 first retransformed rows:\\n\")\n",
    "    print(data_retransformed_df.head(10))\n",
    "    print(\"\\n\") #line break\n",
    " \n",
    "    return data_retransformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for One-Hot Encoding categorical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "df88ce83-988c-46ed-9087-6e21278668ec",
    "id": "2q8SGc6WigDt"
   },
   "source": [
    "- Transform categorical values without notion of order into numerical (binary) features.\n",
    "- Process a single categorical column per function call.\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "- The new columns will be named as the original possible categories.\n",
    "- Each column is a binary variable of the type \"is classified in this category or not\".\n",
    "\n",
    "Therefore, for a category \"A\", a column named \"A\" is created.\n",
    "- If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "- If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dp2axJ_OsfZV"
   },
   "outputs": [],
   "source": [
    "def OneHotEncode_df (df, subset_of_features_to_be_encoded):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    #Start an encoding dictionary empty:\n",
    "    encoding_dict = {}\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df\n",
    "    \n",
    "    #loop through each column of the subset:\n",
    "    for column in subset_of_features_to_be_encoded:\n",
    "        \n",
    "        # Loop through each element (named 'column') of the list of columns to analyze,\n",
    "        # subset_of_features_to_be_encoded\n",
    "        \n",
    "        #We could process the whole subset at once, but it could make us lose information\n",
    "        # about the generated columns\n",
    "        \n",
    "        # set a subset of the dataframe X containing 'column' as the only column:\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X  = df[[column]]\n",
    "        \n",
    "        #Start the OneHotEncoder object:\n",
    "        encoded_X = OneHotEncoder()\n",
    "        \n",
    "        #Fit the object to that column:\n",
    "        encoded_X = encoded_X.fit_transform(X) \n",
    "        \n",
    "        #It will create a scipy sparse matrix full of null values.\n",
    "        #Show encoded categories and store this array. \n",
    "        #It will give the proper columns' names:\n",
    "        encoded_columns = encoded_X.categories_\n",
    "\n",
    "        #encoded_columns is a list containing a single element.\n",
    "        # This element is an array like:\n",
    "        # array(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8'], dtype=object)\n",
    "        # Then, this array is the element of index 0 from the list encoded_columns.\n",
    "        # It is represented as encoded_columns[0]\n",
    "\n",
    "        #Therefore, we actually want the array which is named as encoded_columns[0]\n",
    "        # Each element of this array is the name of one of the encoded columns. In the\n",
    "        # example above, the element 'cat2' would be accessed as encoded_columns[0][1],\n",
    "        # since it is the element of index [1] (second element) from the array \n",
    "        # encoded_columns[0].\n",
    "        \n",
    "        #Update the dictionary to store the original column name as key, and the categories\n",
    "        # array as the value:\n",
    "        encoding_dict.update({column: encoded_columns[0]})\n",
    "\n",
    "        #Create the dense array:\n",
    "        encoded_X = encoded_X.toarray()\n",
    "        #print(\"One-Hot Encoding Matrix:\")\n",
    "        #print(encoded_X)\n",
    "\n",
    "        #Convert it into a dataframe:\n",
    "        encoded_X_df = pd.DataFrame(encoded_X)\n",
    "\n",
    "        #modify the names of the columns for the ones stored in the array encoded_columns[0]\n",
    "        # Simply access the values stored in the dictionary. To access a value, simply pass\n",
    "        # the name of the key (in quotes) inside brackets after the name of the dictionary,\n",
    "        # just as accessing a column from a dataframe:\n",
    "        encoded_X_df.columns = encoding_dict[column]\n",
    "        \n",
    "        #Inner join the new dataset with the encoded dataset.\n",
    "        # Use the index as the key, since indices are necessarily correspondent.\n",
    "        # To use join on index, we apply pandas .concat method.\n",
    "        # To join on a specific key, we could use pandas .merge method with the arguments\n",
    "        # left_on = 'left_key', right_on = 'right_key'; or, if the keys have same name,\n",
    "        # on = 'key':\n",
    "        # Check Pandas merge and concat documentation:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html\n",
    "        \n",
    "        new_df = pd.concat([new_df, encoded_X_df], axis = 1, join = \"inner\")\n",
    "        \n",
    "        print(f\"Successfully encoded column \\'{column}\\' and merged the encoded columns to the dataframe.\")\n",
    "        print(\"Check first 5 rows of the encoded table that was merged:\\n\")\n",
    "        print(encoded_X_df.head())\n",
    "        # The default of the head method, when no parameter is printed, is to show 5 rows; if an\n",
    "        # integer number Y is passed as argument .head(Y), Pandas shows the first Y-rows.\n",
    "    \n",
    "    print(\"Finished One-Hot Encoding. Returning the new transformed dataframe; and an encoding dictionary with the original columns as keys, and arrays containing the categories on those columns as the correspondent values.\")\n",
    "    print(f\"For each category in the columns \\'{subset_of_features_to_be_encoded}\\', a new column has value 1, if it is the actual category of that row; or is 0 if not.\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    print(new_df.head(10))\n",
    "\n",
    "    #return the transformed dataframe and the encoding dictionary:\n",
    "    return new_df, encoding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for scaling the features**\n",
    "- Machine Learning algorithms are extremely sensitive to scale. This function provides 3 methods (modes) of scaling:\n",
    "    - `mode = 'standard'`: applies the standard scaling, which creates a new variable with mean = 0; and standard deviation = 1. Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean of the training samples, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "    - `mode = 'min_max'`: applies min-max normalization, with a resultant feature ranging from 0 to 1. Each value Y is transformed as Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and maximum values of Y, respectively.\n",
    "    - `mode = 'factor'`: divide the whole series by a numeric value provided as argument. For a factor F, the new Y values will be Ytransf = Y/F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling (df, subset_of_features_to_scale, mode = 'standard', scale_with_new_params = True, scaling_params = None, suffix = '_scaled'):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Scikit-learn Preprocessing data guide:\n",
    "    # https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "    # Standard scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    # Min-Max scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.set_params\n",
    "    \n",
    "    ## Machine Learning algorithms are extremely sensitive to scale. \n",
    "    \n",
    "    ## This function provides 3 methods (modes) of scaling:\n",
    "    ## mode = 'standard': applies the standard scaling, \n",
    "    ##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "    ##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "    ##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "    ## mode = 'min_max': applies min-max normalization, with a resultant feature \n",
    "    ## ranging from 0 to 1. each value Y is transformed as \n",
    "    ## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "    ## maximum values of Y, respectively.\n",
    "    \n",
    "    ## mode = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "    ## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #subset_of_features_to_be_scaled: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_scaled = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_scaled = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    # scale_with_new_params = True\n",
    "    # Alternatively, set scale_with_new_params = True if you want to calculate a new\n",
    "    # scaler for the data; or set scale_with_new_params = False if you want to apply \n",
    "    # parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "    # previously trained to another set of data; or wants to simply apply again the same\n",
    "    # scaler).\n",
    "    \n",
    "    # scale_params:\n",
    "    # This variable has effect only when SCALE_WITH_NEW_PARAMS = False\n",
    "    ## WARNING: The mode 'factor' demmands the input of the list of factors that will be \n",
    "    # used for normalizing each column. Therefore, it can be used only \n",
    "    # when scale_with_new_params = False.\n",
    "    \n",
    "    ## For the mode 'factor', declare scaling_params as a dictionary containing the \n",
    "    # column name as the key and the correspondent factor as the value.\n",
    "    # e.g. subset_of_features_to_scale = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "    # and 'col2' will be divided by 3.2,  then:\n",
    "    # scaling_params = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "    ## WARNING: For scaling_params (when scale_with_new_params = False and \n",
    "    # mode = 'standard' or mode = 'min_max'), the dictionary must be declared with the\n",
    "    # column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "    # value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "    # correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "    # scaling_dictionary printed as output when the scaler is trained.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "    # identified as '_scaled'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "      \n",
    "    if (suffix is None):\n",
    "        #set as the default\n",
    "        suffix = '_scaled'\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df\n",
    "    \n",
    "    if (scale_with_new_params == True):\n",
    "            #Let's create a new scaler\n",
    "            \n",
    "            #Start an scaling dictionary empty:\n",
    "            scaling_dict = {}\n",
    "            \n",
    "            if (mode == 'standard'):\n",
    "                \n",
    "                for column in subset_of_features_to_scale:\n",
    "                    # Loop through each element (named 'column') of the list of columns \n",
    "                    # to analyze:\n",
    "                    \n",
    "                    #Create a dataframe X by subsetting only the analyzed column\n",
    "                    # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "                    # or array in the shape for scikit-learn:\n",
    "                    # For doing so, pass a list of columns for column filtering, containing\n",
    "                    # the object column as its single element:\n",
    "                    X = new_df[[column]]\n",
    "                    \n",
    "                    #start the scaler:\n",
    "                    scaler = StandardScaler()\n",
    "                    \n",
    "                    #fit the scaler to the column\n",
    "                    scaler = scaler.fit(X)\n",
    "                    \n",
    "                    #calculate the scaled feature, and store it as new array:\n",
    "                    scaled_feature = scaler.transform(X)\n",
    "                    # scaler.inverse_transform(X) would reverse the scaling.\n",
    "\n",
    "                    # Create the new_column name:\n",
    "                    new_column = column + suffix\n",
    "                    # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "                    # Set the new column as scaled_feature\n",
    "                    new_df[new_column] = scaled_feature\n",
    "                    \n",
    "                    # Get the scaling parameters for that column:\n",
    "                    scaling_params = scaler.get_params(deep=True)\n",
    "                    \n",
    "                    #scaling_params is a dictionary containing the scaling parameters.\n",
    "                    #Update the dictionary to store the original column name as key, \n",
    "                    # and the dictionary of parameters as the value:\n",
    "                    encoding_dict.update({column: scaling_params})\n",
    "                    \n",
    "                    print(f\"Successfully scaled column {column}.\")\n",
    "                \n",
    "                print(\"Successfully scaled the dataframe. Returning the transformed dataframe and the scaling dictionary.\")\n",
    "                print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "                print(new_df.head(10))\n",
    "                print(\"\\n\") # line break\n",
    "                print(\"Check also the scaling dictionary obtained:\\n\")\n",
    "                print(scaling_dict)\n",
    "                \n",
    "                return new_df, scaling_dict\n",
    "                \n",
    "            elif (mode == 'min_max'):\n",
    "                  \n",
    "                for column in subset_of_features_to_scale:\n",
    "                    # Loop through each element (named 'column') of the list of columns \n",
    "                    # to analyze:\n",
    "                    \n",
    "                    #Create a dataframe X by subsetting only the analyzed column\n",
    "                    # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "                    # or array in the shape for scikit-learn:\n",
    "                    # For doing so, pass a list of columns for column filtering, containing\n",
    "                    # the object column as its single element:\n",
    "                    X = new_df[[column]]\n",
    "                    \n",
    "                    #start the scaler:\n",
    "                    scaler = MinMaxScaler()\n",
    "                    \n",
    "                    #fit the scaler to the column\n",
    "                    scaler = scaler.fit(X)\n",
    "                    \n",
    "                    #calculate the scaled feature, and store it as new array:\n",
    "                    scaled_feature = scaler.transform(X)\n",
    "                    # scaler.inverse_transform(X) would reverse the scaling.\n",
    "\n",
    "                    # Create the new_column name:\n",
    "                    new_column = column + suffix\n",
    "                    # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "                    # Set the new column as scaled_feature\n",
    "                    new_df[new_column] = scaled_feature\n",
    "                    \n",
    "                    # Get the scaling parameters for that column:\n",
    "                    scaling_params = scaler.get_params(deep=True)\n",
    "                    \n",
    "                    #scaling_params is a dictionary containing the scaling parameters.\n",
    "                    #Update the dictionary to store the original column name as key, \n",
    "                    # and the dictionary of parameters as the value:\n",
    "                    encoding_dict.update({column: scaling_params})\n",
    "                    \n",
    "                    print(f\"Successfully scaled column {column}.\")\n",
    "                \n",
    "                print(\"Successfully scaled the dataframe. Returning the transformed dataframe and the scaling dictionary.\")\n",
    "                print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "                print(new_df.head(10))\n",
    "                print(\"\\n\") # line break\n",
    "                print(\"Check also the scaling dictionary obtained:\\n\")\n",
    "                print(scaling_dict)\n",
    "                \n",
    "                return new_df, scaling_dict\n",
    "                \n",
    "            else:\n",
    "                print(\"Enter a valid mode, standard or min_max. The mode factor can be only used when scale_with_new_params == False and when a scaling dictionary was input as scaling_params.\")       \n",
    "                return \"error\", \"error\"\n",
    "                \n",
    "    else: \n",
    "        # scale_with_new_params == False\n",
    "        # Use a previously obtained scaling_dict:\n",
    "        \n",
    "        scaling_dict = scaling_params\n",
    "        \n",
    "        if (mode == 'factor'):\n",
    "            \n",
    "            for column in subset_of_features_to_scale:\n",
    "                # Loop through each element (named 'column') of the list of columns \n",
    "                # to analyze:\n",
    "                \n",
    "                # Create the new_column name:\n",
    "                new_column = column + suffix\n",
    "                # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                new_df[new_column] = (new_df[column])/(scaling_dict[column])\n",
    "                \n",
    "                print(f\"Successfully scaled column {column}.\")\n",
    "\n",
    "            print(\"Successfully scaled the dataframe.\")\n",
    "            print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "            print(new_df.head(10))\n",
    "\n",
    "            return new_df\n",
    "        \n",
    "        elif (mode == 'standard'):\n",
    "            \n",
    "            for column in subset_of_features_to_scale:\n",
    "                # Loop through each element (named 'column') of the list of columns \n",
    "                # to analyze:\n",
    "                \n",
    "                #Create a dataframe X by subsetting only the analyzed column\n",
    "                # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "                # or array in the shape for scikit-learn:\n",
    "                # For doing so, pass a list of columns for column filtering, containing\n",
    "                # the object column as its single element:\n",
    "                X = new_df[[column]]\n",
    "                    \n",
    "                #start the scaler:\n",
    "                scaler = StandardScaler()\n",
    "                    \n",
    "                #Get the dictionary of scaling parameters for the feature 'column':\n",
    "                # For that, access the key: 'column' in the scaling_dict dictionary\n",
    "                # to retrieve its value, i.e., the dictionary for that feature:\n",
    "                scaling_params = scaling_dict[column]\n",
    "                    \n",
    "                # Now, set the scaler parameters to be equal to the values retrieved\n",
    "                # as the dictionary scaling_params:\n",
    "                scaler = scaler.set_params(scaling_params)\n",
    "                # Notice that the .set_params method substitute the step where we applied\n",
    "                # the .fit method.\n",
    "                    \n",
    "                #calculate the scaled feature, and store it as new array:\n",
    "                scaled_feature = scaler.transform(X)\n",
    "                # scaler.inverse_transform(X) would reverse the scaling.\n",
    "\n",
    "                # Create the new_column name:\n",
    "                new_column = column + suffix\n",
    "                # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "                # Set the new column as scaled_feature\n",
    "                new_df[new_column] = scaled_feature\n",
    "                    \n",
    "                print(f\"Successfully scaled column {column}.\")\n",
    "                \n",
    "            print(\"Successfully scaled the dataframe.\")\n",
    "            print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "            print(new_df.head(10))\n",
    "                \n",
    "            return new_df\n",
    "        \n",
    "        elif (mode == 'min_max'):\n",
    "            \n",
    "            for column in subset_of_features_to_scale:\n",
    "                # Loop through each element (named 'column') of the list of columns \n",
    "                # to analyze:\n",
    "                \n",
    "                #Create a dataframe X by subsetting only the analyzed column\n",
    "                # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "                # or array in the shape for scikit-learn:\n",
    "                # For doing so, pass a list of columns for column filtering, containing\n",
    "                # the object column as its single element:\n",
    "                X = new_df[[column]]\n",
    "                    \n",
    "                #start the scaler:\n",
    "                scaler = MinMaxScaler()\n",
    "                    \n",
    "                #Get the dictionary of scaling parameters for the feature 'column':\n",
    "                # For that, access the key: 'column' in the scaling_dict dictionary\n",
    "                # to retrieve its value, i.e., the dictionary for that feature:\n",
    "                scaling_params = scaling_dict[column]\n",
    "                    \n",
    "                # Now, set the scaler parameters to be equal to the values retrieved\n",
    "                # as the dictionary scaling_params:\n",
    "                scaler = scaler.set_params(scaling_params)\n",
    "                # Notice that the .set_params method substitute the step where we applied\n",
    "                # the .fit method.\n",
    "                    \n",
    "                #calculate the scaled feature, and store it as new array:\n",
    "                scaled_feature = scaler.transform(X)\n",
    "                # scaler.inverse_transform(X) would reverse the scaling.\n",
    "\n",
    "                # Create the new_column name:\n",
    "                new_column = column + suffix\n",
    "                # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "                # Set the new column as scaled_feature\n",
    "                new_df[new_column] = scaled_feature\n",
    "                    \n",
    "                print(f\"Successfully scaled column {column}.\")\n",
    "                \n",
    "            print(\"Successfully scaled the dataframe.\")\n",
    "            print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "            print(new_df.head(10))\n",
    "                \n",
    "            return new_df\n",
    "        \n",
    "        else:\n",
    "\n",
    "            print(\"Select a valid mode: standard, min_max, or factor.\")\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for reversing the scaling of the features**\n",
    "- `mode = 'standard'`.\n",
    "- `mode = 'min_max'`.\n",
    "- `mode = 'factor'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_feature_scaling (df, subset_of_features_to_scale, scaling_params, mode = 'standard', suffix = '_reverseScaling'):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Scikit-learn Preprocessing data guide:\n",
    "    # https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "    # Standard scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    # Min-Max scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.set_params\n",
    "    \n",
    "    ## Machine Learning algorithms are extremely sensitive to scale. \n",
    "    \n",
    "    ## This function provides 3 methods (modes) of scaling:\n",
    "    ## mode = 'standard': applies the standard scaling, \n",
    "    ##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "    ##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "    ##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "    ## mode = 'min_max': applies min-max normalization, with a resultant feature \n",
    "    ## ranging from 0 to 1. each value Y is transformed as \n",
    "    ## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "    ## maximum values of Y, respectively.\n",
    "    \n",
    "    ## mode = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "    ## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #subset_of_features_to_be_scaled: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_scaled = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_scaled = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    ## WARNING: The mode 'factor' demmands the input of the list of factors that will be \n",
    "    # used for normalizing each column.\n",
    "    \n",
    "    ## For the mode 'factor', declare scaling_params as a dictionary containing the \n",
    "    # column name as the key and the correspondent factor as the value.\n",
    "    # e.g. subset_of_features_to_scale = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "    # and 'col2' will be divided by 3.2,  then:\n",
    "    # scaling_params = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "    ## WARNING: For scaling_params (when scale_with_new_params = False and \n",
    "    # mode = 'standard' or mode = 'min_max'), the dictionary must be declared with the\n",
    "    # column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "    # value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "    # correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "    # scaling_dictionary printed as output when the scaler is trained.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "    # identified as '_reverseScaling'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "      \n",
    "    if (suffix is None):\n",
    "        #set as the default\n",
    "        suffix = '_reverseScaling'\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df\n",
    "    \n",
    "    # Use a previously obtained scaling_dict:\n",
    "        \n",
    "    scaling_dict = scaling_params\n",
    "        \n",
    "    if (mode == 'factor'):\n",
    "            \n",
    "        for column in subset_of_features_to_scale:\n",
    "            # Loop through each element (named 'column') of the list of columns \n",
    "            # to analyze:\n",
    "                \n",
    "            # Create the new_column name:\n",
    "            new_column = column + suffix\n",
    "            # Create the new_column.\n",
    "            # Once the scaling was performed through division, the reverse of it consists\n",
    "            # on a multiplication:\n",
    "            \n",
    "            new_df[new_column] = (new_df[column])*(scaling_dict[column])\n",
    "                \n",
    "            print(f\"Successfully re-scaled column {column}.\")\n",
    "\n",
    "            print(\"Successfully re-scaled the dataframe.\")\n",
    "            print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "            print(new_df.head(10))\n",
    "\n",
    "            return new_df\n",
    "        \n",
    "    elif (mode == 'standard'):\n",
    "            \n",
    "        for column in subset_of_features_to_scale:\n",
    "            # Loop through each element (named 'column') of the list of columns \n",
    "            # to analyze:\n",
    "                \n",
    "            #Create a dataframe X by subsetting only the analyzed column\n",
    "            # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "            # or array in the shape for scikit-learn:\n",
    "            # For doing so, pass a list of columns for column filtering, containing\n",
    "            # the object column as its single element:\n",
    "            X = new_df[[column]]\n",
    "                    \n",
    "            #start the scaler:\n",
    "            scaler = StandardScaler()\n",
    "                    \n",
    "            #Get the dictionary of scaling parameters for the feature 'column':\n",
    "            # For that, access the key: 'column' in the scaling_dict dictionary\n",
    "            # to retrieve its value, i.e., the dictionary for that feature:\n",
    "            scaling_params = scaling_dict[column]\n",
    "                    \n",
    "            # Now, set the scaler parameters to be equal to the values retrieved\n",
    "            # as the dictionary scaling_params:\n",
    "            scaler = scaler.set_params(scaling_params)\n",
    "            # Notice that the .set_params method substitute the step where we applied\n",
    "            # the .fit method.\n",
    "                    \n",
    "            #Invert the scaling of the feature, and store it as new array:\n",
    "            scaled_feature = scaler.inverse_transform(X)\n",
    "            # Notice that this step substitutes the application of the method\n",
    "            # scaler.transform(X), used for scaling the variable.\n",
    "\n",
    "            # Create the new_column name:\n",
    "            new_column = column + suffix\n",
    "            # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "            # Set the new column as scaled_feature\n",
    "            new_df[new_column] = scaled_feature\n",
    "                    \n",
    "            print(f\"Successfully re-scaled column {column}.\")\n",
    "                \n",
    "        print(\"Successfully re-scaled the dataframe.\")\n",
    "        print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "        print(new_df.head(10))\n",
    "                \n",
    "        return new_df\n",
    "        \n",
    "    elif (mode == 'min_max'):\n",
    "            \n",
    "        for column in subset_of_features_to_scale:\n",
    "            # Loop through each element (named 'column') of the list of columns \n",
    "            # to analyze:\n",
    "                \n",
    "            #Create a dataframe X by subsetting only the analyzed column\n",
    "            # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "            # or array in the shape for scikit-learn:\n",
    "            # For doing so, pass a list of columns for column filtering, containing\n",
    "            # the object column as its single element:\n",
    "            X = new_df[[column]]\n",
    "                    \n",
    "            #start the scaler:\n",
    "            scaler = MinMaxScaler()\n",
    "                    \n",
    "            #Get the dictionary of scaling parameters for the feature 'column':\n",
    "            # For that, access the key: 'column' in the scaling_dict dictionary\n",
    "            # to retrieve its value, i.e., the dictionary for that feature:\n",
    "            scaling_params = scaling_dict[column]\n",
    "                    \n",
    "            # Now, set the scaler parameters to be equal to the values retrieved\n",
    "            # as the dictionary scaling_params:\n",
    "            scaler = scaler.set_params(scaling_params)\n",
    "            # Notice that the .set_params method substitute the step where we applied\n",
    "            # the .fit method.\n",
    "                    \n",
    "            #Invert the scaling of the feature, and store it as new array:\n",
    "            scaled_feature = scaler.inverse_transform(X)\n",
    "            # Notice that this step substitutes the application of the method\n",
    "            # scaler.transform(X), used for scaling the variable.\n",
    "                \n",
    "            # Create the new_column name:\n",
    "            new_column = column + suffix\n",
    "            # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "            # Set the new column as scaled_feature\n",
    "            new_df[new_column] = scaled_feature\n",
    "                    \n",
    "            print(f\"Successfully re-scaled column {column}.\")\n",
    "                \n",
    "        print(\"Successfully re-scaled the dataframe.\")\n",
    "        print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "        print(new_df.head(10))\n",
    "                \n",
    "        return new_df\n",
    "        \n",
    "    else:\n",
    "\n",
    "        print(\"Select a valid mode: standard, min_max, or factor.\")\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe (dataframe_to_be_exported, new_file_name_with_csv_extension, file_directory_path = None, export_to_s3_bucket = False, s3_bucket_name = None, desired_s3_file_name_with_csv_extension = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    #boto3 is AWS S3 Python SDK\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all file extensions should be .csv for this function\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # NEW_FILE_NAME_WITH_CSV_EXTENSION - (string, in quotes): input the name of the \n",
    "    # file with the  extension. e.g. FILE_NAME_WITH_CSV_EXTENSION = \"file.csv\"\n",
    "    \n",
    "    # export_to_s3_bucket = False. Alternatively, set as True to export the file to an\n",
    "    # AWS S3 Bucket.\n",
    "\n",
    "    ## The following parameters have effect only when export_to_s3_bucket == True:\n",
    "\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "\n",
    "    # The name desired for the object stored in S3 (string, in quotes). \n",
    "    # Keep it None to set it equals to new_file_name_with_csv_extension. \n",
    "    # Alternatively, set it as a string analogous to new_file_name_with_csv_extension. \n",
    "    # e.g. desired_s3_file_name_with_csv_extension = \"S3_file.csv\"\n",
    "    \n",
    "    if (export_to_s3_bucket == True):\n",
    "        \n",
    "        if (desired_s3_file_name_with_csv_extension is None):\n",
    "            #Repeat new_file_name_with_extension\n",
    "            desired_s3_file_name_with_csv_extension = new_file_name_with_csv_extension\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start S3 client:\n",
    "            print(\"Starting AWS S3 client.\")\n",
    "        \n",
    "            # Let's export the file to a AWS S3 (simple storage service) bucket\n",
    "            # instantiate S3 client and upload to s3\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            # Create a local copy of the file on the root.\n",
    "            local_copy_path = os.path.join(\"/\", new_file_name_with_csv_extension)\n",
    "            dataframe_to_be_exported.to_csv(local_copy_path, index = False)\n",
    "            \n",
    "            print(\"Local copy of the dataframe created on the root path to export to S3.\")\n",
    "            print(\"Simply delete this file from the root path if you only want to keep the S3 version.\")\n",
    "            \n",
    "            # Upload this local copy to S3:\n",
    "            try:\n",
    "                response = s3_client.meta.client.upload_file(local_copy_path, s3_bucket_name, desired_s3_file_name_with_extension)\n",
    "            \n",
    "            except ClientError as e:\n",
    "                logging.error(e)\n",
    "                return False\n",
    "            \n",
    "            print(f\"{desired_s3_file_name_with_csv_extension} successfully exported to {s3_bucket_name} AWS S3 bucket.\")\n",
    "            return True\n",
    "            # Check AWS Documentation:\n",
    "            # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "            \n",
    "            # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "            # the following code, instead:        \n",
    "            # ACCESS_KEY = 'access_key_ID'\n",
    "            # PASSWORD_KEY = 'password_key'\n",
    "            # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "            # s3_client.upload_file(local_copy_path, s3_bucket_name, desired_s3_file_name_with_extension)\n",
    "            \n",
    "    else :\n",
    "        # Do not export to AWS S3. Export to other path.\n",
    "        # Create the complete file path:\n",
    "        file_path = os.path.join(file_directory_path, new_file_name_with_csv_extension)\n",
    "\n",
    "        dataframe_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "        print(f\"Dataframe {new_file_name_with_csv_extension} exported as \\'{file_path}\\'.\")\n",
    "        print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_or_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_file_name = None, directory_path = '/', model_type = 'keras', dict_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.arima.model import ARIMAResults\n",
    "    from keras.models import load_model\n",
    "    from google.colab import files\n",
    "    import pickel as pkl\n",
    "    import dill\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_only' if only a dictionary will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    #model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no \n",
    "    # dictionary will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'sklearn_xgb' for models from sklearn or xgboost (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root:\n",
    "        directory_path = \"/\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'sklearn_xgb'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_tyoe == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                imported_dict = pkl.load(open(colab_files_dict[key], 'rb'))\n",
    "                print(f\"Dictionary {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method             \n",
    "                imported_dict = pkl.load(open(dict_path, 'rb'))\n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn_xgb'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = dill.load(open(colab_files_dict[key], 'rb'))\n",
    "                    print(f\"Scikit-learn or XGBoost model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = dill.load(open(model_path, 'rb'))\n",
    "                    print(f\"Scikit-learn or XGBoost model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                pkl.dump(dict_to_export, open(key, 'wb'))\n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method             \n",
    "                pkl.dump(dict_to_export, open(dict_path, 'wb'))\n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn_xgb'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    dill.dump(model_to_export, open(key, 'wb'))\n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn or XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    dill.dump(model_to_export, open(model_path, 'wb'))\n",
    "                    print(f\"Scikit-learn or XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "                    \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_or_upload_file (source = 'aws', action = 'download', object_to_download_from_colab = None, s3_bucket_name = None, local_path_of_storage = '/', file_name_with_extension = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    from google.colab import files\n",
    "    \n",
    "    # source = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "    # source = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to AWS S3 or to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # object_to_download_from_colab = None. This option has effect only when\n",
    "    # source == 'google'. In this case, this parameter is obbligatory. \n",
    "    # Declare as object_to_download_from_colab the object that you want to download.\n",
    "    # Since it is an object and not a string, it should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = dict.\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = df.\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = keras_model\n",
    "    \n",
    "    ## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "    # to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "    # path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "    # If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "    # will be imported to the root path. Alternatively, input the path as a string \n",
    "    # (in quotes).\n",
    "    # Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "    # LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "    # Notice that only the directories should be declared: do not include the file name and\n",
    "    # its extension.\n",
    "    \n",
    "    # file_name_with_extension: string, in quotes, containing the file name which will be\n",
    "    # downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "    ## This parameter is obbligatory when source == 'aws'\n",
    "    # Examples:\n",
    "    # file_name_with_extension = 'Screen_Shot.png'; file_name_with_extension = 'dataset.csv',\n",
    "    # file_name_with_extension = \"dictionary.pkl\", file_name_with_extension = \"model.h5\",\n",
    "    # file_name_with_extension = 'doc.pdf', file_name_with_extension = 'model.dill'\n",
    "\n",
    "    if (source == 'google'):\n",
    "        \n",
    "        if (action == 'upload'):\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            \n",
    "            colab_files_dict = files.upload()\n",
    "            \n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "                \n",
    "                print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "                print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "                print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "                print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "                print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "                print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "                print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "                print(\"df = pd.read_excel(uploaded_file)\")\n",
    "        \n",
    "        elif (action == 'download'):\n",
    "            \n",
    "            if (object_to_download_from_colab is None):\n",
    "                \n",
    "                #No object was declared\n",
    "                print(\"Please, inform an object to download. Since it is an object, not a string, it should not be declared in quotes.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "                files.download(object_to_download_from_colab)\n",
    "\n",
    "                print(f\"File {object_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(\"Please, select a valid action, download or upload.\")\n",
    "          \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead for starting the client:\n",
    "        \n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # Nextly, the code is the same.\n",
    "        \n",
    "        \n",
    "        # If the path to store is None, also import the bucket content to root path;\n",
    "        # or upload the file from root path to the bucket\n",
    "        if (local_path_of_storage is None):\n",
    "            \n",
    "            local_path_of_storage = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message. The same for the file name with extension:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name.\")\n",
    "        \n",
    "        elif (file_name_with_extension is None):\n",
    "            \n",
    "            print(\"Please, provide a valid file name with its extension. e.g. \\'dataset.csv\\'.\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Obtain the full file path from which the file will be uploaded to S3; or to\n",
    "            # which the file will be downloaded from S3:\n",
    "            file_path = os.path.join(local_path_of_storage, file_name_with_extension)\n",
    "            \n",
    "            # Start S3 client:\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            print(\"Starting AWS S3 client.\")\n",
    "            \n",
    "            if (action == 'upload'):\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).\\\n",
    "                    upload_file(Filename = file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully uploaded to AWS S3 {s3_bucket_name} bucket.\")\n",
    "            \n",
    "            elif (action == 'download'):\n",
    "\n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).download_file(file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully downloaded from AWS S3 {s3_bucket_name} bucket.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"Please, select a valid action, download or upload.\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = '/'\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None, or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'name_of_aws_s3_bucket_to_be_accessed'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_KEY_PREFFIX_FOLDER = None\n",
    "# S3_OBJECT_KEY_PREFFIX_FOLDER = None. Keep it None or as an empty string \n",
    "# (S3_OBJECT_KEY_PREFFIX_FOLDER = '') to import the whole bucket content, instead of a \n",
    "# single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, key_preffix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_key_preffix = S3_OBJECT_KEY_PREFFIX_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "# WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "# txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\"\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# TXT_CSV_COL_SEP = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, TXT_CSV_COL_SEP = \"comma\" for columns separated by comma (\",\")\n",
    "# TXT_CSV_COL_SEP = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, has_header = HAS_HEADER, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering (selecting) or renaming columns of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'filter'\n",
    "# MODE = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "# MODE = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "\n",
    "COLS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "#New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = col_filter_rename (df = DATASET, cols_list = COLS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **log-transforming the variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WARNING: This function will eliminate rows where the selected variables present \n",
    "#### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_log\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"collumn1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "log_transf_df = log_transform (df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)\n",
    "\n",
    "# One curve derived from the normal is the log-normal.\n",
    "# If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "# A log normal curve resembles a normal, but with skewness (distortion); \n",
    "# and kurtosis (long-tail).\n",
    "\n",
    "# Applying the log is a methodology for normalizing the variables: \n",
    "# the sample space gets shrinkled after the transformation, making the data more \n",
    "# adequate for being processed by Machine Learning algorithms. Preferentially apply \n",
    "# the transformation to the whole dataset, so that all variables will be of same order \n",
    "# of magnitude.\n",
    "# Obviously, it is not necessary for variables ranging from -100 to 100 in numerical \n",
    "# value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the log-transform - Exponentially transforming variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"collumn1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_log_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining and applying Box-Cox transform**\n",
    "- Transform a series of data into a series described by a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: no specification limits provided to Box-Cox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "MODE = 'calculate_and_apply'\n",
    "# Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "# transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "# To 'apply_only', lambda_box must be provided.\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_BoxCoxTransf'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "# identified as 'Y_BoxCoxTransf'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "SPECIFICATION_LIMS = None\n",
    "#specification_lims = None if there are no specification limits for the variable being\n",
    "# transformed by the function.\n",
    "#In case there were originally specification limits for the variable (column) being\n",
    "# transformed, declare them as a list, array, or tuple of two numbers (float).\n",
    "# e.g. if the column represents a variable with specifications between 10 to 20 kg, declare\n",
    "# specification_lims = [10, 20]. If it represents a variable which specifications should\n",
    "# be betweewn 0 to 12.5 L, declare specification_lims = [0, 12.5]\n",
    "# Then, the function will return the specifications transformed by the same Box-Cox\n",
    "# transformation applied to the data. Remember: if data were transformed, so should be\n",
    "# the specification limits.\n",
    "\n",
    "#New dataframe saved as data_transformed_df; dictionary saved as data_sum_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "data_transformed_df, data_sum_dict = box_cox_transform (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, mode = MODE, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX, specification_lims = SPECIFICATION_LIMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: specification limits provided to Box-Cox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "MODE = 'calculate_and_apply'\n",
    "# Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "# transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "# To 'apply_only', lambda_box must be provided.\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_BoxCoxTransf'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "# identified as 'Y_BoxCoxTransf'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "SPECIFICATION_LIMS = [None, None]\n",
    "## First element: inferior specification limit (Float)\n",
    "## Second element: superior specification limit (Float)\n",
    "\n",
    "#specification_lims = None if there are no specification limits for the variable being\n",
    "# transformed by the function.\n",
    "#In case there were originally specification limits for the variable (column) being\n",
    "# transformed, declare them as a list, array, or tuple of two numbers (float).\n",
    "# e.g. if the column represents a variable with specifications between 10 to 20 kg, declare\n",
    "# specification_lims = [10, 20]. If it represents a variable which specifications should\n",
    "# be betweewn 0 to 12.5 L, declare specification_lims = [0, 12.5]\n",
    "# Then, the function will return the specifications transformed by the same Box-Cox\n",
    "# transformation applied to the data. Remember: if data were transformed, so should be\n",
    "# the specification limits.\n",
    "\n",
    "#New dataframe saved as data_transformed_df; dictionaries saved as data_sum_dict and\n",
    "# spec_lim_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "data_transformed_df, data_sum_dict, spec_lim_dict = box_cox_transform (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, mode = MODE, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX, specification_lims = SPECIFICATION_LIMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_ReversedBoxCox'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "# identified as 'Y_ReversedBoxCox'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as retransformed_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "retransformed_df = reverse_box_cox (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **One-Hot Encoding the categorical variables**\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.For a category \"A\", a column named \"A\" is created.\n",
    "    - If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "    - If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "#New dataframe saved as one_hot_encoded_df; dictionary saved as encoding_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "one_hot_encoded_df, encoding_dict = OneHotEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaling the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: obtention of a new scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'standard'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "SCALE_WITH_NEW_PARAMS = True\n",
    "# Alternatively, set SCALE_WITH_NEW_PARAMS = True if you want to calculate a new\n",
    "# scaler for the data; or SCALE_WITH_NEW_PARAMS = False if you want to apply \n",
    "# parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "# previously trained to another set of data; or wants to simply apply again the same\n",
    "# scaler).\n",
    "    \n",
    "## WARNING: The MODE 'factor' demmands the input of the list of factors that will be \n",
    "# used for normalizing each column. Therefore, it can be used only \n",
    "# when SCALE_WITH_NEW_PARAMS = False.\n",
    "\n",
    "SCALING_PARAMS = None\n",
    "# This variable has effect only when SCALE_WITH_NEW_PARAMS = False\n",
    "## For the MODE 'factor', declare SCALING_PARAMS as a dictionary containing the \n",
    "# column name as the key and the correspondent factor as the value.\n",
    "# e.g. SUBSET_OF_FEATURES_TO_SCALE = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "# and 'col2' will be divided by 3.2,  then:\n",
    "# SCALING_PARAMS = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "## WARNING: For SCALING_PARAMS when SCALE_WITH_NEW_PARAMS = True and \n",
    "# MODE = 'standard' or MODE = 'min_max', the dictionary must be declared with the\n",
    "# column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "# value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "# correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "# scaling_dictionary printed as output when the scaler is trained.\n",
    "\n",
    "SUFFIX = '_scaled'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "# identified as 'Y_scaled'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as new_df; dictionary saved as scaling_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df, scaling_dict = feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, mode = MODE, scale_with_new_params = SCALE_WITH_NEW_PARAMS, scaling_params = SCALING_PARAMS, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: using scaling parameters previously obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'standard'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "SCALE_WITH_NEW_PARAMS = False\n",
    "# Alternatively, set SCALE_WITH_NEW_PARAMS = True if you want to calculate a new\n",
    "# scaler for the data; or SCALE_WITH_NEW_PARAMS = False if you want to apply \n",
    "# parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "# previously trained to another set of data; or wants to simply apply again the same\n",
    "# scaler).\n",
    "    \n",
    "## WARNING: The MODE 'factor' demmands the input of the list of factors that will be \n",
    "# used for normalizing each column. Therefore, it can be used only \n",
    "# when SCALE_WITH_NEW_PARAMS = False.\n",
    "\n",
    "SCALING_PARAMS = None\n",
    "# This variable has effect only when SCALE_WITH_NEW_PARAMS = False\n",
    "## For the MODE 'factor', declare SCALING_PARAMS as a dictionary containing the \n",
    "# column name as the key and the correspondent factor as the value.\n",
    "# e.g. SUBSET_OF_FEATURES_TO_SCALE = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "# and 'col2' will be divided by 3.2,  then:\n",
    "# SCALING_PARAMS = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "## WARNING: For SCALING_PARAMS when SCALE_WITH_NEW_PARAMS = True and \n",
    "# MODE = 'standard' or MODE = 'min_max', the dictionary must be declared with the\n",
    "# column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "# value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "# correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "# scaling_dictionary printed as output when the scaler is trained.\n",
    "\n",
    "SUFFIX = '_scaled'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "# identified as 'Y_scaled'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as new_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df = feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, mode = MODE, scale_with_new_params = SCALE_WITH_NEW_PARAMS, scaling_params = SCALING_PARAMS, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reversing scaling of the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'standard'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "SCALING_PARAMS = {None}\n",
    "# This variable has effect only when SCALE_WITH_NEW_PARAMS = False\n",
    "## For the MODE 'factor', declare SCALING_PARAMS as a dictionary containing the \n",
    "# column name as the key and the correspondent factor as the value.\n",
    "# e.g. SUBSET_OF_FEATURES_TO_SCALE = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "# and 'col2' will be divided by 3.2,  then:\n",
    "# SCALING_PARAMS = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "## WARNING: For SCALING_PARAMS when SCALE_WITH_NEW_PARAMS = True and \n",
    "# MODE = 'standard' or MODE = 'min_max', the dictionary must be declared with the\n",
    "# column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "# value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "# correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "# scaling_dictionary printed as output when the scaler is trained.\n",
    "\n",
    "SUFFIX = '_reverseScaling'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "# identified as 'Y_reverseScaling'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as new_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df = reverse_feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, scaling_params = SCALING_PARAMS, mode = MODE, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn_xgb' for models from sklearn or xgboost (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "#Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: import only a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn_xgb' for models from sklearn or xgboost (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary saved as imported_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: import a model and a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'keras'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn_xgb' for models from sklearn or xgboost (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary saved as imported_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4: export a model and/or a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn_xgb' for models from sklearn or xgboost (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values = df_gen_charac (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtaining correlation plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SHOW_MASKED_PLOT = True\n",
    "#SHOW_MASKED_PLOT = True - keep as True if you want to see a cleaned version of the plot\n",
    "# where a mask is applied. Alternatively, SHOW_MASKED_PLOT = True, or \n",
    "# SHOW_MASKED_PLOT = False\n",
    "\n",
    "RESPONSES_TO_RETURN_CORR = None\n",
    "#RESPONSES_TO_RETURN_CORR - keep as None to return the full correlation tensor.\n",
    "# If you want to display the correlations for a particular group of features, input them\n",
    "# as a list, even if this list contains a single element. Examples:\n",
    "# responses_to_return_corr = ['response1'] for a single response\n",
    "# responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "# responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "# of a column of the dataset that represents a response variable.\n",
    "# WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "# of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "# Alternatively: a list containing strings (inside quotes) with the names of the response\n",
    "# columns that you want to see the correlations. Declare as a list even if it contains a\n",
    "# single element.\n",
    "\n",
    "SET_RETURNED_LIMIT = None\n",
    "# SET_RETURNED_LIMIT = None - This variable will only present effects in case you have\n",
    "# provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "# to return all of the correlation coefficients; or, alternatively, \n",
    "# provide an integer number to limit the total of coefficients returned. \n",
    "# e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframe saved as correlation_matrix. Simply modify this object on the left of equality:\n",
    "correlation_matrix = correlation_plot (df = DATASET, show_masked_plot = SHOW_MASKED_PLOT, responses_to_return_corr = RESPONSES_TO_RETURN_CORR, set_returned_limit = SET_RETURNED_LIMIT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obtaining scatter plots and simple linear regressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3c9ebe5e-117c-42bb-ae7d-38c206463e1f",
    "tags": [
     "CELL_8"
    ]
   },
   "source": [
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "X1 = DATASET['X1']\n",
    "#Alternatively: None; or other column in quotes, substituting 'X1'\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "SHOW_LINEAR_REG = True\n",
    "#Alternatively: set SHOW_LINEAR_REG = True to plot the linear regressions graphics and show \n",
    "# the linear regressions calculated for each pair Y x X (i.e., each correlation \n",
    "# Y = aX + b, as well as the R² coefficient calculated). \n",
    "# Set SHOW_LINEAR_REG = False to omit both the linear regressions plots on the graphic, and\n",
    "# the correlations and R² coefficients obtained.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframe saved as lin_reg_summary. Simply modify this object on the left of equality:\n",
    "lin_reg_summary = scatter_plot_lin_reg (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_linear_reg = SHOW_LINEAR_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "source": [
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#X1 = dataset.index to use the index as the axis itself\n",
    "X1 = (DATASET['DATE']).astype('datetime64[D]') \n",
    "#Alternatively: None; or other column in quotes, substituting 'DATE'\n",
    "# WARNING: Modify only the object in the first parenthesis: DATASET['DATE']\n",
    "# Do not modify the method .astype('datetime64[D]')\n",
    "#Remove .astype('datetime64[D]') if it is not a datetime.\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "#Notice that there is a data transforming step to guarantee that the 'DATE' was interpreted as a timestamp, not as object or string.\n",
    "#The astype method defines the type of variable as 'datetime64[D]'. If we wanted the timestamps to be resolved in seconds, we should use\n",
    "# 'datetime64[ns]'.\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "ADD_SCATTER_DOTS = False #Alternatively: True or False\n",
    "# If ADD_SCATTER_DOTS = False, the dots (scatter plot) are omitted, so only the lines\n",
    "# correspondent to the series are shown.\n",
    "\n",
    "# Notice that adding the dots and omitting the spline lines is equivalent to obtain a\n",
    "# scatter plot. If you want to do so, consider using the scatter_plot_lin_reg function, \n",
    "# capable of calculating the linear regressions.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "time_series_vis (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: automatically calculate the ideal histogram bin size\n",
    "- The ideal bin interval is calculated through Montgomery's method. Histogram is obtained from this calculated bin size.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "744224c5-7105-4256-a568-6c5443e28cae",
    "tags": [
     "CELL_9"
    ]
   },
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ANALYZED_VARIABLE = DATASET['analyzed_variable']\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# ANALYZED_VARIABLE = DATASET['column1']\n",
    "\n",
    "SET_GRAPHIC_BAR_WIDTH = 2.0\n",
    "# This parameter must be visually adjusted for each particular analyzed variable.\n",
    "# Manually set this parameter until you see only a minimal separation between successive\n",
    "# bars (i.e., you know that the bars are not overlapping, but they are not so distant that\n",
    "# the statistic distribution profile is not clear).\n",
    "# You can input any numeric value, and the order of magnitude will vary depending on the\n",
    "# dispersion and on the width of the sample space.\n",
    "# e.g. SET_GRAPHIC_BAR_WIDTH = 3; SET_GRAPHIC_BAR_WIDTH = 0.003\n",
    "\n",
    "X_AXIS_ROTATION = 70 \n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "DATA_UNITS_LABEL = None\n",
    "# Input a string inside quotes for setting a label for the X-axis, that will represent\n",
    "# The type of data that the histogram is evaluating, i.e., what the statistic distribution\n",
    "# shown by the histogram means.\n",
    "# e.g. if DATA_UNITS_LABEL = \"particle_diameter_in_nm\", the axis X will be labelled with\n",
    "# this string. Then, we can know that the diagram represents the distribution (counts of\n",
    "# data for each defined bin) of particles diameters.\n",
    "\n",
    "Y_TITLE = None\n",
    "#Alternatively: string inside quotes for vertical title. e.g. Y_TITLE = \"Analyzed_values\".\n",
    "\n",
    "HISTOGRAM_TITLE = None\n",
    "#Alternatively: string inside quotes for graphic title. e.g. \n",
    "# HISTOGRAM_TITLE = \"Analyzed_values_histogram\".\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframes saved as general_statistics and frequency_tab.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics, frequency_tab = histogram (y = ANALYZED_VARIABLE, bar_width = SET_GRAPHIC_BAR_WIDTH, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, normal_curve_overlay = NORMAL_CURVE_OVERLAY, data_units_label = DATA_UNITS_LABEL, y_title = Y_TITLE, histogram_title = HISTOGRAM_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: set number of bins\n",
    "- Use this one if the distance between data is too small, or if the histogram function did not return a valid histogram.\n",
    "- Here, the histogram is obtained by manually defining the total of bins (i.e., into how much intervals the sample space should be divided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ANALYZED_VARIABLE = DATASET['analyzed_variable']\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# ANALYZED_VARIABLE = DATASET['column1']\n",
    "\n",
    "TOTAL_OF_BINS = 50\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided. Check comments after the histogram_alternative\n",
    "# function call).\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "\n",
    "SET_GRAPHIC_BAR_WIDTH = 2.0\n",
    "# This parameter must be visually adjusted for each particular analyzed variable.\n",
    "# Manually set this parameter until you see only a minimal separation between successive\n",
    "# bars (i.e., you know that the bars are not overlapping, but they are not so distant that\n",
    "# the statistic distribution profile is not clear).\n",
    "# You can input any numeric value, and the order of magnitude will vary depending on the\n",
    "# dispersion and on the width of the sample space.\n",
    "# e.g. SET_GRAPHIC_BAR_WIDTH = 3; SET_GRAPHIC_BAR_WIDTH = 0.003\n",
    "\n",
    "X_AXIS_ROTATION = 70 \n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "DATA_UNITS_LABEL = None\n",
    "# Input a string inside quotes for setting a label for the X-axis, that will represent\n",
    "# The type of data that the histogram is evaluating, i.e., what the statistic distribution\n",
    "# shown by the histogram means.\n",
    "# e.g. if DATA_UNITS_LABEL = \"particle_diameter_in_nm\", the axis X will be labelled with\n",
    "# this string. Then, we can know that the diagram represents the distribution (counts of\n",
    "# data for each defined bin) of particles diameters.\n",
    "\n",
    "Y_TITLE = None\n",
    "#Alternatively: string inside quotes for vertical title. e.g. Y_TITLE = \"Analyzed_values\".\n",
    "\n",
    "HISTOGRAM_TITLE = None\n",
    "#Alternatively: string inside quotes for graphic title. e.g. \n",
    "# HISTOGRAM_TITLE = \"Analyzed_values_histogram\".\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram_alternative.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframes saved as general_statistics and frequency_tab.\n",
    "# Simply modify this object on the left of equality:\n",
    "general_statistics, frequency_tab = histogram_alternative (y = ANALYZED_VARIABLE, total_of_bins = TOTAL_OF_BINS, bar_width = SET_GRAPHIC_BAR_WIDTH, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, data_units_label = DATA_UNITS_LABEL, y_title = Y_TITLE, histogram_title = HISTOGRAM_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "# Suppose we registered the values corresponding to a given feature / property / attribute\n",
    "# in column Y, and we want to know the Y statistic distribution. the maximum value observed\n",
    "# for Y is named Ymax, whereas the minimum value observed is Ymin.\n",
    "# Therefore, our sample space ranges from Ymin to Ymax. Now, we divide this sample space into\n",
    "# equally-separated intervals, named bins. The width of each bin is the bin_size. The 1st bin\n",
    "# corresponds to the interval Ymin to (Ymin + bin_size), where we can call (Ymin + bin_size)\n",
    "# = Y1. Then, the 2nd interval ranges from Y1 to (Y1 + bin_size),..., until we get to the\n",
    "# last interval, where we find Ymax.\n",
    "# Now, we count how many values of Y belong to each bin. The graphic of count of values in\n",
    "# each bin x the bin interval (or the value correspondent to the half of the bin) is the\n",
    "# histogram. For the first bin this mid-value would be Ymin + bin_size/2, since this value is\n",
    "# exactly in the middle of interval Ymin to (Ymin + bin_size).\n",
    "\n",
    "# In other words we can imagine that each Y value was print on the surface of a ball, and\n",
    "# each bin is a bucket labelled Ymin - (Ymin + bin_size), (Ymin + bin_size) - \n",
    "# (Ymin + 2bin_size), untill we cover the Ymax value. We put every ball inside the bucket,\n",
    "# given that the ball value must be in the interval labelling the bucket. Finally, we count\n",
    "# the balls per bucket, and plot count of balls x (the middle value of the interval \n",
    "# labelling the correspondent bucket). This graphic will be the histogram and will \n",
    "# represent the statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Testing data normality and visualizing probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "Y = DATASET['Y'] \n",
    "#Alternatively: other column in quotes, substituting 'Y'\n",
    "# e.g. Y = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "ALPHA = 0.10\n",
    "# Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "# Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "# results.\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "#New dataframe saved as data_normality_res\n",
    "# Skewness kurtosis and general statistics dictionary returned as general_statistics_dict\n",
    "# Simply modify these objects on the left of equality:\n",
    "data_normality_res, general_statistics_dict = test_data_normality (y = Y, alpha = ALPHA, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting the dataframe as CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all file extensions should be .csv for this function\n",
    "\n",
    "DATAFRAME_TO_BE_EXPORTED = dataset\n",
    "#Alternatively: object containing the dataset to be exported.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITH_CSV_EXTENSION = \"dataset.csv\"\n",
    "# NEW_FILE_NAME_WITH_CSV_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_CSV_EXTENSION = \"file.csv\"\n",
    "\n",
    "EXPORT_TO_S3_BUCKET = False\n",
    "# export_to_s3_bucket = False. Alternatively, set as True to export the file to an\n",
    "# AWS S3 Bucket.\n",
    "    \n",
    "## The following parameters have effect only when export_to_s3_bucket == True:\n",
    "\n",
    "S3_BUCKET_NAME = None    \n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION = None\n",
    "# The name desired for the object stored in S3 (string, in quotes). \n",
    "# Keep it None to set it equals to NEW_FILE_NAME_WITH_CSV_EXTENSION. \n",
    "# Alternatively, set it as a string analogous to NEW_FILE_NAME_WITH_CSV_EXTENSION.\n",
    "# e.g. DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION = \"S3_file.csv\"\n",
    "\n",
    "export_dataframe(dataframe_to_be_exported = DATAFRAME_TO_BE_EXPORTED, new_file_name_with_csv_extension = NEW_FILE_NAME_WITH_CSV_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH, export_to_s3_bucket = EXPORT_TO_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, desired_s3_file_name_with_csv_extension = DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "# SOURCE = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "\n",
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to AWS S3 or to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "OBJECT_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# OBJECT_TO_DOWNLOAD_FROM_COLAB = None. This option has effect only when\n",
    "# SOURCE == 'google'. In this case, this parameter is obbligatory. \n",
    "# Declare as OBJECT_TO_DOWNLOAD_FROM_COLAB the object that you want to download.\n",
    "# Since it is an object and not a string, it should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, OBJECT_TO_DOWNLOAD_FROM_COLAB = dict.\n",
    "# To download a dataframe named df, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = df.\n",
    "# To export a model named keras_model, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = keras_model\n",
    "    \n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "\n",
    "S3_BUCKET_NAME = None\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "LOCAL_PATH_OF_STORAGE = '/'\n",
    "# LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "# to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "# path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "# If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "# will be imported to the root path. Alternatively, input the path as a string (in quotes). \n",
    "# Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "# LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "# Notice that only the directories should be declared: do not include the file name and\n",
    "# its extension.\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = None\n",
    "# FILE_NAME_WITH_EXTENSION: string, in quotes, containing the file name which will be\n",
    "# downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "## This parameter is obbligatory when SOURCE == 'aws'\n",
    "# Examples:\n",
    "# FILE_NAME_WITH_EXTENSION = 'Screen_Shot.png'; FILE_NAME_WITH_EXTENSION = 'dataset.csv',\n",
    "# FILE_NAME_WITH_EXTENSION = \"dictionary.pkl\", FILE_NAME_WITH_EXTENSION = \"model.h5\",\n",
    "# FILE_NAME_WITH_EXTENSION = 'doc.pdf', FILE_NAME_WITH_EXTENSION = 'model.dill'\n",
    "\n",
    "download_or_upload_file (source = SOURCE, action = ACTION, object_to_download_from_colab = OBJECT_TO_DOWNLOAD_FROM_COLAB, s3_bucket_name = S3_BUCKET_NAME, local_path_of_storage = LOCAL_PATH_OF_STORAGE, file_name_with_extension = FILE_NAME_WITH_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **One-Hot Encoding - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "df88ce83-988c-46ed-9087-6e21278668ec",
    "id": "2q8SGc6WigDt"
   },
   "source": [
    "If there are **categorical features**, they should be converted into numerical variables for being processed by the machine learning algorithms.\n",
    "\n",
    "\\- We can assign integer values for each one of the categories. This works well for situations where there is a scale or order for the assignment of the variables (e.g., if there is a satisfaction grade).\n",
    "\n",
    "\\- On the other hand, the results may be compromised if there is no order. That is because the ML algorithms assume that, if two categories have close numbers, then the categories are similar, what is not necessarily true. There are cases where the categories have no relation with each other.\n",
    "\n",
    "\\- In these cases, the best strategy is the One-Hot Encoding. For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "\n",
    "\\- Naturally, the number of columns grow with the number of possible labels. The One-Hot Encoder from Sklearn creates a Scipy Sparse matrix that stores the position of the zeros in the dataset. Then, the computational cost is reduced due to the fact that we are not storing a huge amount of null values.\n",
    "\n",
    "\\- Since each column is a binary variable of the type \"is classified in this category or not\", we expect that the created columns contain more zeros than 1s. That is because if an element belongs to one category (= 1), it does not belong to the others, so its value is zero for all other columns."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
