{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8e0cea1f-1e08-418e-b2e0-53178e19d71a"
   },
   "source": [
    "# **Dataset Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6689b182-e386-4ad3-a19e-215c07e2f6c1"
   },
   "source": [
    "## _ETL Workflow Notebook 3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f04f7076-17fb-41c1-94e9-348059de0fce"
   },
   "source": [
    "## Content:\n",
    "1. Removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type;\n",
    "2. Capitalizing or lowering case of string variables (string homogenizing);\n",
    "3. Adding contractions to the contractions library;\n",
    "4. Correcting contracted strings;\n",
    "5. Substituting (replacing) substrings on string variables;\n",
    "6. Inverting the order of the string characters;\n",
    "7. Slicing the strings;\n",
    "8. Getting the leftest characters from the strings (retrieve last characters);\n",
    "9. Getting the rightest characters from the strings (retrieve first characters);\n",
    "10. Joining strings from a same column into a single string;\n",
    "11. Joining several string columns into a single string column;\n",
    "12. Splitting strings into a list of strings;\n",
    "13. Substituting (replacing or switching) whole strings by different text values (on string variables);\n",
    "14. Replacing strings with Machine Learning: finding similar strings and replacing them by standard strings;\n",
    "15. Searching for Regular Expression (RegEx) within a string column;\n",
    "16. Replacing a Regular Expression (RegEx) from a string column;\n",
    "17. Applying Fast Fourier Transform;\n",
    "18. Generating columns with frequency information;\n",
    "19. Transforming the dataset and reverse transforms: log-transform; \n",
    "20. Exponential transform; \n",
    "21. Box-Cox transform;\n",
    "22. Square-root transform;\n",
    "23. Cube-root transform;\n",
    "24. General power transform;\n",
    "25. One-Hot Encoding;\n",
    "26. Ordinal Encoding;\n",
    "27. Feature scaling; \n",
    "28. Importing or exporting models and dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "azdata_cell_guid": "fec5846c-2b1d-4a60-b6fc-554077ea8582",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Import all needed functions and classes with original names, with no aliases:\n",
    "from idsw import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "81613f50-c3d7-4840-b8c4-e82fc7bb9e53",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9dbf7189-4ba9-4ffa-88a2-965699df42b9",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "# Also, html files and webpages may be also read.\n",
    "\n",
    "# You may input the path for an HTML file containing a table to be read; or \n",
    "# a string containing the address for a webpage containing the table. The address must start\n",
    "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
    "# or as FILE_NAME_WITH_EXTENSION.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e8d2a160-c74b-4d65-b7c2-e50e6f7f32c0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "    \n",
    "METHOD = 'trim'\n",
    "# METHOD = 'trim' will eliminate trailing and leading white spaces from the strings in\n",
    "# COLUMN_TO_ANALYZE.\n",
    "# METHOD = 'substring' will eliminate a defined trailing and leading substring from\n",
    "# COLUMN_TO_ANALYZE.\n",
    "\n",
    "SUBSTRING_TO_ELIMINATE = None\n",
    "# SUBSTRING_TO_ELIMINATE = None. Set as a string (in quotes) if METHOD = 'substring'.\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains time information: each string ends in \" min\":\n",
    "# \"1 min\", \"2 min\", \"3 min\", etc. If SUBSTRING_TO_ELIMINATE = \" min\", this portion will be\n",
    "# eliminated, resulting in: \"1\", \"2\", \"3\", etc. If NEW_VARIABLE_TYPE = None, these values will\n",
    "# continue to be strings. By setting NEW_VARIABLE_TYPE = 'int' or 'float', the series will be\n",
    "# converted to a numeric type.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_trim'\n",
    "# NEW_COLUMN_SUFFIX = \"_trim\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_trim\", the new column will be named as\n",
    "# \"column1_trim\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = trim_spaces_or_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, new_variable_type = NEW_VARIABLE_TYPE, method = METHOD, substring_to_eliminate = SUBSTRING_TO_ELIMINATE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "94411320-e6a4-49b2-aa70-ee39a3630a6b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Capitalizing or lowering case of string variables (string homogenizing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "47e1604b-6bbf-43fe-9e4b-a4e60f2802a5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "METHOD = 'lowercase'\n",
    "# METHOD = 'capitalize' will capitalize all letters from the input string \n",
    "# (turn them to upper case).\n",
    "# METHOD = 'lowercase' will make the opposite: turn all letters to lower case.\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains strings such as 'String One', 'STRING 2',  and\n",
    "# 'string3'. If METHOD = 'capitalize', the output will contain the strings: \n",
    "# 'STRING ONE', 'STRING 2', 'STRING3'. If METHOD = 'lowercase', the outputs will be:\n",
    "# 'string one', 'string 2', 'string3'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_homogenized'\n",
    "# NEW_COLUMN_SUFFIX = \"_homogenized\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_homogenized\", the new column will be named as\n",
    "# \"column1_homogenized\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = capitalize_or_lower_string_case (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, method = METHOD, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Adding contractions to the contractions library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_CONTRACTIONS = [\n",
    "    \n",
    "    {'contracted_expression': None, 'correct_expression': None}, \n",
    "    {'contracted_expression': None, 'correct_expression': None}, \n",
    "    {'contracted_expression': None, 'correct_expression': None}, \n",
    "    {'contracted_expression': None, 'correct_expression': None}\n",
    "\n",
    "]\n",
    "# LIST_OF_CONTRACTIONS = [{'contracted_expression': None, 'correct_expression': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the form as the contraction is usually observed; and the second one \n",
    "# contains the correct (full) string that will replace it.\n",
    "# Since contractions can cause issues when processing text, we can expand them with these functions.\n",
    "        \n",
    "# The object list_of_contractions must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'contracted_expression' for the contraction; and 'correct_expression', \n",
    "# for the strings with the correspondent correction.\n",
    "        \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you want to add more elements\n",
    "# to the contractions library.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'contracted_expression': original_str, 'correct_expression': new_str}, \n",
    "# where original_str and new_str represent the contracted and expanded strings\n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "        \n",
    "# Example:\n",
    "# LIST_OF_CONTRACTIONS = [{'contracted_expression': 'mychange', 'correct_expression': 'my change'}]\n",
    "        \n",
    "\n",
    "add_contractions_to_library (list_of_contractions = LIST_OF_CONTRACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Correcting contracted strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_contractionsFixed'\n",
    "# NEW_COLUMN_SUFFIX = \"_contractionsFixed\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_contractionsFixed\", the new column will be named as\n",
    "# \"column1_contractionsFixed\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = correct_contracted_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Substituting (replacing) substrings on string variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = None\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = ''\n",
    "# SUBSTRING_TO_BE_REPLACED = None; new_substring_for_replacement = ''. \n",
    "# Strings (in quotes): when the sequence of characters SUBSTRING_TO_BE_REPLACED was\n",
    "# found in the strings from column_to_analyze, it will be substituted by the substring\n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT. If None is provided to one of these substring arguments,\n",
    "# it will be substituted by the empty string: ''\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains the following strings, with a spelling error:\n",
    "# \"my collumn 1\", 'his collumn 2', 'her column 3'. We may correct this error by setting:\n",
    "# SUBSTRING_TO_BE_REPLACED = 'collumn' and NEW_SUBSTRING_FOR_REPLACEMENT = 'column'. The\n",
    "# function will search for the wrong group of characters and, if it finds it, will substitute\n",
    "# by the correct sequence: \"my column 1\", 'his column 2', 'her column 3'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_substringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_substringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_substringReplaced\", the new column will be named as\n",
    "# \"column1_substringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = replace_substring (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Inverting the order of the string characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringInverted'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringInverted\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringInverted\", the new column will be named as\n",
    "# \"column1_stringInverted\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = invert_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Slicing the strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "FIRST_CHARACTER_INDEX = None\n",
    "# FIRST_CHARACTER_INDEX = None - integer representing the index of the first character to be\n",
    "# included in the new strings. If None, slicing will start from first character.\n",
    "# Indexing of strings always start from 0. The last index can be represented as -1, the index of\n",
    "# the character before as -2, etc (inverse indexing starts from -1).\n",
    "# example: consider the string \"idsw\", which contains 4 characters. We can represent the indices as:\n",
    "# 'i': index 0; 'd': 1, 's': 2, 'w': 3. Alternatively: 'w': -1, 's': -2, 'd': -3, 'i': -4.\n",
    "\n",
    "LAST_CHARACTER_INDEX = None\n",
    "# LAST_CHARACTER_INDEX = None - integer representing the index of the last character to be\n",
    "# included in the new strings. If None, slicing will go until the last character.\n",
    "# Attention: this is effectively the last character to be added, and not the next index after last\n",
    "# character.\n",
    "        \n",
    "# in the 'idsw' example, if we want a string as 'ds', we want the FIRST_CHARACTER_INDEX = 1 and\n",
    "# LAST_CHARACTER_INDEX = 2.\n",
    "\n",
    "STEP = 1\n",
    "# STEP = 1 - integer representing the slicing step. If step = 1, all characters will be added.\n",
    "# If STEP = 2, then the slicing will pick one element of index i and the element with index (i+2)\n",
    "# (1 index will be 'jumped'), and so on.\n",
    "# If STEP is negative, then the order of the new strings will be inverted.\n",
    "# Example: STEP = -1, and the start and finish indices are None: the output will be the inverted\n",
    "# string, 'wsdi'.\n",
    "# FIRST_CHARACTER_INDEX = 1, LAST_CHARACTER_INDEX = 2, STEP = 1: output = 'ds';\n",
    "# FIRST_CHARACTER_INDEX = None, LAST_CHARACTER_INDEX = None, STEP = 2: output = 'is';\n",
    "# FIRST_CHARACTER_INDEX = None, LAST_CHARACTER_INDEX = None, STEP = 3: output = 'iw';\n",
    "# FIRST_CHARACTER_INDEX = -1, LAST_CHARACTER_INDEX = -2, STEP = -1: output = 'ws';\n",
    "# FIRST_CHARACTER_INDEX = -1, LAST_CHARACTER_INDEX = None, STEP = -2: output = 'wd';\n",
    "# FIRST_CHARACTER_INDEX = -1, LAST_CHARACTER_INDEX = None, STEP = 1: output = 'w'\n",
    "# In this last example, the function tries to access the next element after the character of index\n",
    "# -1. Since -1 is the last character, there are no other characters to be added.\n",
    "# FIRST_CHARACTER_INDEX = -2, LAST_CHARACTER_INDEX = -1, STEP = 1: output = 'sw'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_slicedString'\n",
    "# NEW_COLUMN_SUFFIX = \"_slicedString\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_slicedString\", the new column will be named as\n",
    "# \"column1_slicedString\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = slice_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, first_character_index = FIRST_CHARACTER_INDEX, last_character_index = LAST_CHARACTER_INDEX, step = STEP, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Getting the leftest characters from the strings (retrieve last characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - integer representing the total of characters that will\n",
    "# be retrieved. Here, we will retrieve the leftest characters. If NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1,\n",
    "# only the leftest (last) character will be retrieved.\n",
    "# Consider the string 'idsw'.\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - output: 'w';\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 2 - output: 'sw'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "# So, if the last part of the strings is a number, you can use this argument to directly extract\n",
    "# this part as numeric variable.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_leftChars'\n",
    "# NEW_COLUMN_SUFFIX = \"_leftChars\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_leftChars\", the new column will be named as\n",
    "# \"column1_leftChars\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = left_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, number_of_characters_to_retrieve = NUMBER_OF_CHARACTERS_TO_RETRIEVE, new_variable_type = NEW_VARIABLE_TYPE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Getting the rightest characters from the strings (retrieve first characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - integer representing the total of characters that will\n",
    "# be retrieved. Here, we will retrieve the rightest characters. If NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1,\n",
    "# only the rightest (first) character will be retrieved.\n",
    "# Consider the string 'idsw'.\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - output: 'i';\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 2 - output: 'id'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "# So, if the first part of the strings is a number, you can use this argument to directly extract\n",
    "# this part as numeric variable.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_rightChars'\n",
    "# NEW_COLUMN_SUFFIX = \"_rightChars\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_rightChars\", the new column will be named as\n",
    "# \"column1_rightChars\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = right_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, number_of_characters_to_retrieve = NUMBER_OF_CHARACTERS_TO_RETRIEVE, new_variable_type = NEW_VARIABLE_TYPE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Joining strings from a same column into a single string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SEPARATOR = \" \"\n",
    "# SEPARATOR = \" \" - string containing the separator. Suppose the column contains the\n",
    "# strings: 'a', 'b', 'c', 'd'. If the SEPARATOR is the empty string '', the output will be:\n",
    "# 'abcd' (no separation). If SEPARATOR = \" \" (simple whitespace), the output will be 'a b c d'\n",
    "\n",
    "\n",
    "# The returned string is stored as concat_string:\n",
    "# Simply modify this variable name on the left of equality:\n",
    "concat_string = join_strings_from_column (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, separator = SEPARATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Joining several string columns into a single string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_JOIN = ['column1', 'column2']\n",
    "# LIST_OF_COLUMNS_TO_JOIN: list of strings (inside quotes), \n",
    "# containing the name of the columns with strings to be joined.\n",
    "# Attention: the strings will be joined row by row, i.e. only strings in the same rows will\n",
    "# be concatenated. To join strings from the same column, use function join_strings_from_column\n",
    "# e.g. LIST_OF_COLUMNS_TO_JOIN = [\"column1\", \"column2\"] will join strings from \"column1\" with\n",
    "# the correspondent strings from \"column2\".\n",
    "# Notice that you can concatenate any kind of columns: numeric, dates, texts ,..., but the output\n",
    "# will be a string column.\n",
    "\n",
    "SEPARATOR = \" \"\n",
    "# SEPARATOR = \" \" - string containing the separator. Suppose the column contains the\n",
    "# strings: 'a', 'b', 'c', 'd'. If the SEPARATOR is the empty string '', the output will be:\n",
    "# 'abcd' (no separation). If SEPARATOR = \" \" (simple whitespace), the output will be 'a b c d'\n",
    "\n",
    "NEW_COLUMN_SUFFIX = '_stringConcat'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringConcat\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringConcat\", the new column will be named as\n",
    "# \"column1_stringConcat\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = join_string_columns (df = DATASET, list_of_columns_to_join = LIST_OF_COLUMNS_TO_JOIN, separator = SEPARATOR, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Splitting strings into a list of strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SEPARATOR = \" \"\n",
    "# SEPARATOR = \" \" - string containing the separator. Suppose the column contains the\n",
    "# string: 'a b c d' on a given row. If the SEPARATOR is whitespace ' ', \n",
    "# the output will be a list: ['a', 'b', 'c', 'd']: the function splits the string into a list\n",
    "# of strings (one list per row) every time it finds the separator.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringSplitted'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringSplitted\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringSplitted\", the new column will be named as\n",
    "# \"column1_stringSplitted\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = split_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, separator = SEPARATOR, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a4c5fc1f-f93d-4f21-8fe3-d74c7658cb9b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Substituting (replacing or switching) whole strings by different text values (on string variables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f91631b5-37db-4bd2-a72d-256859c9bd67",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = [\n",
    "    \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = \n",
    "# [{'original_string': None, 'new_string': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original string; and the second one contains the new string\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'original_string', and search these values on the strings\n",
    "# in COLUMN_TO_ANALYZE. When the value is found, it will be replaced (switched) by the correspondent\n",
    "# value in key 'new_string'.\n",
    "    \n",
    "# The object LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'original_string' for the original strings to search on the column \n",
    "# column_to_analyze; and 'new_string', for the strings that will replace the original ones.\n",
    "# Notice that this function will not search for substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_to_analyze' and 'original_string'.\n",
    "# So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'original_string': original_str, 'new_string': new_str}, \n",
    "# where original_str and new_str represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    \n",
    "# Example:\n",
    "# Suppose the COLUMN_TO_ANALYZE contains the values 'sunday', 'monday', 'tuesday', 'wednesday',\n",
    "# 'thursday', 'friday', 'saturday', but you want to obtain data labelled as 'weekend' or 'weekday'.\n",
    "# Set: LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = \n",
    "# [{'original_string': 'sunday', 'new_string': 'weekend'},\n",
    "# {'original_string': 'saturday', 'new_string': 'weekend'},\n",
    "# {'original_string': 'monday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'tuesday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'wednesday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'thursday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'friday', 'new_string': 'weekday'}]\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "# \"column1_stringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = switch_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, list_of_dictionaries_with_original_strings_and_replacements = LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "dd1801c2-ceef-441f-883a-9900a9f5dbb0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Replacing strings with Machine Learning: finding similar strings and replacing them by standard strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1f7e4731-aaba-4ea4-8760-665522f2bb08",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "MODE = 'find_and_replace'\n",
    "# MODE = 'find_and_replace' will find similar strings; and switch them by one of the\n",
    "# standard strings if the similarity between them is higher than or equals to the threshold.\n",
    "# Alternatively: MODE = 'find' will only find the similar strings by calculating the similarity.\n",
    "\n",
    "THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0\n",
    "# THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0 - 0.0% means no similarity and 100% means equal strings.\n",
    "# The THRESHOLD_FOR_PERCENT_OF_SIMILARITY is the minimum similarity calculated from the\n",
    "# Levenshtein (minimum edit) distance algorithm. This distance represents the minimum number of\n",
    "# insertion, substitution or deletion of characters operations that are needed for making two\n",
    "# strings equal.\n",
    "\n",
    "LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT = [\n",
    "    \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}\n",
    "    \n",
    "]\n",
    "# This is a list of dictionaries, where each dictionary contains a single key-value pair:\n",
    "# the key must be always 'standard_string', and the value will be one of the standard strings \n",
    "# for replacement: if a given string on the COLUMN_TO_ANALYZE presents a similarity with one \n",
    "# of the standard string equals or higher than the THRESHOLD_FOR_PERCENT_OF_SIMILARITY, it will be\n",
    "# substituted by this standard string.\n",
    "# For instance, suppose you have a word written in too many ways, making it difficult to use\n",
    "# the function switch_strings: \"EU\" , \"eur\" , \"Europ\" , \"Europa\" , \"Erope\" , \"Evropa\" ...\n",
    "# You can use this function to search strings similar to \"Europe\" and replace them.\n",
    "    \n",
    "# The function will loop through all dictionaries in this list, access the values of the keys \n",
    "# 'standard_string', and search these values on the strings in COLUMN_TO_ANALYZE. When the value \n",
    "# is found, it will be replaced (switched) if the similarity is sufficiently high.\n",
    "    \n",
    "# The object LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'standard_string'.\n",
    "# Notice that this function performs fuzzy matching, so it MAY SEARCH substrings and strings\n",
    "# written with different cases (upper or lower) when this portions or modifications make the\n",
    "# strings sufficiently similar to each other.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same key: {'standard_string': other_std_str}, \n",
    "# where other_std_str represents the string for searching and replacement \n",
    "# (If the key contains None, the new dictionary will be ignored).\n",
    "    \n",
    "# Example:\n",
    "# Suppose the COLUMN_TO_ANALYZE contains the values 'California', 'Cali', 'Calefornia', \n",
    "# 'Calefornie', 'Californie', 'Calfornia', 'Calefernia', 'New York', 'New York City', \n",
    "# but you want to obtain data labelled as the state 'California' or 'New York'.\n",
    "# Set: list_of_dictionaries_with_standard_strings_for_replacement = \n",
    "# [{'standard_string': 'California'},\n",
    "# {'standard_string': 'New York'}]\n",
    "    \n",
    "# ATTENTION: It is advisable for previously searching the similarity to find the best similarity\n",
    "# threshold; set it as high as possible, avoiding incorrect substitutions in a gray area; and then\n",
    "# perform the replacement. It will avoid the repetition of original incorrect strings in the\n",
    "# output dataset, as well as wrong replacement (replacement by one of the standard strings which\n",
    "# is not the correct one).\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "# \"column1_stringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset.\n",
    "# The summary list is saved as summary_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "transf_dataset, summary_list = string_replacement_ml (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, mode = MODE, threshold_for_percent_of_similarity = THRESHOLD_FOR_PERCENT_OF_SIMILARITY, list_of_dictionaries_with_standard_strings_for_replacement = LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Searching for Regular Expression (RegEx) within a string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "REGEX_TO_SEARCH = r\"\"\n",
    "# REGEX_TO_SEARCH = r\"\" - string containing the regular expression (regex) that will be searched\n",
    "# within each string from the column. Declare it with the r before quotes, indicating that the\n",
    "# 'raw' string should be read. That is because the regex contain special characters, such as \\,\n",
    "# which should not be read as scape characters.\n",
    "# example of regex: r'st\\d\\s\\w{3,10}'\n",
    "# Use the regex helper to check: basic theory and most common metacharacters; regex quantifiers;\n",
    "# regex anchoring and finding; regex greedy and non-greedy search; regex grouping and capturing;\n",
    "# regex alternating and non-capturing groups; regex backreferences; and regex lookaround.\n",
    "\n",
    "## ATTENTION: This function returns ONLY the capturing groups from the regex, i.e., portions of the\n",
    "# regex explicitly marked with parentheses (check the regex helper for more details, including how\n",
    "# to convert parentheses into non-capturing groups). If no groups are marked as capturing, the\n",
    "# function will raise an error.\n",
    "\n",
    "SHOW_REGEX_HELPER = False\n",
    "# SHOW_REGEX_HELPER: set SHOW_REGEX_HELPER = True to show a helper guide to the construction of\n",
    "# the regular expression. After finishing the helper, the original dataset itself will be returned\n",
    "# and the function will not proceed. Use it in case of not knowing or not certain on how to input\n",
    "# the regex.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_regex'\n",
    "# NEW_COLUMN_SUFFIX = \"_regex\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_regex\", the new column will be named as\n",
    "# \"column1_regex\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = regex_search (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, regex_to_search = REGEX_TO_SEARCH, show_regex_helper = SHOW_REGEX_HELPER, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Replacing a Regular Expression (RegEx) from a string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "REGEX_TO_SEARCH = r\"\"\n",
    "# REGEX_TO_SEARCH = r\"\" - string containing the regular expression (regex) that will be searched\n",
    "# within each string from the column. Declare it with the r before quotes, indicating that the\n",
    "# 'raw' string should be read. That is because the regex contain special characters, such as \\,\n",
    "# which should not be read as scape characters.\n",
    "# example of regex: r'st\\d\\s\\w{3,10}'\n",
    "# Use the regex helper to check: basic theory and most common metacharacters; regex quantifiers;\n",
    "# regex anchoring and finding; regex greedy and non-greedy search; regex grouping and capturing;\n",
    "# regex alternating and non-capturing groups; regex backreferences; and regex lookaround.\n",
    "\n",
    "STRING_FOR_REPLACEMENT = \"\"\n",
    "# STRING_FOR_REPLACEMENT = \"\" - regular string that will replace the REGEX_TO_SEARCH: \n",
    "# whenever REGEX_TO_SEARCH is found in the string, it is replaced (substituted) by \n",
    "# STRING_FOR_REPLACEMENT. \n",
    "# Example STRING_FOR_REPLACEMENT = \" \" (whitespace).\n",
    "# If STRING_FOR_REPLACEMENT = None, the empty string will be used for replacement.\n",
    "        \n",
    "## ATTENTION: This function process a single regex by call.\n",
    "\n",
    "SHOW_REGEX_HELPER = False\n",
    "# SHOW_REGEX_HELPER: set SHOW_REGEX_HELPER = True to show a helper guide to the construction of\n",
    "# the regular expression. After finishing the helper, the original dataset itself will be returned\n",
    "# and the function will not proceed. Use it in case of not knowing or not certain on how to input\n",
    "# the regex.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_regex'\n",
    "# NEW_COLUMN_SUFFIX = \"_regex\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_regex\", the new column will be named as\n",
    "# \"column1_regex\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = regex_replacement (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, regex_to_search = REGEX_TO_SEARCH, string_for_replacement = STRING_FOR_REPLACEMENT, show_regex_helper = SHOW_REGEX_HELPER, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Applying Fast Fourier Transform**\n",
    "- Determine which frequencies are important by extracting features with <a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\" class=\"external\">Fast Fourier Transform</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "AVERAGE_FREQUENCY_OF_DATA_COLLECTION = 'hour'\n",
    "# AVERAGE_FREQUENCY_OF_DATA_COLLECTION = 'hour' or 'h' for hours; 'day' or 'd' for days;\n",
    "# 'minute' or 'min' for minutes; 'seconds' or 's' for seconds; 'ms' for milliseconds; 'ns' for\n",
    "# nanoseconds; 'year' or 'y' for years; 'month' or 'm' for months.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'capability_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# The results of the Fast Fourier Transform will be stored in the object named fft.\n",
    "# fft_with_freq_df is a dataframe containing the absolute ffts \n",
    "# with the correspondent frequencies in counts per year.\n",
    "# Simply modify this object on the left of equality:\n",
    "fft, fft_with_freq_df = fast_fourier_transform (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, average_frequency_of_data_collection = AVERAGE_FREQUENCY_OF_DATA_COLLECTION, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Generating columns with frequency information**\n",
    "- This gives the model access to the most important frequency features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp\"\n",
    "# TIMESTAMP_TAG_COLUMN = None. string containing the name of the column with the timestamp. \n",
    "# If TIMESTAMP_TAG_COLUMN is None, the index will be used for testing different imputations.\n",
    "# be the time series reference. declare as a string under quotes. This is the column from \n",
    "# which we will extract the timestamps or values with temporal information. e.g.\n",
    "# TIMESTAMP_TAG_COLUMN = 'timestamp' will consider the column 'timestamp' a time column.\n",
    "\n",
    "IMPORTANT_FREQUENCIES = [{'value': 1, 'unit': 'day'}, \n",
    "                         {'value':1, 'unit': 'year'}]\n",
    "\n",
    "# IMPORTANT_FREQUENCIES = [{'value': 1, 'unit': 'day'}, {'value':1, 'unit': 'year'}]\n",
    "# List of dictionaries with the important frequencies to add to the model. You can remove dictionaries,\n",
    "# or add extra dictionaries. The dictionaries must have always the same keys, 'value' and 'unit'.\n",
    "# If the importante frequency is once a day, the value will be 1, and the unit will be 'day' or 'd'.\n",
    "# The possible units are: 'ns', 'ms', 'second' or 's', 'minute' or 'min', 'day' or 'd', 'month' or 'm',\n",
    "# 'year' or 'y'.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "MAX_NUMBER_OF_ENTRIES_TO_PLOT = None\n",
    "# MAX_NUMBER_OF_ENTRIES_TO_PLOT (integer or None): use this argument to limit the number of entries \n",
    "# to plot. If None, all the entries will be plot.\n",
    "# MAX_NUMBER_OF_ENTRIES_TO_PLOT = 25 will plot the first 25 entries, MAX_NUMBER_OF_ENTRIES_TO_PLOT =\n",
    "# 100 will plot the 100 first entries, and so on.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'capability_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# The dataset with new columns containing the frequency information will be stored as dataset.\n",
    "# A dictionary with new features information is returned as timestamp_dict\n",
    "# Simply modify these objects on the left of equality:\n",
    "dataset, timestamp_dict = get_frequency_features (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, important_frequencies = IMPORTANT_FREQUENCIES, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, max_number_of_entries_to_plot = MAX_NUMBER_OF_ENTRIES_TO_PLOT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **log-transforming the variables**\n",
    "- One curve derived from the normal is the log-normal.\n",
    "- If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "- A log normal curve resembles a normal, but with skewness (distortion); and kurtosis (long-tail).\n",
    "\n",
    "Applying the log is a methodology for **normalizing the variables**: the sample space gets shrinkled after the transformation, making the data more adequate for being processed by Machine Learning algorithms.\n",
    "- Preferentially apply the transformation to the whole dataset, so that all variables will be of same order of magnitude.\n",
    "- Obviously, it is not necessary for variables ranging from -100 to 100 in numerical value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8e7fef30-c0ba-4a46-9587-4a46b2240069",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "\n",
    "ADD_CONSTANT = False\n",
    "# ADD_CONSTANT = False - If True, the transformation log(x + C) where C is \n",
    "# a constant will be applied.\n",
    "CONSTANT_TO_ADD = 0\n",
    "# CONSTANT_TO_ADD = 0 - float number which will be added to each value that will be transformed.\n",
    "# Attention: if no constant is added, but there is a negative value, the minimum needed for making\n",
    "# every value positive will be added automatically. If the constant to add results in negative or\n",
    "# zero values, it will be also modified to make all values non-negative (condition for applying\n",
    "# log transform).\n",
    "\n",
    "NEW_COLUMNS_SUFFIX = \"_log\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"column1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "# New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "log_transf_df = log_transform (df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, add_constant = ADD_CONSTANT, constant_to_add = CONSTANT_TO_ADD, new_columns_suffix = NEW_COLUMNS_SUFFIX)\n",
    "\n",
    "# One curve derived from the normal is the log-normal.\n",
    "# If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "# A log normal curve resembles a normal, but with skewness (distortion); \n",
    "# and kurtosis (long-tail).\n",
    "\n",
    "# Applying the log is a methodology for normalizing the variables: \n",
    "# the sample space gets shrinkled after the transformation, making the data more \n",
    "# adequate for being processed by Machine Learning algorithms. Preferentially apply \n",
    "# the transformation to the whole dataset, so that all variables will be of same order \n",
    "# of magnitude.\n",
    "# Obviously, it is not necessary for variables ranging from -100 to 100 in numerical \n",
    "# value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "39ba65a0-cd7c-4123-ac37-c22a027f4149",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the log-transform - Exponentially transforming variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de7066bb-4dfc-46e8-9042-a475cd2cdf71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "ADDED_CONSTANT = 0\n",
    "# ADDED_CONSTANT: constant C added for making the transformation log(x + C). Check the\n",
    "# output of log transform function to verify if a constant was automatically added.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_log_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, added_constant = ADDED_CONSTANT, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b9724a7d-15c6-4472-a0de-a592cda99b0a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining and applying Box-Cox transform**\n",
    "- Transform a series of data into a series that resembles a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "50e549c9-b761-44bc-bdb2-35ffabf9eab2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "ADD_CONSTANT = False\n",
    "# ADD_CONSTANT = False - If True, the transformation log(x + C) where C is \n",
    "# a constant will be applied.\n",
    "CONSTANT_TO_ADD = 0\n",
    "# CONSTANT_TO_ADD = 0 - float number which will be added to each value that will be transformed.\n",
    "# Attention: if no constant is added, but there is a negative value, the minimum needed for making\n",
    "# every value positive will be added automatically. If the constant to add results in negative or\n",
    "# zero values, it will be also modified to make all values non-negative (condition for applying\n",
    "# log transform).\n",
    "\n",
    "\n",
    "MODE = 'calculate_and_apply'\n",
    "# Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "# transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "# To 'apply_only', lambda_box must be provided.\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_BoxCoxTransf'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "# identified as 'Y_BoxCoxTransf'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "SPECIFICATION_LIMITS = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "# specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "# If there are specification limits, input them in this dictionary. Do not modify the keys,\n",
    "# simply substitute None by the lower and/or the upper specification.\n",
    "# e.g. Suppose you have a tank that cannot have more than 10 L. So:\n",
    "# specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': 10}, there is only\n",
    "# an upper specification equals to 10 (do not add units);\n",
    "# Suppose a temperature cannot be lower than 10 ºC, but there is no upper specification. So,\n",
    "# specification_limits = {'lower_spec_lim': 10, 'upper_spec_lim': None}. Finally, suppose\n",
    "# a liquid which pH must be between 6.8 and 7.2:\n",
    "# specification_limits = {'lower_spec_lim': 6.8, 'upper_spec_lim': 7.2}\n",
    "\n",
    "#New dataframe saved as data_transformed_df; dictionary saved as data_sum_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "data_transformed_df, data_sum_dict = box_cox_transform (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, add_constant = ADD_CONSTANT, constant_to_add = CONSTANT_TO_ADD, mode = MODE, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX, specification_limits = SPECIFICATION_LIMITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "60059372-e5bc-4159-961d-e4b79ff0e889",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f6f1e23f-77d1-4194-b59c-2101fd604b93",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "ADDED_CONSTANT = 0\n",
    "# ADDED_CONSTANT: constant C added for making the transformation log(x + C). Check the\n",
    "# output of Box-Cox transform function to verify if a constant was automatically added.\n",
    "\n",
    "SUFFIX = '_ReversedBoxCox'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "# identified as 'Y_ReversedBoxCox'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as retransformed_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "retransformed_df = reverse_box_cox (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, lambda_boxcox = LAMBDA_BOXCOX, added_constant = ADDED_CONSTANT, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Square root-transforming the variables**\n",
    "- Another methodology for **normalizing the variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8e7fef30-c0ba-4a46-9587-4a46b2240069",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "\n",
    "ADD_CONSTANT = False\n",
    "# ADD_CONSTANT = False - If True, the transformation sqrt(x + C) where C is \n",
    "# a constant will be applied.\n",
    "CONSTANT_TO_ADD = 0\n",
    "# CONSTANT_TO_ADD = 0 - float number which will be added to each value that will be transformed.\n",
    "# Attention: if no constant is added, but there is a negative value, the minimum needed for making\n",
    "# every value positive will be added automatically. If the constant to add results in negative or\n",
    "# zero values, it will be also modified to make all values non-negative (condition for applying\n",
    "# square root).\n",
    "\n",
    "NEW_COLUMNS_SUFFIX = \"_sqrt\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"column1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "# New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "square_root_transf_df = square_root_transform (df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, add_constant = ADD_CONSTANT, constant_to_add = CONSTANT_TO_ADD, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "39ba65a0-cd7c-4123-ac37-c22a027f4149",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the square root-transform - Raising to 2nd power**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de7066bb-4dfc-46e8-9042-a475cd2cdf71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "ADDED_CONSTANT = 0\n",
    "# ADDED_CONSTANT: constant C added for making the transformation sqrt(x + C). Check the\n",
    "# output of sqrt transform function to verify if a constant was automatically added.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_square_root_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, added_constant = ADDED_CONSTANT, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Cube root-transforming the variables**\n",
    "- Another methodology for **normalizing the variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8e7fef30-c0ba-4a46-9587-4a46b2240069",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "\n",
    "ADD_CONSTANT = False\n",
    "# ADD_CONSTANT = False - If True, the transformation cbrt(x + C) \n",
    "# where C is a constant will be applied.\n",
    "CONSTANT_TO_ADD = 0\n",
    "# CONSTANT_TO_ADD = 0 = 0 - float number which will be added to each value that will be transformed.\n",
    "        \n",
    "NEW_COLUMNS_SUFFIX = \"_cbrt\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"column1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "# New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "cube_root_transf_df = cube_root_transform (df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, add_constant = ADD_CONSTANT, constant_to_add = CONSTANT_TO_ADD, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "39ba65a0-cd7c-4123-ac37-c22a027f4149",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the cube root-transform - Raising to 3rd power**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de7066bb-4dfc-46e8-9042-a475cd2cdf71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "ADDED_CONSTANT = 0\n",
    "# ADDED_CONSTANT: constant C added for making the transformation cbrt(x + C). Check the\n",
    "# output of sqrt transform function to verify if a constant was automatically added.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_cube_root_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, added_constant = ADDED_CONSTANT, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **(General) Power-transforming the variables**\n",
    "- Positive and fraction power exponents may be used for transforming.\n",
    "- May be used as a method for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8e7fef30-c0ba-4a46-9587-4a46b2240069",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "EXPONENT = 2\n",
    "# EXPONENT = 2 - the exponent of the power function. Positive values or fractions may be used\n",
    "# as exponents. Example: EXPONENT = 10 raises X to 10th power (X^10), EXPONENT = 0.5 raises to 0.5 \n",
    "# (X^0.5 = X ^(1/2) = square root (X)). \n",
    "# EXPONENT = 1/10 calculates the 10th root (X^(1/10))\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "\n",
    "ADD_CONSTANT = False\n",
    "# ADD_CONSTANT = False - If True, the transformation power(x + C)\n",
    "# where C is a constant will be applied.\n",
    "CONSTANT_TO_ADD = 0\n",
    "# CONSTANT_TO_ADD = 0 = 0 - float number which will be added to each value that will be transformed.\n",
    "# Attention: if no constant is added, but there is a negative value that makes it impossible to\n",
    "# apply the power function, the minimum needed for making\n",
    "# every value positive will be added automatically. If the constant to add results in negative or\n",
    "# zero values, it will be also modified to make all values non-negative.\n",
    "        \n",
    "NEW_COLUMNS_SUFFIX = \"_pow\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"column1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "# New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "power_transf_df = power_transform (df = DATASET, exponent = EXPONENT, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, add_constant = ADD_CONSTANT, constant_to_add = CONSTANT_TO_ADD, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "39ba65a0-cd7c-4123-ac37-c22a027f4149",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the power transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de7066bb-4dfc-46e8-9042-a475cd2cdf71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "ORIGINAL_EXPONENT = 2\n",
    "# ORIGINAL_EXPONENT = 2 - the exponent of the power function used for transforming. \n",
    "# Positive values or fractions may be used\n",
    "# as exponents. Set the exact same number used on the original transformation in order to\n",
    "# reverse it. It corresponds to apply the power function with the inverted exponent\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "ADDED_CONSTANT = 0\n",
    "# ADDED_CONSTANT: constant C added for making the transformation power(x + C). Check the\n",
    "# output of sqrt transform function to verify if a constant was automatically added.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_power_transform (df = DATASET, original_exponent = ORIGINAL_EXPONENT, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, added_constant = ADDED_CONSTANT, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "72c3cf5f-dde2-4f3b-929c-681cbe228c1a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **One-Hot Encoding the categorical variables**\n",
    "- Transform categorical values without notion of order into numerical (binary) features.\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "- The new columns will be named as the original columns + \"_\" + possible categories + \"OneHotEnc\".\n",
    "- Each column is a binary variable of the type \"is classified in this category or not\".\n",
    "\n",
    "Therefore, for a category \"A\", a column named \"A\" is created.\n",
    "- If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "- If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "33e0b50c-10c3-46c0-877a-32be0c593d58",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as one_hot_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as OneHot_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "one_hot_encoded_df, OneHot_encoding_list = OneHotEncoding_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fc6b3659-fa74-4c86-8d41-9f20f4232152"
   },
   "source": [
    "### **Reversing the One-Hot Encoding of the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9e3a3262-5ace-4825-af2c-5e9999847baa",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "ENCODING_LIST = [\n",
    "    \n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}}\n",
    "    \n",
    "]\n",
    "# ENCODING_LIST: list in the same format of the one generated by OneHotEncode_df function:\n",
    "# it must be a list of dictionaries where each dictionary contains two keys:\n",
    "# key 'column': string with the original column name (in quotes). If it is None, a column named \n",
    "# 'category_column_i', where i is the index of the dictionary in the encoding_list will be created; \n",
    "# key 'OneHot_encoder': this key must store a nested dictionary.\n",
    "# Even though the nested dictionaries generates by the encoding function present\n",
    "# two keys:  'categories', storing an array with the different categories;\n",
    "# and 'OneHot_enc_obj', storing the encoder object, only the key 'OneHot_enc_obj' is required.\n",
    "## On the other hand, a third key is needed in the nested dictionary:\n",
    "## key 'encoded_columns': this key must store a list or array with the names of the columns\n",
    "# obtained from Encoding.\n",
    "\n",
    "# Alternatively:\n",
    "\"\"\"\n",
    "    ENCODING_LIST = [{'column': None,\n",
    "                'OneHot_encoder': [{'category': None,\n",
    "                                    'encoded_column': column},]}]\n",
    "\"\"\"\n",
    "# Here, enconding_list will be a list of dictionaries, where each dictionary corresponds to one\n",
    "# of the new columns to be encoded. The first key ('column') contains the name of the new column\n",
    "# that will be created. If the value is None, a column named 'category_column_i', where i is the index of\n",
    "# the dictionary in the encoding_list will be created.\n",
    "# The second key, 'OneHot_encoder' will store a list of dictionaries. Each dictionary contains one of the\n",
    "# columns obtained after the One-Hot Encoding, i.e., one of the binary columns that informs if the category\n",
    "# is present or not. Each dictionary contains two keys: 'category' is the category that is encoded by such column.\n",
    "# If the value is None, the category will be labeled as 'i' (string), where i is the index of the category in the\n",
    "# 'OneHot_encoder' list. The 'encoded_column' must contain a string indicating the name (label) of the encoded column\n",
    "# in the original dataset. For instance, suppose column \"label_horse\" is a One-Hot Encoded column that stores value 1\n",
    "# if the label is \"horse\", and value 0 otherwise.  In this case, 'category': 'horse, 'encoded_column': 'label_horse'.\n",
    "\n",
    "\n",
    "# New dataframe saved as reversed_one_hot_encoded_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "reversed_one_hot_encoded_df = reverse_OneHotEncoding (df = DATASET, encoding_list = ENCODING_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3cee6ad2-66f9-4504-8374-b04f3a818bdb"
   },
   "source": [
    "### **Ordinal Encoding the categorical variables**\n",
    "- Transform categorical values with notion of order into numerical (integer) features.\n",
    "- For each column, the Ordinal Encoder creates a new column in the dataset. This new column is represented by a an integer value, where each integer represents a possible categorie.\n",
    "- The new columns will be named as the original column + \"_OrdinalEnc\".\n",
    "\n",
    "#### WARNING: Machine Learning algorithms assume that close values represent similarity and order. If there is no order and no distance associated to each ordinal, use the One-Hot Encoding for converting categorical to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c4f8cdb7-01d0-4d10-b1ee-942ea95f38f3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as ordinal_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as ordinal_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "ordinal_encoded_df, ordinal_encoding_list = OrdinalEncoding_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d3ed1009-13c7-4478-a582-0d795c107afc"
   },
   "source": [
    "### **Reversing the Ordinal Encoding of the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6fcac64f-9dee-4f56-964f-637e777524a7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "ENCODING_LIST = [\n",
    "    \n",
    "    {'original_column_label': None,\n",
    "    'ordinal_encoder': {'categories': None, \n",
    "                        'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'original_column_label': None,\n",
    "    'ordinal_encoder': {'categories': None, \n",
    "                        'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'original_column_label': None,\n",
    "    'ordinal_encoder': {'categories': None, \n",
    "                        'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'original_column_label': None,\n",
    "    'ordinal_encoder': {'categories': None, \n",
    "                        'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    \n",
    "]\n",
    "# ENCODING_LIST: list in the same format of the one generated by OrdinalEncode_df function:\n",
    "# it must be a list of dictionaries where each dictionary contains two keys:\n",
    "# key 'original_column_label': string with the original column name (in quotes); \n",
    "# key 'ordinal_encoder': this key must store a nested dictionary.\n",
    "# Even though the nested dictionaries generates by the encoding function present\n",
    "# two keys:  'categories', storing an array with the different categories;\n",
    "# and 'ordinal_enc_obj', storing the encoder object, only one of them is required,\n",
    "# prefentially the 'categories' one.\n",
    "# Exammple of array or list: 'categories': ['white', 'black', 'blue']\n",
    "\n",
    "## On the other hand, a third key is needed in the nested dictionary:\n",
    "## key 'encoded_column': this key must store a string with the name of the column\n",
    "# obtained from Encoding.\n",
    "# If 'encoded_column' is None, the name of the original column will be used.\n",
    "# On the other hand, when 'original_column_label' is None, the 'encoded_column'\n",
    "# will be used for both. So, at least one of them must be present for the\n",
    "# element not to be ignored.\n",
    "\n",
    "# Alternatively, the list may have the following format:\n",
    "\"\"\"\"\n",
    "ENCODING_LIST = \n",
    "[\n",
    "        {'original_column_label': None,\n",
    "        'encoding': [{'actual_label': None, 'encoded_value': None},]},\n",
    "        {'original_column_label': None,\n",
    "        'encoding': [{'actual_label': None, 'encoded_value': None},]},\n",
    "]\n",
    "\"\"\"\n",
    "# The difference is that the key 'ordinal_encoder' is replaced by the key\n",
    "# 'encoding', which stores a list of dictionaries with encoding information.\n",
    "# You must input a dictionary by possible encoded value (as a list element). \n",
    "# Each dictionary will contain a key 'actual_label' with the real value of the \n",
    "# label, and 'encoded_val'with the correspondent encoding. For example, \n",
    "# if the category 'yellow' was encoded as value 3, then the dictionary would be: \n",
    "# {'actual_label': 'yellow', 'encoded_value': 3}\n",
    "\n",
    "\n",
    "# New dataframe saved as reversed_ordinal_encoded_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "reversed_ordinal_encoded_df = reverse_OrdinalEncoding (df = DATASET, encoding_list = ENCODING_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "89591bf7-2900-4ae0-9f77-0d020cdf9feb"
   },
   "source": [
    "### **Scaling the features - Standard scaler, Min-Max scaler, division by factor**\n",
    "- Machine Learning algorithms are extremely sensitive to scale. This function provides 3 methods (modes) of scaling:\n",
    "    - `mode = 'standard'`: applies the standard scaling, which creates a new variable with mean = 0; and standard deviation = 1. Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean of the training samples, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "    - `mode = 'min_max'`: applies min-max normalization, with a resultant feature ranging from 0 to 1. Each value Y is transformed as Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and maximum values of Y, respectively.\n",
    "    - `mode = 'factor'`: divide the whole series by a numeric value provided as argument. For a factor F, the new Y values will be Ytransf = Y/F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d0a50802-8b35-4d0d-9d33-45b1e44afcd1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor', MODE = 'normalize_by_maximum'\n",
    "## This function provides 4 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "## MODE = 'normalize_by_maximum' is similar to MODE = 'factor', but the factor will be selected\n",
    "# as the maximum value. This mode is available only for SCALE_WITH_NEW_PARAMS = True. If\n",
    "# SCALE_WITH_NEW_PARAMS = False, you should provide the value of the maximum as a division 'factor'.\n",
    "\n",
    "SCALE_WITH_NEW_PARAMS = True\n",
    "# Alternatively, set SCALE_WITH_NEW_PARAMS = True if you want to calculate a new\n",
    "# scaler for the data; or SCALE_WITH_NEW_PARAMS = False if you want to apply \n",
    "# parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "# previously trained to another set of data; or wants to simply apply again the same\n",
    "# scaler).\n",
    "    \n",
    "## WARNING: The MODE 'factor' demmands the input of the list of factors that will be \n",
    "# used for normalizing each column. Therefore, it can be used only \n",
    "# when SCALE_WITH_NEW_PARAMS = False.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = None\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "# The key 'scaler_details' will not create an object: the transform will be directly performed \n",
    "# through vectorial operations.\n",
    "\n",
    "SUFFIX = '_scaled'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "# identified as 'Y_scaled'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as scaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "scaled_df, scaling_list = feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, mode = MODE, scale_with_new_params = SCALE_WITH_NEW_PARAMS, list_of_scaling_params = LIST_OF_SCALING_PARAMS, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e5277523-9e36-4fa2-93c8-4cffab51204c"
   },
   "source": [
    "### **Reversing scaling of the features - Standard scaler, Min-Max scaler, division by factor**\n",
    "- `mode = 'standard'`.\n",
    "- `mode = 'min_max'`.\n",
    "- `mode = 'factor'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9a19ae48-1669-4e22-a680-f7d9ccfb956d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = [\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}},\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}}\n",
    "                            \n",
    "                         ]\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "\n",
    "SUFFIX = '_reverseScaling'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "# identified as 'Y_reverseScaling'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as rescaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df, scaling_list = reverse_feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, list_of_scaling_params = LIST_OF_SCALING_PARAMS, mode = MODE, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "873bf804-9640-4cb2-92ad-b79f8602d265",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f575b478-8930-4400-bb78-36cec4853866"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a7f53fc3-8839-4e22-81e7-f7b3b638f52c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "82a45c25-3b08-4257-8466-2ba8c40087bd"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "70e950c4-eadb-43ea-93d0-f09c5dae61ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1716cb5a-0c72-4cf0-9207-051717b2df4b"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f63a5a18-0797-4c14-ab10-7aa77914f5d3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9a900908-dece-4106-89f3-5a979e50be3d"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "25dadb54-914a-451f-b7b4-85c2ae6beedb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "805a98d9-e0e9-4ebd-bb30-b2e13959b7bf"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "55b24b35-e9ae-437e-858f-aed34972f485",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting dataframes as Excel file tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .xlsx\n",
    "\n",
    "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
    "# (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
    "\n",
    "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None, \n",
    "                    'excel_sheet_name': None},]\n",
    "\n",
    "# exported_tables is a list of dictionaries. User may declare several dictionaries, \n",
    "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
    "      \n",
    "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
    "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
    "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
    "\n",
    "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1, \n",
    "# 'excel_sheet_name': 'sheet1'},]\n",
    "# will export only dataset1 as 'sheet1';\n",
    "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
    "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
    "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
    "\n",
    "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
    "# the sheets will be replaced.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "\n",
    "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2732a0e7-6ffe-495d-a28f-82c921c1a253"
   },
   "source": [
    "# **One-Hot Encoding - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "df88ce83-988c-46ed-9087-6e21278668ec",
    "id": "2q8SGc6WigDt"
   },
   "source": [
    "If there are **categorical features**, they should be converted into numerical variables for being processed by the machine learning algorithms.\n",
    "\n",
    "\\- We can assign integer values for each one of the categories. This works well for situations where there is a scale or order for the assignment of the variables (e.g., if there is a satisfaction grade).\n",
    "\n",
    "\\- On the other hand, the results may be compromised if there is no order. That is because the ML algorithms assume that, if two categories have close numbers, then the categories are similar, what is not necessarily true. There are cases where the categories have no relation with each other.\n",
    "\n",
    "\\- In these cases, the best strategy is the One-Hot Encoding. For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "\n",
    "\\- Naturally, the number of columns grow with the number of possible labels. The One-Hot Encoder from Sklearn creates a Scipy Sparse matrix that stores the position of the zeros in the dataset. Then, the computational cost is reduced due to the fact that we are not storing a huge amount of null values.\n",
    "\n",
    "\\- Since each column is a binary variable of the type \"is classified in this category or not\", we expect that the created columns contain more zeros than 1s. That is because if an element belongs to one category (= 1), it does not belong to the others, so its value is zero for all other columns."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
