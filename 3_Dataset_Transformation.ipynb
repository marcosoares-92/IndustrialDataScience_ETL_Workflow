{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8e0cea1f-1e08-418e-b2e0-53178e19d71a"
   },
   "source": [
    "# **Dataset Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6689b182-e386-4ad3-a19e-215c07e2f6c1"
   },
   "source": [
    "## _ETL Workflow Notebook 3_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f04f7076-17fb-41c1-94e9-348059de0fce"
   },
   "source": [
    "## Content:\n",
    "1. Removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type;\n",
    "2. Capitalizing or lowering case of string variables (string homogenizing);\n",
    "3. Adding contractions to the contractions library;\n",
    "4. Correcting contracted strings;\n",
    "5. Substituting (replacing) substrings on string variables;\n",
    "6. Inverting the order of the string characters;\n",
    "7. Slicing the strings;\n",
    "8. Getting the leftest characters from the strings (retrieve last characters);\n",
    "9. Getting the rightest characters from the strings (retrieve first characters);\n",
    "10. Joining strings from a same column into a single string;\n",
    "11. Joining several string columns into a single string column;\n",
    "12. Splitting strings into a list of strings;\n",
    "13. Substituting (replacing or switching) whole strings by different text values (on string variables);\n",
    "14. Replacing strings with Machine Learning: finding similar strings and replacing them by standard strings;\n",
    "15. Searching for Regular Expression (RegEx) within a string column;\n",
    "16. Replacing a Regular Expression (RegEx) from a string column;\n",
    "17. Applying Fast Fourier Transform;\n",
    "18. Generating columns with frequency information;\n",
    "19. Transforming the dataset and reverse transforms: log-transform; \n",
    "20. Exponential transform; \n",
    "21. Box-Cox transform; \n",
    "22. One-Hot Encoding;\n",
    "23. Ordinal Encoding;\n",
    "24. Feature scaling; \n",
    "25. Importing or exporting models and dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# To install a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow\n",
    "# to update a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow --upgrade\n",
    "# to update pip, unmark and run:\n",
    "# ! pip install pip --upgrade\n",
    "# to show if a library is installed and visualize its information, unmark and run\n",
    "# (e.g. tensorflow):\n",
    "# ! pip show tensorflow\n",
    "# To run a Python file (e.g idsw_etl.py) saved in the notebook's workspace directory,\n",
    "# unmark and run:\n",
    "# import idsw_etl\n",
    "# or:\n",
    "# import idsw_etl as etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "fec5846c-2b1d-4a60-b6fc-554077ea8582",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b812615d-e0ca-45d0-8375-6eab15814d65"
   },
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b6c9eb16-fe46-4b31-bcaf-2872796404a8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f9121ced-ba1a-4b8a-96c5-0d985f24c082"
   },
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e51bb8bc-0396-414f-8781-7ac740957ae2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", load_all_sheets_at_once = False, sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading Excel files:\n",
    "    \n",
    "    # load_all_sheets_at_once = False - This parameter has effect only when for Excel files.\n",
    "    # If load_all_sheets_at_once = True, the function will return a list of dictionaries, each\n",
    "    # dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "    # value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "    # and its value will be the pandas dataframe object obtained from that sheet.\n",
    "    # This argument has preference over sheet_to_load. If it is True, all sheets will be loaded.\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\\n\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\\n\")\n",
    "        # For Excel type files, Pandas automatically detects the decimal separator and requires only the parameter parse_dates.\n",
    "        # Firstly, the argument infer_datetime_format was present on read_excel function, but was removed.\n",
    "        # From version 1.4 (beta, in 10 May 2022), it will be possible to pass the parameter 'decimal' to\n",
    "        # read_excel function for detecting decimal cases in strings. For numeric variables, it is not needed, though\n",
    "        \n",
    "        if (load_all_sheets_at_once == True):\n",
    "            \n",
    "            # Corresponds to setting sheet_name = None\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "            \n",
    "            # xlsx_doc is a dictionary containing the sheet names as keys, and dataframes as items.\n",
    "            # Let's convert it to the desired format.\n",
    "            # Dictionary dict, dict.keys() is the array of keys; dict.values() is an array of the values;\n",
    "            # and dict.items() is an array of tuples with format ('key', value)\n",
    "            \n",
    "            # Create a list of returned datasets:\n",
    "            list_of_datasets = []\n",
    "            \n",
    "            # Let's iterate through the array of tuples. The first element returned is the key, and the\n",
    "            # second is the value\n",
    "            for sheet_name, dataframe in (xlsx_doc.items()):\n",
    "                # sheet_name = key; dataframe = value\n",
    "                # Define the dictionary with the standard format:\n",
    "                df_dict = {'sheet': sheet_name,\n",
    "                            'df': dataframe}\n",
    "                \n",
    "                # Add the dictionary to the list:\n",
    "                list_of_datasets.append(df_dict)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(f\"A total of {len(list_of_datasets)} dataframes were retrieved from the Excel file.\\n\")\n",
    "            print(f\"The dataframes correspond to the following Excel sheets: {list(xlsx_doc.keys())}\\n\")\n",
    "            print(\"Returning a list of dictionaries. Each dictionary contains the key \\'sheet\\', with the original sheet name; and the key \\'df\\', with the Pandas dataframe object obtained.\\n\")\n",
    "            print(f\"Check the 10 first rows of the dataframe obtained from the first sheet, named {list_of_datasets[0]['sheet']}:\\n\")\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            except: # regular mode\n",
    "                print((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            return list_of_datasets\n",
    "            \n",
    "        elif (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1879cff0-4b74-469d-9106-878a8318291a"
   },
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e1e9bbaa-d260-44d8-9974-4c5b86ae5df0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def json_obj_to_pandas_dataframe (json_obj_to_convert, json_obj_type = 'list', json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "    # dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "    # example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "    # structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "    # file containing JSON, you could read the txt and save its content as a string.\n",
    "    # json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "    # 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "    # 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]    \n",
    "\n",
    "    # json_obj_type = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "    # json_obj_type = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "\n",
    "    \n",
    "    if (json_obj_type == 'string'):\n",
    "        # Use the json.loads method to convert the string to json\n",
    "        json_file = json.loads(json_obj_to_convert)\n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "        # like a dataframe.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    \n",
    "    elif (json_obj_type == 'list'):\n",
    "        \n",
    "        # make the json_file the object itself:\n",
    "        json_file = json_obj_to_convert\n",
    "    \n",
    "    else:\n",
    "        print (\"Enter a valid JSON object type: \\'list\\', in case the JSON object is a list of dictionaries in JSON format; or \\'string\\', if the JSON is stored as a text (string variable).\")\n",
    "        return \"error\"\n",
    "    \n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "563d1758-ed67-48ac-ac81-ca1da15ff75b"
   },
   "source": [
    "# **Function for dataframe general characterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "be7b028a-5ef0-44f2-86aa-11d3145b838d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def df_general_characterization (df):\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    # Set a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    # Show dataframe's header\n",
    "    print(\"Dataframe\\'s 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "\n",
    "    # Show dataframe's tail:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    print(\"Dataframe\\'s 10 last rows:\\n\")\n",
    "    try:\n",
    "        display(DATASET.tail(10))\n",
    "    except:\n",
    "        print(DATASET.tail(10))\n",
    "    \n",
    "    # Show dataframe's shape:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_shape  = DATASET.shape\n",
    "    print(\"Dataframe\\'s shape = (number of rows, number of columns) =\\n\")\n",
    "    try:\n",
    "        display(df_shape)\n",
    "    except:\n",
    "        print(df_shape)\n",
    "    \n",
    "    # Show dataframe's columns:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_columns_array = DATASET.columns\n",
    "    print(\"Dataframe\\'s columns =\\n\")\n",
    "    try:\n",
    "        display(df_columns_array)\n",
    "    except:\n",
    "        print(df_columns_array)\n",
    "    \n",
    "    # Show dataframe's columns types:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_dtypes = DATASET.dtypes\n",
    "    # Now, the df_dtypes seroes has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.rename.html#pandas.Index.rename\n",
    "    # To access the Index object, we call the index attribute from Pandas dataframe.\n",
    "    # By setting inplace = True, we modify the object inplace, by simply calling the method:\n",
    "    df_dtypes.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    # Let's also modify the series label or name:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rename.html\n",
    "    df_dtypes.rename('dtype_series', inplace = True)\n",
    "    print(\"Dataframe\\'s variables types:\\n\")\n",
    "    try:\n",
    "        display(df_dtypes)\n",
    "    except:\n",
    "        print(df_dtypes)\n",
    "    \n",
    "    # Show dataframe's general statistics for numerical variables:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    df_general_statistics = DATASET.describe()\n",
    "    print(\"Dataframe\\'s general (summary) statistics for numeric variables:\\n\")\n",
    "    try:\n",
    "        display(df_general_statistics)\n",
    "    except:\n",
    "        print(df_general_statistics)\n",
    "    \n",
    "    # Show total of missing values for each variable:\n",
    "    # Line break before next information:\n",
    "    print(\"\\n\")\n",
    "    total_of_missing_values_series = DATASET.isna().sum()\n",
    "    # This is a series which uses the original column names as index\n",
    "    proportion_of_missing_values_series = DATASET.isna().mean()\n",
    "    percent_of_missing_values_series = proportion_of_missing_values_series * 100\n",
    "    missingness_dict = {'count_of_missing_values': total_of_missing_values_series,\n",
    "                       'proportion_of_missing_values': proportion_of_missing_values_series,\n",
    "                       'percent_of_missing_values': percent_of_missing_values_series}\n",
    "    \n",
    "    df_missing_values = pd.DataFrame(data = missingness_dict)\n",
    "    # Now, the dataframe has the original columns set as index, but this index has no name.\n",
    "    # Let's rename it using the .rename method from Pandas Index object:\n",
    "    df_missing_values.index.rename(name = 'dataframe_column', inplace = True)\n",
    "    \n",
    "    # Create a one row dataframe with the missingness for the whole dataframe:\n",
    "    # Pass the scalars as single-element lists or arrays:\n",
    "    one_row_data = {'dataframe_column': ['missingness_accross_rows'],\n",
    "                    'count_of_missing_values': [len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any'))],\n",
    "                    'proportion_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))],\n",
    "                    'percent_of_missing_values': [(len(DATASET) - len(DATASET.copy(deep = True).dropna(how = 'any')))/(len(DATASET))*100]\n",
    "                    }\n",
    "    one_row_df = pd.DataFrame(data = one_row_data)\n",
    "    one_row_df.set_index('dataframe_column', inplace = True)\n",
    "    \n",
    "    # Append this one_row_df to df_missing_values:\n",
    "    df_missing_values = pd.concat([df_missing_values, one_row_df])\n",
    "    \n",
    "    print(\"Missing values on each feature; and missingness considering all rows from the dataframe:\")\n",
    "    print(\"(note: \\'missingness_accross_rows\\' was calculated by: checking which rows have at least one missing value (NA); and then comparing total rows with NAs with total rows in the dataframe).\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(df_missing_values)\n",
    "    except:\n",
    "        print(df_missing_values)\n",
    "    \n",
    "    return df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2c0adcfb-4fd0-4efc-9f75-156cc2d42486"
   },
   "source": [
    "# **Function for obtaining the correlation plot**\n",
    "- The Pandas method dataset.corr() calculates the Pearson's correlation coefficients R.\n",
    "- Pearson's correlation coefficients R go from -1 to 1.\n",
    "- These coefficients are R, not R².\n",
    "\n",
    "#### To obtain the coefficients R², we raise the results to the 2nd power, i.e., we calculate (dataset.corr())**2\n",
    "- R² goes from 0 to 1, where 1 represents the perfect correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot (df, show_masked_plot = True, responses_to_return_corr = None, set_returned_limit = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    #show_masked_plot = True - keep as True if you want to see a cleaned version of the plot\n",
    "    # where a mask is applied.\n",
    "    \n",
    "    #responses_to_return_corr - keep as None to return the full correlation tensor.\n",
    "    # If you want to display the correlations for a particular group of features, input them\n",
    "    # as a list, even if this list contains a single element. Examples:\n",
    "    # responses_to_return_corr = ['response1'] for a single response\n",
    "    # responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "    # responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "    # of a column of the dataset that represents a response variable.\n",
    "    # WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "    # of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "    \n",
    "    # set_returned_limit = None - This variable will only present effects in case you have\n",
    "    # provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "    # to return all of the correlation coefficients; or, alternatively, \n",
    "    # provide an integer number to limit the total of coefficients returned. \n",
    "    # e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "    \n",
    "    # set a local copy of the dataset to perform the calculations:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    correlation_matrix = DATASET.corr(method = 'pearson')\n",
    "    \n",
    "    if (show_masked_plot == False):\n",
    "        #Show standard plot\n",
    "        \n",
    "        plt.figure(figsize = (12, 8))\n",
    "        sns.heatmap((correlation_matrix)**2, annot = True, fmt = \".2f\")\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    #Oncee the pandas method .corr() calculates R, we raised it to the second power \n",
    "    # to obtain R². R² goes from zero to 1, where 1 represents the perfect correlation.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Show masked (cleaner) plot instead of the standard one\n",
    "        # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize = (12, 8))\n",
    "        # Mask for the upper triangle\n",
    "        mask = np.zeros_like((correlation_matrix)**2)\n",
    "\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        # Generate a custom diverging colormap\n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "\n",
    "        # Heatmap with mask and correct aspect ratio\n",
    "        sns.heatmap(((correlation_matrix)**2), mask = mask, cmap = cmap, center = 0,\n",
    "                    linewidths = .5)\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"correlation_plot\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        #Again, the method dataset.corr() calculates R within the variables of dataset.\n",
    "        #To calculate R², we simply raise it to the second power: (dataset.corr()**2)\n",
    "    \n",
    "    #Sort the values of correlation_matrix in Descending order:\n",
    "    \n",
    "    if (responses_to_return_corr is not None):\n",
    "        \n",
    "        if (type(responses_to_return_corr) == str):\n",
    "            # If a string was input, put it inside a list\n",
    "            responses_to_return_corr = [responses_to_return_corr]\n",
    "        \n",
    "        #Select only the desired responses, by passing the list responses_to_return_corr\n",
    "        # as parameter for column filtering:\n",
    "        correlation_matrix = correlation_matrix[responses_to_return_corr]\n",
    "        # By passing a list as argument, we assure that the output is a dataframe\n",
    "        # and not a series, even if the list contains a single element.\n",
    "        \n",
    "        # Create a list of boolean variables == False, one False correspondent to\n",
    "        # each one of the responses\n",
    "        ascending_modes = [False for i in range(0, len(responses_to_return_corr))]\n",
    "        \n",
    "        #Now sort the values according to the responses, by passing the list\n",
    "        # response\n",
    "        correlation_matrix = correlation_matrix.sort_values(by = responses_to_return_corr, ascending = ascending_modes)\n",
    "        \n",
    "        # If a limit of coefficients was determined, apply it:\n",
    "        if (set_returned_limit is not None):\n",
    "                \n",
    "                correlation_matrix = correlation_matrix.head(set_returned_limit)\n",
    "                #Pandas .head(X) method returns the first X rows of the dataframe.\n",
    "                # Here, it returns the defined limit of coefficients, set_returned_limit.\n",
    "                # The default .head() is X = 5.\n",
    "    \n",
    "    print(\"ATTENTION: The correlation plots show the linear correlations R², which go from 0 (none correlation) to 1 (perfect correlation). Obviously, the main diagonal always shows R² = 1, since the data is perfectly correlated to itself.\\n\")\n",
    "    print(\"The returned correlation matrix, on the other hand, presents the linear coefficients of correlation R, not R². R values go from -1 (perfect negative correlation) to 1 (perfect positive correlation).\\n\")\n",
    "    print(\"None of these coefficients take non-linear relations and the presence of a multiple linear correlation in account. For these cases, it is necessary to calculate R² adjusted, which takes in account the presence of multiple preditors and non-linearities.\\n\")\n",
    "    \n",
    "    print(\"Correlation matrix - numeric results:\\n\")\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(correlation_matrix)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(correlation_matrix)\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d4ddd1f5-74f4-4ce1-84b6-2d67685ebbc4"
   },
   "source": [
    "# **Function for obtaining scatter plots and simple linear regressions**\n",
    "- Here, only a single prediction variable will be analyzed by once.\n",
    "- The plots will show Y x X, where X is the predict or independent variable.\n",
    "- The linear regressions will be of the type Y = aX + b, i.e., a single pair (X, Y) analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "94fa3aa7-204b-4a16-9096-69620d314bba",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def scatter_plot_lin_reg (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], x_axis_rotation = 70, y_axis_rotation = 0, show_linear_reg = True, grid = True, add_splines_lines = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330): \n",
    "    \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from scipy import stats\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "        \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        print(\"Data to be plotted in a same column.\\n\")\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "    \n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "    \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to plot. Please, provide valid arguments.\\n\")\n",
    "        return \"error\" \n",
    "        # we return the value because this function always returns an object.\n",
    "        # In other functions, this return would be omitted.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        \n",
    "        # Now we pre-processed the data, we can obtain a final list of dictionaries, containing\n",
    "        # the linear regression information (it will be plotted only if the user asked to). Start\n",
    "        # a list to store all predictions:\n",
    "        list_of_dictionaries_with_series_and_predictions = []\n",
    "        \n",
    "        # Loop through each dictionary (element) on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            x_is_datetime = False\n",
    "            # boolean that will map if x is a datetime or not. Only change to True when it is.\n",
    "            \n",
    "            # Access keys 'x' and 'y' to retrieve the arrays.\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            \n",
    "            # Check if the elements from array x are np.datetime64 objects. Pick the first\n",
    "            # element to check:\n",
    "            \n",
    "            if (type(x[0]) == np.datetime64):\n",
    "                \n",
    "                x_is_datetime = True\n",
    "                \n",
    "            if (x_is_datetime):\n",
    "                # In this case, performing the linear regression directly in X will\n",
    "                # return an error. We must associate a sequential number to each time.\n",
    "                # to keep the distance between these integers the same as in the original sequence\n",
    "                # let's define a difference of 1 ns as 1. The 1st timestamp will be zero, and the\n",
    "                # addition of 1 ns will be an addition of 1 unit. So a timestamp recorded 10 ns\n",
    "                # after the time zero will have value 10. At the end, we divide every element by\n",
    "                # 10**9, to obtain the correspondent distance in seconds.\n",
    "                \n",
    "                # start a list for the associated integer timescale. Put the number zero,\n",
    "                # associated to the first timestamp:\n",
    "                int_timescale = [0]\n",
    "                \n",
    "                # loop through each element of the array x, starting from index 1:\n",
    "                for i in range(1, len(x)):\n",
    "                    \n",
    "                    # calculate the timedelta between x[i] and x[i-1]:\n",
    "                    # The delta method from the Timedelta class converts the timedelta to\n",
    "                    # nanoseconds, guaranteeing the internal compatibility:\n",
    "                    timedelta = pd.Timedelta(x[i] - x[(i-1)]).delta\n",
    "                    \n",
    "                    # Sum this timedelta (integer number of nanoseconds) to the\n",
    "                    # previous element from int_timescale, and append the result to the list:\n",
    "                    int_timescale.append((timedelta + int_timescale[(i-1)]))\n",
    "                \n",
    "                # Now convert the new scale (that preserves the distance between timestamps)\n",
    "                # to NumPy array:\n",
    "                int_timescale = np.array(int_timescale)\n",
    "                \n",
    "                # Divide by 10**9 to obtain the distances in seconds, reducing the order of\n",
    "                # magnitude of the integer numbers (the division is allowed for arrays)\n",
    "                int_timescale = int_timescale / (10**9)\n",
    "                \n",
    "                # Finally, use this timescale to obtain the linear regression:\n",
    "                lin_reg = stats.linregress(int_timescale, y = y)\n",
    "            \n",
    "            else:\n",
    "                # Obtain the linear regression object directly from x. Since x is not a\n",
    "                # datetime object, we can calculate the regression directly on it:\n",
    "                lin_reg = stats.linregress(x, y = y)\n",
    "                \n",
    "            # Retrieve the equation as a string.\n",
    "            # Access the attributes intercept and slope from the lin_reg object:\n",
    "            lin_reg_equation = \"y = %.2f*x + %.2f\" %((lin_reg).slope, (lin_reg).intercept)\n",
    "            # .2f: float with only two decimals\n",
    "                \n",
    "            # Retrieve R2 (coefficient of correlation) also as a string\n",
    "            r2_lin_reg = \"R²_lin_reg = %.4f\" %(((lin_reg).rvalue) ** 2)\n",
    "            # .4f: 4 decimals. ((lin_reg).rvalue) is the coefficient R. We\n",
    "            # raise it to the second power by doing **2, where ** is the potentiation.\n",
    "                \n",
    "            # Add these two strings to the dictionary\n",
    "            dictionary['lin_reg_equation'] = lin_reg_equation\n",
    "            dictionary['r2_lin_reg'] = r2_lin_reg\n",
    "                \n",
    "            # Now, as final step, let's apply the values x to the linear regression\n",
    "            # equation to obtain the predicted series used to plot the straight line.\n",
    "                \n",
    "            # The lists cannot perform vector operations like element-wise sum or product, \n",
    "            # but numpy arrays can. For example, [1, 2] + 1 would be interpreted as the try\n",
    "            # for concatenation of two lists, resulting in error. But, np.array([1, 2]) + 1\n",
    "            # is allowed, resulting in: np.array[2, 3].\n",
    "            # This and the fact that Scipy and Matplotlib are built on NumPy were the reasons\n",
    "            # why we converted every list to numpy arrays.\n",
    "            \n",
    "            # Save the predicted values as the array y_pred_lin_reg.\n",
    "            # Access the attributes intercept and slope from the lin_reg object.\n",
    "            # The equation is y = (slope * x) + intercept\n",
    "            \n",
    "            # Notice that again we cannot apply the equation directly to a timestamp.\n",
    "            # So once again we will apply the integer scale to obtain the predictions\n",
    "            # if we are dealing with datetime objects:\n",
    "            if (x_is_datetime):\n",
    "                y_pred_lin_reg = ((lin_reg).intercept) + ((lin_reg).slope) * (int_timescale)\n",
    "            \n",
    "            else:\n",
    "                # x is not a timestamp, so we can directly apply it to the regression\n",
    "                # equation:\n",
    "                y_pred_lin_reg = ((lin_reg).intercept) + ((lin_reg).slope) * (x)\n",
    "            \n",
    "            # Add this array to the dictionary with the key 'y_pred_lin_reg':\n",
    "            dictionary['y_pred_lin_reg'] = y_pred_lin_reg\n",
    "            \n",
    "            if (x_is_datetime):\n",
    "            \n",
    "                print(\"For performing the linear regression, a sequence of floats proportional to the timestamps was created. In this sequence, check on the returned object a dictionary containing the timestamps and the correspondent integers, that keeps the distance proportion between successive timestamps. The sequence was created by calculating the timedeltas as an integer number of nanoseconds, which were converted to seconds. The first timestamp was considered time = 0.\")\n",
    "                print(\"Notice that the regression equation is based on the use of this sequence of floats as X.\\n\")\n",
    "                \n",
    "                dictionary['warning'] = \"x is a numeric scale that was obtained from datetimes, preserving the distance relationships. It was obtained for allowing the polynomial fitting.\"\n",
    "                dictionary['numeric_to_datetime_correlation'] = {\n",
    "                    \n",
    "                    'x = 0': x[0],\n",
    "                    f'x = {max(int_timescale)}': x[(len(x) - 1)]\n",
    "                    \n",
    "                }\n",
    "                \n",
    "                dictionary['sequence_of_floats_correspondent_to_timestamps'] = {\n",
    "                                                                                'original_timestamps': x,\n",
    "                                                                                'sequence_of_floats': int_timescale\n",
    "                                                                                }\n",
    "                \n",
    "            # Finally, append this dictionary to list support_list:\n",
    "            list_of_dictionaries_with_series_and_predictions.append(dictionary)\n",
    "        \n",
    "        print(\"Returning a list of dictionaries. Each one contains the arrays of valid series and labels, and the equations, R² and values predicted by the linear regressions.\\n\")\n",
    "        \n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_and_predictions \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes, \n",
    "        # and all the information regarding the linear regression, including the predicted \n",
    "        # values for plotting.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_X\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"X\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_and_predictions:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = \"o\", color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "            if (show_linear_reg == True):\n",
    "                \n",
    "                # Plot the linear regression using the same color.\n",
    "                # Access the array of fitted Y's in the dictionary:\n",
    "                Y_PRED = dictionary['y_pred_lin_reg']\n",
    "                Y_PRED_LABEL = 'lin_reg_' + str(LABEL) # for the case where label is numeric\n",
    "                \n",
    "                ax.plot(X, Y_PRED,  linestyle = '-', marker = '', color = COLOR, alpha = OPACITY, label = Y_PRED_LABEL)\n",
    "\n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "        \n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"scatter_plot_lin_reg\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "        \n",
    "        if (show_linear_reg == True):\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print(\"\\nLinear regression summaries (equations and R²):\\n\")\n",
    "            \n",
    "            for dictionary in list_of_dictionaries_with_series_and_predictions:\n",
    "                \n",
    "                print(f\"Linear regression summary for {dictionary['lab']}:\\n\")\n",
    "                \n",
    "                try:\n",
    "                    display(dictionary['lin_reg_equation'])\n",
    "                    display(dictionary['r2_lin_reg'])\n",
    "\n",
    "                except: # regular mode                  \n",
    "                    print(dictionary['lin_reg_equation'])\n",
    "                    print(dictionary['r2_lin_reg'])\n",
    "                \n",
    "                print(\"\\n\")\n",
    "         \n",
    "        \n",
    "        return list_of_dictionaries_with_series_and_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "99f2486e-b8df-413b-88f2-9d68ae4b89d8"
   },
   "source": [
    "# **Function for time series visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "02914ff7-97da-4453-893b-ea513a7ea510",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def time_series_vis (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "     \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "        \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        print(\"Data to be plotted in a same column.\\n\")\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "    \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to plot. Please, provide valid arguments.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_to_analyze \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        if (add_scatter_dots == True):\n",
    "            MARKER = 'o'\n",
    "            \n",
    "        else:\n",
    "            MARKER = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_timestamp\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"timestamp\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = MARKER, color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"time_series_vis\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        #plt.figure(figsize = (12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3dbe4715-71bd-492a-a457-3bac179b2286"
   },
   "source": [
    "# **Functions for histogram visualization**\n",
    "- Ideal number of bins is calculated through Montgomery's method.\n",
    "    - Douglas C. Montgomery (2009). Introduction to Statistical Process Control, Sixth Edition, John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class capability_analysis:\n",
    "            \n",
    "    # Initialize instance attributes.\n",
    "    # define the Class constructor, i.e., how are its objects:\n",
    "    def __init__ (self, df, column_with_variable_to_be_analyzed, specification_limits, total_of_bins = 10, alpha = 0.10):\n",
    "                \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # If the user passes the argument, use them. Otherwise, use the standard values.\n",
    "        # Set the class objects' attributes.\n",
    "        # Suppose the object is named plot. We can access the attribute as:\n",
    "        # plot.dictionary, for instance.\n",
    "        # So, we can save the variables as objects' attributes.\n",
    "        self.df = df\n",
    "        self.column_with_variable_to_be_analyzed = column_with_variable_to_be_analyzed\n",
    "        self.specification_limits = specification_limits\n",
    "        self.sample_size = df[column_with_variable_to_be_analyzed].count()\n",
    "        self.mu = (df[column_with_variable_to_be_analyzed]).mean() \n",
    "        self.median = (df[column_with_variable_to_be_analyzed]).median()\n",
    "        self.sigma = (df[column_with_variable_to_be_analyzed]).std()\n",
    "        self.lowest = (df[column_with_variable_to_be_analyzed]).min()\n",
    "        self.highest = (df[column_with_variable_to_be_analyzed]).max()\n",
    "        self.total_of_bins = total_of_bins\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Start a dictionary of constants\n",
    "        self.dict_of_constants = {}\n",
    "        # Get parameters to update later:\n",
    "        self.histogram_dict = {}\n",
    "        self.capability_dict = {}\n",
    "        self.normality_dict = {}\n",
    "        \n",
    "        print(\"WARNING: this capability analysis is based on the strong hypothesis that data follows the normal (Gaussian) distribution.\\n\")\n",
    "        \n",
    "    # Define the class methods.\n",
    "    # All methods must take an object from the class (self) as one of the parameters\n",
    "   \n",
    "    # Define a dictionary of constants.\n",
    "    # Each key in the dictionary corresponds to a number of samples in a subgroup.\n",
    "    # sample_size - This variable represents the total of labels or subgroups n. \n",
    "    # If there are multiple labels, this variable will be updated later.\n",
    "    \n",
    "    def check_data_normality (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from scipy import stats\n",
    "        from statsmodels.stats import diagnostic\n",
    "        \n",
    "        alpha = self.alpha\n",
    "        df = self.df\n",
    "        column_with_variable_to_be_analyzed = self.column_with_variable_to_be_analyzed\n",
    "        sample_size = self.sample_size\n",
    "        mu = self.mu \n",
    "        median = self.median\n",
    "        sigma = self.sigma\n",
    "        lowest = self.lowest\n",
    "        highest = self.highest\n",
    "        normality_dict = self.normality_dict # empty dictionary \n",
    "        \n",
    "        print(\"WARNING: The statistical tests require at least 20 samples.\\n\")\n",
    "        print(\"Interpretation:\")\n",
    "        print(\"p-value: probability that data is described by the normal distribution.\")\n",
    "        print(\"Criterion: the series is not described by normal if p < alpha = %.3f.\" %(alpha))\n",
    "        \n",
    "        if (sample_size < 20):\n",
    "            \n",
    "            print(f\"Unable to test series normality: at least 20 samples are needed, but found only {sample_size} entries for this series.\\n\")\n",
    "            normality_dict['WARNING'] = \"Series without the minimum number of elements (20) required to test the normality.\"\n",
    "            \n",
    "        else:\n",
    "            # Let's test the series.\n",
    "            y = df[column_with_variable_to_be_analyzed]\n",
    "            \n",
    "            # Scipy.stats’ normality test\n",
    "            # It is based on D’Agostino and Pearson’s test that combines \n",
    "            # skew and kurtosis to produce an omnibus test of normality.\n",
    "            _, scipystats_test_pval = stats.normaltest(y)\n",
    "            # The underscore indicates an output to be ignored, which is s^2 + k^2, \n",
    "            # where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.\n",
    "            # https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"D\\'Agostino and Pearson\\'s normality test (scipy.stats normality test):\")\n",
    "            print(f\"p-value = {scipystats_test_pval} = {scipystats_test_pval*100}% of probability of being normal.\")\n",
    "            \n",
    "            if (scipystats_test_pval < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(scipystats_test_pval, alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(scipystats_test_pval, alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['dagostino_pearson_p_val'] = scipystats_test_pval\n",
    "            normality_dict['dagostino_pearson_p_in_pct'] = scipystats_test_pval*100\n",
    "            \n",
    "            # Scipy.stats’ Shapiro-Wilk test\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n",
    "            shapiro_test = stats.shapiro(y)\n",
    "            # returns ShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n",
    "             \n",
    "            print(\"\\n\")\n",
    "            print(\"Shapiro-Wilk normality test:\")\n",
    "            print(f\"p-value = {shapiro_test[1]} = {(shapiro_test[1])*100}% of probability of being normal.\")\n",
    "            \n",
    "            if (shapiro_test[1] < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(shapiro_test[1], alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(shapiro_test[1], alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['shapiro_wilk_p_val'] = shapiro_test[1]\n",
    "            normality_dict['shapiro_wilk_p_in_pct'] = (shapiro_test[1])*100\n",
    "            \n",
    "            # Lilliefors’ normality test\n",
    "            lilliefors_test = diagnostic.kstest_normal(y, dist = 'norm', pvalmethod = 'table')\n",
    "            # Returns a tuple: index 0: ksstat: float\n",
    "            # Kolmogorov-Smirnov test statistic with estimated mean and variance.\n",
    "            # index 1: p-value:float\n",
    "            # If the pvalue is lower than some threshold, e.g. 0.10, then we can reject the Null hypothesis that the sample comes from a normal distribution.\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"Lilliefors\\'s normality test:\")\n",
    "            print(f\"p-value = {lilliefors_test[1]} = {(lilliefors_test[1])*100}% of probability of being normal.\")\n",
    "            \n",
    "            if (lilliefors_test[1] < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(lilliefors_test[1], alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(lilliefors_test[1], alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['lilliefors_p_val'] = lilliefors_test[1]\n",
    "            normality_dict['lilliefors_p_in_pct'] = (lilliefors_test[1])*100\n",
    "\n",
    "            # Anderson-Darling normality test\n",
    "            ad_test = diagnostic.normal_ad(y, axis = 0)\n",
    "            # Returns a tuple: index 0 - ad2: float\n",
    "            # Anderson Darling test statistic.\n",
    "            # index 1 - p-val: float\n",
    "            # The p-value for hypothesis that the data comes from a normal distribution with unknown mean and variance.\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(\"Anderson-Darling (AD) normality test:\")\n",
    "            print(f\"p-value = {ad_test[1]} = {(ad_test[1])*100}% of probability of being normal.\")\n",
    "            \n",
    "            if (ad_test[1] < alpha):\n",
    "                \n",
    "                print(\"p = %.3f < %.3f\" %(ad_test[1], alpha))\n",
    "                print(f\"According to this test, data is not described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"p = %.3f >= %.3f\" %(ad_test[1], alpha))\n",
    "                print(f\"According to this test, data is described by the normal distribution, for the {alpha*100}% confidence level defined.\")\n",
    "            \n",
    "            # add this test result to the dictionary:\n",
    "            normality_dict['anderson_darling_p_val'] = ad_test[1]\n",
    "            normality_dict['anderson_darling_p_in_pct'] = (ad_test[1])*100\n",
    "            \n",
    "            # Update the attribute:\n",
    "            self.normality_dict = normality_dict\n",
    "            \n",
    "            return self\n",
    "    \n",
    "    def get_constants (self):\n",
    "        \n",
    "        if (self.sample_size < 2):\n",
    "            \n",
    "            self.sample_size = 2\n",
    "            \n",
    "        if (self.sample_size <= 25):\n",
    "            \n",
    "            dict_of_constants = {\n",
    "                \n",
    "                2: {'A':2.121, 'A2':1.880, 'A3':2.659, 'c4':0.7979, '1/c4':1.2533, 'B3':0, 'B4':3.267, 'B5':0, 'B6':2.606, 'd2':1.128, '1/d2':0.8865, 'd3':0.853, 'D1':0, 'D2':3.686, 'D3':0, 'D4':3.267},\n",
    "                3: {'A':1.732, 'A2':1.023, 'A3':1.954, 'c4':0.8862, '1/c4':1.1284, 'B3':0, 'B4':2.568, 'B5':0, 'B6':2.276, 'd2':1.693, '1/d2':0.5907, 'd3':0.888, 'D1':0, 'D2':4.358, 'D3':0, 'D4':2.574},\n",
    "                4: {'A':1.500, 'A2':0.729, 'A3':1.628, 'c4':0.9213, '1/c4':1.0854, 'B3':0, 'B4':2.266, 'B5':0, 'B6':2.088, 'd2':2.059, '1/d2':0.4857, 'd3':0.880, 'D1':0, 'D2':4.698, 'D3':0, 'D4':2.282},\n",
    "                5: {'A':1.342, 'A2':0.577, 'A3':1.427, 'c4':0.9400, '1/c4':1.0638, 'B3':0, 'B4':2.089, 'B5':0, 'B6':1.964, 'd2':2.326, '1/d2':0.4299, 'd3':0.864, 'D1':0, 'D2':4.918, 'D3':0, 'D4':2.114},\n",
    "                6: {'A':1.225, 'A2':0.483, 'A3':1.287, 'c4':0.9515, '1/c4':1.0510, 'B3':0.030, 'B4':1.970, 'B5':0.029, 'B6':1.874, 'd2':2.534, '1/d2':0.3946, 'd3':0.848, 'D1':0, 'D2':5.078, 'D3':0, 'D4':2.004},\n",
    "                7: {'A':1.134, 'A2':0.419, 'A3':1.182, 'c4':0.9594, '1/c4':1.0423, 'B3':0.118, 'B4':1.882, 'B5':0.113, 'B6':1.806, 'd2':2.704, '1/d2':0.3698, 'd3':0.833, 'D1':0.204, 'D2':5.204, 'D3':0.076, 'D4':1.924},\n",
    "                8: {'A':1.061, 'A2':0.373, 'A3':1.099, 'c4':0.9650, '1/c4':1.0363, 'B3':0.185, 'B4':1.815, 'B5':0.179, 'B6':1.751, 'd2':2.847, '1/d2':0.3512, 'd3':0.820, 'D1':0.388, 'D2':5.306, 'D3':0.136, 'D4':1.864},\n",
    "                9: {'A':1.000, 'A2':0.337, 'A3':1.032, 'c4':0.9693, '1/c4':1.0317, 'B3':0.239, 'B4':1.761, 'B5':0.232, 'B6':1.707, 'd2':2.970, '1/d2':0.3367, 'd3':0.808, 'D1':0.547, 'D2':5.393, 'D3':0.184, 'D4':1.816},\n",
    "                10: {'A':0.949, 'A2':0.308, 'A3':0.975, 'c4':0.9727, '1/c4':1.0281, 'B3':0.284, 'B4':1.716, 'B5':0.276, 'B6':1.669, 'd2':3.078, '1/d2':0.3249, 'd3':0.797, 'D1':0.687, 'D2':5.469, 'D3':0.223, 'D4':1.777},\n",
    "                11: {'A':0.905, 'A2':0.285, 'A3':0.927, 'c4':0.9754, '1/c4':1.0252, 'B3':0.321, 'B4':1.679, 'B5':0.313, 'B6':1.637, 'd2':3.173, '1/d2':0.3152, 'd3':0.787, 'D1':0.811, 'D2':5.535, 'D3':0.256, 'D4':1.744},\n",
    "                12: {'A':0.866, 'A2':0.266, 'A3':0.886, 'c4':0.9776, '1/c4':1.0229, 'B3':0.354, 'B4':1.646, 'B5':0.346, 'B6':1.610, 'd2':3.258, '1/d2':0.3069, 'd3':0.778, 'D1':0.922, 'D2':5.594, 'D3':0.283, 'D4':1.717},\n",
    "                13: {'A':0.832, 'A2':0.249, 'A3':0.850, 'c4':0.9794, '1/c4':1.0210, 'B3':0.382, 'B4':1.618, 'B5':0.374, 'B6':1.585, 'd2':3.336, '1/d2':0.2998, 'd3':0.770, 'D1':1.025, 'D2':5.647, 'D3':0.307, 'D4':1.693},\n",
    "                14: {'A':0.802, 'A2':0.235, 'A3':0.817, 'c4':0.9810, '1/c4':1.0194, 'B3':0.406, 'B4':1.594, 'B5':0.399, 'B6':1.563, 'd2':3.407, '1/d2':0.2935, 'd3':0.763, 'D1':1.118, 'D2':5.696, 'D3':0.328, 'D4':1.672},\n",
    "                15: {'A':0.775, 'A2':0.223, 'A3':0.789, 'c4':0.9823, '1/c4':1.0180, 'B3':0.428, 'B4':1.572, 'B5':0.421, 'B6':1.544, 'd2':3.472, '1/d2':0.2880, 'd3':0.756, 'D1':1.203, 'D2':5.741, 'D3':0.347, 'D4':1.653},\n",
    "                16: {'A':0.750, 'A2':0.212, 'A3':0.763, 'c4':0.9835, '1/c4':1.0168, 'B3':0.448, 'B4':1.552, 'B5':0.440, 'B6':1.526, 'd2':3.532, '1/d2':0.2831, 'd3':0.750, 'D1':1.282, 'D2':5.782, 'D3':0.363, 'D4':1.637},\n",
    "                17: {'A':0.728, 'A2':0.203, 'A3':0.739, 'c4':0.9845, '1/c4':1.0157, 'B3':0.466, 'B4':1.534, 'B5':0.458, 'B6':1.511, 'd2':3.588, '1/d2':0.2787, 'd3':0.744, 'D1':1.356, 'D2':5.820, 'D3':0.378, 'D4':1.622},\n",
    "                18: {'A':0.707, 'A2':0.194, 'A3':0.718, 'c4':0.9854, '1/c4':1.0148, 'B3':0.482, 'B4':1.518, 'B5':0.475, 'B6':1.496, 'd2':3.640, '1/d2':0.2747, 'd3':0.739, 'D1':1.424, 'D2':5.856, 'D3':0.391, 'D4':1.608},\n",
    "                19: {'A':0.688, 'A2':0.187, 'A3':0.698, 'c4':0.9862, '1/c4':1.0140, 'B3':0.497, 'B4':1.503, 'B5':0.490, 'B6':1.483, 'd2':3.689, '1/d2':0.2711, 'd3':0.734, 'D1':1.487, 'D2':5.891, 'D3':0.403, 'D4':1.597},\n",
    "                20: {'A':0.671, 'A2':0.180, 'A3':0.680, 'c4':0.9869, '1/c4':1.0133, 'B3':0.510, 'B4':1.490, 'B5':0.504, 'B6':1.470, 'd2':3.735, '1/d2':0.2677, 'd3':0.729, 'D1':1.549, 'D2':5.921, 'D3':0.415, 'D4':1.585},\n",
    "                21: {'A':0.655, 'A2':0.173, 'A3':0.663, 'c4':0.9876, '1/c4':1.0126, 'B3':0.523, 'B4':1.477, 'B5':0.516, 'B6':1.459, 'd2':3.778, '1/d2':0.2647, 'd3':0.724, 'D1':1.605, 'D2':5.951, 'D3':0.425, 'D4':1.575},\n",
    "                22: {'A':0.640, 'A2':0.167, 'A3':0.647, 'c4':0.9882, '1/c4':1.0119, 'B3':0.534, 'B4':1.466, 'B5':0.528, 'B6':1.448, 'd2':3.819, '1/d2':0.2618, 'd3':0.720, 'D1':1.659, 'D2':5.979, 'D3':0.434, 'D4':1.566},\n",
    "                23: {'A':0.626, 'A2':0.162, 'A3':0.633, 'c4':0.9887, '1/c4':1.0114, 'B3':0.545, 'B4':1.455, 'B5':0.539, 'B6':1.438, 'd2':3.858, '1/d2':0.2592, 'd3':0.716, 'D1':1.710, 'D2':6.006, 'D3':0.443, 'D4':1.557},\n",
    "                24: {'A':0.612, 'A2':0.157, 'A3':0.619, 'c4':0.9892, '1/c4':1.0109, 'B3':0.555, 'B4':1.445, 'B5':0.549, 'B6':1.429, 'd2':3.895, '1/d2':0.2567, 'd3':0.712, 'D1':1.759, 'D2':6.031, 'D3':0.451, 'D4':1.548},\n",
    "                25: {'A':0.600, 'A2':0.153, 'A3':0.606, 'c4':0.9896, '1/c4':1.0105, 'B3':0.565, 'B4':1.435, 'B5':0.559, 'B6':1.420, 'd2':3.931, '1/d2':0.2544, 'd3':0.708, 'D1':1.806, 'D2':6.056, 'D3':0.459, 'D4':1.541},\n",
    "            }\n",
    "            \n",
    "            # Access the key:\n",
    "            dict_of_constants = dict_of_constants[self.sample_size]\n",
    "            \n",
    "        else: #>= 26\n",
    "            \n",
    "            dict_of_constants = {'A':(3/(self.sample_size**(0.5))), 'A2':0.153, \n",
    "                                 'A3':3/((4*(self.sample_size-1)/(4*self.sample_size-3))*(self.sample_size**(0.5))), \n",
    "                                 'c4':(4*(self.sample_size-1)/(4*self.sample_size-3)), \n",
    "                                 '1/c4':1/((4*(self.sample_size-1)/(4*self.sample_size-3))), \n",
    "                                 'B3':(1-3/(((4*(self.sample_size-1)/(4*self.sample_size-3)))*((2*(self.sample_size-1))**(0.5)))), \n",
    "                                 'B4':(1+3/(((4*(self.sample_size-1)/(4*self.sample_size-3)))*((2*(self.sample_size-1))**(0.5)))),\n",
    "                                 'B5':(((4*(self.sample_size-1)/(4*self.sample_size-3)))-3/((2*(self.sample_size-1))**(0.5))), \n",
    "                                 'B6':(((4*(self.sample_size-1)/(4*self.sample_size-3)))+3/((2*(self.sample_size-1))**(0.5))), \n",
    "                                 'd2':3.931, '1/d2':0.2544, 'd3':0.708, 'D1':1.806, 'D2':6.056, 'D3':0.459, 'D4':1.541}\n",
    "        \n",
    "        # Update the attribute\n",
    "        self.dict_of_constants = dict_of_constants\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_histogram_array (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        df = self.df\n",
    "        column_with_variable_to_be_analyzed = self.column_with_variable_to_be_analyzed\n",
    "        y_hist = df[column_with_variable_to_be_analyzed]\n",
    "        lowest = self.lowest\n",
    "        highest = self.highest\n",
    "        sample_size = self.sample_size\n",
    "        \n",
    "        # Number of bins set by the user:\n",
    "        total_of_bins = self.total_of_bins\n",
    "        \n",
    "        # Firstly, get the ideal bin-size according to the Montgomery's method:\n",
    "        # Douglas C. Montgomery (2009). Introduction to Statistical Process Control, \n",
    "        # Sixth Edition, John Wiley & Sons.\n",
    "        # Sort by the column to analyze (ascending order) and reset the index:\n",
    "        y_hist = y_hist.sort_values(ascending = True)\n",
    "        y_hist = y_hist.reset_index(drop = True)\n",
    "        #Calculo do bin size - largura do histograma:\n",
    "        #1: Encontrar o menor (lowest) e o maior (highest) valor dentro da tabela de dados)\n",
    "        #2: Calcular rangehist = highest - lowest\n",
    "        #3: Calcular quantidade de dados (samplesize) de entrada fornecidos\n",
    "        #4: Calcular a quantidade de celulas da tabela de frequencias (ncells)\n",
    "        #ncells = numero inteiro mais proximo da (raiz quadrada de samplesize)\n",
    "        #5: Calcular binsize = (df[column_to_analyze])rangehist/(ncells)\n",
    "        #ATENCAO: Nao se esquecer de converter range, ncells, samplesize e binsize para valores absolutos (modulos)\n",
    "        #isso porque a largura do histograma tem que ser um numero positivo \n",
    "\n",
    "        # bin-size\n",
    "        range_hist = abs(highest - lowest)\n",
    "        n_cells = int(np.rint((sample_size)**(0.5)))\n",
    "        # We must use the int function to guarantee that the ncells will store an\n",
    "        # integer number of cells (we cannot have a fraction of a sentence).\n",
    "        # The int function guarantees that the variable will be stored as an integer.\n",
    "        # The numpy.rint(a) function rounds elements of the array to the nearest integer.\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.rint.html\n",
    "        # For values exactly halfway between rounded decimal values, \n",
    "        # NumPy rounds to the nearest even value. \n",
    "        # Thus 1.5 and 2.5 round to 2.0; -0.5 and 0.5 round to 0.0; etc.\n",
    "        if (n_cells > 3):\n",
    "            \n",
    "            print(f\"Ideal number of histogram bins calculated through Montgomery's method = {n_cells} bins.\\n\")\n",
    "        \n",
    "        # Retrieve the histogram array hist_array\n",
    "        fig, ax = plt.subplots() # (0,0) not to show the plot now:\n",
    "        \n",
    "        # Get a 10-bins histogram:\n",
    "        hist_array = plt.hist(y_hist, bins = total_of_bins)\n",
    "        plt.delaxes(ax) # this will delete ax, so that it will not be plotted.\n",
    "        plt.show()\n",
    "        print(\"\") # use this print not to mix with the final plot\n",
    "\n",
    "        # hist_array is an array of arrays:\n",
    "        # hist_array = (array([count_1, count_2, ..., cont_n]), array([bin_center_1,...,\n",
    "        # bin_center_n])), where n = total_of_bins\n",
    "        # hist_array[0] is the array of countings for each bin, whereas hist_array[1] is\n",
    "        # the array of the bin center, i.e., the central value of the analyzed variable for\n",
    "        # that bin.\n",
    "\n",
    "        # It is possible that the hist_array[0] contains more elements than hist_array[1].\n",
    "        # This happens when the last bins created by the division contain zero elements.\n",
    "        # In this case, we have to pad the sequence of hist_array[0], completing it with zeros.\n",
    "\n",
    "        MAX_LENGTH = max(len(hist_array[0]), len(hist_array[1])) # Get the length of the longest sequence\n",
    "        SEQUENCES = [list(hist_array[0]), list(hist_array[1])] # get a list of sequences to pad.\n",
    "        # Notice that we applied the list attribute to create a list of lists\n",
    "\n",
    "        # We cannot pad with the function pad_sequences from tensorflow because it converts all values\n",
    "        # to integers. Then, we have to pad the sequences by looping through the elements from SEQUENCES:\n",
    "\n",
    "        # Start a support_list\n",
    "        support_list = []\n",
    "\n",
    "        # loop through each sequence in SEQUENCES:\n",
    "        for sequence in SEQUENCES:\n",
    "            # add a zero at the end of the sequence until its length reaches MAX_LENGTH\n",
    "            while (len(sequence) < MAX_LENGTH):\n",
    "\n",
    "                sequence.append(0)\n",
    "\n",
    "            # append the sequence to support_list:\n",
    "            support_list.append(sequence)\n",
    "\n",
    "        # Tuples and arrays are immutable. It means they do not support assignment, i.e., we cannot\n",
    "        # do tuple[0] = variable. Since arrays support vectorial (element-wise) operations, we can\n",
    "        # modify the whole array making it equals to support_list at once by using function np.array:\n",
    "        hist_array = np.array(support_list)\n",
    "\n",
    "        # Get the bin_size as the average difference between successive elements from support_list[1]:\n",
    "\n",
    "        diff_lists = []\n",
    "\n",
    "        for i in range (1, len(support_list[1])):\n",
    "\n",
    "            diff_lists.append(support_list[1][i] - support_list[1][(i-1)])\n",
    "\n",
    "        # Now, get the mean value as the bin_size:\n",
    "        bin_size = np.amax(np.array(diff_lists))\n",
    "\n",
    "        # Let's get the frequency table, which will be saved on DATASET (to get the code\n",
    "        # equivalent to the code for the function 'histogram'):\n",
    "\n",
    "        DATASET = pd.DataFrame(data = {'bin_center': hist_array[1], 'count': hist_array[0]})\n",
    "\n",
    "        # Get a lists of bin_center and column_to_analyze:\n",
    "        list_of_bins = list(hist_array[1])\n",
    "        list_of_counts = list(hist_array[0])\n",
    "\n",
    "        # get the maximum count:\n",
    "        max_count = DATASET['count'].max()\n",
    "        # Get the index of the max count:\n",
    "        max_count_index = list_of_counts.index(max_count)\n",
    "\n",
    "        # Get the value bin_center correspondent to the max count (maximum probability):\n",
    "        bin_of_max_proba = list_of_bins[max_count_index]\n",
    "        bin_after_the_max_proba = list_of_bins[(max_count_index + 1)] # the next bin\n",
    "        number_of_bins = len(DATASET) # Total of elements on the frequency table\n",
    "        \n",
    "        # Obtain a list of differences between bins\n",
    "        bins_diffs = [(list_of_bins[i] - list_of_bins[(i-1)]) for i in range (1, len(list_of_bins))]\n",
    "        # Convert it to Pandas series and use the mean method to retrieve the average bin size:\n",
    "        bin_size = pd.Series(bins_diffs).mean()\n",
    "        \n",
    "        self.histogram_dict = {'df': DATASET, 'list_of_bins': list_of_bins, 'list_of_counts': list_of_counts,\n",
    "                              'max_count': max_count, 'max_count_index': max_count_index,\n",
    "                              'bin_of_max_proba': bin_of_max_proba, 'bin_after_the_max_proba': bin_after_the_max_proba,\n",
    "                              'number_of_bins': number_of_bins, 'bin_size': bin_size}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_desired_normal (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get a normal completely (6s) in the specifications, and centered\n",
    "        # within these limits\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        histogram_dict = self.histogram_dict\n",
    "        max_count = histogram_dict['max_count']\n",
    "        \n",
    "        specification_limits = self.specification_limits\n",
    "        \n",
    "        lower_spec = specification_limits['lower_spec_lim']\n",
    "        upper_spec = specification_limits['upper_spec_lim']\n",
    "        \n",
    "        if (lower_spec is None):\n",
    "            \n",
    "            # There is no lower specification: everything below it is in the specifications.\n",
    "            # Make it mean - 6sigma (virtually infinite).\n",
    "            lower_spec = mu - 6*(sigma)\n",
    "            # Update the dictionary:\n",
    "            specification_limits['lower_spec_lim'] = lower_spec\n",
    "        \n",
    "        if (upper_spec is None):\n",
    "            \n",
    "            # There is no upper specification: everything above it is in the specifications.\n",
    "            # Make it mean + 6sigma (virtually infinite).\n",
    "            upper_spec = mu + 6*(sigma)\n",
    "            # Update the dictionary:\n",
    "            specification_limits['upper_spec_lim'] = upper_spec\n",
    "        \n",
    "        # Desired normal mu: center of the specification limits.\n",
    "        desired_mu = (lower_spec + upper_spec)/2\n",
    "        \n",
    "        # Desired sigma: 6 times the variation within the specific limits\n",
    "        desired_sigma = (upper_spec - lower_spec)/6\n",
    "        \n",
    "        if (desired_sigma == 0):\n",
    "            print(\"Impossible to obtain a normal curve overlayed, because the standard deviation is zero.\\n\")\n",
    "            print(\"The analyzed variable is constant throughout the whole sample space.\\n\")\n",
    "            \n",
    "            # Get a dictionary of empty lists for this case\n",
    "            desired_normal = {'x': [], 'y':[]}\n",
    "            \n",
    "        else:\n",
    "            # create lists to store the normal curve. Center the normal curve in the bin\n",
    "            # of maximum bar (max probability, which will not be the mean if the curve\n",
    "            # is skewed). For normal distributions, this value will be the mean and the median.\n",
    "\n",
    "            # set the lowest value x used for obtaining the normal curve as center_of_bin_of_max_proba - 4*sigma\n",
    "            # the highest x will be center_of_bin_of_max_proba - 4*sigma\n",
    "            # each value will be created by incrementing (0.10)*sigma\n",
    "\n",
    "            # The arrays created by the plt.hist method present the value of the extreme left \n",
    "            # (the beginning) of the histogram bars, not the bin center. So, let's add half of the bin size\n",
    "            # to the bin_of_max_proba, so that the adjusted normal will be positioned on the center of the\n",
    "            # bar of maximum probability. We can do it by taking the average between bin_of_max_proba\n",
    "            # and the following bin, bin_after_the_max_proba:\n",
    "            \n",
    "            # Let's create a normal around the desired mean value. Firstly, create the range X - 4s to\n",
    "            # X + 4s. The probabilities will be calculated for each value in this range:\n",
    "\n",
    "            x = (desired_mu - (4 * desired_sigma))\n",
    "            x_of_normal = [x]\n",
    "\n",
    "            while (x < (desired_mu + (4 * desired_sigma))):\n",
    "\n",
    "                x = x + (0.10)*(desired_sigma)\n",
    "                x_of_normal.append(x)\n",
    "\n",
    "            # Convert the list to a NumPy array, so that it is possible to perform element-wise\n",
    "            # (vectorial) operations:\n",
    "            x_of_normal = np.array(x_of_normal)\n",
    "\n",
    "            # Create an array of the normal curve y, applying the normal curve equation:\n",
    "            # normal curve = 1/(sigma* ((2*pi)**(0.5))) * exp(-((x-mu)**2)/(2*(sigma**2)))\n",
    "            # where pi = 3,14...., and exp is the exponential function (base e)\n",
    "            # Let's center the normal curve on desired_mu:\n",
    "            y_normal = (1 / (desired_sigma* (np.sqrt(2 * (np.pi))))) * (np.exp(-0.5 * (((1 / desired_sigma) * (x_of_normal - desired_mu)) ** 2)))\n",
    "            y_normal = np.array(y_normal)\n",
    "\n",
    "            # Pick the maximum value obtained for y_normal:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "            y_normal_max = np.amax(y_normal)\n",
    "\n",
    "            # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "            # with y_normal_max:\n",
    "            correction_factor = max_count/(y_normal_max)\n",
    "\n",
    "            # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "            y_normal = y_normal * correction_factor\n",
    "            # Now the probability density function (values originally from 0 to 1) has the same \n",
    "            # height as the histogram.\n",
    "            \n",
    "            desired_normal = {'x': x_of_normal, 'y': y_normal}\n",
    "        \n",
    "        # Nest the desired_normal dictionary into specification_limits dictionary:\n",
    "        specification_limits['desired_normal'] = desired_normal\n",
    "        # Update the attribute:\n",
    "        self.specification_limits = specification_limits\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_fitted_normal (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get a normal completely (6s) in the specifications, and centered\n",
    "        # within these limits\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        histogram_dict = self.histogram_dict\n",
    "        max_count = histogram_dict['max_count']\n",
    "        bin_of_max_proba = histogram_dict['bin_of_max_proba']\n",
    "        specification_limits = self.specification_limits\n",
    "        \n",
    "        if (sigma == 0):\n",
    "            print(\"Impossible to obtain a normal curve overlayed, because the standard deviation is zero.\\n\")\n",
    "            print(\"The analyzed variable is constant throughout the whole sample space.\\n\")\n",
    "            \n",
    "            # Get a dictionary of empty lists for this case\n",
    "            fitted_normal = {'x': [], 'y':[]}\n",
    "            \n",
    "        else:\n",
    "            # create lists to store the normal curve. Center the normal curve in the bin\n",
    "            # of maximum bar (max probability, which will not be the mean if the curve\n",
    "            # is skewed). For normal distributions, this value will be the mean and the median.\n",
    "\n",
    "            # set the lowest value x used for obtaining the normal curve as bin_of_max_proba - 4*sigma\n",
    "            # the highest x will be bin_of_max_proba - 4*sigma\n",
    "            # each value will be created by incrementing (0.10)*sigma\n",
    "\n",
    "            x = (bin_of_max_proba - (4 * sigma))\n",
    "            x_of_normal = [x]\n",
    "\n",
    "            while (x < (bin_of_max_proba + (4 * sigma))):\n",
    "\n",
    "                x = x + (0.10)*(sigma)\n",
    "                x_of_normal.append(x)\n",
    "\n",
    "            # Convert the list to a NumPy array, so that it is possible to perform element-wise\n",
    "            # (vectorial) operations:\n",
    "            x_of_normal = np.array(x_of_normal)\n",
    "\n",
    "            # Create an array of the normal curve y, applying the normal curve equation:\n",
    "            # normal curve = 1/(sigma* ((2*pi)**(0.5))) * exp(-((x-mu)**2)/(2*(sigma**2)))\n",
    "            # where pi = 3,14...., and exp is the exponential function (base e)\n",
    "            # Let's center the normal curve on bin_of_max_proba\n",
    "            y_normal = (1 / (sigma* (np.sqrt(2 * (np.pi))))) * (np.exp(-0.5 * (((1 / sigma) * (x_of_normal - bin_of_max_proba)) ** 2)))\n",
    "            y_normal = np.array(y_normal)\n",
    "\n",
    "            # Pick the maximum value obtained for y_normal:\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "            y_normal_max = np.amax(y_normal)\n",
    "\n",
    "            # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "            # with y_normal_max:\n",
    "            correction_factor = max_count/(y_normal_max)\n",
    "\n",
    "            # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "            y_normal = y_normal * correction_factor\n",
    "            \n",
    "            fitted_normal = {'x': x_of_normal, 'y': y_normal}\n",
    "        \n",
    "        # Nest the fitted_normal dictionary into specification_limits dictionary:\n",
    "        specification_limits['fitted_normal'] = fitted_normal\n",
    "        # Update the attribute:\n",
    "        self.specification_limits = specification_limits\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_actual_pdf (self):\n",
    "        \n",
    "        # PDF: probability density function.\n",
    "        # KDE: Kernel density estimation: estimation of the actual probability density\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html#scipy.stats.gaussian_kde\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from scipy import stats\n",
    "        \n",
    "        df = self.df\n",
    "        column_with_variable_to_be_analyzed = self.column_with_variable_to_be_analyzed\n",
    "        array_to_analyze = np.array(df[column_with_variable_to_be_analyzed])\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        lowest = self.lowest\n",
    "        highest = self.highest\n",
    "        sample_size = self.sample_size\n",
    "        \n",
    "        histogram_dict = self.histogram_dict\n",
    "        max_count = histogram_dict['max_count']\n",
    "        specification_limits = self.specification_limits \n",
    "        \n",
    "        # Get the KDE object\n",
    "        kde = stats.gaussian_kde(array_to_analyze)\n",
    "        \n",
    "        # Here, kde may represent a distribution with high skewness and kurtosis. So, let's check\n",
    "        # if the intervals mu - 6s and mu + 6s are represented by the array:\n",
    "        inf_kde_lim = mu - 6*sigma\n",
    "        sup_kde_lim = mu + 6*sigma\n",
    "        \n",
    "        if (inf_kde_lim > min(list(array_to_analyze))):\n",
    "            # make the inferior limit the minimum value from the array:\n",
    "            inf_kde_lim = min(list(array_to_analyze))\n",
    "        \n",
    "        if (sup_kde_lim < max(list(array_to_analyze))):\n",
    "            # make the superior limit the minimum value from the array:\n",
    "            sup_kde_lim = max(list(array_to_analyze))\n",
    "        \n",
    "        # Let's obtain a X array, consisting with all values from which we will calculate the PDF:\n",
    "        new_x = inf_kde_lim\n",
    "        new_x_list = [new_x]\n",
    "        \n",
    "        while ((new_x) < sup_kde_lim):\n",
    "            # There is already the first element, so go to the next one.\n",
    "            new_x = new_x + (0.10)*sigma\n",
    "            new_x_list.append(new_x)\n",
    "        \n",
    "        # Convert the new_x_list to NumPy array, making it the array_to_analyze:\n",
    "        array_to_analyze = np.array(new_x_list)\n",
    "        \n",
    "        # Apply the pdf method to convert the array_to_analyze into the array of probabilities:\n",
    "        # i.e., calculate the probability for each one of the values in array_to_analyze:\n",
    "        # PDF: Probability density function\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.pdf.html#scipy.stats.gaussian_kde.pdf\n",
    "        array_of_probs = kde.pdf(array_to_analyze)\n",
    "        \n",
    "        # Pick the maximum value obtained for array_of_probs:\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax\n",
    "        array_of_probs_max = np.amax(array_of_probs)\n",
    "\n",
    "        # Let's get a correction factor, comparing the maximum of the histogram counting, max_count,\n",
    "        # with array_of_probs_max:\n",
    "        correction_factor = max_count/(array_of_probs_max)\n",
    "\n",
    "        # Now, multiply each value of the array y_normal by the correction factor, to adjust the height:\n",
    "        array_of_probs = array_of_probs * correction_factor\n",
    "        # Now the probability density function (values originally from 0 to 1) has the same \n",
    "        # height as the histogram.\n",
    "        \n",
    "        # Define a dictionary\n",
    "        # X of the probability density plot: values from the series being analyzed.\n",
    "        # Y of the probability density plot: probabilities calculated for each X.\n",
    "        actual_pdf = {'x': array_to_analyze, 'y': array_of_probs}\n",
    "        \n",
    "        # Nest the desired_normal dictionary into specification_limits dictionary:\n",
    "        specification_limits['actual_pdf'] = actual_pdf\n",
    "        # Update the attribute:\n",
    "        self.specification_limits = specification_limits\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_capability_indicators (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Get a normal completely (6s) in the specifications, and centered\n",
    "        # within these limits\n",
    "        \n",
    "        mu = self.mu\n",
    "        sigma = self.sigma\n",
    "        histogram_dict = self.histogram_dict\n",
    "        bin_of_max_proba = histogram_dict['bin_of_max_proba']\n",
    "        bin_after_the_max_proba = histogram_dict['bin_after_the_max_proba']\n",
    "        max_count = histogram_dict['max_count']\n",
    "        \n",
    "        specification_limits = self.specification_limits\n",
    "        lower_spec = specification_limits['lower_spec_lim']\n",
    "        upper_spec = specification_limits['upper_spec_lim']\n",
    "        desired_mu = (lower_spec + upper_spec)/2 \n",
    "        # center of the specification limits: we want the mean to be in the center of the\n",
    "        # specification limits\n",
    "        \n",
    "        range_spec = abs(upper_spec - lower_spec)\n",
    "        \n",
    "        # Get the constant:\n",
    "        self = self.get_constants()\n",
    "        dict_of_constants = self.dict_of_constants\n",
    "        constant = dict_of_constants['1/c4']\n",
    "        \n",
    "        # Calculate corrected sigma:\n",
    "        sigma_corrected = sigma*constant\n",
    "        \n",
    "        # Calculate the capability indicators, adding them to the\n",
    "        # capability_dict\n",
    "        cp = (range_spec)/(6*sigma_corrected)\n",
    "        cr = 100*(6*sigma_corrected)/(range_spec)\n",
    "        cm = (range_spec)/(8*sigma_corrected)\n",
    "        zu = (upper_spec - mu)/(sigma_corrected)\n",
    "        zl = (mu - lower_spec)/(sigma_corrected)\n",
    "        \n",
    "        z_min = min(zu, zl)\n",
    "        cpk = (z_min)/3\n",
    "\n",
    "        cpm_factor = 1 + ((mu - desired_mu)/sigma_corrected)**2\n",
    "        cpm_factor = cpm_factor**(0.5) # square root\n",
    "        cpm = (cp)/(cpm_factor)\n",
    "        \n",
    "        capability_dict = {'indicator': ['cp', 'cr', 'cm', 'zu', 'zl', 'z_min', 'cpk', 'cpm'], \n",
    "                            'value': [cp, cr, cm, zu, zl, z_min, cpk, cpm]}\n",
    "        # Already in format for pd.DataFrame constructor\n",
    "        \n",
    "        # Update the attribute:\n",
    "        self.capability_dict = capability_dict\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def capability_interpretation (self):\n",
    "       \n",
    "        print(\"Capable process: a process which attends its specifications.\")\n",
    "        print(\"Naturally, we want processes capable of attending the specifications.\\n\")\n",
    "        \n",
    "        print(\"Specification range:\")\n",
    "        print(\"Absolute value of the difference between the upper and the lower limits of specification.\\n\")\n",
    "        \n",
    "        print(\"6s interval:\")\n",
    "        print(\"Consider mean value = mu; standard deviation = s\")\n",
    "        print(\"For a normal distribution, 99.7% of the values range from its (mu - 3s) to (mu + 3s).\")\n",
    "        print(\"So, if the process follows the normal distribution, we can consider that virtually all of the data is in this range with 6s width.\\n\")\n",
    "        \n",
    "        print (\"Cp:\")\n",
    "        print (\"Relation between specification range and 6s.\\n\")\n",
    "        \n",
    "        print(\"Cr:\")\n",
    "        print(\"Usually, 6s > specification range.\")\n",
    "        print(\"So, the inverse of Cp is the fraction of 6s correspondent to the specification range.\")\n",
    "        print(\"Example: if 1/Cp = 0.2, then the specification range corresponds to 0.20 (20%) of the 6s interval.\")\n",
    "        print(\"Cr = 100 x (1/Cp) - the percent of 6s correspondent to the specification range.\")\n",
    "        print(\"Again, if 1/Cp = 0.2, then Cr = 20: the specification range corresponds to 20% of the 6s interval.\\n\")\n",
    "        \n",
    "        print(\"Cm:\")\n",
    "        print(\"It is a more generalized version of Cp.\")\n",
    "        print(\"Cm is the relation between specification range and 8s.\")\n",
    "        print(\"Then, even highly distant values from long-tailed curves are analyzed by this indicator.\\n\")\n",
    "        \n",
    "        print(\"Zu:\")\n",
    "        print(\"Represents how far is the mean of the values from the upper specification limit.\")\n",
    "        print(\"Zu = ([upper specification limit] - mu)/s\")\n",
    "        print(\"A higher Zu indicates a mean value lower than (and more distant from) the upper specification.\")\n",
    "        print(\"A negative Zu, in turns, indicates that the mean value is greater than the upper specification (i.e.: in average, specification is not attended).\\n\")\n",
    "        \n",
    "        print(\"Zl:\")\n",
    "        print(\"Represents how far is the mean of the values from the lower specification limit.\")\n",
    "        print(\"Zl = (mu - [lower specification limit])/s\\n\")\n",
    "        print(\"A higher Zl indicates a mean value higher than  (and more distant from) the lower specification.\")\n",
    "        print(\"A negative Zl, in turns, indicates that the mean value is inferior than the lower specification (i.e.: in average, specification is not attended).\\n\")\n",
    "        \n",
    "        print(\"Zmin:\")\n",
    "        print(\"It is the minimum value between Zu and Zl.\")\n",
    "        print(\"So, Zmin indicates which specification is more difficult for the process to attend: the upper or the lower one.\")\n",
    "        print(\"Example: if Zmin = Zl, the mean of the process is closer to the lower specification than it is from the upper specification.\")\n",
    "        print(\"If Zmin, Zu, and Zl are equal, than the process is equally distant from the two specifications.\")\n",
    "        print(\"Again, if Zmin is negative, at least one of the specifications is not attended.\\n\")\n",
    "        \n",
    "        print(\"Cpk:\")\n",
    "        print(\"This is the most fundamental capability indicator.\")\n",
    "        print(\"Consider again that 99.7% of the normally distributed data are within [(mu - 3s), (mu + 3s)].\")\n",
    "        print(\"Cpk = Zmin/3\")\n",
    "        print(\"Cpk = min((([upper specification limit] - mu)/3s), ((mu - [lower specification limit])/3s))\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Cpk simultaneously assess the process centrality, and if the process is capable of attending its specifications.\")\n",
    "        print(\"Here, the process centrality is verified as results which are well and simetrically distributed throughout the mean of the specification limits.\")\n",
    "        print(\"Basically, a perfectly-centralized process has its mean equally distant from both specifications\")\n",
    "        print(\"i.e., the mean is in the center of the specification interval.\")\n",
    "        print(\"Cpk = + 1 is usually considered the minimum value acceptable for a process.\")\n",
    "        print(\"Many quality programs define reaching Cpk = + 1.33 as their goal.\")\n",
    "        print(\"A 6-sigma process, in turns, is defined as a process with Cpk = + 2.\")\n",
    "        print(\"\\n\")\n",
    "        print(\"High values of Cpk indicate that the process is not only centralized, but that the differences\")\n",
    "        print(\"([upper specification limit] - mu) and (mu - [lower specification limit]) are greater than 3s.\")\n",
    "        print(\"Since mu +- 3s is the range for 99.7% of data, it indicates that most of the values generated fall in a range\")\n",
    "        print(\"that is only a fraction of the specification range.\")\n",
    "        print(\"So, it is easier for the process to attend the specifications.\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Cpk values inferior than 1 indicate that at least one of the intervals ([upper specification limit] - mu) and (mu - [lower specification limit])\")\n",
    "        print(\"is lower than 3s, i.e., the process naturally generates values beyond at least one of the specifications.\")\n",
    "        print(\"Low values of Cpk (in particular the negative ones) indicate not-centralized processes and processes not capable of attending their specifications.\")\n",
    "        print(\"So, lower (and, specially, more negative) Cpk: process' outputs more distant from the specifications.\\n\")\n",
    "        \n",
    "        print(\"Cpm:\")\n",
    "        print(\"This indicator is a more generalized version of the Cpk.\")\n",
    "        print(\"It basically consists on a standard normalization of the Cpk.\")\n",
    "        print(\"For that, a normalization factor is defined as:\")\n",
    "        print(\"factor = square root(1 + ((mu - target)/s)**2)\")\n",
    "        print(\"where target is the center of the specification limits, and **2 represents the second power (square)\")\n",
    "        print(\"Cpm = Cpk/(factor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram (df, column_to_analyze, total_of_bins = 10, normal_curve_overlay = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # column_to_analyze: string with the name of the column that will be analyzed.\n",
    "    # column_to_analyze = 'col1' obtain a histogram from column 1.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Sort by the column to analyze (ascending order) and reset the index:\n",
    "    DATASET = DATASET.sort_values(by = column_to_analyze, ascending = True)\n",
    "    \n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # Create an instance (object) from class capability_analysis:\n",
    "    capability_obj = capability_analysis(df = DATASET, column_with_variable_to_be_analyzed = column_to_analyze, specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}, total_of_bins = total_of_bins)\n",
    "     \n",
    "    # Get histogram array:\n",
    "    capability_obj = capability_obj.get_histogram_array()\n",
    "    # Attribute .histogram_dict: dictionary with keys 'list_of_bins' and 'list_of_counts'.\n",
    "    \n",
    "    # Get fitted normal:\n",
    "    capability_obj = capability_obj.get_fitted_normal()\n",
    "    # Now the .specification_limits attribute contains the nested dict desired_normal = {'x': x_of_normal, 'y': y_normal}\n",
    "    # in key 'fitted_normal'.\n",
    "    \n",
    "    # Get the actual probability density function (PDF):\n",
    "    capability_obj = capability_obj.get_actual_pdf()\n",
    "    # Now the dictionary in the attribute .specification_limits has the nested dict actual_pdf = {'x': array_to_analyze, 'y': array_of_probs}\n",
    "    # in key 'actual_pdf'.\n",
    "    \n",
    "    # Retrieve general statistics:\n",
    "    stats_dict = {\n",
    "        \n",
    "        'sample_size': capability_obj.sample_size,\n",
    "        'mu': capability_obj.mu,\n",
    "        'median': capability_obj.median,\n",
    "        'sigma': capability_obj.sigma,\n",
    "        'lowest': capability_obj.lowest,\n",
    "        'highest': capability_obj.highest\n",
    "    }\n",
    "    \n",
    "    # Retrieve the histogram dict:\n",
    "    histogram_dict = capability_obj.histogram_dict\n",
    "    \n",
    "    # Retrieve the specification limits dictionary updated:\n",
    "    specification_limits = capability_obj.specification_limits\n",
    "    # Retrieve the desired normal and actual PDFs dictionaries:\n",
    "    fitted_normal = specification_limits['fitted_normal']\n",
    "    actual_pdf = specification_limits['actual_pdf']\n",
    "    \n",
    "    string_for_title = \" - $\\mu = %.2f$, $\\sigma = %.2f$\" %(stats_dict['mu'], stats_dict['sigma'])\n",
    "    \n",
    "    if not (plot_title is None):\n",
    "        plot_title = plot_title + string_for_title\n",
    "        # %.2f: the number between . and f indicates the number of printed decimal cases\n",
    "        # the notation $\\ - Latex code for printing formatted equations and symbols.\n",
    "    \n",
    "    else:\n",
    "        # Set graphic title\n",
    "        plot_title = f\"histogram_of_{column_to_analyze}\" + string_for_title\n",
    "\n",
    "    if (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        horizontal_axis_title = column_to_analyze\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        vertical_axis_title = \"Counting/Frequency\"\n",
    "        \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    y_hist = DATASET[column_to_analyze]\n",
    "    \n",
    "    # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    #STANDARD MATPLOTLIB METHOD:\n",
    "    #bins = number of bins (intervals) of the histogram. Adjust it manually\n",
    "    #increasing bins will increase the histogram's resolution, but height of bars\n",
    "    \n",
    "    ax.hist(y_hist, bins = total_of_bins, alpha = OPACITY, label = f'counting_of\\n{column_to_analyze}', color = 'darkblue')\n",
    "    #ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #IF GRAPHIC IS NOT SHOWN: THAT IS BECAUSE THE DISTANCES BETWEEN VALUES ARE LOW, AND YOU WILL\n",
    "    #HAVE TO USE THE STANDARD HISTOGRAM METHOD FROM MATPLOTLIB.\n",
    "    #TO DO THAT, UNMARK LINE ABOVE: ax.hist(y, bins=20, width = bar_width, label=xlabel, color='blue')\n",
    "    #AND MARK LINE BELOW AS COMMENT: ax.bar(xhist, yhist, width = bar_width, label=xlabel, color='blue')\n",
    "    \n",
    "    #IF YOU WANT TO CREATE GRAPHIC AS A BAR CHART BASED ON THE CALCULATED DISTRIBUTION TABLE, \n",
    "    #MARK THE LINE ABOVE AS COMMENT AND UNMARK LINE BELOW:\n",
    "    #ax.bar(x_hist, y_hist, label = f'counting_of\\n{column_to_analyze}', color = 'darkblue')\n",
    "    #ajuste manualmente a largura, width, para deixar as barras mais ou menos proximas\n",
    "    \n",
    "    # Plot the probability density function for the data:\n",
    "    pdf_x = actual_pdf['x']\n",
    "    pdf_y = actual_pdf['y']\n",
    "    \n",
    "    ax.plot(pdf_x, pdf_y, color = 'darkgreen', linestyle = '-', alpha = OPACITY, label = 'probability\\ndensity')\n",
    "    \n",
    "    if (normal_curve_overlay == True):\n",
    "        \n",
    "        # Check if a normal curve was obtained:\n",
    "        x_of_normal = fitted_normal['x']\n",
    "        y_normal = fitted_normal['y']\n",
    "\n",
    "        if (len(x_of_normal) > 0):\n",
    "            # Non-empty list, add the normal curve:\n",
    "            ax.plot(x_of_normal, y_normal, color = 'crimson', linestyle = 'dashed', alpha = OPACITY, label = 'expected\\nnormal_curve')\n",
    "\n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(horizontal_axis_title)\n",
    "    ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    ax.grid(grid) # show grid or not\n",
    "    ax.legend(loc = 'upper right')\n",
    "    # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "    # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "    # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"histogram\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "      \n",
    "    stats_dict = {\n",
    "                  'statistics': ['mean', 'median', 'standard_deviation', f'lowest_{column_to_analyze}', \n",
    "                                f'highest_{column_to_analyze}', 'count_of_values', 'number_of_bins', \n",
    "                                 'bin_size', 'bin_of_max_proba', 'count_on_bin_of_max_proba'],\n",
    "                  'value': [stats_dict['mu'], stats_dict['median'], stats_dict['sigma'], \n",
    "                            stats_dict['lowest'], stats_dict['highest'], stats_dict['sample_size'], \n",
    "                            histogram_dict['number_of_bins'], histogram_dict['bin_size'], \n",
    "                            histogram_dict['bin_of_max_proba'], histogram_dict['max_count']]\n",
    "                 }\n",
    "    \n",
    "    # Convert it to a Pandas dataframe setting the list 'statistics' as the index:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "    general_stats = pd.DataFrame(data = stats_dict)\n",
    "    \n",
    "    # Set the column 'statistics' as the index of the dataframe, using set_index method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\n",
    "    \n",
    "    # If inplace = True, modifies the DataFrame in place (do not create a new object).\n",
    "    # Then, we do not create an object equal to the expression. We simply apply the method (so,\n",
    "    # None is returned from the method):\n",
    "    general_stats.set_index(['statistics'], inplace = True)\n",
    "    \n",
    "    print(\"Check the general statistics from the analyzed variable:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(general_stats)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(general_stats)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the frequency table:\\n\")\n",
    "    \n",
    "    freq_table = histogram_dict['df']\n",
    "    \n",
    "    try:    \n",
    "        display(freq_table)    \n",
    "    except:\n",
    "        print(freq_table)\n",
    "\n",
    "    return general_stats, freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a8dd096f-d689-4229-944b-dd87a9ac886d"
   },
   "source": [
    "# **Function for testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_normality (df, column_to_analyze, column_with_labels_to_test_subgroups = None, alpha = 0.10, show_probability_plot = True, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "    # Check https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    # Check https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/reference/generated/scipy.stats.normaltest.html\n",
    "    \n",
    "    # WARNING: The statistical tests require at least 20 samples\n",
    "    \n",
    "    # column_to_analyze: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "    # in quotes.\n",
    "    # e.g. column_to_analyze = 'col1' will analyze a column named 'col1'.\n",
    "    \n",
    "    # column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "    # subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "    # it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "    # will retrieve the labels from 'col2'.\n",
    "    # Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "    # will be tested.\n",
    "    \n",
    "    # Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "    # Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "    # Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "    # results.\n",
    "    \n",
    "    print(\"WARNING: The statistical tests require at least 20 samples.\\n\")\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"p-value: probability that data is described by the normal distribution.\")\n",
    "    print(\"Criterion: the series is not described by normal if p < alpha = %.3f.\" %(alpha))\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Start a list to store the different Pandas series to test:\n",
    "    list_of_dicts = []\n",
    "    \n",
    "    if not (column_with_labels_to_test_subgroups is None):\n",
    "        \n",
    "        # 1. Get the unique values from column_with_labels_to_test_subgroups\n",
    "        # and save it as the list labels_list:\n",
    "        labels_list = list(DATASET[column_with_labels_to_test_subgroups].unique())\n",
    "        \n",
    "        # 2. Loop through each element from labels_list:\n",
    "        for label in labels_list:\n",
    "            \n",
    "            # 3. Create a copy of the DATASET, filtering for entries where \n",
    "            # column_with_labels_to_test_subgroups == label:\n",
    "            filtered_df = (DATASET[DATASET[column_with_labels_to_test_subgroups] == label]).copy(deep = True)\n",
    "            # 4. Reset index of the copied dataframe:\n",
    "            filtered_df = filtered_df.reset_index(drop = True)\n",
    "            # 5. Create a dictionary, with an identification of the series, and the series\n",
    "            # that will be tested:\n",
    "            series_dict = {'series_id': (column_to_analyze + \"_\" + label), \n",
    "                           'series': filtered_df[column_to_analyze],\n",
    "                           'total_elements_to_test': len(filtered_df[column_to_analyze])}\n",
    "            \n",
    "            # 6. Append this dictionary to the list of series:\n",
    "            list_of_dicts.append(series_dict)\n",
    "        \n",
    "    else:\n",
    "        # In this case, the only series is the column itself. So, let's create a dictionary with\n",
    "        # same structure:\n",
    "        series_dict = {'series_id': column_to_analyze, 'series': DATASET[column_to_analyze],\n",
    "                       'total_elements_to_test': len(DATASET[column_to_analyze])}\n",
    "        \n",
    "        # Append this dictionary to the list of series:\n",
    "        list_of_dicts.append(series_dict)\n",
    "    \n",
    "    \n",
    "    # Now, loop through each element from the list of series:\n",
    "    \n",
    "    for series_dict in list_of_dicts:\n",
    "        \n",
    "        # start a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Check if there are at least 20 samples to test:\n",
    "        series_id = series_dict['series_id']\n",
    "        total_elements_to_test = series_dict['total_elements_to_test']\n",
    "        \n",
    "        # Create an instance (object) from class capability_analysis:\n",
    "        capability_obj = capability_analysis(df = DATASET, column_with_variable_to_be_analyzed = series_id, specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}, alpha = alpha)\n",
    "        \n",
    "        # Check data normality:\n",
    "        capability_obj = capability_obj.check_data_normality()\n",
    "        # Attribute .normality_dict: dictionary with results from normality tests\n",
    "        \n",
    "        # Retrieve the normality dictionary:\n",
    "        normality_dict = capability_obj.normality_dict\n",
    "        # Nest it in series_dict:\n",
    "        series_dict['normality_dict'] = normality_dict\n",
    "        \n",
    "        # Finally, append the series dictionary to the support list:\n",
    "        support_list.append(series_dict)\n",
    "        \n",
    "        if ((total_elements_to_test >= 20) & (show_probability_plot == True)):\n",
    "            \n",
    "            y = series_dict['series']\n",
    "        \n",
    "            print(\"\\n\")\n",
    "            #Obtain the probability plot  \n",
    "            fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "            ax.set_title(f\"probability_plot_of_{series_id}_for_normal_distribution\")\n",
    "            \n",
    "            plot_results = stats.probplot(y, dist = 'norm', fit = True, plot = ax)\n",
    "            #This function resturns a tuple, so we must store it into res\n",
    "            \n",
    "            ax.grid(grid)\n",
    "            #ROTATE X AXIS IN XX DEGREES\n",
    "            plt.xticks(rotation = x_axis_rotation)\n",
    "            # XX = 70 DEGREES x_axis (Default)\n",
    "            #ROTATE Y AXIS IN XX DEGREES:\n",
    "            plt.yticks(rotation = y_axis_rotation)\n",
    "            # XX = 0 DEGREES y_axis (Default)   \n",
    "            \n",
    "            # Other distributions to check, see scipy Stats documentation. \n",
    "            # you could test dist=stats.loggamma, where stats was imported from scipy\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html#scipy.stats.probplot\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = \"probability_plot_normal\"\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 330 dpi\n",
    "                    png_resolution_dpi = 330\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            #plt.figure(figsize = (12, 8))\n",
    "            #fig.tight_layout()\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "                \n",
    "            print(\"\\n\")\n",
    "            \n",
    "    # Now we left the for loop, make the list of dicts support list itself:\n",
    "    list_of_dicts = support_list\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Finished normality tests. Returning a list of dictionaries, where each dictionary contains the series analyzed and the p-values obtained.\\n\")\n",
    "    print(\"Now, check general statistics of the data distribution:\\n\")\n",
    "    \n",
    "    # Now, let's obtain general statistics for all of the series, even those without the normality\n",
    "    # test results.\n",
    "    \n",
    "    # start a support list:\n",
    "    support_list = []\n",
    "    \n",
    "    for series_dict in list_of_dicts:\n",
    "        \n",
    "        y = series_dict['series']\n",
    "        # Guarantee it is still a pandas series:\n",
    "        y = pd.Series(y)\n",
    "        # Calculate data skewness and kurtosis\n",
    "    \n",
    "        # Skewness\n",
    "        data_skew = stats.skew(y)\n",
    "        # skewness = 0 : normally distributed.\n",
    "        # skewness > 0 : more weight in the left tail of the distribution.\n",
    "        # skewness < 0 : more weight in the right tail of the distribution.\n",
    "        # https://www.geeksforgeeks.org/scipy-stats-skew-python/\n",
    "\n",
    "        # Kurtosis\n",
    "        data_kurtosis = stats.kurtosis(y, fisher = True)\n",
    "        # scipy.stats.kurtosis(array, axis=0, fisher=True, bias=True) function \n",
    "        # calculates the kurtosis (Fisher or Pearson) of a data set. It is the the fourth \n",
    "        # central moment divided by the square of the variance. \n",
    "        # It is a measure of the “tailedness” i.e. descriptor of shape of probability \n",
    "        # distribution of a real-valued random variable. \n",
    "        # In simple terms, one can say it is a measure of how heavy tail is compared \n",
    "        # to a normal distribution.\n",
    "        # fisher parameter: fisher : Bool; Fisher’s definition is used (normal 0.0) if True; \n",
    "        # else Pearson’s definition is used (normal 3.0) if set to False.\n",
    "        # https://www.geeksforgeeks.org/scipy-stats-kurtosis-function-python/\n",
    "        print(\"A normal distribution should present no skewness (distribution distortion); and no kurtosis (long-tail).\\n\")\n",
    "        print(\"For the data analyzed:\\n\")\n",
    "        print(f\"skewness = {data_skew}\")\n",
    "        print(f\"kurtosis = {data_kurtosis}\\n\")\n",
    "\n",
    "        if (data_skew < 0):\n",
    "\n",
    "            print(f\"Skewness = {data_skew} < 0: more weight in the left tail of the distribution.\")\n",
    "\n",
    "        elif (data_skew > 0):\n",
    "\n",
    "            print(f\"Skewness = {data_skew} > 0: more weight in the right tail of the distribution.\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"Skewness = {data_skew} = 0: no distortion of the distribution.\")\n",
    "                \n",
    "\n",
    "        if (data_kurtosis == 0):\n",
    "\n",
    "            print(\"Data kurtosis = 0. No long-tail effects detected.\\n\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"The kurtosis different from zero indicates long-tail effects on the distribution.\\n\")\n",
    "\n",
    "        #Calculate the mode of the distribution:\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "        data_mode = stats.mode(y, axis = None)[0][0]\n",
    "        # returns an array of arrays. The first array is called mode=array and contains the mode.\n",
    "        # Axis: Default is 0. If None, compute over the whole array.\n",
    "        # we set axis = None to compute the general mode.\n",
    "\n",
    "        #Create general statistics dictionary:\n",
    "        general_statistics_dict = {\n",
    "\n",
    "            \"series_mean\": y.mean(),\n",
    "            \"series_variance\": y.var(),\n",
    "            \"series_standard_deviation\": y.std(),\n",
    "            \"series_skewness\": data_skew,\n",
    "            \"series_kurtosis\": data_kurtosis,\n",
    "            \"series_mode\": data_mode\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Add this dictionary to the series dictionary:\n",
    "        series_dict['general_statistics'] = general_statistics_dict\n",
    "        \n",
    "        # Append the dictionary to support list:\n",
    "        support_list.append(series_dict)\n",
    "    \n",
    "    # Now, make the list of dictionaries support_list itself:\n",
    "    list_of_dicts = support_list\n",
    "\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a653bd0-bbc3-460d-aeb3-17f33ec014a7"
   },
   "source": [
    "# **Function for column filtering (selecting); ordering; or renaming all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "azdata_cell_guid": "466f3105-a638-4287-9c49-ab68738d756e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def select_order_or_rename_columns (df, columns_list, mode = 'select_or_order_columns'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # MODE = 'select_or_order_columns' for filtering only the list of columns passed as columns_list,\n",
    "    # and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "    # the order of elements on the list will be the new order of columns.\n",
    "\n",
    "    # MODE = 'rename_columns' for renaming the columns with the names passed as columns_list. In this\n",
    "    # mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "    # the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "    # will result into columns with incorrect names.\n",
    "    \n",
    "    # columns_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: columns_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{DATASET.columns}\\n\")\n",
    "    \n",
    "    if ((columns_list is None) | (columns_list == np.nan)):\n",
    "        # empty list\n",
    "        columns_list = []\n",
    "    \n",
    "    if (len(columns_list) == 0):\n",
    "        print(\"Please, input a valid list of columns.\\n\")\n",
    "        return DATASET\n",
    "    \n",
    "    if (mode == 'select_or_order_columns'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        DATASET = DATASET[columns_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\\n\")\n",
    "        print(\"Check the new dataframe:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET)\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET)\n",
    "        \n",
    "    elif (mode == 'rename_columns'):\n",
    "        \n",
    "        # Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(columns_list) == len(DATASET.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\\n\")\n",
    "            return DATASET\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            DATASET.columns = columns_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\\n\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\\n\")\n",
    "            print(\"Check the new dataframe:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(DATASET)\n",
    "\n",
    "            except: # regular mode\n",
    "                print(DATASET)\n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'select_or_order_columns\\' or \\'rename_columns\\'.\")\n",
    "        return DATASET\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a37eac94-88c6-46a8-a647-c6228d0be157"
   },
   "source": [
    "# **Function for renaming specific columns from the dataframe; or cleaning columns' labels**\n",
    "- The function `select_order_or_rename_columns` requires the user to pass a list containing the names from all columns.\n",
    "- Also, this list must contain the columns in the correct order (the order they appear in the dataframe).\n",
    "- This function may manipulate one or several columns by call, and is not dependent on their order.\n",
    "- This function can also be used for cleaning the columns' labels: capitalize (upper case) or lower cases of all columns' names; replace substrings on columns' names; or eliminating trailing and leading white spaces or characters from columns' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "azdata_cell_guid": "acc6d13f-1331-4915-8037-60b1cd52e5d4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def rename_or_clean_columns_labels (df, mode = 'set_new_names', substring_to_be_replaced = ' ', new_substring_for_replacement = '_', trailing_substring = None, list_of_columns_labels = [{'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}, {'column_name': None, 'new_column_name': None}]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    # Pandas .rename method:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "    \n",
    "    # mode = 'set_new_names' will change the columns according to the specifications in\n",
    "    # list_of_columns_labels.\n",
    "    \n",
    "    # list_of_columns_labels = [{'column_name': None, 'new_column_name': None}]\n",
    "    # This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "    # the first one contains the original column name; and the second one contains the new name\n",
    "    # that will substitute the original one. The function will loop through all dictionaries in\n",
    "    # this list, access the values of the keys 'column_name', and it will be replaced (switched) \n",
    "    # by the correspondent value in key 'new_column_name'.\n",
    "    \n",
    "    # The object list_of_columns_labels must be declared as a list, \n",
    "    # in brackets, even if there is a single dictionary.\n",
    "    # Use always the same keys: 'column_name' for the original label; \n",
    "    # and 'new_column_name', for the correspondent new label.\n",
    "    # Notice that this function will not search substrings: it will substitute a value only when\n",
    "    # there is perfect correspondence between the string in 'column_name' and one of the columns\n",
    "    # labels. So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "    # values.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'column_name': original_col, 'new_column_name': new_col}, \n",
    "    # where original_col and new_col represent the strings for searching and replacement \n",
    "    # (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    # Example: list_of_columns_labels = [{'column_name': 'col1', 'new_column_name': 'col'}] will\n",
    "    # rename 'col1' as 'col'.\n",
    "    \n",
    "    \n",
    "    # mode = 'capitalize_columns' will capitalize all columns names (i.e., they will be put in\n",
    "    # upper case). e.g. a column named 'column' will be renamed as 'COLUMN'\n",
    "    \n",
    "    # mode = 'lowercase_columns' will lower the case of all columns names. e.g. a column named\n",
    "    # 'COLUMN' will be renamed as 'column'.\n",
    "    \n",
    "    # mode = 'replace_substring' will search on the columns names (strings) for the \n",
    "    # substring_to_be_replaced (which may be a character or a string); and will replace it by \n",
    "    # new_substring_for_replacement (which again may be either a character or a string). \n",
    "    # Numbers (integers or floats) will be automatically converted into strings.\n",
    "    # As an example, consider the default situation where we search for a whitespace ' ' \n",
    "    # and replace it by underscore '_': \n",
    "    # substring_to_be_replaced = ' ', new_substring_for_replacement = '_'  \n",
    "    # In this case, a column named 'new column' will be renamed as 'new_column'.\n",
    "    \n",
    "    # mode = 'trim' will remove all trailing or leading whitespaces from column names.\n",
    "    # e.g. a column named as ' col1 ' will be renamed as 'col1'; 'col2 ' will be renamed as\n",
    "    # 'col2'; and ' col3' will be renamed as 'col3'.\n",
    "    \n",
    "    # mode = 'eliminate_trailing_characters' will eliminate a defined trailing and leading \n",
    "    # substring from the columns' names. \n",
    "    # The substring must be indicated as trailing_substring, and its default, when no value\n",
    "    # is provided, is equivalent to mode = 'trim' (eliminate white spaces). \n",
    "    # e.g., if trailing_substring = '_test' and you have a column named 'col_test', it will be \n",
    "    # renamed as 'col'.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the columns were read as strings:\n",
    "    DATASET.columns = (DATASET.columns).astype(str)\n",
    "    # dataframe.columns is a Pandas Index object, so it has the dtype attribute as other Pandas\n",
    "    # objects. So, we can use the astype method to set its type as str or 'object' (or \"O\").\n",
    "    # Notice that there are situations with int Index, when integers are used as column names or\n",
    "    # as row indices. So, this portion guarantees that we can call the str attribute to apply string\n",
    "    # methods.\n",
    "    \n",
    "    if (mode == 'set_new_names'):\n",
    "        \n",
    "        # Start a mapping dictionary:\n",
    "        mapping_dict = {}\n",
    "        # This dictionary will be in the format required by .rename method: old column name as key,\n",
    "        # and new name as value.\n",
    "\n",
    "        # Loop through each element from list_of_columns_labels:\n",
    "        for dictionary in list_of_columns_labels:\n",
    "\n",
    "            # Access the values in keys:\n",
    "            column_name = dictionary['column_name']\n",
    "            new_column_name = dictionary['new_column_name']\n",
    "\n",
    "            # Check if neither is None:\n",
    "            if ((column_name is not None) & (new_column_name is not None)):\n",
    "                \n",
    "                # Guarantee that both were read as strings:\n",
    "                column_name = str(column_name)\n",
    "                new_column_name = str(new_column_name)\n",
    "\n",
    "                # Add it to the mapping dictionary setting column_name as key, and the new name as the\n",
    "                # value:\n",
    "                mapping_dict[column_name] = new_column_name\n",
    "\n",
    "        # Now, the dictionary is in the correct format for the method. Let's apply it:\n",
    "        DATASET.rename(columns = mapping_dict, inplace = True)\n",
    "    \n",
    "    elif (mode == 'capitalize_columns'):\n",
    "        \n",
    "        DATASET.rename(str.upper, axis = 'columns', inplace = True)\n",
    "    \n",
    "    elif (mode == 'lowercase_columns'):\n",
    "        \n",
    "        DATASET.rename(str.lower, axis = 'columns', inplace = True)\n",
    "    \n",
    "    elif (mode == 'replace_substring'):\n",
    "        \n",
    "        if (substring_to_be_replaced is None):\n",
    "            # set as the default (whitespace):\n",
    "            substring_to_be_replaced = ' '\n",
    "        \n",
    "        if (new_substring_for_replacement is None):\n",
    "            # set as the default (underscore):\n",
    "            new_substring_for_replacement = '_'\n",
    "        \n",
    "        # Apply the str attribute to guarantee that numbers were read as strings:\n",
    "        substring_to_be_replaced = str(substring_to_be_replaced)\n",
    "        new_substring_for_replacement = str(new_substring_for_replacement)\n",
    "        # Replace the substrings in the columns' names:\n",
    "        substring_replaced_series = (pd.Series(DATASET.columns)).str.replace(substring_to_be_replaced, new_substring_for_replacement)\n",
    "        # The Index object is not callable, and applying the str attribute to a np.array or to a list\n",
    "        # will result in a single string concatenating all elements from the array. So, we convert\n",
    "        # the columns index to a pandas series for performing a element-wise string replacement.\n",
    "        \n",
    "        # Now, convert the columns to the series with the replaced substrings:\n",
    "        DATASET.columns = substring_replaced_series\n",
    "        \n",
    "    elif (mode == 'trim'):\n",
    "        # Use the strip method from str attribute with no argument, correspondening to the\n",
    "        # Trim function.\n",
    "        DATASET.rename(str.strip, axis = 'columns', inplace = True)\n",
    "    \n",
    "    elif (mode == 'eliminate_trailing_characters'):\n",
    "        \n",
    "        if ((trailing_substring is None) | (trailing_substring == np.nan)):\n",
    "            # Apply the str.strip() with no arguments:\n",
    "            DATASET.rename(str.strip, axis = 'columns', inplace = True)\n",
    "        \n",
    "        else:\n",
    "            # Apply the str attribute to guarantee that numbers were read as strings:\n",
    "            trailing_substring = str(trailing_substring)\n",
    "\n",
    "            # Apply the strip method:\n",
    "            stripped_series = (pd.Series(DATASET.columns)).str.strip(trailing_substring)\n",
    "            # The Index object is not callable, and applying the str attribute to a np.array or to a list\n",
    "            # will result in a single string concatenating all elements from the array. So, we convert\n",
    "            # the columns index to a pandas series for performing a element-wise string replacement.\n",
    "\n",
    "            # Now, convert the columns to the series with the stripped strings:\n",
    "            DATASET.columns = stripped_series\n",
    "    \n",
    "    else:\n",
    "        print(\"Select a valid mode: \\'set_new_names\\', \\'capitalize_columns\\', \\'lowercase_columns\\', \\'replace_substrings\\', \\'trim\\', or \\'eliminate_trailing_characters\\'.\\n\")\n",
    "        return \"error\"\n",
    "    \n",
    "    print(\"Finished renaming dataframe columns.\\n\")\n",
    "    print(\"Check the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET)\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET)\n",
    "        \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "376911de-63e0-49a1-8fc6-fdeea08fa1c2"
   },
   "source": [
    "# **Function for removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "azdata_cell_guid": "8d0428ea-7022-43b3-8abd-5ce04016ba4d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def trim_spaces_or_characters (df, column_to_analyze, new_variable_type = None, method = 'trim', substring_to_eliminate = None, create_new_column = True, new_column_suffix = \"_trim\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # new_variable_type = None. String (in quotes) that represents a given data type for the column\n",
    "    # after transformation. Set:\n",
    "    # - new_variable_type = 'int' to convert the column to integer type after the transform;\n",
    "    # - new_variable_type = 'float' to convert the column to float (decimal number);\n",
    "    # - new_variable_type = 'datetime' to convert it to date or timestamp;\n",
    "    # - new_variable_type = 'category' to convert it to Pandas categorical variable.\n",
    "    \n",
    "    # method = 'trim' will eliminate trailing and leading white spaces from the strings in\n",
    "    # column_to_analyze.\n",
    "    # method = 'substring' will eliminate a defined trailing and leading substring from\n",
    "    # column_to_analyze.\n",
    "    \n",
    "    # substring_to_eliminate = None. Set as a string (in quotes) if method = 'substring'.\n",
    "    # e.g. suppose column_to_analyze contains time information: each string ends in \" min\":\n",
    "    # \"1 min\", \"2 min\", \"3 min\", etc. If substring_to_eliminate = \" min\", this portion will be\n",
    "    # eliminated, resulting in: \"1\", \"2\", \"3\", etc. If new_variable_type = None, these values will\n",
    "    # continue to be strings. By setting new_variable_type = 'int' or 'float', the series will be\n",
    "    # converted to a numeric type.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_column = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_trim\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_column_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_trim\", the new column will be named as\n",
    "    # \"column1_trim\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    if (method == 'substring'):\n",
    "        \n",
    "        if (substring_to_eliminate is None):\n",
    "            \n",
    "            method = 'trim'\n",
    "            print(\"No valid substring input. Modifying method to \\'trim\\'.\\n\")\n",
    "    \n",
    "    if (method == 'substring'):\n",
    "        \n",
    "        print(\"ATTENTION: Operations of string strip (removal) or replacement are all case-sensitive. There must be correct correspondence between cases and spaces for the strings being removed or replaced.\\n\")\n",
    "        # For manipulating strings, call the str attribute and, then, the method to be applied:\n",
    "        new_series = new_series.str.strip(substring_to_eliminate)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        new_series = new_series.str.strip()\n",
    "    \n",
    "    # Check if a the series type should be modified:\n",
    "    if (new_variable_type is not None):\n",
    "        \n",
    "        if (new_variable_type == 'int'):\n",
    "\n",
    "            new_type = np.int64\n",
    "\n",
    "        elif (new_variable_type == 'float'):\n",
    "            \n",
    "            new_type = np.float64\n",
    "        \n",
    "        elif (new_variable_type == 'datetime'):\n",
    "            \n",
    "            new_type = np.datetime64\n",
    "        \n",
    "        elif (new_variable_type == 'category'):\n",
    "            \n",
    "            new_type = new_variable_type\n",
    "        \n",
    "        # Try converting the type:\n",
    "        try:\n",
    "            new_series = new_series.astype(new_type)\n",
    "            print(f\"Successfully converted the series to the type {new_variable_type}.\\n\")\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if (create_new_column):\n",
    "        \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_trim\"\n",
    "                \n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        DATASET[column_to_analyze] = new_series\n",
    "    \n",
    "    # Now, we are in the main code.\n",
    "    print(\"Finished removing leading and trailing spaces or characters (substrings).\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "77160d3d-7f68-4e2e-9ef8-cd0e25c7cddb"
   },
   "source": [
    "# **Function for capitalizing or lowering case of string variables (string homogenizing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "azdata_cell_guid": "4e556a53-7335-4f0c-ab7b-7b9467ceb7b5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def capitalize_or_lower_string_case (df, column_to_analyze, method = 'lowercase', create_new_column = True, new_column_suffix = \"_homogenized\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # method = 'capitalize' will capitalize all letters from the input string \n",
    "    # (turn them to upper case).\n",
    "    # method = 'lowercase' will make the opposite: turn all letters to lower case.\n",
    "    # e.g. suppose column_to_analyze contains strings such as 'String One', 'STRING 2',  and\n",
    "    # 'string3'. If method = 'capitalize', the output will contain the strings: \n",
    "    # 'STRING ONE', 'STRING 2', 'STRING3'. If method = 'lowercase', the outputs will be:\n",
    "    # 'string one', 'string 2', 'string3'.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_homogenized\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_column_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_homogenized\", the new column will be named as\n",
    "    # \"column1_homogenized\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    if (method == 'capitalize'):\n",
    "        \n",
    "        print(\"Capitalizing the string (moving all characters to upper case).\\n\")\n",
    "        # For manipulating strings, call the str attribute and, then, the method to be applied:\n",
    "        new_series = new_series.str.upper()\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Lowering the string case (moving all characters to lower case).\\n\")\n",
    "        new_series = new_series.str.lower()\n",
    "        \n",
    "    if (create_new_column):\n",
    "        \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_homogenized\"\n",
    "                \n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        DATASET[column_to_analyze] = new_series\n",
    "    \n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished homogenizing the string case of {column_to_analyze}, giving value consistency.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for adding contractions to the contractions library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contractions_to_library (list_of_contractions = [{'contracted_expression': None, 'correct_expression': None}, {'contracted_expression': None, 'correct_expression': None}, {'contracted_expression': None, 'correct_expression': None}, {'contracted_expression': None, 'correct_expression': None}]):\n",
    "    \n",
    "    import contractions\n",
    "    # contractions library: https://github.com/kootenpv/contractions\n",
    "    \n",
    "    # list_of_contractions = \n",
    "    # [{'contracted_expression': None, 'correct_expression': None}]\n",
    "    # This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "    # the first one contains the form as the contraction is usually observed; and the second one \n",
    "    # contains the correct (full) string that will replace it.\n",
    "    # Since contractions can cause issues when processing text, we can expand them with these functions.\n",
    "    \n",
    "    # The object list_of_contractions must be declared as a list, \n",
    "    # in brackets, even if there is a single dictionary.\n",
    "    # Use always the same keys: 'contracted_expression' for the contraction; and 'correct_expression', \n",
    "    # for the strings with the correspondent correction.\n",
    "    \n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you want to add more elements\n",
    "    # to the contractions library.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'contracted_expression': original_str, 'correct_expression': new_str}, \n",
    "    # where original_str and new_str represent the contracted and expanded strings\n",
    "    # (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Example:\n",
    "    # list_of_contractions = [{'contracted_expression': 'mychange', 'correct_expression': 'my change'}]\n",
    "    \n",
    "    \n",
    "    for dictionary in list_of_contractions:\n",
    "        \n",
    "        contraction = dictionary['contracted_expression']\n",
    "        correction = dictionary['correct_expression']\n",
    "        \n",
    "        if ((contraction is not None) & (correction is not None)):\n",
    "    \n",
    "            contractions.add(contraction, correction)\n",
    "            print(f\"Successfully included the contracted expression {contraction} to the contractions library.\")\n",
    "\n",
    "    print(\"Now, the function for contraction correction will be able to process it within the strings.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for correcting contracted strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def correct_contracted_strings (df, column_to_analyze, create_new_column = True, new_column_suffix = \"_contractionsFixed\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import contractions\n",
    "    \n",
    "    # contractions library: https://github.com/kootenpv/contractions\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "   \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_contractionsFixed\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_contractionsFixed\", the new column will be named as\n",
    "    # \"column1_contractionsFixed\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    # Contractions operate at one string at once:\n",
    "    correct_contractions_list = [contractions.fix(new_series[i], slang = True) for i in range (0, len(DATASET))]\n",
    "    \n",
    "    # Make this list the new_series itself:\n",
    "    new_series = pd.Series(correct_contractions_list)\n",
    "    \n",
    "    if (create_new_column):\n",
    "            \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_contractionsFixed\"\n",
    "\n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "\n",
    "        DATASET[column_to_analyze] = new_series\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished correcting the contracted strings from column {column_to_analyze}.\")\n",
    "    print(\"Check the 10 first elements (10 lists) from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c05922a0-8c5f-42e3-8782-385564dbaec0"
   },
   "source": [
    "# **Function for substituting (replacing) substrings on string variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "azdata_cell_guid": "ec8e340b-187e-423e-9532-378add71c2ef",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def replace_substring (df, column_to_analyze, substring_to_be_replaced = None, new_substring_for_replacement = '', create_new_column = True, new_column_suffix = \"_substringReplaced\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # substring_to_be_replaced = None; new_substring_for_replacement = ''. \n",
    "    # Strings (in quotes): when the sequence of characters substring_to_be_replaced was\n",
    "    # found in the strings from column_to_analyze, it will be substituted by the substring\n",
    "    # new_substring_for_replacement. If None is provided to one of these substring arguments,\n",
    "    # it will be substituted by the empty string: ''\n",
    "    # e.g. suppose column_to_analyze contains the following strings, with a spelling error:\n",
    "    # \"my collumn 1\", 'his collumn 2', 'her column 3'. We may correct this error by setting:\n",
    "    # substring_to_be_replaced = 'collumn' and new_substring_for_replacement = 'column'. The\n",
    "    # function will search for the wrong group of characters and, if it finds it, will substitute\n",
    "    # by the correct sequence: \"my column 1\", 'his column 2', 'her column 3'.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_substringReplaced\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_column_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_substringReplaced\", the new column will be named as\n",
    "    # \"column1_substringReplaced\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    print(\"ATTENTION: Operations of string strip (removal) or replacement are all case-sensitive. There must be correct correspondence between cases and spaces for the strings being removed or replaced.\\n\")\n",
    "        \n",
    "    # If one of the input substrings is None, make it the empty string:\n",
    "    if (substring_to_be_replaced is None):\n",
    "        substring_to_be_replaced = ''\n",
    "    \n",
    "    if (new_substring_for_replacement is None):\n",
    "        new_substring_for_replacement = ''\n",
    "    \n",
    "    # Guarantee that both were read as strings (they may have been improperly read as \n",
    "    # integers or floats):\n",
    "    substring_to_be_replaced = str(substring_to_be_replaced)\n",
    "    new_substring_for_replacement = str(new_substring_for_replacement)\n",
    "    \n",
    "    # For manipulating strings, call the str attribute and, then, the method to be applied:\n",
    "    new_series = new_series.str.replace(substring_to_be_replaced, new_substring_for_replacement)\n",
    "        \n",
    "    if (create_new_column):\n",
    "        \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_substringReplaced\"\n",
    "                \n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        DATASET[column_to_analyze] = new_series\n",
    "    \n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished replacing the substring {substring_to_be_replaced} by {new_substring_for_replacement}.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for inverting the order of the string characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def invert_strings (df, column_to_analyze, create_new_column = True, new_column_suffix = \"_stringInverted\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_stringInverted\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_stringInverted\", the new column will be named as\n",
    "    # \"column1_stringInverted\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    # Pandas slice: start from -1 (last character) and go to the last element with -1 step\n",
    "    # walk through the string 'backwards':\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice.html\n",
    "    \n",
    "    new_series = new_series.str.slice(start = -1, step = -1)\n",
    "    \n",
    "    if (create_new_column):\n",
    "            \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_stringInverted\"\n",
    "\n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "\n",
    "        DATASET[column_to_analyze] = new_series\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished inversion of the strings.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for slicing the strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def slice_strings (df, column_to_analyze, first_character_index = None, last_character_index = None, step = 1, create_new_column = True, new_column_suffix = \"_slicedString\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_slicedString\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_slicedString\", the new column will be named as\n",
    "    # \"column1_slicedString\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # first_character_index = None - integer representing the index of the first character to be\n",
    "    # included in the new strings. If None, slicing will start from first character.\n",
    "    # Indexing of strings always start from 0. The last index can be represented as -1, the index of\n",
    "    # the character before as -2, etc (inverse indexing starts from -1).\n",
    "    # example: consider the string \"idsw\", which contains 4 characters. We can represent the indices as:\n",
    "    # 'i': index 0; 'd': 1, 's': 2, 'w': 3. Alternatively: 'w': -1, 's': -2, 'd': -3, 'i': -4.\n",
    "    \n",
    "    # last_character_index = None - integer representing the index of the last character to be\n",
    "    # included in the new strings. If None, slicing will go until the last character.\n",
    "    # Attention: this is effectively the last character to be added, and not the next index after last\n",
    "    # character.\n",
    "    \n",
    "    # in the 'idsw' example, if we want a string as 'ds', we want the first_character_index = 1 and\n",
    "    # last_character_index = 2.\n",
    "    \n",
    "    # step = 1 - integer representing the slicing step. If step = 1, all characters will be added.\n",
    "    # If step = 2, then the slicing will pick one element of index i and the element with index (i+2)\n",
    "    # (1 index will be 'jumped'), and so on.\n",
    "    # If step is negative, then the order of the new strings will be inverted.\n",
    "    # Example: step = -1, and the start and finish indices are None: the output will be the inverted\n",
    "    # string, 'wsdi'.\n",
    "    # first_character_index = 1, last_character_index = 2, step = 1: output = 'ds';\n",
    "    # first_character_index = None, last_character_index = None, step = 2: output = 'is';\n",
    "    # first_character_index = None, last_character_index = None, step = 3: output = 'iw';\n",
    "    # first_character_index = -1, last_character_index = -2, step = -1: output = 'ws';\n",
    "    # first_character_index = -1, last_character_index = None, step = -2: output = 'wd';\n",
    "    # first_character_index = -1, last_character_index = None, step = 1: output = 'w'\n",
    "    # In this last example, the function tries to access the next element after the character of index\n",
    "    # -1. Since -1 is the last character, there are no other characters to be added.\n",
    "    # first_character_index = -2, last_character_index = -1, step = 1: output = 'sw'.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    # Pandas slice:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice.html\n",
    "    \n",
    "    \n",
    "    if (step is None):\n",
    "        # set as 1\n",
    "        step = 1\n",
    "    \n",
    "    if (last_character_index is not None):\n",
    "        if (last_character_index == -1):\n",
    "            # In this case, we cannot sum 1, because it would result in index 0 (1st character).\n",
    "            # So, we will proceed without last index definition, to stop only at the end.\n",
    "            last_character_index = None\n",
    "    \n",
    "    # Now, make the checking again:\n",
    "            \n",
    "    if ((first_character_index is None) & (last_character_index is None)):\n",
    "        \n",
    "        new_series = new_series.str.slice(step = step)\n",
    "        \n",
    "    elif (first_character_index is None):\n",
    "        # Only this is None:\n",
    "        new_series = new_series.str.slice(stop = (last_character_index + 1), step = step)\n",
    "    \n",
    "    elif (last_character_index is None):\n",
    "        \n",
    "        new_series = new_series.str.slice(start = first_character_index, step = step)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        new_series = new_series.str.slice(start = first_character_index, stop = (last_character_index + 1), step = step)\n",
    "    \n",
    "    # Slicing from index i to index j includes index i, but does not include \n",
    "    # index j (ends in j-1). So, we add 1 to the last index to include it.\n",
    "    # automatically included.\n",
    "\n",
    "    if (create_new_column):\n",
    "            \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_slicedString\"\n",
    "\n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "\n",
    "        DATASET[column_to_analyze] = new_series\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished slicing the strings from character {first_character_index} to character {last_character_index}.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for getting the leftest characters from the strings (retrieve last characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def left_characters (df, column_to_analyze, number_of_characters_to_retrieve = 1, new_variable_type = None, create_new_column = True, new_column_suffix = \"_leftChars\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_leftChars\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_leftChars\", the new column will be named as\n",
    "    # \"column1_leftChars\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # number_of_characters_to_retrieve = 1 - integer representing the total of characters that will\n",
    "    # be retrieved. Here, we will retrieve the leftest characters. If number_of_characters_to_retrieve = 1,\n",
    "    # only the leftest (last) character will be retrieved.\n",
    "    # Consider the string 'idsw'.\n",
    "    # number_of_characters_to_retrieve = 1 - output: 'w';\n",
    "    # number_of_characters_to_retrieve = 2 - output: 'sw'.\n",
    "    \n",
    "    # new_variable_type = None. String (in quotes) that represents a given data type for the column\n",
    "    # after transformation. Set:\n",
    "    # - new_variable_type = 'int' to convert the extracted column to integer;\n",
    "    # - new_variable_type = 'float' to convert the column to float (decimal number);\n",
    "    # - new_variable_type = 'datetime' to convert it to date or timestamp;\n",
    "    # - new_variable_type = 'category' to convert it to Pandas categorical variable.\n",
    "    \n",
    "    # So, if the last part of the strings is a number, you can use this argument to directly extract\n",
    "    # this part as numeric variable.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    # Pandas slice:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice.html\n",
    "    \n",
    "    if (number_of_characters_to_retrieve is None):\n",
    "        # set as 1\n",
    "        number_of_characters_to_retrieve = 1\n",
    "    \n",
    "    # last_character_index = -1 would be the index of the last character.\n",
    "    # If we want the last N = 2 characters, we should go from index -2 to -1, -2 = -1 - (N-1);\n",
    "    # If we want the last N = 3 characters, we should go from index -3 to -1, -2 = -1 - (N-1);\n",
    "    # If we want only the last (N = 1) character, we should go from -1 to -1, -1 = -1 - (N-1).\n",
    "    \n",
    "    # N = number_of_characters_to_retrieve\n",
    "    first_character_index = -1 - (number_of_characters_to_retrieve - 1)\n",
    "    \n",
    "    # Perform the slicing without setting the limit, to slice until the end of the string:\n",
    "    new_series = new_series.str.slice(start = first_character_index, step = 1)\n",
    "    \n",
    "    # Check if a the series type should be modified:\n",
    "    if (new_variable_type is not None):\n",
    "        \n",
    "        if (new_variable_type == 'int'):\n",
    "\n",
    "            new_type = np.int64\n",
    "\n",
    "        elif (new_variable_type == 'float'):\n",
    "            \n",
    "            new_type = np.float64\n",
    "        \n",
    "        elif (new_variable_type == 'datetime'):\n",
    "            \n",
    "            new_type = np.datetime64\n",
    "        \n",
    "        elif (new_variable_type == 'category'):\n",
    "            \n",
    "            new_type = new_variable_type\n",
    "        \n",
    "        # Try converting the type:\n",
    "        try:\n",
    "            new_series = new_series.astype(new_type)\n",
    "            print(f\"Successfully converted the series to the type {new_variable_type}.\\n\")\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if (create_new_column):\n",
    "            \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_leftChars\"\n",
    "\n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "\n",
    "        DATASET[column_to_analyze] = new_series\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished extracting the {number_of_characters_to_retrieve} leftest characters.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for getting the rightest characters from the strings (retrieve first characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def right_characters (df, column_to_analyze, number_of_characters_to_retrieve = 1, new_variable_type = None, create_new_column = True, new_column_suffix = \"_rightChars\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_rightChars\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_rightChars\", the new column will be named as\n",
    "    # \"column1_rightChars\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # number_of_characters_to_retrieve = 1 - integer representing the total of characters that will\n",
    "    # be retrieved. Here, we will retrieve the rightest characters. If number_of_characters_to_retrieve = 1,\n",
    "    # only the rightest (first) character will be retrieved.\n",
    "    # Consider the string 'idsw'.\n",
    "    # number_of_characters_to_retrieve = 1 - output: 'i';\n",
    "    # number_of_characters_to_retrieve = 2 - output: 'id'.\n",
    "    \n",
    "    # new_variable_type = None. String (in quotes) that represents a given data type for the column\n",
    "    # after transformation. Set:\n",
    "    # - new_variable_type = 'int' to convert the extracted column to integer;\n",
    "    # - new_variable_type = 'float' to convert the column to float (decimal number);\n",
    "    # - new_variable_type = 'datetime' to convert it to date or timestamp;\n",
    "    # - new_variable_type = 'category' to convert it to Pandas categorical variable.\n",
    "    \n",
    "    # So, if the first part of the strings is a number, you can use this argument to directly extract\n",
    "    # this part as numeric variable.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    # Pandas slice:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice.html\n",
    "    \n",
    "    if (number_of_characters_to_retrieve is None):\n",
    "        # set as 1\n",
    "        number_of_characters_to_retrieve = 1\n",
    "    \n",
    "    # first_character_index = 0 would be the index of the first character.\n",
    "    # If we want the last N = 2 characters, we should go from index 0 to 1, 1 = (N-1);\n",
    "    # If we want the last N = 3 characters, we should go from index 0 to 2, 2 = (N-1);\n",
    "    # If we want only the last (N = 1) character, we should go from 0 to 0, 0 = (N-1).\n",
    "    \n",
    "    # N = number_of_characters_to_retrieve\n",
    "    last_character_index = number_of_characters_to_retrieve - 1\n",
    "    \n",
    "    # Perform the slicing without setting the limit, to slice from the 1st character:\n",
    "    new_series = new_series.str.slice(stop = (last_character_index + 1), step = 1)\n",
    "    \n",
    "    # Check if a the series type should be modified:\n",
    "    if (new_variable_type is not None):\n",
    "        \n",
    "        if (new_variable_type == 'int'):\n",
    "\n",
    "            new_type = np.int64\n",
    "\n",
    "        elif (new_variable_type == 'float'):\n",
    "            \n",
    "            new_type = np.float64\n",
    "        \n",
    "        elif (new_variable_type == 'datetime'):\n",
    "            \n",
    "            new_type = np.datetime64\n",
    "        \n",
    "        elif (new_variable_type == 'category'):\n",
    "            \n",
    "            new_type = new_variable_type\n",
    "        \n",
    "        # Try converting the type:\n",
    "        try:\n",
    "            new_series = new_series.astype(new_type)\n",
    "            print(f\"Successfully converted the series to the type {new_variable_type}.\\n\")\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if (create_new_column):\n",
    "            \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_rightChars\"\n",
    "\n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "\n",
    "        DATASET[column_to_analyze] = new_series\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished extracting the {number_of_characters_to_retrieve} rightest characters.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for joining strings from a same column into a single string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def join_strings_from_column (df, column_to_analyze, separator = \" \"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # separator = \" \" - string containing the separator. Suppose the column contains the\n",
    "    # strings: 'a', 'b', 'c', 'd'. If the separator is the empty string '', the output will be:\n",
    "    # 'abcd' (no separation). If separator = \" \" (simple whitespace), the output will be 'a b c d'\n",
    "    \n",
    "    \n",
    "    if (separator is None):\n",
    "        # make it a whitespace:\n",
    "        separator = \" \"\n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    concat_string = separator.join(new_series)\n",
    "    # sep.join(list_of_strings) method: join all the strings, separating them by sep.\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished joining strings from column {column_to_analyze}.\")\n",
    "    print(\"Check the 10 first characters from the new string:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(concat_string[:10])\n",
    "\n",
    "    except: # regular mode\n",
    "        print(concat_string[:10])\n",
    "\n",
    "    return concat_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for joining several string columns into a single string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def join_string_columns (df, list_of_columns_to_join, separator = \" \", new_column_suffix = \"_stringConcat\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # list_of_columns_to_join: list of strings (inside quotes), \n",
    "    # containing the name of the columns with strings to be joined.\n",
    "    # Attention: the strings will be joined row by row, i.e. only strings in the same rows will\n",
    "    # be concatenated. To join strings from the same column, use function join_strings_from_column\n",
    "    # e.g. list_of_columns_to_join = [\"column1\", \"column2\"] will join strings from \"column1\" with\n",
    "    # the correspondent strings from \"column2\".\n",
    "    # Notice that you can concatenate any kind of columns: numeric, dates, texts ,..., but the output\n",
    "    # will be a string column.\n",
    "    \n",
    "    # separator = \" \" - string containing the separator. Suppose the columns contain the\n",
    "    # strings: 'a', 'b', 'c', 'd' on a given row. If the separator is the empty string '', \n",
    "    # the output will be: 'abcd' (no separation). If separator = \" \" (simple whitespace), \n",
    "    # the output will be 'a b c d'\n",
    "    \n",
    "    # new_column_suffix = \"_stringConcat\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_stringConcat\", the new column will be named as\n",
    "    # \"column1_stringConcat\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    if (separator is None):\n",
    "        # make it a whitespace:\n",
    "        separator = \" \"\n",
    "        \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Start a string pandas series from DATASET, but without connections with it. It will contain\n",
    "    # only empty strings.\n",
    "    second_copy_df = DATASET.copy(deep = True)\n",
    "    second_copy_df['concat_string'] = ''\n",
    "    # Also, create a separator series from it, and make it constant and equals to the separator:\n",
    "    second_copy_df['separator'] = separator\n",
    "    \n",
    "    new_series = second_copy_df['concat_string']\n",
    "    sep_series = second_copy_df['separator']\n",
    "    \n",
    "    col = list_of_columns_to_join[0]\n",
    "    new_series = new_series + (DATASET[col]).astype(str)\n",
    "    \n",
    "    # Now, loop through the columns in the list:\n",
    "    for i in range(1, len(list_of_columns_to_join)):\n",
    "        # We already picked the 1st column (index 0). Now, we pick the second one and go\n",
    "        # until len(list_of_columns_to_join) - 1, index of the last column of the list.\n",
    "        \n",
    "        # concatenate the column with new_series, adding the separator to the left.\n",
    "        # As we add the separator before, there will be no extra separator after the last string.\n",
    "        # Convert the columns to strings for concatenation.\n",
    "        new_series = new_series + sep_series + (DATASET[col]).astype(str)\n",
    "        # The sep.join(list_of_strings) method can only be applied to array-like objects. It cannot\n",
    "        # be used for this operation.\n",
    "            \n",
    "    if (new_column_suffix is None):\n",
    "        new_column_suffix = \"_stringConcat\"\n",
    "\n",
    "    # Add the suffix to the name of the first column\n",
    "    new_column_name = list_of_columns_to_join[0] + new_column_suffix\n",
    "    DATASET[new_column_name] = new_series\n",
    "    \n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished concatenating strings from columns {list_of_columns_to_join}.\")\n",
    "    print(\"Check the 10 first elements from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for splitting strings into a list of strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def split_strings (df, column_to_analyze, separator = \" \", create_new_column = True, new_column_suffix = \"_stringSplitted\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "   \n",
    "    # separator = \" \" - string containing the separator. Suppose the column contains the\n",
    "    # string: 'a b c d' on a given row. If the separator is whitespace ' ', \n",
    "    # the output will be a list: ['a', 'b', 'c', 'd']: the function splits the string into a list\n",
    "    # of strings (one list per row) every time it finds the separator.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_stringSplitted\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_stringSplitted\", the new column will be named as\n",
    "    # \"column1_stringSplitted\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    if (separator is None):\n",
    "        # make it a whitespace:\n",
    "        separator = \" \"\n",
    "        \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    # Split the strings from new_series, getting a list of strings per column:\n",
    "    new_series = new_series.str.split(separator)\n",
    "    \n",
    "    if (create_new_column):\n",
    "            \n",
    "        if (new_column_suffix is None):\n",
    "            new_column_suffix = \"_stringSplitted\"\n",
    "\n",
    "        new_column_name = column_to_analyze + new_column_suffix\n",
    "        DATASET[new_column_name] = new_series\n",
    "            \n",
    "    else:\n",
    "\n",
    "        DATASET[column_to_analyze] = new_series\n",
    "\n",
    "    # Now, we are in the main code.\n",
    "    print(f\"Finished splitting strings from column {column_to_analyze}.\")\n",
    "    print(\"Check the 10 first elements (10 lists) from the series:\\n\")\n",
    "\n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_series.head(10))\n",
    "\n",
    "    except: # regular mode\n",
    "        print(new_series.head(10))\n",
    "\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for substituting (replacing or switching) whole strings by different text values (on string variables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def switch_strings (df, column_to_analyze, list_of_dictionaries_with_original_strings_and_replacements = [{'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}, {'original_string': None, 'new_string': None}], create_new_column = True, new_column_suffix = \"_stringReplaced\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # list_of_dictionaries_with_original_strings_and_replacements = \n",
    "    # [{'original_string': None, 'new_string': None}]\n",
    "    # This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "    # the first one contains the original string; and the second one contains the new string\n",
    "    # that will substitute the original one. The function will loop through all dictionaries in\n",
    "    # this list, access the values of the keys 'original_string', and search these values on the strings\n",
    "    # in column_to_analyze. When the value is found, it will be replaced (switched) by the correspondent\n",
    "    # value in key 'new_string'.\n",
    "    \n",
    "    # The object list_of_dictionaries_with_original_strings_and_replacements must be declared as a list, \n",
    "    # in brackets, even if there is a single dictionary.\n",
    "    # Use always the same keys: 'original_string' for the original strings to search on the column \n",
    "    # column_to_analyze; and 'new_string', for the strings that will replace the original ones.\n",
    "    # Notice that this function will not search substrings: it will substitute a value only when\n",
    "    # there is perfect correspondence between the string in 'column_to_analyze' and 'original_string'.\n",
    "    # So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "    # values.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'original_string': original_str, 'new_string': new_str}, \n",
    "    # where original_str and new_str represent the strings for searching and replacement \n",
    "    # (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Example:\n",
    "    # Suppose the column_to_analyze contains the values 'sunday', 'monday', 'tuesday', 'wednesday',\n",
    "    # 'thursday', 'friday', 'saturday', but you want to obtain data labelled as 'weekend' or 'weekday'.\n",
    "    # Set: list_of_dictionaries_with_original_strings_and_replacements = \n",
    "    # [{'original_string': 'sunday', 'new_string': 'weekend'},\n",
    "    # {'original_string': 'saturday', 'new_string': 'weekend'},\n",
    "    # {'original_string': 'monday', 'new_string': 'weekday'},\n",
    "    # {'original_string': 'tuesday', 'new_string': 'weekday'},\n",
    "    # {'original_string': 'wednesday', 'new_string': 'weekday'},\n",
    "    # {'original_string': 'thursday', 'new_string': 'weekday'},\n",
    "    # {'original_string': 'friday', 'new_string': 'weekday'}]\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_stringReplaced\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "    # \"column1_stringReplaced\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "    \n",
    "    print(\"ATTENTION: Operations of string strip (removal) or replacement are all case-sensitive. There must be correct correspondence between cases and spaces for the strings being removed or replaced.\\n\")\n",
    "     \n",
    "    # Create the mapping dictionary for the str.replace method:\n",
    "    mapping_dict = {}\n",
    "    # The key of the mapping dict must be an string, whereas the value must be the new string\n",
    "    # that will replace it.\n",
    "        \n",
    "    # Loop through each element on the list list_of_dictionaries_with_original_strings_and_replacements:\n",
    "    \n",
    "    for i in range (0, len(list_of_dictionaries_with_original_strings_and_replacements)):\n",
    "        # from i = 0 to i = len(list_of_dictionaries_with_original_strings_and_replacements) - 1, index of the\n",
    "        # last element from the list\n",
    "            \n",
    "        # pick the i-th dictionary from the list:\n",
    "        dictionary = list_of_dictionaries_with_original_strings_and_replacements[i]\n",
    "            \n",
    "        # access 'original_string' and 'new_string' keys from the dictionary:\n",
    "        original_string = dictionary['original_string']\n",
    "        new_string = dictionary['new_string']\n",
    "        \n",
    "        # check if they are not None:\n",
    "        if ((original_string is not None) & (new_string is not None)):\n",
    "            \n",
    "            #Guarantee that both are read as strings:\n",
    "            original_string = str(original_string)\n",
    "            new_string = str(new_string)\n",
    "            \n",
    "            # add them to the mapping dictionary, using the original_string as key and\n",
    "            # new_string as the correspondent value:\n",
    "            mapping_dict[original_string] = new_string\n",
    "    \n",
    "    # Now, the input list was converted into a dictionary with the correct format for the method.\n",
    "    # Check if there is at least one key in the dictionary:\n",
    "    if (len(mapping_dict) > 0):\n",
    "        # len of a dictionary returns the amount of key:value pairs stored. If nothing is stored,\n",
    "        # len = 0. dictionary.keys() method (no arguments in parentheses) returns an array containing\n",
    "        # the keys; whereas dictionary.values() method returns the arrays of the values.\n",
    "        \n",
    "        new_series = new_series.replace(mapping_dict)\n",
    "        # For replacing the whole strings using a mapping dictionary, do not call the str\n",
    "        # attribute\n",
    "    \n",
    "        if (create_new_column):\n",
    "            \n",
    "            if (new_column_suffix is None):\n",
    "                new_column_suffix = \"_substringReplaced\"\n",
    "\n",
    "            new_column_name = column_to_analyze + new_column_suffix\n",
    "            DATASET[new_column_name] = new_series\n",
    "            \n",
    "        else:\n",
    "\n",
    "            DATASET[column_to_analyze] = new_series\n",
    "\n",
    "        # Now, we are in the main code.\n",
    "        print(f\"Finished replacing the substrings accordingly to the mapping: {mapping_dict}.\")\n",
    "        print(\"Check the 10 first elements from the series:\\n\")\n",
    "\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(new_series.head(10))\n",
    "\n",
    "        except: # regular mode\n",
    "            print(new_series.head(10))\n",
    "\n",
    "        return DATASET\n",
    "    \n",
    "    else:\n",
    "        print(\"Input at least one dictionary containing a pair of original string, in the key \\'original_string\\', and the correspondent new string as key \\'new_string\\'.\")\n",
    "        print(\"The dictionaries must be elements from the list list_of_dictionaries_with_original_strings_and_replacements.\\n\")\n",
    "        \n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fe3339b5-15d8-417b-b8e3-c0ad92c0d17b"
   },
   "source": [
    "# **Function for string replacement with Machine Learning: find similar strings and replace them by standard strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "azdata_cell_guid": "38b8e52a-e7f4-4faf-8e1a-0d01c5b439b0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def string_replacement_ml (df, column_to_analyze, mode = 'find_and_replace', threshold_for_percent_of_similarity = 80.0, list_of_dictionaries_with_standard_strings_for_replacement = [{'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}, {'standard_string': None}], create_new_column = True, new_column_suffix = \"_stringReplaced\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from fuzzywuzzy import process\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # mode = 'find_and_replace' will find similar strings; and switch them by one of the\n",
    "    # standard strings if the similarity between them is higher than or equals to the threshold.\n",
    "    # Alternatively: mode = 'find' will only find the similar strings by calculating the similarity.\n",
    "    \n",
    "    # threshold_for_percent_of_similarity = 80.0 - 0.0% means no similarity and 100% means equal strings.\n",
    "    # The threshold_for_percent_of_similarity is the minimum similarity calculated from the\n",
    "    # Levenshtein (minimum edit) distance algorithm. This distance represents the minimum number of\n",
    "    # insertion, substitution or deletion of characters operations that are needed for making two\n",
    "    # strings equal.\n",
    "    \n",
    "    # list_of_dictionaries_with_standard_strings_for_replacement =\n",
    "    # [{'standard_string': None}]\n",
    "    # This is a list of dictionaries, where each dictionary contains a single key-value pair:\n",
    "    # the key must be always 'standard_string', and the value will be one of the standard strings \n",
    "    # for replacement: if a given string on the column_to_analyze presents a similarity with one \n",
    "    # of the standard string equals or higher than the threshold_for_percent_of_similarity, it will be\n",
    "    # substituted by this standard string.\n",
    "    # For instance, suppose you have a word written in too many ways, making it difficult to use\n",
    "    # the function switch_strings: \"EU\" , \"eur\" , \"Europ\" , \"Europa\" , \"Erope\" , \"Evropa\" ...\n",
    "    # You can use this function to search strings similar to \"Europe\" and replace them.\n",
    "    \n",
    "    # The function will loop through all dictionaries in\n",
    "    # this list, access the values of the keys 'standard_string', and search these values on the strings\n",
    "    # in column_to_analyze. When the value is found, it will be replaced (switched) if the similarity\n",
    "    # is sufficiently high.\n",
    "    \n",
    "    # The object list_of_dictionaries_with_standard_strings_for_replacement must be declared as a list, \n",
    "    # in brackets, even if there is a single dictionary.\n",
    "    # Use always the same keys: 'standard_string'.\n",
    "    # Notice that this function performs fuzzy matching, so it MAY SEARCH substrings and strings\n",
    "    # written with different cases (upper or lower) when this portions or modifications make the\n",
    "    # strings sufficiently similar to each other.\n",
    "    \n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "    # values.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same key: {'standard_string': other_std_str}, \n",
    "    # where other_std_str represents the string for searching and replacement \n",
    "    # (If the key contains None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Example:\n",
    "    # Suppose the column_to_analyze contains the values 'California', 'Cali', 'Calefornia', \n",
    "    # 'Calefornie', 'Californie', 'Calfornia', 'Calefernia', 'New York', 'New York City', \n",
    "    # but you want to obtain data labelled as the state 'California' or 'New York'.\n",
    "    # Set: list_of_dictionaries_with_standard_strings_for_replacement = \n",
    "    # [{'standard_string': 'California'},\n",
    "    # {'standard_string': 'New York'}]\n",
    "    \n",
    "    # ATTENTION: It is advisable for previously searching the similarity to find the best similarity\n",
    "    # threshold; set it as high as possible, avoiding incorrect substitutions in a gray area; and then\n",
    "    # perform the replacement. It will avoid the repetition of original incorrect strings in the\n",
    "    # output dataset, as well as wrong replacement (replacement by one of the standard strings which\n",
    "    # is not the correct one).\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_stringReplaced\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "    # \"column1_stringReplaced\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    print(\"Performing fuzzy replacement based on the Levenshtein (minimum edit) distance algorithm.\")\n",
    "    print(\"This distance represents the minimum number of insertion, substitution or deletion of characters operations that are needed for making two strings equal.\\n\")\n",
    "    \n",
    "    print(\"This means that substrings or different cases (upper or higher) may be searched and replaced, as long as the similarity threshold is reached.\\n\")\n",
    "    \n",
    "    print(\"ATTENTION!\\n\")\n",
    "    print(\"It is advisable for previously searching the similarity to find the best similarity threshold.\\n\")\n",
    "    print(\"Set the threshold as high as possible, and only then perform the replacement.\\n\")\n",
    "    print(\"It will avoid the repetition of original incorrect strings in the output dataset, as well as wrong replacement (replacement by one of the standard strings which is not the correct one.\\n\")\n",
    "    \n",
    "    # Set a local copy of dataframe to manipulate\n",
    "    DATASET = df.copy(deep = True)\n",
    "    # Guarantee that the column to analyze was read as string:\n",
    "    DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "    new_series = DATASET[column_to_analyze].copy()\n",
    "\n",
    "    # Get the unique values present in column_to_analyze:\n",
    "    unique_types = new_series.unique()\n",
    "    \n",
    "    # Create the summary_list:\n",
    "    summary_list = []\n",
    "        \n",
    "    # Loop through each element on the list list_of_dictionaries_with_original_strings_and_replacements:\n",
    "    \n",
    "    for i in range (0, len(list_of_dictionaries_with_standard_strings_for_replacement)):\n",
    "        # from i = 0 to i = len(list_of_dictionaries_with_standard_strings_for_replacement) - 1, index of the\n",
    "        # last element from the list\n",
    "            \n",
    "        # pick the i-th dictionary from the list:\n",
    "        dictionary = list_of_dictionaries_with_standard_strings_for_replacement[i]\n",
    "            \n",
    "        # access 'standard_string' key from the dictionary:\n",
    "        standard_string = dictionary['standard_string']\n",
    "        \n",
    "        # check if it is not None:\n",
    "        if (standard_string is not None):\n",
    "            \n",
    "            # Guarantee that it was read as a string:\n",
    "            standard_string = str(standard_string)\n",
    "            \n",
    "            # Calculate the similarity between each one of the unique_types and standard_string:\n",
    "            similarity_list = process.extract(standard_string, unique_types, limit = len(unique_types))\n",
    "            \n",
    "            # Add the similarity list to the dictionary:\n",
    "            dictionary['similarity_list'] = similarity_list\n",
    "            # This is a list of tuples with the format (tested_string, percent_of_similarity_with_standard_string)\n",
    "            # e.g. ('asiane', 92) for checking similarity with string 'asian'\n",
    "            \n",
    "            if (mode == 'find_and_replace'):\n",
    "                \n",
    "                # If an invalid value was set for threshold_for_percent_of_similarity, correct it to 80% standard:\n",
    "                \n",
    "                if(threshold_for_percent_of_similarity is None):\n",
    "                    threshold_for_percent_of_similarity = 80.0\n",
    "                \n",
    "                if((threshold_for_percent_of_similarity == np.nan) | (threshold_for_percent_of_similarity < 0)):\n",
    "                    threshold_for_percent_of_similarity = 80.0\n",
    "                \n",
    "                list_of_replacements = []\n",
    "                # Let's replace the matches in the series by the standard_string:\n",
    "                # Iterate through the list of matches\n",
    "                for match in similarity_list:\n",
    "                    # Check whether the similarity score is greater than or equal to threshold_for_percent_of_similarity.\n",
    "                    # The similarity score is the second element (index 1) from the tuples:\n",
    "                    if (match[1] >= threshold_for_percent_of_similarity):\n",
    "                        # If it is, select all rows where the column_to_analyze is spelled as\n",
    "                        # match[0] (1st Tuple element), and set it to standard_string:\n",
    "                        boolean_filter = (new_series == match[0])\n",
    "                        new_series.loc[boolean_filter] = standard_string\n",
    "                        print(f\"Found {match[1]}% of similarity between {match[0]} and {standard_string}.\")\n",
    "                        print(f\"Then, {match[0]} was replaced by {standard_string}.\\n\")\n",
    "                        \n",
    "                        # Add match to the list of replacements:\n",
    "                        list_of_replacements.append(match)\n",
    "                \n",
    "                # Add the list_of_replacements to the dictionary, if its length is higher than zero:\n",
    "                if (len(list_of_replacements) > 0):\n",
    "                    dictionary['list_of_replacements_by_std_str'] = list_of_replacements\n",
    "            \n",
    "            # Add the dictionary to the summary_list:\n",
    "            summary_list.append(dictionary)\n",
    "      \n",
    "    # Now, let's replace the original column or create a new one if mode was set as replace:\n",
    "    if (mode == 'find_and_replace'):\n",
    "    \n",
    "        if (create_new_column):\n",
    "            \n",
    "            if (new_column_suffix is None):\n",
    "                new_column_suffix = \"_substringReplaced\"\n",
    "\n",
    "            new_column_name = column_to_analyze + new_column_suffix\n",
    "            DATASET[new_column_name] = new_series\n",
    "            \n",
    "        else:\n",
    "\n",
    "            DATASET[column_to_analyze] = new_series\n",
    "\n",
    "        # Now, we are in the main code.\n",
    "        print(f\"Finished replacing the strings by the provided standards. Returning the new dataset and a summary list.\\n\")\n",
    "        print(\"In summary_list, you can check the calculated similarities in keys \\'similarity_list\\' from the dictionaries.\\n\")\n",
    "        print(\"The similarity list is a list of tuples, where the first element is the string compared against the value on key \\'standard_string\\'; and the second element is the similarity score, the percent of similarity between the tested and the standard string.\\n\")\n",
    "        print(\"Check the 10 first elements from the new series, with strings replaced:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(new_series.head(10))\n",
    "\n",
    "        except: # regular mode\n",
    "            print(new_series.head(10))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Finished mapping similarities. Returning the original dataset and a summary list.\\n\")\n",
    "        print(\"Check the similarities below, in keys \\'similarity_list\\' from the dictionaries.\\n\")\n",
    "        print(\"The similarity list is a list of tuples, where the first element is the string compared against the value on key \\'standard_string\\'; and the second element is the similarity score, the percent of similarity between the tested and the standard string.\\n\")\n",
    "        \n",
    "        try:\n",
    "            display(summary_list)\n",
    "        except:\n",
    "            print(summary_list)\n",
    "    \n",
    "    return DATASET, summary_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for searching for Regular Expression (RegEx) within a string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regex_help:\n",
    "\n",
    "    def __init__ (self, start_helper = True, helper_screen = 0):\n",
    "        \n",
    "        # from DataCamp course Regular Expressions in Python\n",
    "        # https://www.datacamp.com/courses/regular-expressions-in-python#!\n",
    "\n",
    "        self.start_helper = start_helper\n",
    "        self.helper_screen = helper_screen\n",
    "        \n",
    "        self.helper_menu_1 = \"\"\"\n",
    "\n",
    "Regular Expressions (RegEx) Helper\n",
    "                \n",
    "Input the number in the text box and press enter to visualize help and examples for a topic:\n",
    "\n",
    "    1. regex basic theory and most common metacharacters\n",
    "    2. regex quantifiers\n",
    "    3. regex anchoring and finding\n",
    "    4. regex greedy and non-greedy search\n",
    "    5. regex grouping and capturing\n",
    "    6. regex alternating and non-capturing groups\n",
    "    7. regex backreferences\n",
    "    8. regex lookaround\n",
    "    9. print all topics at once\n",
    "    10. Finish regex helper\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "        # regex basic theory and most common metacharacters\n",
    "        self.help_text_1 = \"\"\"\n",
    "REGular EXpression or regex:\n",
    "String containing a combination of normal characters and special metacharacters that\n",
    "describes patterns to find text or positions within a text.\n",
    "\n",
    "Example:\n",
    "\n",
    "r'st\\d\\s\\w{3,10}'\n",
    "- In Python, the r at the beginning indicates a raw string. It is always advisable to use it.\n",
    "- We said that a regex contains normal characters, or literal characters we already know. \n",
    "    - The normal characters match themselves. \n",
    "    - In the case shown above, 'st' exactly matches an 's' followed by a 't'.\n",
    "\n",
    "- Most important metacharacters:\n",
    "    - \\d: digit (number);\n",
    "    - \\D: non-digit;\n",
    "    - \\s: whitespace;\n",
    "    - \\s+: one or more consecutive whitespaces.\n",
    "    - \\S: non-whitespace;\n",
    "    - \\w: (word) character;\n",
    "    - \\W: non-word character.\n",
    "    - {N, M}: indicates that the character on the left appears from N to M consecutive times.\n",
    "        - \\w{3,10}: a word character that appears 3, 4, 5,..., or 10 consecutive times.\n",
    "    - {N}: indicates that the character on the left appears exactly N consecutive times.\n",
    "        - \\d{4}: a digit appears 4 consecutive times.\n",
    "    - {N,}: indicates that the character appears at least N times.\n",
    "        - \\d{4,}: a digit appears 4 or more times.\n",
    "        - phone_number = \"John: 1-966-847-3131 Michelle: 54-908-42-42424\"\n",
    "        - re.findall(r\"\\d{1,2}-\\d{3}-\\d{2,3}-\\d{4,}\", phone_number) - returns: ['1-966-847-3131', '54-908-42-42424']\n",
    "\n",
    "ATTENTION: Using metacharacters in regular expressions will allow you to match types of characters such as digits. \n",
    "- You can encounter many forms of whitespace such as tabs, space or new line. \n",
    "- To make sure you match all of them always specify whitespaces as \\s.\n",
    "\n",
    "re module: Python standard library module to search regex within individual strings.\n",
    "\n",
    "- .findall method: search all occurrences of the regex within the string, returning a list of strings.\n",
    "- Syntax: re.findall(r\"regex\", string)\n",
    "    - Example: re.findall(r\"#movies\", \"Love #movies! I had fun yesterday going to the #movies\")\n",
    "        - Returns: ['#movies', '#movies']\n",
    "\n",
    "- .split method: splits the string at each occurrence of the regex, returning a list of strings.\n",
    "- Syntax: re.split(r\"regex\", string)\n",
    "    - Example: re.split(r\"!\", \"Nice Place to eat! I'll come back! Excellent meat!\")\n",
    "        - Returns: ['Nice Place to eat', \" I'll come back\", ' Excellent meat', '']\n",
    "\n",
    "- .sub method: replace one or many matches of the regex with a given string (returns a replaced string).\n",
    "- Syntax: re.sub((r\"regex\", new_substring, original_string))\n",
    "    - Example: re.sub(r\"yellow\", \"nice\", \"I have a yellow car and a yellow house in a yellow neighborhood\")\n",
    "    - Returns: 'I have a nice car and a nice house in a nice neighborhood'\n",
    "\n",
    "- .search and .match methods: they have the same syntax and are used to find a match. \n",
    "    - Both methods return an object with the match found. \n",
    "    - The difference is that .match is anchored at the beginning of the string.\n",
    "- Syntax: re.search(r\"regex\", string) and re.match(r\"regex\", string)\n",
    "    - Example 1: re.search(r\"\\d{4}\", \"4506 people attend the show\")\n",
    "    - Returns: <_sre.SRE_Match object; span=(0, 4), match='4506'>\n",
    "    - re.match(r\"\\d{4}\", \"4506 people attend the show\")\n",
    "    - Returns: <_sre.SRE_Match object; span=(0, 4), match='4506'>\n",
    "        - In this example, we use both methods to find a digit appearing four times. \n",
    "        - Both methods return an object with the match found.\n",
    "    \n",
    "    - Example 2: re.search(r\"\\d+\", \"Yesterday, I saw 3 shows\")\n",
    "    - Returns: <_sre.SRE_Match object; span=(17, 18), match='3'>\n",
    "    - re.match(r\"\\d+\",\"Yesterday, I saw 3 shows\")\n",
    "    - Returns: None\n",
    "        - In this example,, we used them to find a match for a digit. \n",
    "        - In this case, .search finds a match, but .match does not. \n",
    "        - This is because the first characters do not match the regex.\n",
    "\n",
    "- .group method: detailed in Section 7 (Backreferences).\n",
    "    - Retrieves the groups captured.\n",
    "- Syntax: searched_string = re.search(r\"regex\", string)\n",
    "    re.group(N) - returns N-th group captured (group 0 is the regex itself).\n",
    "    \n",
    "    Example: text = \"Python 3.0 was released on 12-03-2008.\"\n",
    "    information = re.search('(\\d{1,2})-(\\d{2})-(\\d{4})', text)\n",
    "    information.group(3) - returns: '2008'\n",
    "- .group can only be used with .search and .match methods.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. re.findall(r\"User\\d\", \"The winners are: User9, UserN, User8\")\n",
    "    ['User9', 'User8']\n",
    "2. re.findall(r\"User\\D\", \"The winners are: User9, UserN, User8\")\n",
    "    ['UserN']\n",
    "3. re.findall(r\"User\\w\", \"The winners are: User9, UserN, User8\")\n",
    "    ['User9', 'UserN', 'User8']\n",
    "4. re.findall(r\"\\W\\d\", \"This skirt is on sale, only $5 today!\")\n",
    "    ['$5']\n",
    "5. re.findall(r\"Data\\sScience\", \"I enjoy learning Data Science\")\n",
    "    ['Data Science']\n",
    "6. re.sub(r\"ice\\Scream\", \"ice cream\", \"I really like ice-cream\")\n",
    "    'I really like ice cream'\n",
    "\n",
    "7. regex that matches the user mentions that starts with @ and follows the pattern @robot3!.\n",
    "\n",
    "regex = r\"@robot\\d\\W\"\n",
    "\n",
    "8. regex that matches the number of user mentions given as, for example: User_mentions:9.\n",
    "\n",
    "regex = r\"User_mentions:\\d\"\n",
    "\n",
    "9. regex that matches the number of likes given as, for example, likes: 5.\n",
    "\n",
    "regex = r\"likes:\\s\\d\"\n",
    "\n",
    "10. regex that matches the number of retweets given as, for example, number of retweets: 4.\n",
    "\n",
    "regex = r\"number\\sof\\sretweets:\\s\\d\"\n",
    "\n",
    "11. regex that matches the user mentions that starts with @ and follows the pattern @robot3!.\n",
    "\n",
    "regex_sentence = r\"\\W\\dbreak\\W\"\n",
    "\n",
    "12. regex that matches the pattern #newH\n",
    "\n",
    "regex_words = r\"\\Wnew\\w\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        # regex quantifiers\n",
    "        self.help_text_2 = \"\"\"\n",
    "Quantifiers: \n",
    "A metacharacter that tells the regex engine how many times to match a character immediately to its left.\n",
    "\n",
    "    1. +: Once or more times.\n",
    "        - text = \"Date of start: 4-3. Date of registration: 10-04.\"\n",
    "        - re.findall(r\"\\d+-\\d+\", text) - returns: ['4-3', '10-04']\n",
    "        - Again, \\s+ represents one or more consecutive whitespaces.\n",
    "    2. *: Zero times or more.\n",
    "        - my_string = \"The concert was amazing! @ameli!a @joh&&n @mary90\"\n",
    "        - re.findall(r\"@\\w+\\W*\\w+\", my_string) - returns: ['@ameli!a', '@joh&&n', '@mary90']\n",
    "    3. ?: Zero times or once: ?\n",
    "        - text = \"The color of this image is amazing. However, the colour blue could be brighter.\"\n",
    "        - re.findall(r\"colou?r\", text) - returns: ['color', 'colour']\n",
    "    \n",
    "The quantifier refers to the character immediately on the left:\n",
    "- r\"apple+\" : + applies to 'e' and not to 'apple'.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. Most of the times, links start with 'http' and do not contain any whitespace, e.g. https://www.datacamp.com. \n",
    "- regex to find all the matches of http links appearing:\n",
    "    - regex = r\"http\\S+\"\n",
    "    - \\S is very useful to use when you know a pattern does not contain spaces and you have reached the end when you do find one.\n",
    "\n",
    "2. User mentions in Twitter start with @ and can have letters and numbers only, e.g. @johnsmith3.\n",
    "- regex to find all the matches of user mentions appearing:\n",
    "    - regex = r\"@\\w*\\d*\"\n",
    "\n",
    "3. regex that finds all dates in a format similar to 27 minutes ago or 4 hours ago.\n",
    "- regex = r\"\\d{1,2}\\s\\w+\\sago\"\n",
    "\n",
    "4. regex that finds all dates in a format similar to 23rd june 2018.\n",
    "- regex = r\"\\d{1,2}\\w{2}\\s\\w+\\s\\d{4}\"\n",
    "\n",
    "5. regex that finds all dates in a format similar to 1st september 2019 17:25.\n",
    "- regex = r\"\\d{1,2}\\w{2}\\s\\w+\\s\\d{4}\\s\\d{1,2}:\\d{2}\"\n",
    "\n",
    "6. Hashtags start with a # symbol and contain letters and numbers but never whitespace.\n",
    "- regex that matches the described hashtag pattern.\n",
    "    - regex = r\"#\\w+\"\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "        # regex anchoring and finding\n",
    "        self.help_text_3 = \"\"\"\n",
    "- Anchoring and Finding Metacharacters\n",
    "\n",
    "    1. . (dot): Match any character (except newline).\n",
    "        - my_links = \"Just check out this link: www.amazingpics.com. It has amazing photos!\"\n",
    "        - re.findall(r\"www.+com\", my_links) - returns: ['www.amazingpics.com']\n",
    "            - The dot . metacharacter is very useful when we want to match all repetitions of any character. \n",
    "            - However, we need to be very careful how we use it.\n",
    "    2. ^: Anchoring on start of the string.\n",
    "        - my_string = \"the 80s music was much better that the 90s\"\n",
    "        - If we do re.findall(r\"the\\s\\d+s\", my_string) - returns: ['the 80s', 'the 90s']\n",
    "        - Using ^: re.findall(r\"^the\\s\\d+s\", my_string) - returns: ['the 80s']\n",
    "    3. $: Anchoring at the end of the string.\n",
    "        - my_string = \"the 80s music hits were much better that the 90s\"\n",
    "        - re.findall(r\"the\\s\\d+s$\", my_string) - returns: ['the 90s']\n",
    "    4. \\: Escape special characters.\n",
    "        - my_string = \"I love the music of Mr.Go. However, the sound was too loud.\"\n",
    "            - re.split(r\".\\s\", my_string) - returns: ['', 'lov', 'th', 'musi', 'o', 'Mr.Go', 'However', 'th', 'soun', 'wa', 'to', 'loud.']\n",
    "            - re.split(r\"\\.\\s\", my_string) - returns: ['I love the music of Mr.Go', 'However, the sound was too loud.']\n",
    "    5. |: OR Operator\n",
    "        - my_string = \"Elephants are the world's largest land animal! I would love to see an elephant one day\"\n",
    "        - re.findall(r\"Elephant|elephant\", my_string) - returns: ['Elephant', 'elephant']\n",
    "    6. []: set of characters representing the OR Operator.\n",
    "        Example 1 - my_string = \"Yesterday I spent my afternoon with my friends: MaryJohn2 Clary3\"\n",
    "        - re.findall(r\"[a-zA-Z]+\\d\", my_string) - returns: ['MaryJohn2', 'Clary3']\n",
    "        Example 2 - my_string = \"My&name&is#John Smith. I%live$in#London.\"\n",
    "        - re.sub(r\"[#$%&]\", \" \", my_string) - returns: 'My name is John Smith. I live in London.'\n",
    "        \n",
    "        Note 1: within brackets, the characters to be found should not be separated, as in [#$%&].\n",
    "            - Whitespaces or other separators would be interpreted as characters to be found.\n",
    "        Note 2: [a-z] represents all word characters from 'a' to 'z', lowercase.\n",
    "                - [A-Z] represents all word characters from 'A' to 'Z', uppercase.\n",
    "                - Since lower and uppercase are different, we must declare [a-zA-Z] or [A-Za-z] to capture all word characters.\n",
    "                - [0-9] represents all digits from 0 to 9.\n",
    "                - Something like [a-zA-Z0-9] or [a-z0-9A-Z] will search all word characters and all numbers.\n",
    "    \n",
    "    7. [^ ]: OR operator combined to ^ transforms the expression to negative.\n",
    "        - my_links = \"Bad website: www.99.com. Favorite site: www.hola.com\"\n",
    "        - re.findall(r\"www[^0-9]+com\", my_links) - returns: ['www.hola.com']\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. You want to find names of files that appear at the start of the string; \n",
    "    - always start with a sequence of 2 or 3 upper or lowercase vowels (a e i o u); \n",
    "    - and always finish with the txt ending.\n",
    "        - Write a regex that matches the pattern of the text file names, e.g. aemyfile.txt.\n",
    "        # . = match any character\n",
    "        regex = r\"^[aeiouAEIOU]{2,3}.+txt\"\n",
    "\n",
    "2. When a user signs up on the company website, they must provide a valid email address.\n",
    "    - The company puts some rules in place to verify that the given email address is valid:\n",
    "    - The first part can contain: Upper A-Z or lowercase letters a-z; \n",
    "    - Numbers; Characters: !, #, %, &, *, $, . Must have @. Domain: Can contain any word characters;\n",
    "    - But only .com ending is allowed. \n",
    "        - Write a regular expression to match valid email addresses.\n",
    "        - Match the regex to the elements contained in emails, and print out the message indicating if it is a valid email or not \n",
    "    \n",
    "    # Write a regex to match a valid email address\n",
    "    regex = r\"^[A-Za-z0-9!#%&*$.]+@\\w+\\.com\"\n",
    "\n",
    "    for example in emails:\n",
    "        # Match the regex to the string\n",
    "        if re.match(regex, example):\n",
    "            # Complete the format method to print out the result\n",
    "            print(\"The email {email_example} is a valid email\".format(email_example=example))\n",
    "        else:\n",
    "            print(\"The email {email_example} is invalid\".format(email_example=example))\n",
    "    \n",
    "    # Notice that we used the .match() method. \n",
    "    # The reason is that we want to match the pattern from the beginning of the string.\n",
    "\n",
    "3. Rules in order to verify valid passwords: it can contain lowercase a-z and uppercase letters A-Z;\n",
    "    - It can contain numbers; it can contain the symbols: *, #, $, %, !, &, .\n",
    "    - It must be at least 8 characters long but not more than 20.\n",
    "        - Write a regular expression to check if the passwords are valid according to the description.\n",
    "        - Search the elements in the passwords list to find out if they are valid passwords.\n",
    "        - Print out the message indicating if it is a valid password or not, complete .format() statement.\n",
    "    \n",
    "    # Write a regex to check if the password is valid\n",
    "    regex = r\"[a-z0-9A-Z*#$%!&.]{8,20}\"\n",
    "\n",
    "    for example in passwords:\n",
    "        # Scan the strings to find a match\n",
    "        if re.match(regex, example):\n",
    "            # Complete the format method to print out the result\n",
    "            print(\"The password {pass_example} is a valid password\".format(pass_example=example))\n",
    "        else:\n",
    "            print(\"The password {pass_example} is invalid\".format(pass_example=example))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        # regex greedy and non-greedy search\n",
    "        self.help_text_4 = \"\"\"\n",
    "There are two types of matching methods: greedy and non-greedy (also called lazy) operators. \n",
    "\n",
    "Greedy operators\n",
    "- The standard quantifiers are greedy by default, meaning that they will attempt to match as many characters as possible.\n",
    "    - Standard quantifiers: * , + , ? , {num, num}\n",
    "    - Example: re.match(r\"\\d+\", \"12345bcada\") - returns: <_sre.SRE_Match object; span=(0, 5), match='12345'>\n",
    "    - We can explain this in the following way: our quantifier will start by matching the first digit found, '1'. \n",
    "    - Because it is greedy, it will keep going to find 'more' digits and stop only when no other digit can be matched, returning '12345'.\n",
    "- If the greedy quantifier has matched so many characters that can not match the rest of pattern, it will backtrack, giving up characters matched earlier one at a time and try again. \n",
    "- Backtracking is like driving a car without a map. If you drive through a path and hit a dead end street, you need to backtrack along your road to an earlier point to take another street. \n",
    "    - Example: re.match(r\".*hello\", \"xhelloxxxxxx\") - returns: <_sre.SRE_Match object; span=(0, 6), match='xhello'>\n",
    "    - We use the greedy quantifier .* to find anything, zero or more times, followed by the letters \"h\" \"e\" \"l\" \"l\" \"o\". \n",
    "    - We can see here that it returns the pattern 'xhello'. \n",
    "    - So our greedy quantifier will start by matching as much as possible, the entire string. \n",
    "    - Then, it tries to match the h, but there are no characters left. So it backtracks, giving up one matched character. \n",
    "    - Trying again, it still doesn't match the h, so it backtracks one more step repeatedly until it finally matches the h in the regex, and the rest of the characters.\n",
    "\n",
    "Non-greedy (lazy) operators\n",
    "- Because they have lazy behavior, non-greedy quantifiers will attempt to match as few characters as needed, returning the shortest match. \n",
    "- To obtain non-greedy quantifiers, we can append a question mark at the end of the greedy quantifiers to convert them into lazy. \n",
    "    - Example: re.match(r\"\\d+?\", \"12345bcada\") - returns: <_sre.SRE_Match object; span=(0, 1), match='1'>\n",
    "    - Now, our non-greedy quantifier will return the pattern '1'. \n",
    "    - In this case, our quantifier will start by matching the first digit found, '1'. \n",
    "    - Because it is non-greedy, it will stop there, as we stated that we want 'one or more', and 1 is as few as needed.\n",
    "- Non-greedy quantifiers also backtrack. \n",
    "- In this case, if they have matched so few characters that the rest of the pattern cannot match, they backtrack, expand the matched character one at a time, and try again. \n",
    "- In the example above: this time we use the lazy quantifier .*?. Interestingly, we obtain the same match 'xhello'. \n",
    "- But, how this match was obtained is different from the first time: the lazy quantifier first matches as little as possible, nothing, leaving the entire string unmatched. \n",
    "- Then it tries to match the 'h', but it doesn't work. \n",
    "- So, it backtracks, matching one more character, the 'x'. Then, it tries again, this time matching the 'h', and afterwards, the rest of the regex.\n",
    "\n",
    "- Even though greedy quantifiers lead to longer matches, they are sometimes the best option. \n",
    "- Because lazy quantifiers match as few as possible, they return a shorter match than we expected.\n",
    "    - Example: if you want to extract a word starting with 'a' and ending with 'e' in the string 'I like apple pie', you may think that applying the greedy regex r\"a.+e\" will return 'apple'. \n",
    "    - However, your match will be 'apple pie'. A way to overcome this is to make it lazy by using '?'' which will return 'apple'.\n",
    "- On the other hand, using greedy quantifiers always leads to longer matches that sometimes are not desired. \n",
    "    - Making quantifiers lazy by adding '?' to match a shorter pattern is a very important consideration to keep in mind when handling data for text mining.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. You want to extract the number contained in the sentence 'I was born on April 24th'. \n",
    "    - A lazy quantifier will make the regex return 2 and 4, because they will match as few characters as needed. \n",
    "    - However, a greedy quantifier will return the entire 24 due to its need to match as much as possible.\n",
    "\n",
    "    1.1. Use a lazy quantifier to match all numbers that appear in the variable sentiment_analysis:\n",
    "    numbers_found_lazy = re.findall(r\"[0-9]+?\", sentiment_analysis)\n",
    "    - Output: ['5', '3', '6', '1', '2']\n",
    "    \n",
    "    1.2. Now, use a greedy quantifier to match all numbers that appear in the variable sentiment_analysis.\n",
    "    numbers_found_greedy = re.findall(r\"[0-9]+\", sentiment_analysis)\n",
    "    - Output: ['536', '12']\n",
    "\n",
    "2.1. Use a greedy quantifier to match text that appears within parentheses in the variable sentiment_analysis.\n",
    "    \n",
    "    sentences_found_greedy = re.findall(r\"\\(.+\\)\", sentiment_analysis)\n",
    "    - Output: [\"(They were so cute) a few yrs ago. PC crashed, and now I forget the name of the site ('I'm crying)\"]\n",
    "\n",
    "2.2. Now, use a lazy quantifier to match text that appears within parentheses in the variable sentiment_analysis.\n",
    "\n",
    "    sentences_found_lazy = re.findall(r\"\\(.+?\\)\", sentiment_analysis)\n",
    "    - Output: [\"(They were so cute)\", \"('I'm crying)\"]\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "        # regex grouping and capturing\n",
    "        self.help_text_5 = \"\"\"\n",
    "Capturing groups in regular expressions\n",
    "- Let's say that we have the following text:\n",
    "    \n",
    "    text = \"Clary has 2 friends who she spends a lot time with. Susan has 3 brothers while John has 4 sisters.\"\n",
    "    \n",
    "- We want to extract information about a person, how many and which type of relationships they have. \n",
    "- So, we want to extract Clary 2 friends, Susan 3 brothers and John 4 sisters.\n",
    "- If we do: re.findall(r'[A-Za-z]+\\s\\w+\\s\\d+\\s\\w+', text), the output will be: ['Clary has 2 friends', 'Susan has 3 brothers', 'John has 4 sisters']\n",
    "    - The output is quite close, but we do not want the word 'has'.\n",
    "\n",
    "- We start simple, by trying to extract only the names. We can place parentheses to group those characters, capture them, and retrieve only that group:\n",
    "    - re.findall(r'([A-Za-z]+)\\s\\w+\\s\\d+\\s\\w+', text) - returns: ['Clary', 'Susan', 'John']\n",
    "- Actually, we can place parentheses around the three groups that we want to capture. \n",
    "    - re.findall(r'([A-Za-z]+)\\s\\w+\\s(\\d+)\\s(\\w+)', text)\n",
    "    \n",
    "    - Each group will receive a number: \n",
    "        - The entire expression will always be group 0. \n",
    "        - The first group: 1; the second: 2; and the third: 3.\n",
    "    \n",
    "    - The result returned is: [('Clary', '2', 'friends'), ('Susan', '3', 'brothers'), ('John', '4', 'sisters')]\n",
    "        - We got a list of tuples: \n",
    "            - The first element of each tuple is the match captured corresponding to group 1. \n",
    "            - The second, to group 2. The last, to group 3.\n",
    "    \n",
    "    - We can use capturing groups to match a specific subpattern in a pattern. \n",
    "    - We can use this information for retrieving the groups by numbers; or to organize data.\n",
    "        - Example: pets = re.findall(r'([A-Za-z]+)\\s\\w+\\s(\\d+)\\s(\\w+)', \"Clary has 2 dogs but John has 3 cats\")\n",
    "                    pets[0][0] == 'Clary'\n",
    "                    - In the code, we placed the parentheses to capture the name of the owner, the number and which type of pets each one has. \n",
    "                    - We can access the information retrieved by using indexing and slicing as seen in the code. \n",
    "   \n",
    "- Capturing groups have one important feature. \n",
    "    - Remember that quantifiers apply to the character immediately to the left. \n",
    "    - So, we can place parentheses to group characters and then apply the quantifier to the entire group. \n",
    "    \n",
    "    Example: re.search(r\"(\\d[A-Za-z])+\", \"My user name is 3e4r5fg\")\n",
    "        - returns: <_sre.SRE_Match object; span=(16, 22), match='3e4r5f'>\n",
    "        - In the code, we have placed parentheses to match the group containing a number and any letter. \n",
    "        - We applied the plus quantifier to specify that we want this group repeated once or more times. \n",
    "    \n",
    "- ATTENTION: It's not the same to capture a repeated group AND to repeat a capturing group. \n",
    "    \n",
    "    my_string = \"My lucky numbers are 8755 and 33\"\n",
    "    - re.findall(r\"(\\d)+\", my_string) - returns: ['5', '3']\n",
    "    - re.findall(r\"(\\d+)\", my_string) - returns: ['8755', '33']\n",
    "    \n",
    "    - In the first code, we use findall to match a capturing group containing one number. \n",
    "        - We want this capturing group to be repeated once or more times. \n",
    "        - We get 5 and 3 as an output, because these numbers are repeated consecutively once or more times. \n",
    "    - In the second code, we specify that we should capture a group containing one or more repetitions of a number. \n",
    "\n",
    "- Placing a subpattern inside parenthesis will capture that content and stores it temporarily in memory. This can be later reused.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. You want to extract the first part of the email. E.g. if you have the email marysmith90@gmail.com, you are only interested in marysmith90.\n",
    "- You need to match the entire expression. So you make sure to extract only names present in emails. Also, you are only interested in names containing upper (e.g. A,B, Z) or lowercase letters (e.g. a, d, z) and numbers.\n",
    "- regex to match the email capturing only the name part. The name part appears before the @.\n",
    "    - regex_email = r\"([a-z0-9A-Z]+)@\\S+\"\n",
    "\n",
    "2. Text follows a pattern: \"Here you have your boarding pass LA4214 AER-CDB 06NOV.\"\n",
    "- You need to extract the information about the flight: \n",
    "    - The two letters indicate the airline (e.g LA); the 4 numbers are the flight number (e.g. 4214);\n",
    "    - The three letters correspond to the departure (e.g AER); the destination (CDB); the date (06NOV) of the flight.\n",
    "    - All letters are always uppercase.\n",
    "\n",
    "- Regular expression to match and capture all the flight information required.\n",
    "- Find all the matches corresponding to each piece of information about the flight. Assign it to flight_matches.\n",
    "- Complete the format method with the elements contained in flight_matches: \n",
    "    - In the first line print the airline and the flight number. \n",
    "    - In the second line, the departure and destination. In the third line, the date.\n",
    "\n",
    "# Import re\n",
    "import re\n",
    "\n",
    "# Write regex to capture information of the flight\n",
    "regex = r\"([A-Z]{2})(\\d{4})\\s([A-Z]{3})-([A-Z]{3})\\s(\\d{2}[A-Z]{3})\"\n",
    "\n",
    "# Find all matches of the flight information\n",
    "flight_matches = re.findall(regex, flight)\n",
    "    \n",
    "#Print the matches\n",
    "print(\"Airline: {} Flight number: {}\".format(flight_matches[0][0], flight_matches[0][1]))\n",
    "print(\"Departure: {} Destination: {}\".format(flight_matches[0][2], flight_matches[0][3]))\n",
    "print(\"Date: {}\".format(flight_matches[0][4]))\n",
    "\n",
    "    - findall() returns a list of tuples. \n",
    "    - The nth element of each tuple is the element corresponding to group n. \n",
    "    - This provides us with an easy way to access and organize our data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        # regex alternating and non-capturing groups\n",
    "        self.help_text_6 = \"\"\"\n",
    "Alternating and non-capturing groups\n",
    "\n",
    "- Vertical bar or pipe operator\n",
    "    - Suppose we have the following string, and we want to find all matches for pet names. \n",
    "    - We can use the pipe operator to specify that we want to match cat or dog or bird:\n",
    "        - my_string = \"I want to have a pet. But I don't know if I want a cat, a dog or a bird.\"\n",
    "        - re.findall(r\"cat|dog|bird\", my_string) - returns: ['cat', 'dog', 'bird']\n",
    "    \n",
    "     - Now, we changed the string a little bit, and once more we want to find all the pet names, but only those that come after a number and a whitespace. \n",
    "     - So, if we specify this again with the pipe operator, we get the wrong output: \n",
    "        - my_string = \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"\n",
    "        - re.findall(r\"\\d+\\scat|dog|bird\", my_string) - returns: ['2 cat', 'dog', 'bird']\n",
    "     \n",
    "     - That is because the pipe operator works by comparing everything that is to its left (digit whitespace cat) with everything to the right, dog.\n",
    "     - In order to solve this, we can use alternation. \n",
    "         - In simpler terms, we can use parentheses again to group the optional characters:\n",
    "         \n",
    "         - my_string = \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"\n",
    "         - re.findall(r\"\\d+\\s(cat|dog|bird)\", my_string) - returns: ['cat', 'dog']\n",
    "         \n",
    "         In the code, now the parentheses are added to group cat or dog or bird.\n",
    "    \n",
    "    - In the previous example, we may also want to match the number. \n",
    "    - In that case, we need to place parentheses to capture the digit group:\n",
    "    \n",
    "        - my_string = \"I want to have a pet. But I don't know if I want 2 cats, 1 dog or a bird.\"\n",
    "        - re.findall(r\"(\\d)+\\s(cat|dog|bird)\", my_string) - returns: [('2', 'cat'), ('1', 'dog')]\n",
    "    \n",
    "        - In the code, we now use two pair of parentheses and we use findall in the string, so we get a list with two tuples.\n",
    "    \n",
    "- Non-capturing groups\n",
    "    - Sometimes, we need to group characters using parentheses, but we are not going to reference back to this group. \n",
    "    - For these cases, there are a special type of groups called non-capturing groups. \n",
    "    - For using them, we just need to add question mark colon inside the parenthesis but before the regex.\n",
    "    \n",
    "    regex = r\"(?:regex)\"\n",
    "    \n",
    "    - Example: we have the following string, and we want to find all matches of numbers. \n",
    "    \n",
    "        my_string = \"John Smith: 34-34-34-042-980, Rebeca Smith: 10-10-10-434-425\"\n",
    "    \n",
    "    - We see that the pattern consists of two numbers and dash repeated three times. After that, three numbers, dash, four numbers. \n",
    "    - We want to extract only the last part, without the first repeated elements. \n",
    "    - We need to group the first two elements to indicate repetitions, but we do not want to capture them. \n",
    "    - So, we use non-capturing groups to group \\d repeated two times and dash. Then we indicate this group should be repeated three times. Then, we group \\d repeated three times, dash, \\d repeated three times:\n",
    "    \n",
    "        re.findall(r\"(?:\\d{2}-){3}(\\d{3}-\\d{3})\", my_string) - returns: ['042-980', '434-425']\n",
    "    \n",
    "- Alternation\n",
    "    - We can combine non-capturing groups and alternation together. \n",
    "    - Remember that alternation implies using parentheses and the pipe operand to group optional characters. \n",
    "    - Let's suppose that we have the following string. We want to match all the numbers of the day. \n",
    "    \n",
    "        my_date = \"Today is 23rd May 2019. Tomorrow is 24th May 19.\"\n",
    "    \n",
    "    - We know that they are followed by 'th' or 'rd', but we only want to capture the number, and not the letters that follow it. \n",
    "    - We write our regex to capture inside parentheses \\d repeated once or more times. Then, we can use a non-capturing group. \n",
    "    - Inside, we use the pipe operator to choose between 'th' or 'rd':\n",
    "    \n",
    "        re.findall(r\"(\\d+)(?:th|rd)\", my_date) - returns: ['23', '24']\n",
    "\n",
    "- Non-capturing groups are very often used together with alternation. \n",
    "- Sometimes, you have optional patterns and you need to group them. \n",
    "- However, you are not interested in keeping them. It's a nice feature of regex.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. Sentiment analysis project: firstly, you want to identify positive tweets about movies and concerts.\n",
    "- You plan to find all the sentences that contain the words 'love', 'like', or 'enjoy', and capture that word. \n",
    "- You will limit the tweets by focusing on those that contain the words 'movie' or 'concert' by keeping the word in another group. \n",
    "- You will also save the movie or concert name.\n",
    "    - For example, if you have the sentence: 'I love the movie Avengers', you match and capture 'love'. \n",
    "    - You need to match and capture 'movie'. Afterwards, you match and capture anything until the dot.\n",
    "    - The list sentiment_analysis contains the text of tweets.\n",
    "- Regular expression to capture the words 'love', 'like', or 'enjoy'; \n",
    "    - match and capture the words 'movie' or 'concert'; \n",
    "    - match and capture anything appearing until the '.'.\n",
    "\n",
    "    regex_positive = r\"(love|like|enjoy).+?(movie|concert)\\s(.+?)\\.\"\n",
    "\n",
    "    - The pipe operator works by comparing everything that is to its left with everything to the right. \n",
    "    - Grouping optional patterns is the way to get the correct result.\n",
    "\n",
    "2. After finding positive tweets, you want to do it for negative tweets. \n",
    "- Your plan now is to find sentences that contain the words 'hate', 'dislike' or 'disapprove'. \n",
    "- You will again save the movie or concert name. \n",
    "- You will get the tweet containing the words 'movie' or 'concert', but this time, you do not plan to save the word.\n",
    "    - For example, if you have the sentence: 'I dislike the movie Avengers a lot.', you match and capture 'dislike'. \n",
    "    - You will match, but not capture, the word 'movie'. Afterwards, you match and capture anything until the dot.\n",
    "- Regular expression to capture the words 'hate', 'dislike' or 'disapprove'; \n",
    "    - Match, but do not capture, the words 'movie' or 'concert'; \n",
    "    - Match and capture anything appearing until the '.'.\n",
    "    \n",
    "    regex_negative = r\"(hate|dislike|disapprove).+?(?:movie|concert)\\s(.+?)\\.\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # regex backreferences\n",
    "        self.help_text_7 = \"\"\"\n",
    "Backreferences\n",
    "- How we can backreference capturing groups.\n",
    "\n",
    "Numbered groups\n",
    "- Imagine we come across this text, and we want to extract the date: \n",
    "    \n",
    "    text = \"Python 3.0 was released on 12-03-2008. It was a major revision of the language. Many of its major features were backported to Python 2.6.x and 2.7.x version series.\"\n",
    "    \n",
    "- We want to extract only the numbers. So, we can place parentheses in a regex to capture these groups:\n",
    "    \n",
    "    regex = r\"(\\d{1,2})-(\\d{1,2})-(\\d{4})\"\n",
    "\n",
    "- We have also seen that each of these groups receive a number. \n",
    "- The whole expression is group 0; the first group, 1; and so on.\n",
    "\n",
    "- Let's use .search to match the pattern to the text. \n",
    "- To retrieve the groups captured, we can use the method .group specifying the number of a group we want. \n",
    "\n",
    "Again: .group method retrieves the groups captured.\n",
    "    - Syntax: searched_string = re.search(r\"regex\", string)\n",
    "    re.group(N) - returns N-th group captured (group 0 is the regex itself).\n",
    "\n",
    "Example: text = \"Python 3.0 was released on 12-03-2008.\"\n",
    "\n",
    "    information = re.search('(\\d{1,2})-(\\d{2})-(\\d{4})', text)\n",
    "    information.group(3) - returns: '2008'\n",
    "    information.group(0) - returns: '12-03-2008' (regex itself, the entire expression).\n",
    "\n",
    "- .group can only be used with .search and .match methods.\n",
    "\n",
    "Named groups\n",
    "- We can also give names to our capturing groups. \n",
    "- Inside the parentheses, we write '?P', and the name inside angle brackets:\n",
    "\n",
    "    regex = r\"(?P<name>regex)\"\n",
    "\n",
    "- Let's say we have the following string, and we want to match the name of the city and zipcode in different groups. \n",
    "- We can use capturing groups and assign them the name 'city' and 'zipcode'. \n",
    "- We retrieve the information by using .group, and we indicate the name of the group. \n",
    "    \n",
    "    text = \"Austin, 78701\"\n",
    "    cities = re.search(r\"(?P<city>[A-Za-z]+).*?(?P<zipcode>\\d{5})\", text)\n",
    "    cities.group(\"city\") - returns: 'Austin'\n",
    "    cities.group(\"zipcode\") - returns: '78701'\n",
    "\n",
    "Backreferences\n",
    "- There is another way to backreference groups. \n",
    "- In fact, the matched group can be reused inside the same regex or outside for substitution. \n",
    "- We can do this using backslash and the number of the group:\n",
    "\n",
    "    regex = r'(\\d{1,2})-(\\d{2})-(\\d{4})'\n",
    "    \n",
    "    - we can backreference the groups as:\n",
    "        (\\d{1,2}): (\\1);\n",
    "        (\\d{2}): (\\2)\n",
    "        (\\d{4}): (\\3)\n",
    "\n",
    "- Example: we have the following string, and we want to find all matches of repeated words. \n",
    "- In the code, we specify that we want to capture a sequence of word characters, then a whitespace.\n",
    "- Finally, we write \\1. This will indicate that we want to match the first group captured again. \n",
    "- In other words, it says: 'match that sequence of characters that was previously captured once more.' \n",
    "    \n",
    "    sentence = \"I wish you a happy happy birthday!\"\n",
    "    re.findall(r\"(\\w+)\\s\\1\", sentence) - returns: ['happy'] \n",
    "\n",
    "- We get the word 'happy' as an output: this was the repeated word in our string.\n",
    "\n",
    "- Now, we want to replace the repeated word with one occurrence of the same word. \n",
    "- In the code, we use the same regex as before, but this time, we use the .sub method. \n",
    "- In the replacement part, we can also reference back to the captured group: \n",
    "    - We write r\"\\1\" to say: 'replace the entire expression match with the first captured group.' \n",
    "    \n",
    "    re.sub(r\"(\\w+)\\s\\1\", r\"\\1\", sentence) - returns: 'I wish you a happy birthday!'\n",
    "    - In the output string, we have only one occurrence of the word 'happy'.\n",
    "    \n",
    "- We can also use named groups for backreferencing. \n",
    "- To do this, we use ?P= the group name. \n",
    "\n",
    "    regex = r\"(?P=name)\"\n",
    "\n",
    "Example:\n",
    "    sentence = \"Your new code number is 23434. Please, enter 23434 to open the door.\"\n",
    "    re.findall(r\"(?P<code>\\d{5}).*?(?P=code)\", sentence) - returns: ['23434']\n",
    "\n",
    "- In the code, we want to find all matches of the same number. \n",
    "- We use a capturing group and name it 'code'. \n",
    "- Later, we reference back to this group, and we obtain the number as an output.\n",
    "\n",
    "- To reference the group back for replacement, we need to use \\g and the group name inside angle brackets. \n",
    "\n",
    "    regex = r\"(\\g<name>)\"\n",
    "\n",
    "Example:\n",
    "    sentence = \"This app is not working! It's repeating the last word word.\"\n",
    "    re.sub(r\"(?P<word>\\w+)\\s(?P=word)\", r\"\\g<word>\", sentence) - returns: 'This app is not working! It's repeating the last word.'\n",
    "    \n",
    "- In the code, we want to replace repeated words by one occurrence of the same word. \n",
    "- Inside the regex, we use the previous syntax. \n",
    "- In the replacement field, we need to use this new syntax as seen in the code.\n",
    "- Backreferences are very helpful when you need to reuse part of the regex match inside the regex.\n",
    "- You should remember that the group zero stands for the entire expression matched. \n",
    "    - It is always helpful to keep that in mind. Sometimes you will need to use it.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. Parsing PDF files: your company gave you some PDF files of signed contracts. The goal of the project is to create a database with the information you parse from them. \n",
    "- Three of these columns should correspond to the day, month, and year when the contract was signed.\n",
    "- The dates appear as 'Signed on 05/24/2016' ('05' indicating the month, '24' the day). \n",
    "- You decide to use capturing groups to extract this information. Also, you would like to retrieve that information so you can store it separately in different variables.\n",
    "- The variable contract contains the text of one contract.\n",
    "\n",
    "- Write a regex that captures the month, day, and year in which the contract was signed. \n",
    "- Scan contract for matches.\n",
    "- Assign each captured group to the corresponding keys in the dictionary.\n",
    "- Complete the positional method to print out the captured groups. \n",
    "- Use the values corresponding to each key in the dictionary.\n",
    "\n",
    "    # Write regex and scan contract to capture the dates described\n",
    "    regex_dates = r\"Signed\\son\\s(\\d{2})/(\\d{2})/(\\d{4})\"\n",
    "    dates = re.search(regex_dates, contract)\n",
    "\n",
    "    # Assign to each key the corresponding match\n",
    "    signature = {\n",
    "        \"day\": dates.group(2),\n",
    "        \"month\": dates.group(1),\n",
    "        \"year\": dates.group(3)\n",
    "    }\n",
    "    # Complete the format method to print-out\n",
    "    print(\"Our first contract is dated back to {data[year]}. Particularly, the day {data[day]} of the month {data[month]}.\".format(data=signature))\n",
    "\n",
    "- Remember that each capturing group is assigned a number according to its position in the regex. \n",
    "- Only if you use .search() and .match(), you can use .group() to retrieve the groups.\n",
    "\n",
    "2. The company is going to develop a new product which will help developers automatically check the code they are writing. \n",
    "- You need to write a short script for checking that every HTML tag that is open has its proper closure.\n",
    "- You have an example of a string containing HTML tags: \"<title>The Data Science Company</title>\"\n",
    "- You learn that an opening HTML tag is always at the beginning of the string, and appears inside \"<>\". \n",
    "- A closing tag also appears inside \"<>\", but it is preceded by \"/\".\n",
    "- The list html_tags, contains strings with HTML tags.\n",
    "\n",
    "- Regex to match closed HTML tags: find if there is a match in each string of the list html_tags. Assign the result to match_tag;\n",
    "    - If a match is found, print the first group captured and saved in match_tag;\n",
    "- If no match is found, regex to match only the text inside the HTML tag. Assign it to notmatch_tag.\n",
    "    - Print the first group captured by the regex and save it in notmatch_tag.\n",
    "    - To capture the text inside <>, place parenthesis around the expression: r\"<(text)>. To confirm that the same text appears in the closing tag, reference back to the m group captured by using '\\m'.\n",
    "    - To print the 'm' group captured, use .group(m).\n",
    "\n",
    "    for string in html_tags:\n",
    "        # Complete the regex and find if it matches a closed HTML tags\n",
    "        match_tag =  re.match(r\"<(\\w+)>.*?</\\1>\", string)\n",
    "\n",
    "        if match_tag:\n",
    "            # If it matches print the first group capture\n",
    "            print(\"Your tag {} is closed\".format(match_tag.group(1))) \n",
    "        else:\n",
    "            # If it doesn't match capture only the tag \n",
    "            notmatch_tag = re.match(r\"<(\\w+)>\",string)\n",
    "            # Print the first group capture\n",
    "            print(\"Close your {} tag!\".format(notmatch_tag.group(1)))\n",
    "\n",
    "3. Your task is to replace elongated words that appear in the tweets. \n",
    "- We define an elongated word as a word that contains a repeating character twice or more times. \n",
    "    - e.g. \"Awesoooome\".\n",
    "- Replacing those words is very important since a classifier will treat them as a different term from the source words, lowering their frequency.\n",
    "- To find them, you will use capturing groups and reference them back using numbers. E.g \\4.\n",
    "- If you want to find a match for 'Awesoooome', you firstly need to capture 'Awes'. \n",
    "    - Then, match 'o' and reference the same character back, and then, 'me'.\n",
    "- The list sentiment_analysis contains the text tweets.\n",
    "- Regular expression to match an elongated word as described.\n",
    "- Search the elements in sentiment_analysis list to find out if they contain elongated words. Assign the result to match_elongated.\n",
    "- Assign the captured group number zero to the variable elongated_word.\n",
    "    - Print the result contained in the variable elongated_word.\n",
    "\n",
    "    # Complete the regex to match an elongated word\n",
    "    regex_elongated = r\"\\w*(\\w)\\1*me\\w*\"\n",
    "\n",
    "    for tweet in sentiment_analysis:\n",
    "        # Find if there is a match in each tweet \n",
    "        match_elongated = re.search(regex_elongated, tweet)\n",
    "\n",
    "        if match_elongated:\n",
    "            # Assign the captured group zero \n",
    "            elongated_word = match_elongated.group(0)\n",
    "\n",
    "            # Complete the format method to print the word\n",
    "            print(\"Elongated word found: {word}\".format(word=elongated_word))\n",
    "        else:\n",
    "            print(\"No elongated word found\") \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # regex lookaround\n",
    "        self.help_text_8 = \"\"\"\n",
    "Lookaround\n",
    "- There are specific types of non-capturing groups that help us look around an expression.\n",
    "- Look-around will look for what is behind or ahead of a pattern. \n",
    "- Imagine that we have the following string:\n",
    "    \n",
    "    text = \"the white cat sat on the chair\"\n",
    "\n",
    "- We want to see what is surrounding a specific word. \n",
    "- For example, we position ourselves in the word 'cat'. \n",
    "- So look-around will let us answer the following problem: \n",
    "    - At my current position, look ahead and search if 'sat' is there. \n",
    "    - Or, look behind and search if 'white' is there.\n",
    "    \n",
    "- In other words, looking around allows us to confirm that a sub-pattern is ahead or behind the main pattern.\n",
    "- \"At my current position in the matching process, look ahead or behind and examine whether some pattern matches or not match before continuing.\"\n",
    "- In the previous example, we are looking for the word 'cat'. \n",
    "- The look ahead expression can be either positive or negative. For positive we use ?=. For negative, ?!.\n",
    "    - positive: (?=sat)\n",
    "    - negative: (?!run)\n",
    "\n",
    "- Look-ahead\n",
    "- This non-capturing group checks whether the first part of the expression is followed or not by the lookahead expression. \n",
    "- As a consequence, it will return the first part of the expression. \n",
    "    - Let's imagine that we have a string containing file names and the status of that file. \n",
    "    - We want to extract only those files that are followed by the word 'transferred'. \n",
    "    - So we start building the regex by indicating any word character followed by .txt.\n",
    "    - We now indicate we want the first part to be followed by the word transferred. \n",
    "    - We do so by writing ?= and then whitespace transferred all inside the parenthesis:\n",
    "    \n",
    "    my_text =\"tweets.txt transferred, mypass.txt transferred, keywords.txt error\"\n",
    "    re.findall(r\"\\w+\\.txt(?=\\stransferred)\", my_text) - returns: ['tweets.txt', 'mypass.txt']\n",
    "\n",
    "- Negative look-ahead\n",
    "    - Now, let's use negative lookahead in the same example.\n",
    "    - In this case, we will say that we want those matches that are NOT followed by the expression 'transferred'. \n",
    "    - We use, instead, ?! inside parenthesis:\n",
    "\n",
    "    my_text = \"tweets.txt transferred, mypass.txt transferred, keywords.txt error\"\n",
    "    re.findall(r\"\\w+\\.txt(?!\\stransferred)\", my_text) - returns: ['keywords.txt']\n",
    "\n",
    "- Look-behind\n",
    "- The non-capturing group look-behind gets all matches that are preceded or not by a specific pattern.\n",
    "- As a consequence, it will return the matches after the look expression.\n",
    "- Look behind expression can also be either positive or negative. \n",
    "    - For positive, we use ?<=. For negative, ?<!.\n",
    "    - So, we add an intermediate '<' (angle bracket) sign. In the previous example, we can look before the word 'cat': \n",
    "        - positive: (?<=white)\n",
    "        - negative: (?<!brown)\n",
    "    \n",
    "- Positive look-behind\n",
    "    - Let's look at the following string, in which we want to find all matches of the names that are preceded by the word 'member'. \n",
    "    - We construct our regex with positive look-behind. \n",
    "    - At the end of the regex, we indicate that we want a sequence of word characters whitespace another sequence of word characters:\n",
    "    \n",
    "    my_text = \"Member: Angus Young, Member: Chris Slade, Past: Malcolm Young, Past: Cliff Williams.\"\n",
    "    re.findall(r\"(?<=Member:\\s)\\w+\\s\\w+\", my_text) - returns: ['Angus Young', 'Chris Slade']\n",
    "    \n",
    "    - Pay attention to the code: the look-behind expression goes before that expression. \n",
    "    - We indicate ?<= followed by member, colon, and whitespace. All inside parentheses. \n",
    "    - In that way we get the two names that were preceded by the word member, as shown in the output.\n",
    "\n",
    "- Negative look-behind\n",
    "- Now, we have other string, in which will use negative look-behind. \n",
    "- We will find all matches of the word 'cat' or 'dog' that are not preceded by the word 'brown'. \n",
    "- In this example, we use ?<!, followed by brown, whitespace. All inside the parenthesis. \n",
    "- Then, we indicate our alternation group: 'cat' or 'dog'. \n",
    "\n",
    "    my_text = \"My white cat sat at the table. However, my brown dog was lying on the couch.\"\n",
    "    re.findall(r\"(?<!brown\\s)(cat|dog)\", my_text) - returns: ['cat']\n",
    "\n",
    "    - Consequently, we get 'cat' as an output, the 'cat' or 'dog' word that is not after the word 'brown'.\n",
    "\n",
    "In summary:\n",
    "- Positive lookahead (?=) makes sure that first part of the expression is followed by the lookahead expression. \n",
    "- Positive lookbehind (?<=) returns all matches that are preceded by the specified pattern.\n",
    "- It is important to know that positive lookahead will return the text matched by the first part of the expression after asserting that it is followed by the lookahead expression,\n",
    "    - while positive lookbehind will return all matches that follow a specific pattern.\n",
    "- Negative lookarounds work in a similar way to positive lookarounds. \n",
    "    - They are very helpful when we are looking to exclude certain patterns from our analysis.\n",
    "\n",
    "Examples of regex:\n",
    "\n",
    "1. You are interested in the words surrounding 'python'. You want to count how many times a specific words appears right before and after it.\n",
    "- The variable sentiment_analysis contains the text of one tweet.\n",
    "- Get all the words that are followed by the word 'python' in sentiment_analysis. \n",
    "- Print out the word found.\n",
    "    - In re.findall(). Use \\w+ to match the words followed by the word 'python';\n",
    "    - In re.findall() first argument, include \\spython within parentheses to indicate that everything after the word 'python' should be matched.\n",
    "\n",
    "    # Positive lookahead\n",
    "    look_ahead = re.findall(r\"\\w+(?=\\spython)\", sentiment_analysis)\n",
    "\n",
    "    # Print out\n",
    "    print(look_ahead)\n",
    " \n",
    "1.2. Get all the words that are preceded by the word 'python' or 'Python' in sentiment_analysis. Print out the words found.\n",
    "- In re.findall() first argument, include [Pp]ython\\s within parentheses to indicate that everything before the word 'python' (or 'Python') should be matched.\n",
    "\n",
    "    # Positive lookbehind\n",
    "    look_behind = re.findall(r\"(?<=[pP]ython\\s)\\w+\", sentiment_analysis)\n",
    "\n",
    "    # Print out\n",
    "    print(look_behind)\n",
    "\n",
    "2. You need to write a script for a cell-phone searcher. \n",
    "- It should scan a list of phone numbers and return those that meet certain characteristics.\n",
    "- The phone numbers in the list have the structure:\n",
    "    - Optional area code: 3 numbers\n",
    "    - Prefix: 4 numbers\n",
    "    - Line number: 6 numbers\n",
    "    - Optional extension: 2 numbers\n",
    "    - E.g. 654-8764-439434-01.\n",
    "- You decide to use .findall() and the non-capturing group's negative lookahead (?!) and negative lookbehind (?<!).\n",
    "- The list cellphones, contains three phone numbers:\n",
    "    cellphones = ['4564-646464-01', '345-5785-544245', '6476-579052-01']\n",
    "\n",
    "- Get all cell phones numbers that are not preceded by the optional area code.\n",
    "    - In re.findall() first argument, you use a negative lookbehind ?<! within parentheses () indicating the optional area code.\n",
    "\n",
    "    for phone in cellphones:\n",
    "        # Get all phone numbers not preceded by area code\n",
    "        number = re.findall(r\"(?<!\\d{3}-)\\d{4}-\\d{6}-\\d{2}\", phone)\n",
    "        print(number)\n",
    " \n",
    "2.1. Get all the cell phones numbers that are not followed by the optional extension.\n",
    "    - In re.findall() first argument, you use a negative lookahead ?! within parentheses () indicating the optional extension.\n",
    "\n",
    "    for phone in cellphones:\n",
    "        # Get all phone numbers not followed by optional extension\n",
    "        number = re.findall(r\"\\d{3}-\\d{4}-\\d{6}(?!-\\d{2})\", phone)\n",
    "        print(number)\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "    def show_screen (self):\n",
    "            \n",
    "        helper_screen = self.helper_screen\n",
    "        helper_menu_1 = self.helper_menu_1\n",
    "            \n",
    "        if (helper_screen == 0):\n",
    "                \n",
    "            # Start screen\n",
    "            print(self.helper_menu_1)\n",
    "            print(\"\\n\")\n",
    "            # For the input, strip all whitespaces and, and so convert it to integer:\n",
    "            helper_screen = int(str(input(\"Next screen:\")).strip())\n",
    "                \n",
    "            # the object.__dict__ method returns all attributes from an object as a dictionary.\n",
    "            # Analogously, the vars function applied to an object vars(object) returns the same\n",
    "            # dictionary. We can access an attribute from the object by passing the key of this\n",
    "            # dictionary:\n",
    "            # vars(object)['key']\n",
    "                \n",
    "            while (helper_screen != 10):\n",
    "                    \n",
    "                if (helper_screen not in range(0, 11)):\n",
    "                    # range (0, 11): integers from 0 to 10\n",
    "                        \n",
    "                    helper_screen = int(str(input(\"Input a valid number, from 0 to 10:\")).strip())\n",
    "                    \n",
    "                else:\n",
    "                        \n",
    "                    if (helper_screen == 9):\n",
    "                        # print all at once:\n",
    "                        for screen_number in range (1, 9):\n",
    "                            # integers from 1 to 8\n",
    "                            key = \"help_text_\" + str(screen_number)\n",
    "                            # apply the vars function to get the dictionary of attributes, and call the\n",
    "                            # attribute by passing its name as key from the dictionary:\n",
    "                            screen_text = vars(self)[key]\n",
    "                            # Notice that we cannot directly call the attribute as a string. We would have to\n",
    "                            # create an if else for each of the 8 attributes.\n",
    "                            print(screen_text)\n",
    "                            \n",
    "                        # Now, make helper_screen = 10 for finishing this step:\n",
    "                        helper_screen = 10\n",
    "                        \n",
    "                    else:\n",
    "                        key = \"help_text_\" + str(helper_screen)\n",
    "                        screen_text = vars(self)[key]\n",
    "                        print(screen_text)\n",
    "                        helper_screen = int(str(input(\"Next screen:\")).strip())\n",
    "            \n",
    "        print(\"Finishing regex assistant.\\n\")\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def regex_search (df, column_to_analyze, regex_to_search = r\"\", show_regex_helper = False, create_new_column = True, new_column_suffix = \"_regex\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # regex_to_search = r\"\" - string containing the regular expression (regex) that will be searched\n",
    "    # within each string from the column. Declare it with the r before quotes, indicating that the\n",
    "    # 'raw' string should be read. That is because the regex contain special characters, such as \\,\n",
    "    # which should not be read as scape characters.\n",
    "    # example of regex: r'st\\d\\s\\w{3,10}'\n",
    "    # Use the regex helper to check: basic theory and most common metacharacters; regex quantifiers;\n",
    "    # regex anchoring and finding; regex greedy and non-greedy search; regex grouping and capturing;\n",
    "    # regex alternating and non-capturing groups; regex backreferences; and regex lookaround.\n",
    "    \n",
    "    ## ATTENTION: This function returns ONLY the capturing groups from the regex, i.e., portions of the\n",
    "    # regex explicitly marked with parentheses (check the regex helper for more details, including how\n",
    "    # to convert parentheses into non-capturing groups). If no groups are marked as capturing, the\n",
    "    # function will raise an error.\n",
    "\n",
    "    # show_regex_helper: set show_regex_helper = True to show a helper guide to the construction of\n",
    "    # the regular expression. After finishing the helper, the original dataset itself will be returned\n",
    "    # and the function will not proceed. Use it in case of not knowing or not certain on how to input\n",
    "    # the regex.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_regex\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_regex\", the new column will be named as\n",
    "    # \"column1_regex\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    if (show_regex_helper): # run if True\n",
    "        \n",
    "        # Create an instance (object) from class regex_help:\n",
    "        helper = regex_help()\n",
    "        # Run helper object:\n",
    "        helper = helper.show_screen()\n",
    "        print(\"Interrupting the function and returning the dataframe itself.\")\n",
    "        print(\"Use the regex helper instructions to obtain the regex.\")\n",
    "        print(\"Do not forget to declare it as r'regex', with the r before quotes.\")\n",
    "        print(\"It indicates a raw expression. It is important for not reading the regex metacharacters as regular string scape characters.\")\n",
    "        print(\"Also, notice that this function returns only the capturing groups (marked with parentheses).\")\n",
    "        print(\"If no groups are marked as capturing groups (with parentheses) within the regex, the function will raise an error.\\n\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Set a local copy of dataframe to manipulate\n",
    "        DATASET = df.copy(deep = True)\n",
    "        DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "        new_series = DATASET[column_to_analyze].copy()\n",
    "        \n",
    "        # Search for the regex within new_series:\n",
    "        new_series = new_series.str.extract(regex_to_search, expand = True)\n",
    "        \n",
    "        # https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html\n",
    "        # setting expand = True returns a dataframe with one column per capture group, if the\n",
    "        # regex contains more than 1 capture group.\n",
    "        \n",
    "        # The shape attribute is a tuple (N,) for a Pandas Series, and (N, M) for a dataframe,\n",
    "        # where N is the number of rows, and M is the number of columns.\n",
    "        # Let's try to access the number of columns. It will only be possible if the object is a\n",
    "        # dataframe (index 1 from shape tuple):\n",
    "        try:\n",
    "            \n",
    "            total_new_cols = new_series.shape[1]\n",
    "            \n",
    "            if (new_column_suffix is None):\n",
    "                new_column_suffix = \"_regex\"\n",
    "            \n",
    "            new_column_suffix = str(column_to_analyze) + new_column_suffix + \"_group_\"\n",
    "            \n",
    "            # In the regex, the group 0 is the expression itself, whereas the first group is group 1.\n",
    "            # range (0, total_new_cols) goes from 0 to total_new_cols-1;\n",
    "            # range (1, total_new_cols + 1) goes from group 1 to group total_new_cols\n",
    "            # (both cases result in total_new_cols elements):\n",
    "            \n",
    "            # Create a list of columns:\n",
    "            new_columns_list = [(new_column_suffix + str(i)) for i in range (1, (total_new_cols + 1))]\n",
    "            \n",
    "            # Make this list the new columns' names:\n",
    "            new_series.columns = new_columns_list\n",
    "            \n",
    "            # Concatenate this dataframe to the original one (add columns to the right of DATASET):\n",
    "            DATASET = pd.concat([DATASET, new_series], axis = 1, join = \"inner\")\n",
    "        \n",
    "        \n",
    "        except IndexError:\n",
    "            \n",
    "            # There is no second dimension, because it is a series.\n",
    "            # The regex finds a single group\n",
    "            \n",
    "            if (create_new_column):\n",
    "\n",
    "                if (new_column_suffix is None):\n",
    "                    new_column_suffix = \"_regex\"\n",
    "\n",
    "                new_column_name = column_to_analyze + new_column_suffix\n",
    "                DATASET[new_column_name] = new_series\n",
    "\n",
    "            else:\n",
    "\n",
    "                DATASET[column_to_analyze] = new_series\n",
    "\n",
    "        # Now, we are in the main code.\n",
    "        print(f\"Finished searching the regex {regex_to_search} within {column_to_analyze}.\")\n",
    "        print(\"Check the 10 first elements from the output:\\n\")\n",
    "\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(new_series.head(10))\n",
    "\n",
    "        except: # regular mode\n",
    "            print(new_series.head(10))\n",
    "\n",
    "        return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e072a1b8-e3ad-40f4-bb0d-acf887037357"
   },
   "source": [
    "# **Function for replacing a Regular Expression (RegEx) in a string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "azdata_cell_guid": "b8f0b822-7c44-4712-9d80-157dd7ed1bb5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def regex_replacement (df, column_to_analyze, regex_to_search = r\"\", string_for_replacement = \"\", show_regex_helper = False, create_new_column = True, new_column_suffix = \"_regex\"):\n",
    "     \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # column_to_analyze: string (inside quotes), \n",
    "    # containing the name of the column that will be analyzed. \n",
    "    # e.g. column_to_analyze = \"column1\" will analyze the column named as 'column1'.\n",
    "    \n",
    "    # regex_to_search = r\"\" - string containing the regular expression (regex) that will be searched\n",
    "    # within each string from the column. Declare it with the r before quotes, indicating that the\n",
    "    # 'raw' string should be read. That is because the regex contain special characters, such as \\,\n",
    "    # which should not be read as scape characters.\n",
    "    # example of regex: r'st\\d\\s\\w{3,10}'\n",
    "    # Use the regex helper to check: basic theory and most common metacharacters; regex quantifiers;\n",
    "    # regex anchoring and finding; regex greedy and non-greedy search; regex grouping and capturing;\n",
    "    # regex alternating and non-capturing groups; regex backreferences; and regex lookaround.\n",
    "    \n",
    "    # string_for_replacement = \"\" - regular string that will replace the regex_to_search: \n",
    "    # whenever regex_to_search is found in the string, it is replaced (substituted) by \n",
    "    # string_or_regex_for_replacement. \n",
    "    # Example string_for_replacement = \" \" (whitespace).\n",
    "    # If string_for_replacement = None, the empty string will be used for replacement.\n",
    "    \n",
    "    ## ATTENTION: This function process a single regex by call.\n",
    "    \n",
    "    # show_regex_helper: set show_regex_helper = True to show a helper guide to the construction of\n",
    "    # the regular expression. After finishing the helper, the original dataset itself will be returned\n",
    "    # and the function will not proceed. Use it in case of not knowing or not certain on how to input\n",
    "    # the regex.\n",
    "    \n",
    "    # create_new_column = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into a new\n",
    "    # column. Or set create_new_column = False to overwrite the existing column.\n",
    "    \n",
    "    # new_column_suffix = \"_regex\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_regex\", the new column will be named as\n",
    "    # \"column1_regex\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    if (show_regex_helper): # run if True\n",
    "        \n",
    "        # Create an instance (object) from class regex_help:\n",
    "        helper = regex_help()\n",
    "        # Run helper object:\n",
    "        helper = helper.show_screen()\n",
    "        print(\"Interrupting the function and returning the dataframe itself.\")\n",
    "        print(\"Use the regex helper instructions to obtain the regex.\")\n",
    "        print(\"Do not forget to declare it as r'regex', with the r before quotes.\")\n",
    "        print(\"It indicates a raw expression. It is important for not reading the regex metacharacters as regular string scape characters.\\n\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if (string_for_replacement is None):\n",
    "            # make it the empty string\n",
    "            string_for_replacement = \"\"\n",
    "        \n",
    "        # Set a local copy of dataframe to manipulate\n",
    "        DATASET = df.copy(deep = True)\n",
    "        DATASET[column_to_analyze] = (DATASET[column_to_analyze]).astype(str)\n",
    "        new_series = DATASET[column_to_analyze].copy()\n",
    "        \n",
    "        new_series = new_series.str.replace(regex_to_search, string_for_replacement, regex = True)\n",
    "        # set regex = True to replace a regular expression\n",
    "        # https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html\n",
    "            \n",
    "        if (create_new_column):\n",
    "\n",
    "            if (new_column_suffix is None):\n",
    "                new_column_suffix = \"_regex\"\n",
    "\n",
    "            new_column_name = column_to_analyze + new_column_suffix\n",
    "            DATASET[new_column_name] = new_series\n",
    "\n",
    "        else:\n",
    "\n",
    "            DATASET[column_to_analyze] = new_series\n",
    "\n",
    "        # Now, we are in the main code.\n",
    "        print(f\"Finished searching the regex {regex_to_search} within {column_to_analyze}.\")\n",
    "        print(\"Check the 10 first elements from the output:\\n\")\n",
    "\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(new_series.head(10))\n",
    "\n",
    "        except: # regular mode\n",
    "            print(new_series.head(10))\n",
    "\n",
    "        return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bb586d5f-cc06-4aa6-8033-ab7c3389e2cb"
   },
   "source": [
    "# **Function for applying Fast Fourier Transform**\n",
    "- Determine which frequencies are important by extracting features with <a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\" class=\"external\">Fast Fourier Transform</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_fourier_transform (df, column_to_analyze, average_frequency_of_data_collection = 'hour', x_axis_rotation = 0, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    \n",
    "    # average_frequency_of_data_collection = 'hour' or 'h' for hours; 'day' or 'd' for days;\n",
    "    # 'minute' or 'min' for minutes; 'seconds' or 's' for seconds; 'ms' for milliseconds; 'ns' for\n",
    "    # nanoseconds; 'year' or 'y' for years; 'month' or 'm' for months.\n",
    "    \n",
    "    \n",
    "    average_frequency_of_data_collection = str(average_frequency_of_data_collection).lower()\n",
    "    \n",
    "    if ((average_frequency_of_data_collection == 'year')|(average_frequency_of_data_collection == 'y')):\n",
    "        count_per_year = 1\n",
    "        xtick_list = [1]\n",
    "        labels_list = ['1/year']\n",
    "    \n",
    "    elif ((average_frequency_of_data_collection == 'month')|(average_frequency_of_data_collection == 'm')):\n",
    "        count_per_year = 12\n",
    "        xtick_list = [1, count_per_year]\n",
    "        labels_list = ['1/year', '1/month']\n",
    "    \n",
    "    elif ((average_frequency_of_data_collection == 'day')|(average_frequency_of_data_collection == 'd')):\n",
    "        count_per_year = 365.2524\n",
    "        xtick_list = [1, count_per_year]\n",
    "        labels_list = ['1/year', '1/day']\n",
    "    \n",
    "    elif ((average_frequency_of_data_collection == 'hour')|(average_frequency_of_data_collection == 'h')):\n",
    "        count_per_year = 24 * 365.2524\n",
    "        xtick_list = [1, 365.2524, count_per_year]\n",
    "        labels_list = ['1/year', '1/day', '1/h']\n",
    "    \n",
    "    elif ((average_frequency_of_data_collection == 'minute')|(average_frequency_of_data_collection == 'min')):\n",
    "        count_per_year = 60 * 24 * 365.2524\n",
    "        xtick_list = [1, 365.2524, (24 * 365.2524), count_per_year]\n",
    "        labels_list = ['1/year', '1/day', '1/h', '1/min']\n",
    "    \n",
    "    elif ((average_frequency_of_data_collection == 'second')|(average_frequency_of_data_collection == 's')):\n",
    "        count_per_year = 60 * 60 * 24 * 365.2524\n",
    "        xtick_list = [1, 365.2524, (24 * 365.2524), (60 * 24 * 365.2524), count_per_year]\n",
    "        labels_list = ['1/year', '1/day', '1/h', '1/min', '1/s']\n",
    "    \n",
    "    elif (average_frequency_of_data_collection == 'ms'):\n",
    "        count_per_year = 60 * 60 * 24 * 365.2524 * (10**3)\n",
    "        xtick_list = [1, 365.2524, (24 * 365.2524), (60 * 24 * 365.2524), (60 * 60 * 24 * 365.2524), count_per_year]\n",
    "        labels_list = ['1/year', '1/day', '1/h', '1/min', '1/s', '1/ms']\n",
    "    \n",
    "    elif (average_frequency_of_data_collection == 'ns'):\n",
    "        count_per_year = 60 * 60 * 24 * 365.2524 * (10**9)\n",
    "        xtick_list = [1, 365.2524, (24 * 365.2524), (60 * 24 * 365.2524), (60 * 60 * 24 * 365.2524), (60 * 60 * 24 * 365.2524 * (10**3)), count_per_year]\n",
    "        labels_list = ['1/year', '1/day', '1/h', '1/min', '1/s', '1/ms', '1/ns']\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid frequency input. Considering frequency in h.\\n\")\n",
    "        count_per_year = 24 * 365.2524\n",
    "        xtick_list = [1, 365.2524, count_per_year]\n",
    "        labels_list = ['1/year', '1/day', '1/h']\n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    #Subtract the average value of column to analyze from each entry, to eliminate a possible offset\n",
    "    avg_value = DATASET[column_to_analyze].mean()\n",
    "    DATASET[column_to_analyze] = DATASET[column_to_analyze] - avg_value\n",
    "    \n",
    "    # Perform the Fourier transform\n",
    "    fft = tf.signal.rfft(DATASET[column_to_analyze])\n",
    "    f_per_dataset = np.arange(0, len(fft))\n",
    "\n",
    "    n_samples = len(DATASET[column_to_analyze])\n",
    "    years_per_dataset = n_samples/(count_per_year)\n",
    "\n",
    "    f_per_year = f_per_dataset/years_per_dataset\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    if (plot_title is None):\n",
    "        # Set graphic title\n",
    "        plot_title = \"obtained_frequencies\"\n",
    "\n",
    "    if (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        horizontal_axis_title = \"frequency_log_scale\"\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        vertical_axis_title = \"abs(fft)\"\n",
    "    \n",
    "    # fft is a complex tensor. Let's pick the absolute value of each complex:\n",
    "    abs_fft = np.abs(fft)\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    ax.step(f_per_year, abs_fft, color = 'crimson', linestyle = '-', alpha = OPACITY)\n",
    "    \n",
    "    # Set limits of the axes:\n",
    "    # Y from 0 to a value 1% higher than the maximum\n",
    "    # X from 0.1, close to zero, to the maximum. Zero cannot be present in log scale\n",
    "    \n",
    "    plt.xlim([0.1, max(plt.xlim())])\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xticks(xtick_list, labels = labels_list)\n",
    "        \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(horizontal_axis_title)\n",
    "    ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    ax.grid(grid) # show grid or not\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"fast_fourier_transform\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Attention: the frequency is in counts per year: 1 count per year corresponds to 1 year; 12 counts: months per year; 365.2524 counts: days per year, etc.\\n\")\n",
    "    \n",
    "    # Also, return a tuple combining the absolute value of fft with the corresponding count per year\n",
    "    return fft, tuple(zip(abs_fft, f_per_year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bb586d5f-cc06-4aa6-8033-ab7c3389e2cb"
   },
   "source": [
    "# **Function for generating columns with frequency information**\n",
    "- This gives the model access to the most important frequency features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_features (df, timestamp_tag_column, important_frequencies = [{'value': 1, 'unit': 'day'}, {'value':1, 'unit': 'year'}], x_axis_rotation = 0, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # important_frequencies = [{'value': 1, 'unit': 'day'}, {'value':1, 'unit': 'year'}]\n",
    "    # List of dictionaries with the important frequencies to add to the model. You can remove dictionaries,\n",
    "    # or add extra dictionaries. The dictionaries must have always the same keys, 'value' and 'unit'.\n",
    "    # If the importante frequency is once a day, the value will be 1, and the unit will be 'day' or 'd'.\n",
    "    # The possible units are: 'ns', 'ms', 'second' or 's', 'minute' or 'min', 'day' or 'd', 'month' or 'm',\n",
    "    # 'year' or 'y'.\n",
    "    \n",
    "    # the Date Time column is very useful, but not in this string form. \n",
    "    # Start by converting it to seconds:\n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    timestamp_s = DATASET[timestamp_tag_column].map(pd.Timestamp.timestamp)\n",
    "    # the time in seconds is not a useful model input. \n",
    "    # It may have daily and yearly periodicity, for instance. \n",
    "    # To deal with periodicity, you can get usable signals by using sine and cosine transforms \n",
    "    # to clear \"Time of day\" and \"Time of year\" signals:\n",
    "    \n",
    "    columns_to_plot = []\n",
    "    \n",
    "    for freq_dict in important_frequencies:\n",
    "        \n",
    "        value = freq_dict['value']\n",
    "        unit = freq_dict['unit']\n",
    "        \n",
    "        if ((value is not None) & (unit is not None)):\n",
    "            \n",
    "            unit = str(unit).lower()\n",
    "            \n",
    "            column_name1 = unit + \"_sin\"\n",
    "            column_name2 = unit + \"_cos\"\n",
    "            \n",
    "            column_tuple = (column_name1, column_name2)\n",
    "            columns_to_plot.append(column_tuple)\n",
    "            \n",
    "            if (unit == 'ns'):\n",
    "                # convert to seconds:\n",
    "                factor = 10 ** (-9)\n",
    "            \n",
    "            elif (unit == 'ms'):\n",
    "                # convert to seconds:\n",
    "                factor = 10 ** (-3)\n",
    "            \n",
    "            elif ((unit == 's')|(unit == 'second')):\n",
    "                # convert to seconds:\n",
    "                factor = 1\n",
    "            \n",
    "            elif ((unit == 'min')|(unit == 'minute')):\n",
    "                # convert to seconds:\n",
    "                factor = 60\n",
    "            \n",
    "            elif ((unit == 'hour')|(unit == 'h')):\n",
    "                # convert to seconds:\n",
    "                factor = 60 * 60\n",
    "            \n",
    "            elif ((unit == 'month')|(unit == 'm')):\n",
    "                # convert to seconds, considering a (365.2425)-day year, divided by 12:\n",
    "                factor = 60 * 60 * 24 * (365.2425)/12\n",
    "                print(f\"Attention: considering an average month of {(365.2425)/12} days.\\n\")\n",
    "            \n",
    "            elif ((unit == 'year')|(unit == 'y')):\n",
    "                # convert to seconds, considering a (365.2425)-day year:\n",
    "                factor = 60 * 60 * 24 * (365.2425)\n",
    "            \n",
    "            else:\n",
    "                # unit == 'day', or 'd', the default case\n",
    "                # convert to seconds:\n",
    "                factor = 60 * 60 * 24\n",
    "            \n",
    "            DATASET[column_name1] = np.sin(timestamp_s * (2 * np.pi / factor))\n",
    "            DATASET[column_name2] = np.cos(timestamp_s * (2 * np.pi / factor))\n",
    "            \n",
    "    # There are 8 possible frequencies to plot, i.e, 16 possible sin and cos plots.\n",
    "    # List of tuples, containing the pairs of colors to be used:\n",
    "    colors = [('crimson', 'darkblue'), \n",
    "                ('fuchsia', 'black'),\n",
    "                ('red', 'blue'),\n",
    "                ('darkgreen', 'magenta'),\n",
    "                ('aqua', 'violet'),\n",
    "                ('navy', 'purple'),\n",
    "                ('green', 'firebrick'),\n",
    "                ('blue', 'plum')]\n",
    "    \n",
    "    # Slice the colors list so that it has the same amount of elements as columns_to_plot:\n",
    "    colors = colors[:(len(columns_to_plot))]\n",
    "    # Now, we can zip both to create an iterable containing a tuple of plots and a correspondent\n",
    "    # tuple of colors.\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    if (plot_title is None):\n",
    "        # Set graphic title\n",
    "        plot_title = f\"frequency_signals\"\n",
    "\n",
    "    if (horizontal_axis_title is None):\n",
    "        # Set horizontal axis title\n",
    "        horizontal_axis_title = \"time\"\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Set vertical axis title\n",
    "        vertical_axis_title = \"signal\"\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    fig = plt.figure(figsize = (12, 8))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    for columns_tuple, colors_tuple in zip(columns_to_plot, colors):\n",
    "        \n",
    "        ax.plot(np.array(DATASET[columns_tuple[0]])[:25], linestyle = \"-\", marker = '', color = colors_tuple[0], alpha = OPACITY, label = columns_tuple[0])\n",
    "        ax.plot(np.array(DATASET[columns_tuple[1]])[:25], linestyle = \"-\", marker = '', color = colors_tuple[1], alpha = OPACITY, label = columns_tuple[1])\n",
    "        \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 0 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel(horizontal_axis_title)\n",
    "    ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "    ax.grid(grid) # show grid or not\n",
    "    ax.legend(loc = 'upper right')\n",
    "    # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "    # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "    # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "\n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "\n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"frequency_signals\"\n",
    "\n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "\n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    #plt.figure(figsize = (12, 8))\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6c672226-726a-4d40-bacd-bb2358159eca"
   },
   "source": [
    "# **Function for log-transforming the variables**\n",
    "- One curve derived from the normal is the log-normal.\n",
    "- If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "- A log normal curve resembles a normal, but with skewness (distortion); and kurtosis (long-tail).\n",
    "\n",
    "Applying the log is a methodology for **normalizing the variables**: the sample space gets shrinkled after the transformation, making the data more adequate for being processed by Machine Learning algorithms.\n",
    "- Preferentially apply the transformation to the whole dataset, so that all variables will be of same order of magnitude.\n",
    "- Obviously, it is not necessary for variables ranging from -100 to 100 in numerical value, where most outputs from the log transformation are.\n",
    "\n",
    "#### **WARNING**: This function will eliminate rows where the selected variables present values lower or equal to zero (condition for the logarithm to be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "azdata_cell_guid": "bdcdc35c-6401-4b3d-9669-0f6c123bca30",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def log_transform (df, subset = None, create_new_columns = True, new_columns_suffix = \"_log\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #### WARNING: This function will eliminate rows where the selected variables present \n",
    "    #### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "    \n",
    "    # subset = None\n",
    "    # Set subset = None to transform the whole dataset. Alternatively, pass a list with \n",
    "    # columns names for the transformation to be applied. For instance:\n",
    "    # subset = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "    # as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "    # Declaring the full list of columns is equivalent to setting subset = None.\n",
    "    \n",
    "    # create_new_columns = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into new\n",
    "    # columns. Or set create_new_columns = False to overwrite the existing columns\n",
    "    \n",
    "    # new_columns_suffix = \"_log\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "    # \"column1_log\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Check if a subset was defined. If so, make columns_list = subset \n",
    "    if not (subset is None):\n",
    "        \n",
    "        columns_list = subset\n",
    "    \n",
    "    else:\n",
    "        #There is no declared subset. Then, make columns_list equals to the list of\n",
    "        # numeric columns of the dataframe.\n",
    "        columns_list = list(DATASET.columns)\n",
    "        \n",
    "    # Let's check if there are categorical columns in columns_list. Only numerical\n",
    "    # columns should remain\n",
    "    # Start a support list:\n",
    "    support_list = []\n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    # Loop through each column in columns_list:\n",
    "    for column in columns_list:\n",
    "        \n",
    "        # Check the Pandas series (column) data type:\n",
    "        column_type = DATASET[column].dtype\n",
    "            \n",
    "        # If it is not categorical (object), append it to the support list:\n",
    "        if (column_type in numeric_dtypes):\n",
    "                \n",
    "            support_list.append(column)\n",
    "    \n",
    "    # Finally, make the columns_list support_list itself:\n",
    "    columns_list = support_list\n",
    "    \n",
    "    #Loop through each column to apply the transform:\n",
    "    for column in columns_list:\n",
    "        #access each element in the list column_list. The element is named 'column'.\n",
    "        \n",
    "        #boolean filter to check if the entry is higher than zero, condition for the log\n",
    "        # to be applied\n",
    "        boolean_filter = (DATASET[column] > 0)\n",
    "        #This filter is equals True only for the rows where the column is higher than zero.\n",
    "        \n",
    "        #Apply the boolean filter to the dataframe, removing the entries where the column\n",
    "        # cannot be log transformed.\n",
    "        # The boolean_filter selects only the rows for which the filter values are True.\n",
    "        DATASET = DATASET[boolean_filter]\n",
    "        \n",
    "        #Check if a new column will be created, or if the original column should be\n",
    "        # substituted.\n",
    "        if (create_new_columns == True):\n",
    "            # Create a new column.\n",
    "            \n",
    "            # The new column name will be set as column + new_columns_suffix\n",
    "            new_column_name = column + new_columns_suffix\n",
    "        \n",
    "        else:\n",
    "            # Overwrite the existing column. Simply set new_column_name as the value 'column'\n",
    "            new_column_name = column\n",
    "        \n",
    "        # Calculate the column value as the log transform of the original series (column)\n",
    "        DATASET[new_column_name] = np.log(DATASET[column])\n",
    "    \n",
    "    # Reset the index:\n",
    "    DATASET.reset_index(drop = True)\n",
    "    \n",
    "    print(\"The columns were successfully log-transformed. Check the 10 first rows of the new dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET\n",
    "\n",
    "    # One curve derived from the normal is the log-normal.\n",
    "    # If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "    # A log normal curve resembles a normal, but with skewness (distortion); \n",
    "    # and kurtosis (long-tail).\n",
    "\n",
    "    # Applying the log is a methodology for normalizing the variables: \n",
    "    # the sample space gets shrinkled after the transformation, making the data more \n",
    "    # adequate for being processed by Machine Learning algorithms. Preferentially apply \n",
    "    # the transformation to the whole dataset, so that all variables will be of same order \n",
    "    # of magnitude.\n",
    "    # Obviously, it is not necessary for variables ranging from -100 to 100 in numerical \n",
    "    # value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8877bfc2-101b-4e44-9e57-f1cf25902b36"
   },
   "source": [
    "# **Function for reversing the log-transform - applying the exponential transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "azdata_cell_guid": "da8b94bf-90c1-4bf6-a7be-51d881d9fd18",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_log_transform (df, subset = None, create_new_columns = True, new_columns_suffix = \"_originalScale\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #### WARNING: This function will eliminate rows where the selected variables present \n",
    "    #### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "    \n",
    "    # subset = None\n",
    "    # Set subset = None to transform the whole dataset. Alternatively, pass a list with \n",
    "    # columns names for the transformation to be applied. For instance:\n",
    "    # subset = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "    # as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "    # Declaring the full list of columns is equivalent to setting subset = None.\n",
    "    \n",
    "    # create_new_columns = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into new\n",
    "    # columns. Or set create_new_columns = False to overwrite the existing columns\n",
    "    \n",
    "    # new_columns_suffix = \"_originalScale\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_originalScale\", the new column will be named \n",
    "    # as \"column1_originalScale\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Check if a subset was defined. If so, make columns_list = subset \n",
    "    if not (subset is None):\n",
    "        \n",
    "        columns_list = subset\n",
    "    \n",
    "    else:\n",
    "        #There is no declared subset. Then, make columns_list equals to the list of\n",
    "        # numeric columns of the dataframe.\n",
    "        columns_list = list(DATASET.columns)\n",
    "        \n",
    "    # Let's check if there are categorical columns in columns_list. Only numerical\n",
    "    # columns should remain\n",
    "    # Start a support list:\n",
    "    support_list = []\n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "\n",
    "    # Loop through each column in columns_list:\n",
    "    for column in columns_list:\n",
    "        \n",
    "        # Check the Pandas series (column) data type:\n",
    "        column_type = DATASET[column].dtype\n",
    "            \n",
    "        # If it is not categorical (object), append it to the support list:\n",
    "        if (column_type in numeric_dtypes):\n",
    "                \n",
    "            support_list.append(column)\n",
    "    \n",
    "    # Finally, make the columns_list support_list itself:\n",
    "    columns_list = support_list\n",
    "    \n",
    "    #Loop through each column to apply the transform:\n",
    "    for column in columns_list:\n",
    "        #access each element in the list column_list. The element is named 'column'.\n",
    "        \n",
    "        # The exponential transformation can be applied to zero and negative values,\n",
    "        # so we remove the boolean filter.\n",
    "        \n",
    "        #Check if a new column will be created, or if the original column should be\n",
    "        # substituted.\n",
    "        if (create_new_columns == True):\n",
    "            # Create a new column.\n",
    "            \n",
    "            # The new column name will be set as column + new_columns_suffix\n",
    "            new_column_name = column + new_columns_suffix\n",
    "        \n",
    "        else:\n",
    "            # Overwrite the existing column. Simply set new_column_name as the value 'column'\n",
    "            new_column_name = column\n",
    "        \n",
    "        # Calculate the column value as the log transform of the original series (column)\n",
    "        DATASET[new_column_name] = np.exp(DATASET[column])\n",
    "    \n",
    "    print(\"The log_transform was successfully reversed through the exponential transformation. Check the 10 first rows of the new dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bb586d5f-cc06-4aa6-8033-ab7c3389e2cb"
   },
   "source": [
    "# **Function for obtaining and applying Box-Cox transform**\n",
    "- Transform data into a series that are represented by the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "azdata_cell_guid": "0d55cdcf-4fa6-4069-9a22-0cbf6d4bc067",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def box_cox_transform (df, column_to_transform, mode = 'calculate_and_apply', lambda_boxcox = None, suffix = '_BoxCoxTransf', specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from statsmodels.stats import diagnostic\n",
    "    from scipy import stats\n",
    "    \n",
    "    # This function will process a single column column_to_transform \n",
    "    # of the dataframe df per call.\n",
    "    \n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "    ## Box-Cox transform is given by:\n",
    "    ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "    ## log(x),                  for lmbda = 0\n",
    "    \n",
    "    # column_to_transform must be a string with the name of the column.\n",
    "    # e.g. column_to_transform = 'column1' to transform a column named as 'column1'\n",
    "    \n",
    "    # mode = 'calculate_and_apply'\n",
    "    # Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "    # transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "    # To 'apply_only', lambda_box must be provided.\n",
    "    \n",
    "    # lambda_boxcox must be a float value. e.g. lamda_boxcox = 1.7\n",
    "    # If you calculated lambda from the function box_cox_transform and saved the\n",
    "    # transformation data summary dictionary as data_sum_dict, simply set:\n",
    "    # lambda_boxcox = data_sum_dict['lambda_boxcox']\n",
    "    # This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "    # contains the lambda. \n",
    "    \n",
    "    # Analogously, spec_lim_dict['Inf_spec_lim_transf'] access\n",
    "    # the inferior specification limit transformed; and spec_lim_dict['Sup_spec_lim_transf'] \n",
    "    # access the superior specification limit transformed.\n",
    "    \n",
    "    # If lambda_boxcox is None, \n",
    "    # the mode will be automatically set as 'calculate_and_apply'.\n",
    "    \n",
    "    # suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "    # identified as 'Y_BoxCoxTransf'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "    \n",
    "    # specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "    # If there are specification limits, input them in this dictionary. Do not modify the keys,\n",
    "    # simply substitute None by the lower and/or the upper specification.\n",
    "    # e.g. Suppose you have a tank that cannot have more than 10 L. So:\n",
    "    # specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': 10}, there is only\n",
    "    # an upper specification equals to 10 (do not add units);\n",
    "    # Suppose a temperature cannot be lower than 10 ºC, but there is no upper specification. So,\n",
    "    # specification_limits = {'lower_spec_lim': 10, 'upper_spec_lim': None}. Finally, suppose\n",
    "    # a liquid which pH must be between 6.8 and 7.2:\n",
    "    # specification_limits = {'lower_spec_lim': 6.8, 'upper_spec_lim': 7.2}\n",
    "    \n",
    "    if not (suffix is None):\n",
    "        #only if a suffix was declared\n",
    "        #concatenate the column name to the suffix\n",
    "        new_col = column_to_transform + suffix\n",
    "    \n",
    "    else:\n",
    "        #concatenate the column name to the standard '_BoxCoxTransf' suffix\n",
    "        new_col = column_to_transform + '_BoxCoxTransf'\n",
    "    \n",
    "    boolean_check = (mode != 'calculate_and_apply') & (mode != 'apply_only')\n",
    "    # & is the 'and' operator. != is the 'is different from' operator.\n",
    "    #Check if neither 'calculate_and_apply' nor 'apply_only' were selected\n",
    "    \n",
    "    if ((lambda_boxcox is None) & (mode == 'apply_only')):\n",
    "        print(\"Invalid value set for \\'lambda_boxcox'\\. Setting mode to \\'calculate_and_apply\\'.\\n\")\n",
    "        mode = 'calculate_and_apply'\n",
    "    \n",
    "    elif (boolean_check == True):\n",
    "        print(\"Invalid value set for \\'mode'\\. Setting mode to \\'calculate_and_apply\\'.\\n\")\n",
    "        mode = 'calculate_and_apply'\n",
    "    \n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    print(\"Box-Cox transformation must be applied only to values higher than zero.\\n\")\n",
    "    print(\"That is because it is a logarithmic transformation.\\n\")\n",
    "    print(f\"So, filtering out all values from {column_to_transform} lower than or equal to zero.\\n\")\n",
    "    DATASET = DATASET[DATASET[column_to_transform] > 0]\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    y = DATASET[column_to_transform]\n",
    "    \n",
    "    \n",
    "    if (mode == 'calculate_and_apply'):\n",
    "        # Calculate lambda_boxcox\n",
    "        lambda_boxcox = stats.boxcox_normmax(y, method = 'pearsonr')\n",
    "        #calcula o lambda da transformacao box-cox utilizando o metodo da maxima verossimilhanca\n",
    "        #por meio da maximizacao do coeficiente de correlacao de pearson da funcao\n",
    "        #y = boxcox(x), onde boxcox representa a transformacao\n",
    "    \n",
    "    # For other cases, we will apply the lambda_boxcox set as the function parameter.\n",
    "\n",
    "    #Calculo da variavel transformada\n",
    "    y_transform = stats.boxcox(y, lmbda = lambda_boxcox, alpha = None)\n",
    "    #Calculo da transformada\n",
    "    \n",
    "    DATASET[new_col] = y_transform\n",
    "    #dataframe contendo os dados transformados\n",
    "    \n",
    "    print(\"Data successfully transformed. Check the 10 first transformed rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "        \n",
    "    print(\"\\n\") #line break\n",
    "    \n",
    "    # Start a dictionary to store the summary results of the transform and the normality\n",
    "    # tests:\n",
    "    data_sum_dict = {'lambda_boxcox': lambda_boxcox}\n",
    "    \n",
    "    # Test normality of the transformed variable:\n",
    "    # Scipy.stats’ normality test\n",
    "    # It is based on D’Agostino and Pearson’s test that combines \n",
    "    # skew and kurtosis to produce an omnibus test of normality.\n",
    "    _, scipystats_test_pval = stats.normaltest(y_transform) \n",
    "    # add this test result to the dictionary:\n",
    "    data_sum_dict['dagostino_pearson_p_val'] = scipystats_test_pval\n",
    "            \n",
    "    # Scipy.stats’ Shapiro-Wilk test\n",
    "    shapiro_test = stats.shapiro(y_transform)\n",
    "    data_sum_dict['shapiro_wilk_p_val'] = shapiro_test[1]\n",
    "    \n",
    "    # Lilliefors’ normality test\n",
    "    lilliefors_test = diagnostic.kstest_normal(y_transform, dist = 'norm', pvalmethod = 'table')\n",
    "    data_sum_dict['lilliefors_p_val'] = lilliefors_test[1]\n",
    "    \n",
    "    # Anderson-Darling normality test\n",
    "    ad_test = diagnostic.normal_ad(y_transform, axis = 0)\n",
    "    data_sum_dict['anderson_darling_p_val'] = ad_test[1]\n",
    "     \n",
    "    print(\"Box-Cox Transformation Summary:\\n\")\n",
    "    try:\n",
    "        display(data_sum_dict)     \n",
    "    except:\n",
    "        print(data_sum_dict)\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    \n",
    "    if not ((specification_limits['lower_spec_lim'] is None) & (specification_limits['upper_spec_lim'] is None)):\n",
    "        # Convert it to a list of specs:\n",
    "        list_of_specs = []\n",
    "        \n",
    "        if not (specification_limits['lower_spec_lim'] is None):\n",
    "            \n",
    "            if (specification_limits['lower_spec_lim'] <= 0):\n",
    "                print(\"Box-Cox transform can only be applied to values higher than zero. So, ignoring the lower specification.\\n\")\n",
    "            \n",
    "            else:\n",
    "                list_of_specs.append(specification_limits['lower_spec_lim'])\n",
    "        \n",
    "        if not (specification_limits['upper_spec_lim'] is None):\n",
    "            \n",
    "            if (specification_limits['upper_spec_lim'] <= 0):\n",
    "                print(\"Box-Cox transform can only be applied to values higher than zero. So, ignoring the upper specification.\\n\")\n",
    "            \n",
    "            else:\n",
    "                list_of_specs.append(specification_limits['upper_spec_lim'])\n",
    "        \n",
    "        # Notice that the list may have 1 or 2 elements.\n",
    "        \n",
    "        # Convert the list of specifications into a NumPy array:\n",
    "        spec_lim_array = np.array(list_of_specs)\n",
    "        \n",
    "        # If the array has a single element, we cannot apply stats method. So, let's transform\n",
    "        # manually:\n",
    "        ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "        ## log(x),                  for lmbda = 0\n",
    "        if (lambda_boxcox == 0):\n",
    "            \n",
    "            spec_lim_array = np.log(spec_lim_array)\n",
    "        \n",
    "        else:\n",
    "            spec_lim_array = ((spec_lim_array**lambda_boxcox) - 1)/(lambda_boxcox)\n",
    "        \n",
    "        # Start a dictionary to store the transformed limits:\n",
    "        spec_lim_dict = {}\n",
    "        \n",
    "        if not (specification_limits['lower_spec_lim'] is None):\n",
    "            # First element of the array is the lower specification. Add it to the\n",
    "            # dictionary:\n",
    "            spec_lim_dict['lower_spec_lim_transf'] = spec_lim_array[0]\n",
    "            \n",
    "            if not (specification_limits['upper_spec_lim'] is None):\n",
    "                # Second element of the array is the upper specification. Add it to the\n",
    "                # dictionary:\n",
    "                spec_lim_dict['upper_spec_lim_transf'] = spec_lim_array[1]\n",
    "        \n",
    "        else:\n",
    "            # The array contains only one element, which is the upper specification. Add\n",
    "            # it to the dictionary:\n",
    "            spec_lim_dict['upper_spec_lim_transf'] = spec_lim_array[0]\n",
    "        \n",
    "        print(\"New specification limits successfully obtained:\\n\")\n",
    "        try:\n",
    "            display(spec_lim_dict)     \n",
    "        except:\n",
    "            print(spec_lim_dict)\n",
    "        \n",
    "        # Add spec_lim_dict as a new element from data_sum_dict:\n",
    "        data_sum_dict['spec_lim_dict'] = spec_lim_dict\n",
    "    \n",
    "    \n",
    "    return DATASET, data_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0b8e7690-02e7-4593-b4d3-16a9671e66cd"
   },
   "source": [
    "# **Function for reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "azdata_cell_guid": "f383502b-0e94-4a04-b34c-49c3917155f7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_box_cox (df, column_to_transform, lambda_boxcox, suffix = '_ReversedBoxCox'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # This function will process a single column column_to_transform \n",
    "    # of the dataframe df per call.\n",
    "    \n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "    ## Box-Cox transform is given by:\n",
    "    ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "    ## log(x),                  for lmbda = 0\n",
    "    \n",
    "    # column_to_transform must be a string with the name of the column.\n",
    "    # e.g. column_to_transform = 'column1' to transform a column named as 'column1'\n",
    "    \n",
    "    # lambda_boxcox must be a float value. e.g. lamda_boxcox = 1.7\n",
    "    # If you calculated lambda from the function box_cox_transform and saved the\n",
    "    # transformation data summary dictionary as data_sum_dict, simply set:\n",
    "    # lambda_boxcox = data_sum_dict['lambda_boxcox']\n",
    "    # This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "    # contains the lambda. \n",
    "    \n",
    "    # Analogously, spec_lim_dict['Inf_spec_lim_transf'] access\n",
    "    # the inferior specification limit transformed; and spec_lim_dict['Sup_spec_lim_transf'] \n",
    "    # access the superior specification limit transformed.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "    # identified as '_ReversedBoxCox'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "    \n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    y = DATASET[column_to_transform]\n",
    "    \n",
    "    if (lambda_boxcox == 0):\n",
    "        #ytransf = np.log(y), according to Box-Cox definition. Then\n",
    "        #y_retransform = np.exp(y)\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = np.exp(y)\n",
    "    \n",
    "    else:\n",
    "        #apply Box-Cox function:\n",
    "        #y_transf = (y**lmbda - 1) / lmbda. Then,\n",
    "        #y_retransf ** (lmbda) = (y_transf * lmbda) + 1\n",
    "        #y_retransf = ((y_transf * lmbda) + 1) ** (1/lmbda), where ** is the potentiation\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = ((y * lambda_boxcox) + 1) ** (1/lambda_boxcox)\n",
    "    \n",
    "    if not (suffix is None):\n",
    "        #only if a suffix was declared\n",
    "        #concatenate the column name to the suffix\n",
    "        new_col = column_to_transform + suffix\n",
    "    \n",
    "    else:\n",
    "        #concatenate the column name to the standard '_ReversedBoxCox' suffix\n",
    "        new_col = column_to_transform + '_ReversedBoxCox'\n",
    "    \n",
    "    DATASET[new_col] = y_transform\n",
    "    #dataframe contendo os dados transformados\n",
    "    \n",
    "    print(\"Data successfully retransformed. Check the 10 first retransformed rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    " \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e6d569b6-aa9b-4b20-bae7-364e8220c5ac"
   },
   "source": [
    "# **Function for One-Hot Encoding categorical features**\n",
    "- Transform categorical values without notion of order into numerical (binary) features.\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "- The new columns will be named as the original columns + \"_\" + possible categories + \"OneHotEnc\".\n",
    "- Each column is a binary variable of the type \"is classified in this category or not\".\n",
    "\n",
    "Therefore, for a category \"A\", a column named \"A\" is created.\n",
    "- If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "- If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "azdata_cell_guid": "a1dfb85b-b74d-4b16-9f78-7f931812468d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def OneHotEncode_df (df, subset_of_features_to_be_encoded):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    #Start an encoding list empty (it will be a JSON object):\n",
    "    encoding_list = []\n",
    "    \n",
    "    # Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display  \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #loop through each column of the subset:\n",
    "    for column in subset_of_features_to_be_encoded:\n",
    "        \n",
    "        # Start two empty dictionaries:\n",
    "        encoding_dict = {}\n",
    "        nested_dict = {}\n",
    "        \n",
    "        # Add the column to encoding_dict as the key 'column':\n",
    "        encoding_dict['column'] = column\n",
    "        \n",
    "        # Loop through each element (named 'column') of the list of columns to analyze,\n",
    "        # subset_of_features_to_be_encoded\n",
    "        \n",
    "        # We could process the whole subset at once, but it could make us lose information\n",
    "        # about the generated columns\n",
    "        \n",
    "        # set a subset of the dataframe X containing 'column' as the only column:\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X  = df[[column]]\n",
    "        \n",
    "        #Start the OneHotEncoder object:\n",
    "        OneHot_enc_obj = OneHotEncoder()\n",
    "        \n",
    "        #Fit the object to that column:\n",
    "        OneHot_enc_obj = OneHot_enc_obj.fit(X)\n",
    "        # Get the transformed columns as a SciPy sparse matrix: \n",
    "        transformed_columns = OneHot_enc_obj.transform(X)\n",
    "        # Convert the sparse matrix to a NumPy dense array:\n",
    "        transformed_columns = transformed_columns.toarray()\n",
    "        \n",
    "        # Now, let's retrieve the encoding information and save it:\n",
    "        # Show encoded categories and store this array. \n",
    "        # It will give the proper columns' names:\n",
    "        encoded_columns = OneHot_enc_obj.categories_\n",
    "\n",
    "        # encoded_columns is an array containing a single element.\n",
    "        # This element is an array like:\n",
    "        # array(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8'], dtype=object)\n",
    "        # Then, this array is the element of index 0 from the list encoded_columns.\n",
    "        # It is represented as encoded_columns[0]\n",
    "\n",
    "        # Therefore, we actually want the array which is named as encoded_columns[0]\n",
    "        # Each element of this array is the name of one of the encoded columns. In the\n",
    "        # example above, the element 'cat2' would be accessed as encoded_columns[0][1],\n",
    "        # since it is the element of index [1] (second element) from the array \n",
    "        # encoded_columns[0].\n",
    "        \n",
    "        new_columns = encoded_columns[0]\n",
    "        # To identify the column that originated these new columns, we can join the\n",
    "        # string column to each element from new_columns:\n",
    "        \n",
    "        # Update the nested dictionary: store the new_columns as the key 'categories':\n",
    "        nested_dict['categories'] = new_columns\n",
    "        # Store the encoder object as the key 'OneHot_enc_obj'\n",
    "        # Add the encoder object to the dictionary:\n",
    "        nested_dict['OneHot_enc_obj'] = OneHot_enc_obj\n",
    "        \n",
    "        # Store the nested dictionary in the encoding_dict as the key 'OneHot_encoder':\n",
    "        encoding_dict['OneHot_encoder'] = nested_dict\n",
    "        # Append the encoding_dict as an element from list encoding_list:\n",
    "        encoding_list.append(encoding_dict)\n",
    "        \n",
    "        # Now we saved all encoding information, let's transform the data:\n",
    "        \n",
    "        # Start a support_list to store the concatenated strings:\n",
    "        support_list = []\n",
    "        \n",
    "        for encoded_col in new_columns:\n",
    "            # Use the str attribute to guarantee that the array stores only strings.\n",
    "            # Add an underscore \"_\" to separate the strings and an identifier of the transform:\n",
    "            new_column = column + \"_\" + str(encoded_col) + \"_OneHotEnc\"\n",
    "            # Append it to the support_list:\n",
    "            support_list.append(new_column)\n",
    "            \n",
    "        # Convert the support list to NumPy array, and make new_columns the support list itself:\n",
    "        new_columns = np.array(support_list)\n",
    "        \n",
    "        # Crete a Pandas dataframe from the array transformed_columns:\n",
    "        encoded_X_df = pd.DataFrame(transformed_columns)\n",
    "        \n",
    "        # Modify the name of the columns to make it equal to new_columns:\n",
    "        encoded_X_df.columns = new_columns\n",
    "        \n",
    "        #Inner join the new dataset with the encoded dataset.\n",
    "        # Use the index as the key, since indices are necessarily correspondent.\n",
    "        # To use join on index, we apply pandas .concat method.\n",
    "        # To join on a specific key, we could use pandas .merge method with the arguments\n",
    "        # left_on = 'left_key', right_on = 'right_key'; or, if the keys have same name,\n",
    "        # on = 'key':\n",
    "        # Check Pandas merge and concat documentation:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html\n",
    "        \n",
    "        new_df = pd.concat([new_df, encoded_X_df], axis = 1, join = \"inner\")\n",
    "        # When axis = 0, the .concat operation occurs in the row level, so the rows\n",
    "        # of the second dataframe are added to the bottom of the first one.\n",
    "        # It is the SQL union, and creates a dataframe with more rows, and\n",
    "        # total of columns equals to the total of columns of the first dataframe\n",
    "        # plus the columns of the second one that were not in the first dataframe.\n",
    "        # When axis = 1, the operation occurs in the column level: the two\n",
    "        # dataframes are laterally merged using the index as the key, \n",
    "        # preserving all columns from both dataframes. Therefore, the number of\n",
    "        # rows will be the total of rows of the dataframe with more entries,\n",
    "        # and the total of columns will be the sum of the total of columns of\n",
    "        # the first dataframe with the total of columns of the second dataframe.\n",
    "        \n",
    "        print(f\"Successfully encoded column \\'{column}\\' and merged the encoded columns to the dataframe.\\n\")\n",
    "        print(\"Check first 5 rows of the encoded table that was merged:\\n\")\n",
    "        \n",
    "        try:\n",
    "            display(encoded_X_df.head())\n",
    "        except: # regular mode\n",
    "            print(encoded_X_df.head())\n",
    "        \n",
    "        # The default of the head method, when no parameter is printed, is to show 5 rows; if an\n",
    "        # integer number Y is passed as argument .head(Y), Pandas shows the first Y-rows.\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"Finished One-Hot Encoding. Returning the new transformed dataframe; and an encoding list.\\n\")\n",
    "    print(\"Each element from this list is a dictionary with the original column name as key \\'column\\', and a nested dictionary as the key \\'OneHot_encoder\\'.\\n\")\n",
    "    print(\"In turns, the nested dictionary shows the different categories as key \\'categories\\' and the encoder object as the key \\'OneHot_enc_obj\\'.\\n\")\n",
    "    print(\"Use the encoder object to inverse the One-Hot Encoding in the correspondent function.\\n\")\n",
    "    print(f\"For each category in the columns \\'{subset_of_features_to_be_encoded}\\', a new column has value 1, if it is the actual category of that row; or is 0 if not.\\n\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(new_df.head(10))\n",
    "    except:\n",
    "        print(new_df.head(10))\n",
    "\n",
    "    #return the transformed dataframe and the encoding dictionary:\n",
    "    return new_df, encoding_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f7eb7b4c-fc69-4a27-84c2-5326b0de652b"
   },
   "source": [
    "# **Function for Reversing One-Hot Encoding of categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "azdata_cell_guid": "b1a903d4-2b26-4b5b-90d1-58c3c827b986",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_OneHotEncode (df, encoding_list):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # encoding_list: list in the same format of the one generated by OneHotEncode_df function:\n",
    "    # it must be a list of dictionaries where each dictionary contains two keys:\n",
    "    # key 'column': string with the original column name (in quotes); \n",
    "    # key 'OneHot_encoder': this key must store a nested dictionary.\n",
    "    # Even though the nested dictionaries generates by the encoding function present\n",
    "    # two keys:  'categories', storing an array with the different categories;\n",
    "    # and 'OneHot_enc_obj', storing the encoder object, only the key 'OneHot_enc_obj' is required.\n",
    "    ## On the other hand, a third key is needed in the nested dictionary:\n",
    "    ## key 'encoded_columns': this key must store a list or array with the names of the columns\n",
    "    # obtained from Encoding.\n",
    "    \n",
    "    \n",
    "    # Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display  \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for encoder_dict in encoding_list:\n",
    "        \n",
    "        try:\n",
    "            # Check if the required arguments are present:\n",
    "            if ((encoder_dict['column'] is not None) & (encoder_dict['OneHot_encoder']['OneHot_enc_obj'] is not None) & (encoder_dict['OneHot_encoder']['encoded_columns'] is not None)):\n",
    "\n",
    "                # Access the column name:\n",
    "                col_name = encoder_dict['column']\n",
    "\n",
    "                # Access the nested dictionary:\n",
    "                nested_dict = encoder_dict['OneHot_encoder']\n",
    "                # Access the encoder object on the dictionary\n",
    "                OneHot_enc_obj = nested_dict['OneHot_enc_obj']\n",
    "                # Access the list of encoded columns:\n",
    "                list_of_encoded_cols = list(nested_dict['encoded_columns'])\n",
    "\n",
    "                # Get a subset of the encoded columns\n",
    "                X = new_df.copy(deep = True)\n",
    "                X = X[list_of_encoded_cols]\n",
    "\n",
    "                # Reverse the encoding:\n",
    "                reversed_array = OneHot_enc_obj.inverse_transform(X)\n",
    "\n",
    "                # Add the reversed array as the column col_name on the dataframe:\n",
    "                new_df[col_name] = reversed_array\n",
    "                \n",
    "                print(f\"Reversed the encoding for {col_name}. Check the 5 first rows of the re-transformed series:\\n\")\n",
    "                \n",
    "                try:\n",
    "                    display(new_df[[col_name]].head())\n",
    "                except:\n",
    "                    print(new_df[[col_name]].head())\n",
    "                \n",
    "                print(\"\\n\")\n",
    "            \n",
    "        except:\n",
    "            print(\"Detected dictionary with incorrect keys or format. Unable to reverse encoding. Please, correct it.\\n\")\n",
    "    \n",
    "    print(\"Finished reversing One-Hot Encoding. Returning the new transformed dataframe.\\n\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_df.head(10))\n",
    "\n",
    "    #return the transformed dataframe:\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d9d06c66-8758-45d7-a01b-3b2d4420a8b4"
   },
   "source": [
    "# **Function for Ordinal Encoding categorical features**\n",
    "- Transform categorical values with notion of order into numerical (integer) features.\n",
    "- For each column, the Ordinal Encoder creates a new column in the dataset. This new column is represented by a an integer value, where each integer represents a possible categorie.\n",
    "- The new columns will be named as the original column + \"_OrdinalEnc\".\n",
    "\n",
    "#### WARNING: Machine Learning algorithms assume that close values represent similarity and order. If there is no order and no distance associated to each ordinal, use the One-Hot Encoding for converting categorical to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "azdata_cell_guid": "4a0c4f57-fdc9-4d33-9199-d94d3e4798d0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def OrdinalEncode_df (df, subset_of_features_to_be_encoded):\n",
    "\n",
    "    # Ordinal encoding: let's associate integer sequential numbers to the categorical column\n",
    "    # to apply the advanced encoding techniques. Even though the one-hot encoding could perform\n",
    "    # the same task and would, in fact, better, since there may be no ordering relation, the\n",
    "    # ordinal encoding is simpler and more suitable for this particular task:    \n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder \n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    #Start an encoding list empty (it will be a JSON object):\n",
    "    encoding_list = []\n",
    "    \n",
    "    # Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display  \n",
    "    except:\n",
    "        pass\n",
    "   \n",
    "    #loop through each column of the subset:\n",
    "    for column in subset_of_features_to_be_encoded:\n",
    "        \n",
    "        # Start two empty dictionaries:\n",
    "        encoding_dict = {}\n",
    "        nested_dict = {}\n",
    "        \n",
    "        # Add the column to encoding_dict as the key 'column':\n",
    "        encoding_dict['column'] = column\n",
    "        \n",
    "        # Loop through each element (named 'column') of the list of columns to analyze,\n",
    "        # subset_of_features_to_be_encoded\n",
    "        \n",
    "        # We could process the whole subset at once, but it could make us lose information\n",
    "        # about the generated columns\n",
    "        \n",
    "        # set a subset of the dataframe X containing 'column' as the only column:\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X  = new_df[[column]]\n",
    "        \n",
    "        #Start the OrdinalEncoder object:\n",
    "        ordinal_enc_obj = OrdinalEncoder()\n",
    "        \n",
    "        # Fit the ordinal encoder to the dataframe X:\n",
    "        ordinal_enc_obj = ordinal_enc_obj.fit(X)\n",
    "        # Get the transformed dataframe X: \n",
    "        transformed_X = ordinal_enc_obj.transform(X)\n",
    "        # transformed_X is an array of arrays like: [[0.], [0.], ..., [8.]]\n",
    "        # We want all the values in the first position of the internal arrays:\n",
    "        transformed_X = transformed_X[:,0]\n",
    "        # Get the encoded series as a NumPy array:\n",
    "        encoded_series = np.array(transformed_X)\n",
    "        \n",
    "        # Get a name for the new encoded column:\n",
    "        new_column = column + \"_OrdinalEnc\"\n",
    "        # Add this column to the dataframe:\n",
    "        new_df[new_column] = encoded_series\n",
    "        \n",
    "        # Now, let's retrieve the encoding information and save it:\n",
    "        # Show encoded categories and store this array. \n",
    "        # It will give the proper columns' names:\n",
    "        encoded_categories = ordinal_enc_obj.categories_\n",
    "\n",
    "        # encoded_categories is an array of strings containing the information of\n",
    "        # encoded categories and their values.\n",
    "        \n",
    "        # Update the nested dictionary: store the categories as the key 'categories':\n",
    "        nested_dict['categories'] = encoded_categories\n",
    "        # Store the encoder object as the key 'ordinal_enc_obj'\n",
    "        # Add the encoder object to the dictionary:\n",
    "        nested_dict['ordinal_enc_obj'] = ordinal_enc_obj\n",
    "        \n",
    "        # Store the nested dictionary in the encoding_dict as the key 'ordinal_encoder':\n",
    "        encoding_dict['ordinal_encoder'] = nested_dict\n",
    "        # Append the encoding_dict as an element from list encoding_list:\n",
    "        encoding_list.append(encoding_dict)\n",
    "        \n",
    "        print(f\"Successfully encoded column \\'{column}\\' and added the encoded column to the dataframe.\\n\")\n",
    "        print(\"Check first 5 rows of the encoded series that was merged:\\n\")\n",
    "        \n",
    "        try:\n",
    "            display(new_df[[new_column]].head())\n",
    "        except:\n",
    "            print(new_df[[new_column]].head())\n",
    "        \n",
    "        # The default of the head method, when no parameter is printed, is to show 5 rows; if an\n",
    "        # integer number Y is passed as argument .head(Y), Pandas shows the first Y-rows.\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"Finished Ordinal Encoding. Returning the new transformed dataframe; and an encoding list.\\n\")\n",
    "    print(\"Each element from this list is a dictionary with the original column name as key \\'column\\', and a nested dictionary as the key \\'ordinal_encoder\\'.\\n\")\n",
    "    print(\"In turns, the nested dictionary shows the different categories as key \\'categories\\' and the encoder object as the key \\'ordinal_enc_obj\\'.\\n\")\n",
    "    print(\"Use the encoder object to inverse the Ordinal Encoding in the correspondent function.\\n\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(new_df.head(10))\n",
    "    except:\n",
    "        print(new_df.head(10))\n",
    "    \n",
    "    #return the transformed dataframe and the encoding dictionary:\n",
    "    return new_df, encoding_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "329d671a-7fc9-4459-a460-ce9c3298c642"
   },
   "source": [
    "# **Function for Reversing Ordinal Encoding of categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "azdata_cell_guid": "7532858e-aa20-4d63-a2d6-4d2922952eef",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_OrdinalEncode (df, encoding_list):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # encoding_list: list in the same format of the one generated by OrdinalEncode_df function:\n",
    "    # it must be a list of dictionaries where each dictionary contains two keys:\n",
    "    # key 'column': string with the original column name (in quotes); \n",
    "    # key 'ordinal_encoder': this key must store a nested dictionary.\n",
    "    # Even though the nested dictionaries generates by the encoding function present\n",
    "    # two keys:  'categories', storing an array with the different categories;\n",
    "    # and 'ordinal_enc_obj', storing the encoder object, only the key 'ordinal_enc_obj' is required.\n",
    "    ## On the other hand, a third key is needed in the nested dictionary:\n",
    "    ## key 'encoded_column': this key must store a string with the name of the column\n",
    "    # obtained from Encoding.\n",
    "    \n",
    "    \n",
    "    # Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display  \n",
    "    except:\n",
    "        pass\n",
    "   \n",
    "    for encoder_dict in encoding_list:\n",
    "        \n",
    "        try:\n",
    "            # Check if the required arguments are present:\n",
    "            if ((encoder_dict['column'] is not None) & (encoder_dict['ordinal_encoder']['ordinal_enc_obj'] is not None) & (encoder_dict['ordinal_encoder']['encoded_column'] is not None)):\n",
    "\n",
    "                # Access the column name:\n",
    "                col_name = encoder_dict['column']\n",
    "\n",
    "                # Access the nested dictionary:\n",
    "                nested_dict = encoder_dict['ordinal_encoder']\n",
    "                # Access the encoder object on the dictionary\n",
    "                ordinal_enc_obj = nested_dict['ordinal_enc_obj']\n",
    "                # Access the encoded column and save it as a list:\n",
    "                list_of_encoded_cols = [nested_dict['encoded_column']]\n",
    "                # In OneHotEncoding we have an array of strings. Applying the list\n",
    "                # attribute would convert the array to list. Here, in turns, we have a simple\n",
    "                # string, which is also an iterable object. Applying the list attribute to a string\n",
    "                # creates a list of characters of that string.\n",
    "                # So, here we create a list with the string as its single element.\n",
    "\n",
    "                # Get a subset of the encoded column\n",
    "                X = new_df.copy(deep = True)\n",
    "                X = X[list_of_encoded_cols]\n",
    "\n",
    "                # Reverse the encoding:\n",
    "                reversed_array = ordinal_enc_obj.inverse_transform(X)\n",
    "\n",
    "                # Add the reversed array as the column col_name on the dataframe:\n",
    "                new_df[col_name] = reversed_array\n",
    "                    \n",
    "                print(f\"Reversed the encoding for {col_name}. Check the 5 first rows of the re-transformed series:\\n\")\n",
    "                \n",
    "                try:\n",
    "                    display(new_df[[col_name]].head())\n",
    "                except:\n",
    "                    print(new_df[[col_name]].head())\n",
    "\n",
    "                print(\"\\n\")\n",
    "                   \n",
    "        except:\n",
    "            print(\"Detected dictionary with incorrect keys or format. Unable to reverse encoding. Please, correct it.\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"Finished reversing Ordinal Encoding. Returning the new transformed dataframe.\\n\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(new_df.head(10))\n",
    "    except:\n",
    "        print(new_df.head(10))\n",
    "\n",
    "    #return the transformed dataframe:\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "32445308-6a42-415e-ba15-4cd94defb392"
   },
   "source": [
    "# **Function for scaling the features**\n",
    "- Machine Learning algorithms are extremely sensitive to scale. This function provides 3 methods (modes) of scaling:\n",
    "    - `mode = 'standard'`: applies the standard scaling, which creates a new variable with mean = 0; and standard deviation = 1. Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean of the training samples, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "    - `mode = 'min_max'`: applies min-max normalization, with a resultant feature ranging from 0 to 1. Each value Y is transformed as Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and maximum values of Y, respectively.\n",
    "    - `mode = 'factor'`: divide the whole series by a numeric value provided as argument. For a factor F, the new Y values will be Ytransf = Y/F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "azdata_cell_guid": "65fbbbd0-8242-4c6d-8133-7456fd83ecb7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def feature_scaling (df, subset_of_features_to_scale, mode = 'min_max', scale_with_new_params = True, list_of_scaling_params = None, suffix = '_scaled'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Scikit-learn Preprocessing data guide:\n",
    "    # https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "    # Standard scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    # Min-Max scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.set_params\n",
    "    \n",
    "    ## Machine Learning algorithms are extremely sensitive to scale. \n",
    "    \n",
    "    ## This function provides 4 methods (modes) of scaling:\n",
    "    ## mode = 'standard': applies the standard scaling, \n",
    "    ##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "    ##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "    ##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "    ## mode = 'min_max': applies min-max normalization, with a resultant feature \n",
    "    ## ranging from 0 to 1. each value Y is transformed as \n",
    "    ## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "    ## maximum values of Y, respectively.\n",
    "    \n",
    "    ## mode = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "    ## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "    \n",
    "    ## mode = 'normalize_by_maximum' is similar to mode = 'factor', but the factor will be selected\n",
    "    # as the maximum value. This mode is available only for scale_with_new_params = True. If\n",
    "    # scale_with_new_params = False, you should provide the value of the maximum as a division 'factor'.\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # subset_of_features_to_be_scaled: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_scaled = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_scaled = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    # scale_with_new_params = True\n",
    "    # Alternatively, set scale_with_new_params = True if you want to calculate a new\n",
    "    # scaler for the data; or set scale_with_new_params = False if you want to apply \n",
    "    # parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "    # previously trained to another set of data; or wants to simply apply again the same\n",
    "    # scaler).\n",
    "    \n",
    "    # list_of_scaling_params:\n",
    "    # This variable has effect only when SCALE_WITH_NEW_PARAMS = False\n",
    "    ## WARNING: The mode 'factor' demmands the input of the list of factors that will be \n",
    "    # used for normalizing each column. Therefore, it can be used only \n",
    "    # when scale_with_new_params = False.\n",
    "    \n",
    "    # list_of_scaling_params is a list of dictionaries with the same format of the list returned\n",
    "    # from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "    # but the list do not have to be in the same order of the columns - it will check one of the\n",
    "    # dictionary keys.\n",
    "    # The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "    # name of the column that will be scaled.\n",
    "    # the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "    # one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "    # numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "    # must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "    # two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "    # For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "    # standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "    # factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "    # Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "    # division.\n",
    "    # The key 'scaler_details' will not create an object: the transform will be directly performed \n",
    "    # through vectorial operations.\n",
    "    \n",
    "    # suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "    # identified as '_scaled'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "      \n",
    "    if (suffix is None):\n",
    "        #set as the default\n",
    "        suffix = '_scaled'\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    #Start an scaling list empty (it will be a JSON object):\n",
    "    scaling_list = []\n",
    "    \n",
    "    for column in subset_of_features_to_scale:\n",
    "        \n",
    "        # Create a dataframe X by subsetting only the analyzed column\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X = new_df[[column]]\n",
    "        \n",
    "        if (scale_with_new_params == False):\n",
    "            \n",
    "            # Use a previously obtained scaler.\n",
    "            # Loop through each element of the list:\n",
    "            \n",
    "            for scaling_dict in list_of_scaling_params:\n",
    "                \n",
    "                # check if the dictionary is from that column:\n",
    "                if (scaling_dict['column'] == column):\n",
    "                    \n",
    "                    # We found the correct dictionary. Let's retrieve the information:\n",
    "                    # retrieve the nested dictionary:\n",
    "                    nested_dict = scaling_dict['scaler']\n",
    "                    \n",
    "                    # try accessing the scaler object:\n",
    "                    try:\n",
    "                        scaler = nested_dict['scaler_obj']\n",
    "                        #calculate the scaled feature, and store it as new array:\n",
    "                        scaled_feature = scaler.transform(X)\n",
    "                        \n",
    "                        # Add the parameters to the nested dictionary:\n",
    "                        nested_dict['scaling_params'] = scaler.get_params(deep = True)\n",
    "                        \n",
    "                        if (mode == 'standard'):\n",
    "                            \n",
    "                            nested_dict['scaler_details'] = {\n",
    "                                'mu': X[column].mean(),\n",
    "                                'sigma': X[column].std()\n",
    "                            }\n",
    "                        \n",
    "                        elif (mode == 'min_max'):\n",
    "                            \n",
    "                            nested_dict['scaler_details'] = {\n",
    "                                'min': X[column].min(),\n",
    "                                'max': X[column].max()\n",
    "                            }\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        try:\n",
    "                            # As last alternative, let's try accessing the scaler details dict\n",
    "                            scaler_details = nested_dict['scaler_details']\n",
    "                                \n",
    "                            if (mode == 'standard'):\n",
    "                                \n",
    "                                nested_dict['scaling_params'] = 'standard_scaler_manually_defined'\n",
    "                                mu = scaler_details['mu']\n",
    "                                sigma = scaler_details['sigma']\n",
    "                                    \n",
    "                                if (sigma != 0):\n",
    "                                    scaled_feature = (X - mu)/sigma\n",
    "                                else:\n",
    "                                    scaled_feature = (X - mu)\n",
    "                                \n",
    "                            elif (mode == 'min_max'):\n",
    "                                    \n",
    "                                nested_dict['scaling_params'] = 'min_max_scaler_manually_defined'\n",
    "                                minimum = scaler_details['min']\n",
    "                                maximum = scaler_details['max']\n",
    "                                    \n",
    "                                if ((maximum - minimum) != 0):\n",
    "                                    scaled_feature = (X - minimum)/(maximum - minimum)\n",
    "                                else:\n",
    "                                    scaled_feature = X/maximum\n",
    "                                \n",
    "                            elif (mode == 'factor'):\n",
    "                                \n",
    "                                nested_dict['scaling_params'] = 'normalization_by_factor'\n",
    "                                factor = scaler_details['factor']\n",
    "                                scaled_feature = X/(factor)\n",
    "                                \n",
    "                            else:\n",
    "                                print(\"Select a valid mode: standard, min_max, or factor.\\n\")\n",
    "                                return \"error\", \"error\"\n",
    "                            \n",
    "                        except:\n",
    "                                \n",
    "                            print(f\"No valid scaling dictionary was input for column {column}.\\n\")\n",
    "                            return \"error\", \"error\"\n",
    "            \n",
    "        elif (mode == 'normalize_by_maximum'):\n",
    "            \n",
    "            #Start an scaling dictionary empty:\n",
    "            scaling_dict = {}\n",
    "\n",
    "            # add the column to the scaling dictionary:\n",
    "            scaling_dict['column'] = column\n",
    "\n",
    "            # Start a nested dictionary:\n",
    "            nested_dict = {}\n",
    "            \n",
    "            factor = X[column].max()\n",
    "            scaled_feature = X/(factor)\n",
    "            nested_dict['scaling_params'] = 'normalization_by_factor'\n",
    "            nested_dict['scaler_details'] = {'factor': factor, 'description': 'division_by_maximum_detected_value'}\n",
    "    \n",
    "        else:\n",
    "            # Create a new scaler:\n",
    "            \n",
    "            #Start an scaling dictionary empty:\n",
    "            scaling_dict = {}\n",
    "\n",
    "            # add the column to the scaling dictionary:\n",
    "            scaling_dict['column'] = column\n",
    "            \n",
    "            # Start a nested dictionary:\n",
    "            nested_dict = {}\n",
    "                \n",
    "            #start the scaler object:\n",
    "            if (mode == 'standard'):\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                scaler_details = {'mu': X[column].mean(), 'sigma': X[column].std()}\n",
    "\n",
    "            elif (mode == 'min_max'):\n",
    "                \n",
    "                scaler = MinMaxScaler()\n",
    "                scaler_details = {'min': X[column].min(), 'max': X[column].max()}\n",
    "                \n",
    "            # fit the scaler to the column\n",
    "            scaler = scaler.fit(X)\n",
    "                    \n",
    "            # calculate the scaled feature, and store it as new array:\n",
    "            scaled_feature = scaler.transform(X)\n",
    "            # scaler.inverse_transform(X) would reverse the scaling.\n",
    "                \n",
    "            # Get the scaling parameters for that column:\n",
    "            scaling_params = scaler.get_params(deep = True)\n",
    "                    \n",
    "            # scaling_params is a dictionary containing the scaling parameters.\n",
    "            # Add the scaling parameters to the nested dictionary:\n",
    "            nested_dict['scaling_params'] = scaling_params\n",
    "                \n",
    "            # add the scaler object to the nested dictionary:\n",
    "            nested_dict['scaler_obj'] = scaler\n",
    "            \n",
    "            # Add the scaler_details dictionary:\n",
    "            nested_dict['scaler_details'] = scaler_details\n",
    "            \n",
    "            # Now, all steps are the same for all cases, so we can go back to the main\n",
    "            # for loop:\n",
    "    \n",
    "        # Create the new_column name:\n",
    "        new_column = column + suffix\n",
    "        # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "        # Set the new column as scaled_feature\n",
    "        new_df[new_column] = scaled_feature\n",
    "                \n",
    "        # Add the nested dictionary to the scaling_dict:\n",
    "        scaling_dict['scaler'] = nested_dict\n",
    "                \n",
    "        # Finally, append the scaling_dict to the list scaling_list:\n",
    "        scaling_list.append(scaling_dict)\n",
    "                    \n",
    "        print(f\"Successfully scaled column {column}.\\n\")\n",
    "                \n",
    "    print(\"Successfully scaled the dataframe. Returning the transformed dataframe and the scaling dictionary.\\n\")\n",
    "    print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_df.head(10))\n",
    " \n",
    "    return new_df, scaling_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3f9bfa47-697d-4fec-80e9-03e2774a6ced"
   },
   "source": [
    "# **Function for reversing the scaling of the features**\n",
    "- `mode = 'standard'`.\n",
    "- `mode = 'min_max'`.\n",
    "- `mode = 'factor'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "azdata_cell_guid": "db9369fd-d483-408e-80a0-a9c05e776059",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_feature_scaling (df, subset_of_features_to_scale, list_of_scaling_params, mode = 'min_max', suffix = '_reverseScaling'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Scikit-learn Preprocessing data guide:\n",
    "    # https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "    # Standard scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    # Min-Max scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.set_params\n",
    "    \n",
    "    ## mode = 'standard': reverses the standard scaling, \n",
    "    ##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "    ##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "    ##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "    ## mode = 'min_max': reverses min-max normalization, with a resultant feature \n",
    "    ## ranging from 0 to 1. each value Y is transformed as \n",
    "    ## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "    ## maximum values of Y, respectively.\n",
    "    ## mode = 'factor': reverses the division of the whole series by a numeric value \n",
    "    # provided as argument. \n",
    "    ## For a factor F, the new Y transformed values are Ytransf = Y/F.\n",
    "    # Notice that if the original mode was 'normalize_by_maximum', then the maximum value used\n",
    "    # must be declared as any other factor.\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # subset_of_features_to_be_scaled: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_scaled = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_scaled = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    # list_of_scaling_params is a list of dictionaries with the same format of the list returned\n",
    "    # from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "    # but the list do not have to be in the same order of the columns - it will check one of the\n",
    "    # dictionary keys.\n",
    "    # The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "    # name of the column that will be scaled.\n",
    "    # the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "    # one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "    # numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "    # must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "    # two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "    # For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "    # standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "    # factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "    # Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "    # division.\n",
    "    # The key 'scaler_details' will not create an object: the transform will be directly performed \n",
    "    # through vectorial operations.\n",
    "    \n",
    "    # suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "    # identified as '_reverseScaling'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "      \n",
    "    if (suffix is None):\n",
    "        #set as the default\n",
    "        suffix = '_reverseScaling'\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    #Start an scaling list empty (it will be a JSON object):\n",
    "    scaling_list = []\n",
    "    \n",
    "    # Use a previously obtained scaler:\n",
    "    \n",
    "    for column in subset_of_features_to_scale:\n",
    "        \n",
    "        # Create a dataframe X by subsetting only the analyzed column\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X = new_df[[column]]\n",
    "\n",
    "        # Loop through each element of the list:\n",
    "            \n",
    "        for scaling_dict in list_of_scaling_params:\n",
    "                \n",
    "            # check if the dictionary is from that column:\n",
    "            if (scaling_dict['column'] == column):\n",
    "                    \n",
    "                # We found the correct dictionary. Let's retrieve the information:\n",
    "                # retrieve the nested dictionary:\n",
    "                nested_dict = scaling_dict['scaler']\n",
    "                    \n",
    "                # try accessing the scaler object:\n",
    "                try:\n",
    "                    scaler = nested_dict['scaler_obj']\n",
    "                    #calculate the reversed scaled feature, and store it as new array:\n",
    "                    rev_scaled_feature = scaler.inverse_transform(X)\n",
    "                        \n",
    "                    # Add the parameters to the nested dictionary:\n",
    "                    nested_dict['scaling_params'] = scaler.get_params(deep = True)\n",
    "                        \n",
    "                    if (mode == 'standard'):\n",
    "                            \n",
    "                        nested_dict['scaler_details'] = {\n",
    "                                'mu': rev_scaled_feature.mean(),\n",
    "                                'sigma': rev_scaled_feature.std()\n",
    "                            }\n",
    "                        \n",
    "                    elif (mode == 'min_max'):\n",
    "                            \n",
    "                        nested_dict['scaler_details'] = {\n",
    "                                'min': rev_scaled_feature.min(),\n",
    "                                'max': rev_scaled_feature.max()\n",
    "                            }\n",
    "                    \n",
    "                except:\n",
    "                        \n",
    "                    try:\n",
    "                        # As last alternative, let's try accessing the scaler details dict\n",
    "                        scaler_details = nested_dict['scaler_details']\n",
    "                                \n",
    "                        if (mode == 'standard'):\n",
    "                                \n",
    "                            nested_dict['scaling_params'] = 'standard_scaler_manually_defined'\n",
    "                            mu = scaler_details['mu']\n",
    "                            sigma = scaler_details['sigma']\n",
    "                                    \n",
    "                            if (sigma != 0):\n",
    "                                # scaled_feature = (X - mu)/sigma\n",
    "                                rev_scaled_feature = (X * sigma) + mu\n",
    "                            else:\n",
    "                                # scaled_feature = (X - mu)\n",
    "                                rev_scaled_feature = (X + mu)\n",
    "                                \n",
    "                        elif (mode == 'min_max'):\n",
    "                                    \n",
    "                            nested_dict['scaling_params'] = 'min_max_scaler_manually_defined'\n",
    "                            minimum = scaler_details['min']\n",
    "                            maximum = scaler_details['max']\n",
    "                                    \n",
    "                            if ((maximum - minimum) != 0):\n",
    "                                # scaled_feature = (X - minimum)/(maximum - minimum)\n",
    "                                rev_scaled_feature = (X * (maximum - minimum)) + minimum\n",
    "                            else:\n",
    "                                # scaled_feature = X/maximum\n",
    "                                rev_scaled_feature = (X * maximum)\n",
    "                                \n",
    "                        elif (mode == 'factor'):\n",
    "                                \n",
    "                            nested_dict['scaling_params'] = 'normalization_by_factor'\n",
    "                            factor = scaler_details['factor']\n",
    "                            # scaled_feature = X/(factor)\n",
    "                            rev_scaled_feature = (X * factor)\n",
    "                                \n",
    "                        else:\n",
    "                            print(\"Select a valid mode: standard, min_max, or factor.\\n\")\n",
    "                            return \"error\", \"error\"\n",
    "                            \n",
    "                    except:\n",
    "                                \n",
    "                        print(f\"No valid scaling dictionary was input for column {column}.\\n\")\n",
    "                        return \"error\", \"error\"\n",
    "         \n",
    "                # Create the new_column name:\n",
    "                new_column = column + suffix\n",
    "                # Create the new_column by dividing the previous column by the scaling factor:\n",
    "\n",
    "                # Set the new column as rev_scaled_feature\n",
    "                new_df[new_column] = rev_scaled_feature\n",
    "\n",
    "                # Add the nested dictionary to the scaling_dict:\n",
    "                scaling_dict['scaler'] = nested_dict\n",
    "\n",
    "                # Finally, append the scaling_dict to the list scaling_list:\n",
    "                scaling_list.append(scaling_dict)\n",
    "\n",
    "                print(f\"Successfully re-scaled column {column}.\\n\")\n",
    "                \n",
    "    print(\"Successfully re-scaled the dataframe.\\n\")\n",
    "    print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_df.head(10))\n",
    "                \n",
    "    return new_df, scaling_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a51119a2-d5b5-4897-b075-cd631f45a7e4"
   },
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "baa1ec11-492a-4ced-bda3-8799a2e63049",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def export_pd_dataframe_as_csv (dataframe_obj_to_be_exported, new_file_name_without_extension, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_obj_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # new_file_name_without_extension - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "    # will export a file 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name_without_extension)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_obj_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_without_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5ddec55a-b8a4-4d33-95d6-c6e3fdbf7e0c"
   },
   "source": [
    "# **Function for importing or exporting models, lists, or dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_list_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_or_list_file_name = None, directory_path = '', model_type = 'keras', dict_or_list_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    import dill\n",
    "    import tarfile\n",
    "    import tensorflow as tf\n",
    "    from zipfile import ZipFile\n",
    "    # https://docs.python.org/3/library/tarfile.html#tar-examples\n",
    "    # https://docs.python.org/3/library/zipfile.html#zipfile-objects\n",
    "    # pickle and dill save the file in binary (bits) serialized mode. So, we must use\n",
    "    # open 'rb' or 'wb' when calling the context manager. The 'b' stands for 'binary',\n",
    "    # informing the context manager (with statement) that a bit-file will be processed\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_or_list_only' if only a dictionary or list will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    # model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_or_list_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_or_list_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep dictionary_or_list_file_name = None if no \n",
    "    # dictionary or list will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type = 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "    # custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_or_list_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_or_list_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_or_list_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root (empty string):\n",
    "        directory_path = \"\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_or_list_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_or_list_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary or list.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_or_list_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pickle.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary or list {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pickle.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary or list successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_general'):\n",
    "                \n",
    "                print(\"Warning, save the model in a directory called 'saved_model' (before compressing.)\\n\")\n",
    "                # Create a temporary folder in case it does not exist:\n",
    "                # https://www.geeksforgeeks.org/python-os-makedirs-method/\n",
    "                # Set exist_ok = True\n",
    "                os.makedirs(\"tmp/\", exist_ok = True)\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_file_name\n",
    "                    \n",
    "                    try:\n",
    "                        model_extension = \".tar\"\n",
    "                        key = key + model_extension\n",
    "                        model_path = colab_files_dict[key]\n",
    "                        # Open the context manager\n",
    "                        with tarfile.open (model_path, 'r:') as compressed_model:\n",
    "                            #extract all to the tmp directory:\n",
    "                            compressed_model.extractall(\"tmp/\")\n",
    "                        \n",
    "                        # if you were not using the context manager, it would be necessary to apply\n",
    "                        # close method: tar = tarfile.open(fname, \"r:gz\"); tar.extractall(); tar.close()\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        try:\n",
    "                            # try tar.gz extension\n",
    "                            model_extension = \".tar.gz\"\n",
    "                            key = key + model_extension\n",
    "                            model_path = colab_files_dict[key]\n",
    "                            \n",
    "                            # Open the context manager\n",
    "                            with tarfile.open (model_path, 'r:gz') as compressed_model:\n",
    "                                #extract all to the tmp directory:\n",
    "                                compressed_model.extractall(\"tmp/\")\n",
    "                        \n",
    "                        except:\n",
    "                            # try .zip extension\n",
    "                            try:\n",
    "                                model_extension = \".zip\"\n",
    "                                key = key + model_extension\n",
    "                                model_path = colab_files_dict[key]\n",
    "                                \n",
    "                                # Open the context manager\n",
    "                                with ZipFile (model_path, 'r') as compressed_model:\n",
    "                                    #extract all to the tmp directory:\n",
    "                                    compressed_model.extractall(\"tmp/\")\n",
    "                            \n",
    "                            except:\n",
    "                                print(\"Failed to load the model. Compress it as zip, tar or tar.gz file.\\n\")\n",
    "                    \n",
    "                    \n",
    "                    # Compress the directory using tar\n",
    "                    # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    try:\n",
    "                        model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                        print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                    except:\n",
    "                        print(\"Failed to load the model. Save it in a directory named 'saved_model' before compressing.\\n\")\n",
    "                    \n",
    "                else:\n",
    "                    #standard method\n",
    "                    \n",
    "                    # Try simply accessing the directory:\n",
    "                    try:\n",
    "                        model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        try:\n",
    "                            model = tf.keras.models.load_model(model_file_name)\n",
    "                        \n",
    "                        except:\n",
    "                            \n",
    "                            # It is compressed\n",
    "                            try:\n",
    "                                model_extension = \".tar\"\n",
    "                                model_path = model_file_name\n",
    "\n",
    "                                # Open the context manager\n",
    "                                with tarfile.open (model_path, 'r:') as compressed_model:\n",
    "                                    #extract all to the tmp directory:\n",
    "                                    compressed_model.extractall(\"tmp/\")\n",
    "\n",
    "                                # if you were not using the context manager, it would be necessary to apply\n",
    "                                # close method: tar = tarfile.open(fname, \"r:gz\"); tar.extractall(); tar.close()\n",
    "\n",
    "                            except:\n",
    "\n",
    "                                try:\n",
    "                                    # try tar.gz extension\n",
    "                                    model_extension = \".tar.gz\"\n",
    "                                    model_path = model_file_name\n",
    "\n",
    "                                    # Open the context manager\n",
    "                                    with tarfile.open (model_path, 'r:gz') as compressed_model:\n",
    "                                        #extract all to the tmp directory:\n",
    "                                        compressed_model.extractall(\"tmp/\")\n",
    "\n",
    "                                except:\n",
    "                                    # try .zip extension\n",
    "                                    try:\n",
    "                                        model_extension = \".zip\"\n",
    "                                        model_path = model_file_name\n",
    "\n",
    "                                        # Open the context manager\n",
    "                                        with ZipFile (model_path, 'r') as compressed_model:\n",
    "                                            #extract all to the tmp directory:\n",
    "                                            compressed_model.extractall(\"tmp/\")\n",
    "\n",
    "                                    except:\n",
    "                                        print(\"Failed to load the model. Compress it as zip, tar or tar.gz file.\\n\")\n",
    "\n",
    "                    \n",
    "                    try:\n",
    "                        model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                        print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                    except:\n",
    "                        print(\"Failed to load the model. Save it in a directory named 'saved_model' before compressing.\\n\")\n",
    "                    \n",
    "                    \n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "\n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "\n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_or_list_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pickle.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary or list {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pickle.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary or list successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_general'):\n",
    "                \n",
    "                # Save your model in the SavedModel format\n",
    "                # Save as a directory named 'saved_model'\n",
    "                model_to_export.save('saved_model')\n",
    "                model_path = 'saved_model'\n",
    "            \n",
    "                try:\n",
    "                    model_path = model_path + \".tar.gz\"\n",
    "                    \n",
    "                    # Open the context manager\n",
    "                    with tarfile.open (model_path, 'w:gz') as compressed_model:\n",
    "                        #Add the folder:\n",
    "                        compressed_model.add('saved_model/')    \n",
    "                        # if you were not using the context manager, it would be necessary to apply\n",
    "                        # close method: tar = tarfile.open(fname, \"r:gz\"); tar.extractall(); tar.close()\n",
    "                \n",
    "                except:\n",
    "                    # try compressing as tar:\n",
    "                    try:\n",
    "                        model_path = model_path + \".tar\"\n",
    "                        # Open the context manager\n",
    "                        with tarfile.open (model_path, 'w:') as compressed_model:\n",
    "                            #Add the folder:\n",
    "                            compressed_model.add('saved_model/') \n",
    "                    \n",
    "                    except:\n",
    "                        # compress as zip:\n",
    "                        model_path = model_path + \".zip\"\n",
    "                        with ZipFile (model_path, 'w') as compressed_model:\n",
    "                            compressed_model.write('saved_model/')\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_path\n",
    "                    files.download(key)\n",
    "                    print(f\"TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    print(f\"TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1ed8f5da-7308-409e-95cb-c81c46fbdca8"
   },
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "55705356-3127-4fdb-aa29-aeca482ac8b4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "27be83b1-e61b-4a94-8c9e-7b2073f11812"
   },
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7e92d874-423a-483e-8249-8c39eb156a73",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "81613f50-c3d7-4840-b8c4-e82fc7bb9e53",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7dcbbeac-5d1b-483a-83c1-a0a85c4a908f",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "cf7dd573-3da1-4bec-95c0-46fc3d27b7ff",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9dbf7189-4ba9-4ffa-88a2-965699df42b9",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "29759209-4342-4e2c-addd-34acf841b18c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3afd2423-139d-431b-9a56-8e75400b2491",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e8d2a160-c74b-4d65-b7c2-e50e6f7f32c0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Removing trailing or leading white spaces or characters (trim) from string variables, and modifying the variable type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "    \n",
    "METHOD = 'trim'\n",
    "# METHOD = 'trim' will eliminate trailing and leading white spaces from the strings in\n",
    "# COLUMN_TO_ANALYZE.\n",
    "# METHOD = 'substring' will eliminate a defined trailing and leading substring from\n",
    "# COLUMN_TO_ANALYZE.\n",
    "\n",
    "SUBSTRING_TO_ELIMINATE = None\n",
    "# SUBSTRING_TO_ELIMINATE = None. Set as a string (in quotes) if METHOD = 'substring'.\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains time information: each string ends in \" min\":\n",
    "# \"1 min\", \"2 min\", \"3 min\", etc. If SUBSTRING_TO_ELIMINATE = \" min\", this portion will be\n",
    "# eliminated, resulting in: \"1\", \"2\", \"3\", etc. If NEW_VARIABLE_TYPE = None, these values will\n",
    "# continue to be strings. By setting NEW_VARIABLE_TYPE = 'int' or 'float', the series will be\n",
    "# converted to a numeric type.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_trim'\n",
    "# NEW_COLUMN_SUFFIX = \"_trim\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_trim\", the new column will be named as\n",
    "# \"column1_trim\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = trim_spaces_or_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, new_variable_type = NEW_VARIABLE_TYPE, method = METHOD, substring_to_eliminate = SUBSTRING_TO_ELIMINATE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "94411320-e6a4-49b2-aa70-ee39a3630a6b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Capitalizing or lowering case of string variables (string homogenizing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "47e1604b-6bbf-43fe-9e4b-a4e60f2802a5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "METHOD = 'lowercase'\n",
    "# METHOD = 'capitalize' will capitalize all letters from the input string \n",
    "# (turn them to upper case).\n",
    "# METHOD = 'lowercase' will make the opposite: turn all letters to lower case.\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains strings such as 'String One', 'STRING 2',  and\n",
    "# 'string3'. If METHOD = 'capitalize', the output will contain the strings: \n",
    "# 'STRING ONE', 'STRING 2', 'STRING3'. If METHOD = 'lowercase', the outputs will be:\n",
    "# 'string one', 'string 2', 'string3'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_homogenized'\n",
    "# NEW_COLUMN_SUFFIX = \"_homogenized\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_homogenized\", the new column will be named as\n",
    "# \"column1_homogenized\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = capitalize_or_lower_string_case (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, method = METHOD, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Adding contractions to the contractions library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_CONTRACTIONS = [\n",
    "    \n",
    "    {'contracted_expression': None, 'correct_expression': None}, \n",
    "    {'contracted_expression': None, 'correct_expression': None}, \n",
    "    {'contracted_expression': None, 'correct_expression': None}, \n",
    "    {'contracted_expression': None, 'correct_expression': None}\n",
    "\n",
    "]\n",
    "# LIST_OF_CONTRACTIONS = [{'contracted_expression': None, 'correct_expression': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the form as the contraction is usually observed; and the second one \n",
    "# contains the correct (full) string that will replace it.\n",
    "# Since contractions can cause issues when processing text, we can expand them with these functions.\n",
    "        \n",
    "# The object list_of_contractions must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'contracted_expression' for the contraction; and 'correct_expression', \n",
    "# for the strings with the correspondent correction.\n",
    "        \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you want to add more elements\n",
    "# to the contractions library.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'contracted_expression': original_str, 'correct_expression': new_str}, \n",
    "# where original_str and new_str represent the contracted and expanded strings\n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "        \n",
    "# Example:\n",
    "# LIST_OF_CONTRACTIONS = [{'contracted_expression': 'mychange', 'correct_expression': 'my change'}]\n",
    "        \n",
    "\n",
    "add_contractions_to_library (list_of_contractions = LIST_OF_CONTRACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Correcting contracted strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_contractionsFixed'\n",
    "# NEW_COLUMN_SUFFIX = \"_contractionsFixed\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_contractionsFixed\", the new column will be named as\n",
    "# \"column1_contractionsFixed\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = correct_contracted_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Substituting (replacing) substrings on string variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = None\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = ''\n",
    "# SUBSTRING_TO_BE_REPLACED = None; new_substring_for_replacement = ''. \n",
    "# Strings (in quotes): when the sequence of characters SUBSTRING_TO_BE_REPLACED was\n",
    "# found in the strings from column_to_analyze, it will be substituted by the substring\n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT. If None is provided to one of these substring arguments,\n",
    "# it will be substituted by the empty string: ''\n",
    "# e.g. suppose COLUMN_TO_ANALYZE contains the following strings, with a spelling error:\n",
    "# \"my collumn 1\", 'his collumn 2', 'her column 3'. We may correct this error by setting:\n",
    "# SUBSTRING_TO_BE_REPLACED = 'collumn' and NEW_SUBSTRING_FOR_REPLACEMENT = 'column'. The\n",
    "# function will search for the wrong group of characters and, if it finds it, will substitute\n",
    "# by the correct sequence: \"my column 1\", 'his column 2', 'her column 3'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_substringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_substringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_substringReplaced\", the new column will be named as\n",
    "# \"column1_substringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = replace_substring (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Inverting the order of the string characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringInverted'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringInverted\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringInverted\", the new column will be named as\n",
    "# \"column1_stringInverted\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = invert_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Slicing the strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "FIRST_CHARACTER_INDEX = None\n",
    "# FIRST_CHARACTER_INDEX = None - integer representing the index of the first character to be\n",
    "# included in the new strings. If None, slicing will start from first character.\n",
    "# Indexing of strings always start from 0. The last index can be represented as -1, the index of\n",
    "# the character before as -2, etc (inverse indexing starts from -1).\n",
    "# example: consider the string \"idsw\", which contains 4 characters. We can represent the indices as:\n",
    "# 'i': index 0; 'd': 1, 's': 2, 'w': 3. Alternatively: 'w': -1, 's': -2, 'd': -3, 'i': -4.\n",
    "\n",
    "LAST_CHARACTER_INDEX = None\n",
    "# LAST_CHARACTER_INDEX = None - integer representing the index of the last character to be\n",
    "# included in the new strings. If None, slicing will go until the last character.\n",
    "# Attention: this is effectively the last character to be added, and not the next index after last\n",
    "# character.\n",
    "        \n",
    "# in the 'idsw' example, if we want a string as 'ds', we want the FIRST_CHARACTER_INDEX = 1 and\n",
    "# LAST_CHARACTER_INDEX = 2.\n",
    "\n",
    "STEP = 1\n",
    "# STEP = 1 - integer representing the slicing step. If step = 1, all characters will be added.\n",
    "# If STEP = 2, then the slicing will pick one element of index i and the element with index (i+2)\n",
    "# (1 index will be 'jumped'), and so on.\n",
    "# If STEP is negative, then the order of the new strings will be inverted.\n",
    "# Example: STEP = -1, and the start and finish indices are None: the output will be the inverted\n",
    "# string, 'wsdi'.\n",
    "# FIRST_CHARACTER_INDEX = 1, LAST_CHARACTER_INDEX = 2, STEP = 1: output = 'ds';\n",
    "# FIRST_CHARACTER_INDEX = None, LAST_CHARACTER_INDEX = None, STEP = 2: output = 'is';\n",
    "# FIRST_CHARACTER_INDEX = None, LAST_CHARACTER_INDEX = None, STEP = 3: output = 'iw';\n",
    "# FIRST_CHARACTER_INDEX = -1, LAST_CHARACTER_INDEX = -2, STEP = -1: output = 'ws';\n",
    "# FIRST_CHARACTER_INDEX = -1, LAST_CHARACTER_INDEX = None, STEP = -2: output = 'wd';\n",
    "# FIRST_CHARACTER_INDEX = -1, LAST_CHARACTER_INDEX = None, STEP = 1: output = 'w'\n",
    "# In this last example, the function tries to access the next element after the character of index\n",
    "# -1. Since -1 is the last character, there are no other characters to be added.\n",
    "# FIRST_CHARACTER_INDEX = -2, LAST_CHARACTER_INDEX = -1, STEP = 1: output = 'sw'.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_slicedString'\n",
    "# NEW_COLUMN_SUFFIX = \"_slicedString\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_slicedString\", the new column will be named as\n",
    "# \"column1_slicedString\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = slice_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, first_character_index = FIRST_CHARACTER_INDEX, last_character_index = LAST_CHARACTER_INDEX, step = STEP, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Getting the leftest characters from the strings (retrieve last characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - integer representing the total of characters that will\n",
    "# be retrieved. Here, we will retrieve the leftest characters. If NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1,\n",
    "# only the leftest (last) character will be retrieved.\n",
    "# Consider the string 'idsw'.\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - output: 'w';\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 2 - output: 'sw'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "# So, if the last part of the strings is a number, you can use this argument to directly extract\n",
    "# this part as numeric variable.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_leftChars'\n",
    "# NEW_COLUMN_SUFFIX = \"_leftChars\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_leftChars\", the new column will be named as\n",
    "# \"column1_leftChars\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = left_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, number_of_characters_to_retrieve = NUMBER_OF_CHARACTERS_TO_RETRIEVE, new_variable_type = NEW_VARIABLE_TYPE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Getting the rightest characters from the strings (retrieve first characters)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3c30781-388b-4955-9019-17cd797fac2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - integer representing the total of characters that will\n",
    "# be retrieved. Here, we will retrieve the rightest characters. If NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1,\n",
    "# only the rightest (first) character will be retrieved.\n",
    "# Consider the string 'idsw'.\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 1 - output: 'i';\n",
    "# NUMBER_OF_CHARACTERS_TO_RETRIEVE = 2 - output: 'id'.\n",
    "\n",
    "NEW_VARIABLE_TYPE = None\n",
    "# NEW_VARIABLE_TYPE = None. String (in quotes) that represents a given data type for the column\n",
    "# after transformation. Set:\n",
    "# - NEW_VARIABLE_TYPE = 'int' to convert the column to integer type after the transform;\n",
    "# - NEW_VARIABLE_TYPE = 'float' to convert the column to float (decimal number);\n",
    "# - NEW_VARIABLE_TYPE = 'datetime' to convert it to date or timestamp;\n",
    "# - NEW_VARIABLE_TYPE = 'category' to convert it to Pandas categorical variable.\n",
    "# So, if the first part of the strings is a number, you can use this argument to directly extract\n",
    "# this part as numeric variable.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_rightChars'\n",
    "# NEW_COLUMN_SUFFIX = \"_rightChars\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_rightChars\", the new column will be named as\n",
    "# \"column1_rightChars\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = right_characters (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, number_of_characters_to_retrieve = NUMBER_OF_CHARACTERS_TO_RETRIEVE, new_variable_type = NEW_VARIABLE_TYPE, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Joining strings from a same column into a single string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SEPARATOR = \" \"\n",
    "# SEPARATOR = \" \" - string containing the separator. Suppose the column contains the\n",
    "# strings: 'a', 'b', 'c', 'd'. If the SEPARATOR is the empty string '', the output will be:\n",
    "# 'abcd' (no separation). If SEPARATOR = \" \" (simple whitespace), the output will be 'a b c d'\n",
    "\n",
    "\n",
    "# The returned string is stored as concat_string:\n",
    "# Simply modify this variable name on the left of equality:\n",
    "concat_string = join_strings_from_column (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, separator = SEPARATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Joining several string columns into a single string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "LIST_OF_COLUMNS_TO_JOIN = ['column1', 'column2']\n",
    "# LIST_OF_COLUMNS_TO_JOIN: list of strings (inside quotes), \n",
    "# containing the name of the columns with strings to be joined.\n",
    "# Attention: the strings will be joined row by row, i.e. only strings in the same rows will\n",
    "# be concatenated. To join strings from the same column, use function join_strings_from_column\n",
    "# e.g. LIST_OF_COLUMNS_TO_JOIN = [\"column1\", \"column2\"] will join strings from \"column1\" with\n",
    "# the correspondent strings from \"column2\".\n",
    "# Notice that you can concatenate any kind of columns: numeric, dates, texts ,..., but the output\n",
    "# will be a string column.\n",
    "\n",
    "SEPARATOR = \" \"\n",
    "# SEPARATOR = \" \" - string containing the separator. Suppose the column contains the\n",
    "# strings: 'a', 'b', 'c', 'd'. If the SEPARATOR is the empty string '', the output will be:\n",
    "# 'abcd' (no separation). If SEPARATOR = \" \" (simple whitespace), the output will be 'a b c d'\n",
    "\n",
    "NEW_COLUMN_SUFFIX = '_stringConcat'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringConcat\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringConcat\", the new column will be named as\n",
    "# \"column1_stringConcat\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = join_string_columns (df = DATASET, list_of_columns_to_join = LIST_OF_COLUMNS_TO_JOIN, separator = SEPARATOR, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Splitting strings into a list of strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "SEPARATOR = \" \"\n",
    "# SEPARATOR = \" \" - string containing the separator. Suppose the column contains the\n",
    "# string: 'a b c d' on a given row. If the SEPARATOR is whitespace ' ', \n",
    "# the output will be a list: ['a', 'b', 'c', 'd']: the function splits the string into a list\n",
    "# of strings (one list per row) every time it finds the separator.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringSplitted'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringSplitted\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringSplitted\", the new column will be named as\n",
    "# \"column1_stringSplitted\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = split_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, separator = SEPARATOR, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a4c5fc1f-f93d-4f21-8fe3-d74c7658cb9b",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Substituting (replacing or switching) whole strings by different text values (on string variables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f91631b5-37db-4bd2-a72d-256859c9bd67",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = [\n",
    "    \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}, \n",
    "    {'original_string': None, 'new_string': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = \n",
    "# [{'original_string': None, 'new_string': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original string; and the second one contains the new string\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'original_string', and search these values on the strings\n",
    "# in COLUMN_TO_ANALYZE. When the value is found, it will be replaced (switched) by the correspondent\n",
    "# value in key 'new_string'.\n",
    "    \n",
    "# The object LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'original_string' for the original strings to search on the column \n",
    "# column_to_analyze; and 'new_string', for the strings that will replace the original ones.\n",
    "# Notice that this function will not search for substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_to_analyze' and 'original_string'.\n",
    "# So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'original_string': original_str, 'new_string': new_str}, \n",
    "# where original_str and new_str represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "    \n",
    "# Example:\n",
    "# Suppose the COLUMN_TO_ANALYZE contains the values 'sunday', 'monday', 'tuesday', 'wednesday',\n",
    "# 'thursday', 'friday', 'saturday', but you want to obtain data labelled as 'weekend' or 'weekday'.\n",
    "# Set: LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS = \n",
    "# [{'original_string': 'sunday', 'new_string': 'weekend'},\n",
    "# {'original_string': 'saturday', 'new_string': 'weekend'},\n",
    "# {'original_string': 'monday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'tuesday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'wednesday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'thursday', 'new_string': 'weekday'},\n",
    "# {'original_string': 'friday', 'new_string': 'weekday'}]\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "# \"column1_stringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = switch_strings (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, list_of_dictionaries_with_original_strings_and_replacements = LIST_OF_DICTIONARIES_WITH_ORIGINAL_STRINGS_AND_REPLACEMENTS, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "dd1801c2-ceef-441f-883a-9900a9f5dbb0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Replacing strings with Machine Learning: finding similar strings and replacing them by standard strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1f7e4731-aaba-4ea4-8760-665522f2bb08",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "MODE = 'find_and_replace'\n",
    "# MODE = 'find_and_replace' will find similar strings; and switch them by one of the\n",
    "# standard strings if the similarity between them is higher than or equals to the threshold.\n",
    "# Alternatively: MODE = 'find' will only find the similar strings by calculating the similarity.\n",
    "\n",
    "THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0\n",
    "# THRESHOLD_FOR_PERCENT_OF_SIMILARITY = 80.0 - 0.0% means no similarity and 100% means equal strings.\n",
    "# The THRESHOLD_FOR_PERCENT_OF_SIMILARITY is the minimum similarity calculated from the\n",
    "# Levenshtein (minimum edit) distance algorithm. This distance represents the minimum number of\n",
    "# insertion, substitution or deletion of characters operations that are needed for making two\n",
    "# strings equal.\n",
    "\n",
    "LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT = [\n",
    "    \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None},\n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}, \n",
    "    {'standard_string': None}\n",
    "    \n",
    "]\n",
    "# This is a list of dictionaries, where each dictionary contains a single key-value pair:\n",
    "# the key must be always 'standard_string', and the value will be one of the standard strings \n",
    "# for replacement: if a given string on the COLUMN_TO_ANALYZE presents a similarity with one \n",
    "# of the standard string equals or higher than the THRESHOLD_FOR_PERCENT_OF_SIMILARITY, it will be\n",
    "# substituted by this standard string.\n",
    "# For instance, suppose you have a word written in too many ways, making it difficult to use\n",
    "# the function switch_strings: \"EU\" , \"eur\" , \"Europ\" , \"Europa\" , \"Erope\" , \"Evropa\" ...\n",
    "# You can use this function to search strings similar to \"Europe\" and replace them.\n",
    "    \n",
    "# The function will loop through all dictionaries in this list, access the values of the keys \n",
    "# 'standard_string', and search these values on the strings in COLUMN_TO_ANALYZE. When the value \n",
    "# is found, it will be replaced (switched) if the similarity is sufficiently high.\n",
    "    \n",
    "# The object LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'standard_string'.\n",
    "# Notice that this function performs fuzzy matching, so it MAY SEARCH substrings and strings\n",
    "# written with different cases (upper or lower) when this portions or modifications make the\n",
    "# strings sufficiently similar to each other.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same key: {'standard_string': other_std_str}, \n",
    "# where other_std_str represents the string for searching and replacement \n",
    "# (If the key contains None, the new dictionary will be ignored).\n",
    "    \n",
    "# Example:\n",
    "# Suppose the COLUMN_TO_ANALYZE contains the values 'California', 'Cali', 'Calefornia', \n",
    "# 'Calefornie', 'Californie', 'Calfornia', 'Calefernia', 'New York', 'New York City', \n",
    "# but you want to obtain data labelled as the state 'California' or 'New York'.\n",
    "# Set: list_of_dictionaries_with_standard_strings_for_replacement = \n",
    "# [{'standard_string': 'California'},\n",
    "# {'standard_string': 'New York'}]\n",
    "    \n",
    "# ATTENTION: It is advisable for previously searching the similarity to find the best similarity\n",
    "# threshold; set it as high as possible, avoiding incorrect substitutions in a gray area; and then\n",
    "# perform the replacement. It will avoid the repetition of original incorrect strings in the\n",
    "# output dataset, as well as wrong replacement (replacement by one of the standard strings which\n",
    "# is not the correct one).\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_stringReplaced'\n",
    "# NEW_COLUMN_SUFFIX = \"_stringReplaced\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_stringReplaced\", the new column will be named as\n",
    "# \"column1_stringReplaced\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset.\n",
    "# The summary list is saved as summary_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "transf_dataset, summary_list = string_replacement_ml (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, mode = MODE, threshold_for_percent_of_similarity = THRESHOLD_FOR_PERCENT_OF_SIMILARITY, list_of_dictionaries_with_standard_strings_for_replacement = LIST_OF_DICTIONARIES_WITH_STANDARD_STRINGS_FOR_REPLACEMENT, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Searching for Regular Expression (RegEx) within a string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "REGEX_TO_SEARCH = r\"\"\n",
    "# REGEX_TO_SEARCH = r\"\" - string containing the regular expression (regex) that will be searched\n",
    "# within each string from the column. Declare it with the r before quotes, indicating that the\n",
    "# 'raw' string should be read. That is because the regex contain special characters, such as \\,\n",
    "# which should not be read as scape characters.\n",
    "# example of regex: r'st\\d\\s\\w{3,10}'\n",
    "# Use the regex helper to check: basic theory and most common metacharacters; regex quantifiers;\n",
    "# regex anchoring and finding; regex greedy and non-greedy search; regex grouping and capturing;\n",
    "# regex alternating and non-capturing groups; regex backreferences; and regex lookaround.\n",
    "\n",
    "## ATTENTION: This function returns ONLY the capturing groups from the regex, i.e., portions of the\n",
    "# regex explicitly marked with parentheses (check the regex helper for more details, including how\n",
    "# to convert parentheses into non-capturing groups). If no groups are marked as capturing, the\n",
    "# function will raise an error.\n",
    "\n",
    "SHOW_REGEX_HELPER = False\n",
    "# SHOW_REGEX_HELPER: set SHOW_REGEX_HELPER = True to show a helper guide to the construction of\n",
    "# the regular expression. After finishing the helper, the original dataset itself will be returned\n",
    "# and the function will not proceed. Use it in case of not knowing or not certain on how to input\n",
    "# the regex.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_regex'\n",
    "# NEW_COLUMN_SUFFIX = \"_regex\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_regex\", the new column will be named as\n",
    "# \"column1_regex\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = regex_search (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, regex_to_search = REGEX_TO_SEARCH, show_regex_helper = SHOW_REGEX_HELPER, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d68bdbe5-72ce-4feb-aaab-685ee02e55b2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Replacing a Regular Expression (RegEx) from a string column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ada5c9e7-5846-4e84-8860-6769c7243f55",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "REGEX_TO_SEARCH = r\"\"\n",
    "# REGEX_TO_SEARCH = r\"\" - string containing the regular expression (regex) that will be searched\n",
    "# within each string from the column. Declare it with the r before quotes, indicating that the\n",
    "# 'raw' string should be read. That is because the regex contain special characters, such as \\,\n",
    "# which should not be read as scape characters.\n",
    "# example of regex: r'st\\d\\s\\w{3,10}'\n",
    "# Use the regex helper to check: basic theory and most common metacharacters; regex quantifiers;\n",
    "# regex anchoring and finding; regex greedy and non-greedy search; regex grouping and capturing;\n",
    "# regex alternating and non-capturing groups; regex backreferences; and regex lookaround.\n",
    "\n",
    "STRING_FOR_REPLACEMENT = \"\"\n",
    "# STRING_FOR_REPLACEMENT = \"\" - regular string that will replace the REGEX_TO_SEARCH: \n",
    "# whenever REGEX_TO_SEARCH is found in the string, it is replaced (substituted) by \n",
    "# STRING_FOR_REPLACEMENT. \n",
    "# Example STRING_FOR_REPLACEMENT = \" \" (whitespace).\n",
    "# If STRING_FOR_REPLACEMENT = None, the empty string will be used for replacement.\n",
    "        \n",
    "## ATTENTION: This function process a single regex by call.\n",
    "\n",
    "SHOW_REGEX_HELPER = False\n",
    "# SHOW_REGEX_HELPER: set SHOW_REGEX_HELPER = True to show a helper guide to the construction of\n",
    "# the regular expression. After finishing the helper, the original dataset itself will be returned\n",
    "# and the function will not proceed. Use it in case of not knowing or not certain on how to input\n",
    "# the regex.\n",
    "\n",
    "CREATE_NEW_COLUMN = True\n",
    "# CREATE_NEW_COLUMN = True\n",
    "# Alternatively, set CREATE_NEW_COLUMN = True to store the transformed data into a new\n",
    "# column. Or set CREATE_NEW_COLUMN = False to overwrite the existing column.\n",
    "NEW_COLUMN_SUFFIX = '_regex'\n",
    "# NEW_COLUMN_SUFFIX = \"_regex\"\n",
    "# This value has effect only if CREATE_NEW_COLUMN = True.\n",
    "# The new column name will be set as column + NEW_COLUMN_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_regex\", the new column will be named as\n",
    "# \"column1_regex\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named transf_dataset:\n",
    "# Simply modify this object on the left of equality:\n",
    "transf_dataset = regex_replacement (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, regex_to_search = REGEX_TO_SEARCH, string_for_replacement = STRING_FOR_REPLACEMENT, show_regex_helper = SHOW_REGEX_HELPER, create_new_column = CREATE_NEW_COLUMN, new_column_suffix = NEW_COLUMN_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Applying Fast Fourier Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze'\n",
    "# COLUMN_TO_ANALYZE: string (inside quotes), \n",
    "# containing the name of the column that will be analyzed. \n",
    "# e.g. COLUMN_TO_ANALYZE = \"column1\" will analyze the column named as 'column1'.\n",
    "\n",
    "AVERAGE_FREQUENCY_OF_DATA_COLLECTION = 'hour'\n",
    "# AVERAGE_FREQUENCY_OF_DATA_COLLECTION = 'hour' or 'h' for hours; 'day' or 'd' for days;\n",
    "# 'minute' or 'min' for minutes; 'seconds' or 's' for seconds; 'ms' for milliseconds; 'ns' for\n",
    "# nanoseconds; 'year' or 'y' for years; 'month' or 'm' for months.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'capability_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# The results of the Fast Fourier Transform will be stored in the object named fft.\n",
    "# Simply modify this object on the left of equality:\n",
    "fft = fast_fourier_transform (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, average_frequency_of_data_collection = AVERAGE_FREQUENCY_OF_DATA_COLLECTION, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Generating columns with frequency information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "TIMESTAMP_TAG_COLUMN = \"timestamp\"\n",
    "# TIMESTAMP_TAG_COLUMN = None. string containing the name of the column with the timestamp. \n",
    "# If TIMESTAMP_TAG_COLUMN is None, the index will be used for testing different imputations.\n",
    "# be the time series reference. declare as a string under quotes. This is the column from \n",
    "# which we will extract the timestamps or values with temporal information. e.g.\n",
    "# TIMESTAMP_TAG_COLUMN = 'timestamp' will consider the column 'timestamp' a time column.\n",
    "\n",
    "IMPORTANT_FREQUENCIES = [{'value': 1, 'unit': 'day'}, \n",
    "                         {'value':1, 'unit': 'year'}]\n",
    "\n",
    "# IMPORTANT_FREQUENCIES = [{'value': 1, 'unit': 'day'}, {'value':1, 'unit': 'year'}]\n",
    "# List of dictionaries with the important frequencies to add to the model. You can remove dictionaries,\n",
    "# or add extra dictionaries. The dictionaries must have always the same keys, 'value' and 'unit'.\n",
    "# If the importante frequency is once a day, the value will be 1, and the unit will be 'day' or 'd'.\n",
    "# The possible units are: 'ns', 'ms', 'second' or 's', 'minute' or 'min', 'day' or 'd', 'month' or 'm',\n",
    "# 'year' or 'y'.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'capability_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# The dataset with new columns containing the frequency information will be stored as dataset.\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = get_frequency_features (df = DATASET, timestamp_tag_column = TIMESTAMP_TAG_COLUMN, important_frequencies = IMPORTANT_FREQUENCIES, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a60c8a00-6f89-424f-92c4-9de8304a8065",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **log-transforming the variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8e7fef30-c0ba-4a46-9587-4a46b2240069",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#### WARNING: This function will eliminate rows where the selected variables present \n",
    "#### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_log\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_log\", the new column will be named as\n",
    "# \"column1_log\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "# New dataframe saved as log_transf_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "log_transf_df = log_transform (df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)\n",
    "\n",
    "# One curve derived from the normal is the log-normal.\n",
    "# If the values Y follow a log-normal distribution, their log follow a normal.\n",
    "# A log normal curve resembles a normal, but with skewness (distortion); \n",
    "# and kurtosis (long-tail).\n",
    "\n",
    "# Applying the log is a methodology for normalizing the variables: \n",
    "# the sample space gets shrinkled after the transformation, making the data more \n",
    "# adequate for being processed by Machine Learning algorithms. Preferentially apply \n",
    "# the transformation to the whole dataset, so that all variables will be of same order \n",
    "# of magnitude.\n",
    "# Obviously, it is not necessary for variables ranging from -100 to 100 in numerical \n",
    "# value, where most outputs from the log transformation are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "39ba65a0-cd7c-4123-ac37-c22a027f4149",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the log-transform - Exponentially transforming variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de7066bb-4dfc-46e8-9042-a475cd2cdf71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_log_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b9724a7d-15c6-4472-a0de-a592cda99b0a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Obtaining and applying Box-Cox transform**\n",
    "- Transform a series of data into a series described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "50e549c9-b761-44bc-bdb2-35ffabf9eab2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "MODE = 'calculate_and_apply'\n",
    "# Aternatively, mode = 'calculate_and_apply' to calculate lambda and apply Box-Cox\n",
    "# transform; mode = 'apply_only' to apply the transform for a known lambda.\n",
    "# To 'apply_only', lambda_box must be provided.\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_BoxCoxTransf'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_BoxCoxTransf', the transformed column will be\n",
    "# identified as 'Y_BoxCoxTransf'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "SPECIFICATION_LIMITS = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "# specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': None}\n",
    "# If there are specification limits, input them in this dictionary. Do not modify the keys,\n",
    "# simply substitute None by the lower and/or the upper specification.\n",
    "# e.g. Suppose you have a tank that cannot have more than 10 L. So:\n",
    "# specification_limits = {'lower_spec_lim': None, 'upper_spec_lim': 10}, there is only\n",
    "# an upper specification equals to 10 (do not add units);\n",
    "# Suppose a temperature cannot be lower than 10 ºC, but there is no upper specification. So,\n",
    "# specification_limits = {'lower_spec_lim': 10, 'upper_spec_lim': None}. Finally, suppose\n",
    "# a liquid which pH must be between 6.8 and 7.2:\n",
    "# specification_limits = {'lower_spec_lim': 6.8, 'upper_spec_lim': 7.2}\n",
    "\n",
    "#New dataframe saved as data_transformed_df; dictionary saved as data_sum_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "data_transformed_df, data_sum_dict = box_cox_transform (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, mode = MODE, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX, specification_limits = SPECIFICATION_LIMITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "60059372-e5bc-4159-961d-e4b79ff0e889",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f6f1e23f-77d1-4194-b59c-2101fd604b93",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_ReversedBoxCox'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "# identified as 'Y_ReversedBoxCox'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as retransformed_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "retransformed_df = reverse_box_cox (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "72c3cf5f-dde2-4f3b-929c-681cbe228c1a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **One-Hot Encoding the categorical variables**\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.For a category \"A\", a column named \"A\" is created.\n",
    "    - If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "    - If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "33e0b50c-10c3-46c0-877a-32be0c593d58",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as one_hot_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as OneHot_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "one_hot_encoded_df, OneHot_encoding_list = OneHotEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fc6b3659-fa74-4c86-8d41-9f20f4232152"
   },
   "source": [
    "### **Reversing the One-Hot Encoding of the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9e3a3262-5ace-4825-af2c-5e9999847baa",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "ENCODING_LIST = [\n",
    "    \n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}},\n",
    "    {'column': None,\n",
    "    'OneHot_encoder': {'OneHot_enc_obj': None, 'encoded_columns': None}}\n",
    "    \n",
    "]\n",
    "# ENCODING_LIST: list in the same format of the one generated by OneHotEncode_df function:\n",
    "# it must be a list of dictionaries where each dictionary contains two keys:\n",
    "# key 'column': string with the original column name (in quotes); \n",
    "# key 'OneHot_encoder': this key must store a nested dictionary.\n",
    "# Even though the nested dictionaries generates by the encoding function present\n",
    "# two keys:  'categories', storing an array with the different categories;\n",
    "# and 'OneHot_enc_obj', storing the encoder object, only the key 'OneHot_enc_obj' is required.\n",
    "## On the other hand, a third key is needed in the nested dictionary:\n",
    "## key 'encoded_columns': this key must store a list or array with the names of the columns\n",
    "# obtained from Encoding.\n",
    "\n",
    "# New dataframe saved as reversed_one_hot_encoded_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "reversed_one_hot_encoded_df = reverse_OneHotEncode (df = DATASET, encoding_list = ENCODING_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3cee6ad2-66f9-4504-8374-b04f3a818bdb"
   },
   "source": [
    "### **Ordinal Encoding the categorical variables**\n",
    "- Transform categorical values with notion of order into numerical (integer) features.\n",
    "- For each column, the Ordinal Encoder creates a new column in the dataset. This new column is represented by a an integer value, where each integer represents a possible categorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c4f8cdb7-01d0-4d10-b1ee-942ea95f38f3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as ordinal_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as ordinal_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "ordinal_encoded_df, ordinal_encoding_list = OrdinalEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d3ed1009-13c7-4478-a582-0d795c107afc"
   },
   "source": [
    "### **Reversing the Ordinal Encoding of the categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6fcac64f-9dee-4f56-964f-637e777524a7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "ENCODING_LIST = [\n",
    "    \n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}},\n",
    "    {'column': None,\n",
    "    'ordinal_encoder': {'ordinal_enc_obj': None, 'encoded_column': None}}\n",
    "    \n",
    "]\n",
    "# ENCODING_LIST: list in the same format of the one generated by OrdinalEncode_df function:\n",
    "# it must be a list of dictionaries where each dictionary contains two keys:\n",
    "# key 'column': string with the original column name (in quotes); \n",
    "# key 'ordinal_encoder': this key must store a nested dictionary.\n",
    "# Even though the nested dictionaries generates by the encoding function present\n",
    "# two keys:  'categories', storing an array with the different categories;\n",
    "# and 'ordinal_enc_obj', storing the encoder object, only the key 'ordinal_enc_obj' is required.\n",
    "## On the other hand, a third key is needed in the nested dictionary:\n",
    "## key 'encoded_column': this key must store a string with the name of the column\n",
    "# obtained from Encoding.\n",
    "\n",
    "# New dataframe saved as reversed_ordinal_encoded_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "reversed_ordinal_encoded_df = reverse_OrdinalEncode (df = DATASET, encoding_list = ENCODING_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "89591bf7-2900-4ae0-9f77-0d020cdf9feb"
   },
   "source": [
    "### **Scaling the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d0a50802-8b35-4d0d-9d33-45b1e44afcd1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor', MODE = 'normalize_by_maximum'\n",
    "## This function provides 4 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "## MODE = 'normalize_by_maximum' is similar to MODE = 'factor', but the factor will be selected\n",
    "# as the maximum value. This mode is available only for SCALE_WITH_NEW_PARAMS = True. If\n",
    "# SCALE_WITH_NEW_PARAMS = False, you should provide the value of the maximum as a division 'factor'.\n",
    "\n",
    "SCALE_WITH_NEW_PARAMS = True\n",
    "# Alternatively, set SCALE_WITH_NEW_PARAMS = True if you want to calculate a new\n",
    "# scaler for the data; or SCALE_WITH_NEW_PARAMS = False if you want to apply \n",
    "# parameters previously obtained to the data (i.e., if you want to apply the scaler\n",
    "# previously trained to another set of data; or wants to simply apply again the same\n",
    "# scaler).\n",
    "    \n",
    "## WARNING: The MODE 'factor' demmands the input of the list of factors that will be \n",
    "# used for normalizing each column. Therefore, it can be used only \n",
    "# when SCALE_WITH_NEW_PARAMS = False.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = None\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "# The key 'scaler_details' will not create an object: the transform will be directly performed \n",
    "# through vectorial operations.\n",
    "\n",
    "SUFFIX = '_scaled'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_scaled', the transformed column will be\n",
    "# identified as 'Y_scaled'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as scaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "scaled_df, scaling_list = feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, mode = MODE, scale_with_new_params = SCALE_WITH_NEW_PARAMS, list_of_scaling_params = LIST_OF_SCALING_PARAMS, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e5277523-9e36-4fa2-93c8-4cffab51204c"
   },
   "source": [
    "### **Reversing scaling of the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9a19ae48-1669-4e22-a680-f7d9ccfb956d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = [\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}},\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}}\n",
    "                            \n",
    "                         ]\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "\n",
    "SUFFIX = '_reverseScaling'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "# identified as 'Y_reverseScaling'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as rescaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df, scaling_list = reverse_feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, list_of_scaling_params = LIST_OF_SCALING_PARAMS, mode = MODE, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "873bf804-9640-4cb2-92ad-b79f8602d265",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f575b478-8930-4400-bb78-36cec4853866"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a7f53fc3-8839-4e22-81e7-f7b3b638f52c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "82a45c25-3b08-4257-8466-2ba8c40087bd"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "70e950c4-eadb-43ea-93d0-f09c5dae61ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1716cb5a-0c72-4cf0-9207-051717b2df4b"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f63a5a18-0797-4c14-ab10-7aa77914f5d3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9a900908-dece-4106-89f3-5a979e50be3d"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "25dadb54-914a-451f-b7b4-85c2ae6beedb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8d5503e7-4826-4e84-bbcb-ff5fd96f9d1a"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6246a5bf-49f4-4cba-9f3c-bf102403f659",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d7b63e08-99c4-47ba-a00b-d209faef3843"
   },
   "source": [
    "### **Renaming specific columns from the dataframe; or cleaning columns' labels**\n",
    "- The function `select_order_or_rename_columns` requires the user to pass a list containing the names from all columns.\n",
    "- Also, this list must contain the columns in the correct order (the order they appear in the dataframe).\n",
    "- This function may manipulate one or several columns by call, and is not dependent on their order.\n",
    "- This function can also be used for cleaning the columns' labels: capitalize (upper case) or lower cases of all columns' names; replace substrings on columns' names; or eliminating trailing and leading white spaces or characters from columns' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "242ee25f-dabc-49dd-9a44-38527fe186e6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'set_new_names'\n",
    "# MODE = 'set_new_names' will change the columns according to the specifications in\n",
    "# LIST_OF_COLUMNS_LABELS.\n",
    "\n",
    "# MODE = 'capitalize_columns' will capitalize all columns names (i.e., they will be put in\n",
    "# upper case). e.g. a column named 'column' will be renamed as 'COLUMN'\n",
    "\n",
    "# MODE = 'lowercase_columns' will lower the case of all columns names. e.g. a column named\n",
    "# 'COLUMN' will be renamed as 'column'.\n",
    "\n",
    "# MODE = 'replace_substring' will search on the columns names (strings) for the \n",
    "# SUBSTRING_TO_BE_REPLACED (which may be a character or a string); and will replace it by \n",
    "# NEW_SUBSTRING_FOR_REPLACEMENT (which again may be either a character or a string). \n",
    "# Numbers (integers or floats) will be automatically converted into strings.\n",
    "# As an example, consider the default situation where we search for a whitespace ' ' and replace it\n",
    "# by underscore '_': SUBSTRING_TO_BE_REPLACED = ' ', NEW_SUBSTRING_FOR_REPLACEMENT = '_'  \n",
    "# In this case, a column named 'new column' will be renamed as 'new_column'.\n",
    "\n",
    "# MODE = 'trim' will remove all trailing or leading whitespaces from column names.\n",
    "# e.g. a column named as ' col1 ' will be renamed as 'col1'; 'col2 ' will be renamed as\n",
    "# 'col2'; and ' col3' will be renamed as 'col3'.\n",
    "\n",
    "# MODE = 'eliminate_trailing_characters' will eliminate a defined trailing and leading \n",
    "# substring from the columns' names. \n",
    "# The substring must be indicated as TRAILING_SUBSTRING, and its default, when no value\n",
    "# is provided, is equivalent to mode = 'trim' (eliminate white spaces). \n",
    "# e.g., if TRAILING_SUBSTRING = '_test' and you have a column named 'col_test', it will be \n",
    "# renamed as 'col'.\n",
    "\n",
    "SUBSTRING_TO_BE_REPLACED = ' '\n",
    "NEW_SUBSTRING_FOR_REPLACEMENT = '_'\n",
    "\n",
    "TRAILING_SUBSTRING = None\n",
    "\n",
    "LIST_OF_COLUMNS_LABELS = [\n",
    "    \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None},\n",
    "    {'column_name': None, 'new_column_name': None}, \n",
    "    {'column_name': None, 'new_column_name': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_COLUMNS_LABELS = [{'column_name': None, 'new_column_name': None}]\n",
    "# This is a list of dictionaries, where each dictionary contains two key-value pairs:\n",
    "# the first one contains the original column name; and the second one contains the new name\n",
    "# that will substitute the original one. The function will loop through all dictionaries in\n",
    "# this list, access the values of the keys 'column_name', and it will be replaced (switched) \n",
    "# by the correspondent value in key 'new_column_name'.\n",
    "    \n",
    "# The object LIST_OF_COLUMNS_LABELS must be declared as a list, \n",
    "# in brackets, even if there is a single dictionary.\n",
    "# Use always the same keys: 'column_name' for the original label; \n",
    "# and 'new_column_name', for the correspondent new label.\n",
    "# Notice that this function will not search substrings: it will substitute a value only when\n",
    "# there is perfect correspondence between the string in 'column_name' and one of the columns\n",
    "# labels. So, the cases (upper or lower) must be the same.\n",
    "    \n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to replace more\n",
    "# values.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'column_name': original_col, 'new_column_name': new_col}, \n",
    "# where original_col and new_col represent the strings for searching and replacement \n",
    "# (If one of the keys contains None, the new dictionary will be ignored).\n",
    "# Example: LIST_OF_COLUMNS_LABELS = [{'column_name': 'col1', 'new_column_name': 'col'}] will\n",
    "# rename 'col1' as 'col'.\n",
    "\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = rename_or_clean_columns_labels (df = DATASET, mode = MODE, substring_to_be_replaced = SUBSTRING_TO_BE_REPLACED, new_substring_for_replacement = NEW_SUBSTRING_FOR_REPLACEMENT, trailing_substring = TRAILING_SUBSTRING, list_of_columns_labels = LIST_OF_COLUMNS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8551a853-4b4a-4708-a471-a679711acb78"
   },
   "source": [
    "### **Characterizing the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b996a36b-9f62-46d5-82d4-d828ced2527e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#New dataframes saved as df_shape, df_columns_list, df_dtypes, df_general_statistics, df_missing_values.\n",
    "# Simply modify this object on the left of equality:\n",
    "df_shape, df_columns_array, df_dtypes, df_general_statistics, df_missing_values = df_general_characterization (df = DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a0bb249a-5d13-483d-adb4-b7f25e9b8ca4"
   },
   "source": [
    "### **Obtaining correlation plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9ade47a0-205c-485e-b0d5-ad010823d3bc",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SHOW_MASKED_PLOT = True\n",
    "#SHOW_MASKED_PLOT = True - keep as True if you want to see a cleaned version of the plot\n",
    "# where a mask is applied. Alternatively, SHOW_MASKED_PLOT = True, or \n",
    "# SHOW_MASKED_PLOT = False\n",
    "\n",
    "RESPONSES_TO_RETURN_CORR = None\n",
    "#RESPONSES_TO_RETURN_CORR - keep as None to return the full correlation tensor.\n",
    "# If you want to display the correlations for a particular group of features, input them\n",
    "# as a list, even if this list contains a single element. Examples:\n",
    "# responses_to_return_corr = ['response1'] for a single response\n",
    "# responses_to_return_corr = ['response1', 'response2', 'response3'] for multiple\n",
    "# responses. Notice that 'response1',... should be substituted by the name ('string')\n",
    "# of a column of the dataset that represents a response variable.\n",
    "# WARNING: The returned coefficients will be ordered according to the order of the list\n",
    "# of responses. i.e., they will be firstly ordered based on 'response1'\n",
    "# Alternatively: a list containing strings (inside quotes) with the names of the response\n",
    "# columns that you want to see the correlations. Declare as a list even if it contains a\n",
    "# single element.\n",
    "\n",
    "SET_RETURNED_LIMIT = None\n",
    "# SET_RETURNED_LIMIT = None - This variable will only present effects in case you have\n",
    "# provided a response feature to be returned. In this case, keep set_returned_limit = None\n",
    "# to return all of the correlation coefficients; or, alternatively, \n",
    "# provide an integer number to limit the total of coefficients returned. \n",
    "# e.g. if set_returned_limit = 10, only the ten highest coefficients will be returned. \n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'correlation_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "#New dataframe saved as correlation_matrix. Simply modify this object on the left of equality:\n",
    "correlation_matrix = correlation_plot (df = DATASET, show_masked_plot = SHOW_MASKED_PLOT, responses_to_return_corr = RESPONSES_TO_RETURN_CORR, set_returned_limit = SET_RETURNED_LIMIT, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ff763e69-c3d3-4029-b25e-c43789785967"
   },
   "source": [
    "### **Obtaining scatter plots and simple linear regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "22290618-2884-44ef-a588-d653076996f3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "SHOW_LINEAR_REG = True\n",
    "#Alternatively: set SHOW_LINEAR_REG = True to plot the linear regressions graphics and show \n",
    "# the linear regressions calculated for each pair Y x X (i.e., each correlation \n",
    "# Y = aX + b, as well as the R² coefficient calculated). \n",
    "# Set SHOW_LINEAR_REG = False to omit both the linear regressions plots on the graphic, and\n",
    "# the correlations and R² coefficients obtained.\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = False #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# JSON-formatted list containing all series converted to NumPy arrays, \n",
    "#  with timestamps parsed as datetimes, and all the information regarding the linear regressions, \n",
    "# including the predicted values for plotting, returned as list_of_dictionaries_with_series_and_predictions. \n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dictionaries_with_series_and_predictions = scatter_plot_lin_reg (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, show_linear_reg = SHOW_LINEAR_REG, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0dfc5f4a-0d7b-46c7-8671-960fa6936567"
   },
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "94265248-c714-454e-8fd8-2636a46220e6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c9625bd5-c612-46de-9642-afd9be336ce6"
   },
   "source": [
    "### **Visualizing histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER: A histogram is the representation of a statistical distribution \n",
    "# of a given variable.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'analyzed_variable'\n",
    "#Alternatively: other column in quotes, substituting 'analyzed_variable'\n",
    "# e.g., if the analyzed variable is in a column named 'column1':\n",
    "# COLUMN_TO_ANALYZE = 'column1'\n",
    "\n",
    "TOTAL_OF_BINS = 10\n",
    "# This parameter must be an integer number: it represents the total of bins of the \n",
    "# histogram, i.e., the number of divisions of the sample space (in how much intervals\n",
    "# the sample space will be divided.\n",
    "# Manually adjust this parameter to obtain more or less resolution of the statistical\n",
    "# distribution: less bins tend to result into higher counting of values per bin, since\n",
    "# a larger interval of values is grouped. After modifying the total of bins, do not forget\n",
    "# to adjust the bar width in SET_GRAPHIC_BAR_WIDTH.\n",
    "# Examples: TOTAL_OF_BINS = 50, to divide the sample space into 50 equally-separated \n",
    "# intervals; TOTAL_OF_BINS = 10 to divide it into 10 intervals; TOTAL_OF_BINS = 100 to\n",
    "# divide it into 100 intervals.\n",
    "NORMAL_CURVE_OVERLAY = True\n",
    "#Alternatively: set NORMAL_CURVE_OVERLAY = True to show a normal curve overlaying the\n",
    "# histogram; or set NORMAL_CURVE_OVERLAY = False to omit the normal curve (show only\n",
    "# the histogram).\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'histogram.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "#New dataframes saved as general_stats and frequency_table.\n",
    "# Simply modify these objects on the left of equality:\n",
    "general_stats, frequency_table = histogram (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, total_of_bins = TOTAL_OF_BINS, normal_curve_overlay = NORMAL_CURVE_OVERLAY, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "acc57636-2d61-4aaf-8b97-ebb9bff6ebca"
   },
   "source": [
    "### **Testing data normality and visualizing the probability plot**\n",
    "- Check the probability that data is actually described by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: The statistical tests require at least 20 samples\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "COLUMN_TO_ANALYZE = 'column_to_analyze' \n",
    "# COLUMN_TO_ANALYZE: column (variable) of the dataset that will be tested. Declare as a string,\n",
    "# in quotes.\n",
    "# e.g. COLUMN_TO_ANALYZE = 'col1' will analyze a column named 'col1'.\n",
    "\n",
    "COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS = None\n",
    "# column_with_labels_to_test_subgroups: if there is a column with labels or\n",
    "# subgroup indication, and the normality should be tested separately for each label, indicate\n",
    "# it here as a string (in quotes). e.g. column_with_labels_to_test_subgroups = 'col2' \n",
    "# will retrieve the labels from 'col2'.\n",
    "# Keep column_with_labels_to_test_subgroups = None if a single series (the whole column)\n",
    "# will be tested.\n",
    "    \n",
    "ALPHA = 0.10\n",
    "# Confidence level = 1 - ALPHA. For ALPHA = 0.10, we get a 0.90 = 90% confidence\n",
    "# Set ALPHA = 0.05 to get 0.95 = 95% confidence in the analysis.\n",
    "# Notice that, when less trust is needed, we can increase ALPHA to get less restrictive\n",
    "# results.\n",
    "\n",
    "SHOW_PROBABILITY_PLOT = True\n",
    "#Alternatively: set SHOW_PROBABILITY_PLOT = True to obtain the probability plot for the\n",
    "# variable Y (normal distribution tested). \n",
    "# Set SHOW_PROBABILITY_PLOT = False to omit the probability plot.\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'probability_plot_normal.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "# List of dictionaries containing the series, p-values, skewness and kurtosis returned as\n",
    "# list_of_dicts\n",
    "# Simply modify this object on the left of equality:\n",
    "list_of_dicts = test_data_normality (df = DATASET, column_to_analyze = COLUMN_TO_ANALYZE, column_with_labels_to_test_subgroups = COLUMN_WITH_LABELS_TO_TEST_SUBGROUPS, alpha = ALPHA, show_probability_plot = SHOW_PROBABILITY_PLOT, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "805a98d9-e0e9-4ebd-bb30-b2e13959b7bf"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "55b24b35-e9ae-437e-858f-aed34972f485",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a6708648-6942-4da5-987a-a8ebb61fd537"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "db69901d-3841-4e8d-a60f-834537d58aa3"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f3cd0390-3343-4ad9-8001-f514e629b782",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e678c975-8acc-4bd3-9457-e9a66e8359b2"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c48fb828-a6d4-4efe-a517-539c383434ba",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bbb977f9-fb1e-4a1b-89e0-919914d64b9b"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a36fba56-6e01-402a-b2c8-455e5b046a95",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2732a0e7-6ffe-495d-a28f-82c921c1a253"
   },
   "source": [
    "# **One-Hot Encoding - Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "df88ce83-988c-46ed-9087-6e21278668ec",
    "id": "2q8SGc6WigDt"
   },
   "source": [
    "If there are **categorical features**, they should be converted into numerical variables for being processed by the machine learning algorithms.\n",
    "\n",
    "\\- We can assign integer values for each one of the categories. This works well for situations where there is a scale or order for the assignment of the variables (e.g., if there is a satisfaction grade).\n",
    "\n",
    "\\- On the other hand, the results may be compromised if there is no order. That is because the ML algorithms assume that, if two categories have close numbers, then the categories are similar, what is not necessarily true. There are cases where the categories have no relation with each other.\n",
    "\n",
    "\\- In these cases, the best strategy is the One-Hot Encoding. For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "\n",
    "\\- Naturally, the number of columns grow with the number of possible labels. The One-Hot Encoder from Sklearn creates a Scipy Sparse matrix that stores the position of the zeros in the dataset. Then, the computational cost is reduced due to the fact that we are not storing a huge amount of null values.\n",
    "\n",
    "\\- Since each column is a binary variable of the type \"is classified in this category or not\", we expect that the created columns contain more zeros than 1s. That is because if an element belongs to one category (= 1), it does not belong to the others, so its value is zero for all other columns."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
